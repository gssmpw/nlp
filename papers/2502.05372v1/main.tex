\documentclass[preprint,12pt]{elsarticle}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{mathrsfs}
\usepackage{caption}

\usepackage[bottom]{footmisc}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{mathtools}
\usepackage{subcaption}

\usepackage{float}
\usepackage{adjustbox}
\usepackage[margin=1in]{geometry}
\usepackage[small,compact]{titlesec}
\usepackage[labelfont=bf,labelsep=colon,skip=2pt]{caption} 
\usepackage[titles]{tocloft}
\usepackage{bold-extra}
\usepackage{hyperref}

\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}
\begin{frontmatter}
\title{Active Learning of Model Discrepancy with Bayesian Experimental Design}

\author{Huchen Yang}
\author{Chuanqi Chen}
\author{Jin-Long Wu\corref{cor1}} \ead{jinlong.wu@wisc.edu} 
\cortext[cor1]{Corresponding author}

\address{Department of Mechanical Engineering, University of Wisconsinâ€“Madison, Madison, WI 53706}

\begin{abstract}
Digital twins have been actively explored in many engineering applications, such as manufacturing and autonomous systems. However, model discrepancy is ubiquitous in most digital twin models and has significant impacts on the performance of using those models. In recent years, data-driven modeling techniques have been demonstrated promising in characterizing the model discrepancy in existing models, while the training data for the learning of model discrepancy is often obtained in an empirical way and an active approach of gathering informative data can potentially benefit the learning of model discrepancy. On the other hand, Bayesian experimental design (BED) provides a systematic approach to gathering the most informative data, but its performance is often negatively impacted by the model discrepancy. In this work, we build on sequential BED and propose an efficient approach to iteratively learn the model discrepancy based on the data from the BED. The performance of the proposed method is validated by a classical numerical example governed by a convection-diffusion equation, for which full BED is still feasible. The proposed method is then further studied in the same numerical example with a high-dimensional model discrepancy, which serves as a demonstration for the scenarios where full BED is not practical anymore. An ensemble-based approximation of information gain is further utilized to assess the data informativeness and to enhance learning model discrepancy. The results show that the proposed method is efficient and robust to the active learning of high-dimensional model discrepancy, using data suggested by the sequential BED. We also demonstrate that the proposed method is compatible with both classical numerical solvers and modern auto-differentiable solvers.
\end{abstract}

\begin{keyword}
Model discrepancy \sep Bayesian experimental design \sep Active learning \sep Neural ODE \sep Ensemble Kalman method
\end{keyword}

\end{frontmatter}

%\textcolor{red}{Todo list: (1) Get a LaTex template from Chuanqi or Xinghao and make sure everything shows correctly with the new template, (2) revise the results part based on the new structure, (3) Address all the red color comments, (4) Update the appendix based on the notations in Section 2.}

%\textcolor{red}{Try to add at least 10 more selected references to the introduction of Bayesian OED and sequential Bayesian OED. You have worked on this topic for a year, and the references list does not reflect the efforts of a PhD student who spent a year on a topic. Ref. [17] is a good starting point to check for all relevant references.}

\section{Introduction}

%\textcolor{blue}{Latest introduction}:
%\textcolor{red}{The optimization part reduces the computational cost of estimating the structural error of a model, while the Bayesian inference part provides randomness that can potentially mitigate the possibility of converging to local minimum.}

Data-driven modeling techniques have been demonstrated promising in many science and engineering applications~\cite{kutz2016dynamic,peherstorfer2016data,wang2017physics,wu2018physics,duraisamy2019turbulence,montans_data-driven_2019,raissi2019physics,brunton2020machine,li2020fourier,lu2021learning,kochkov2021machine, chen2023ceboosting, chen2024cgnsde, chen2024cgkn}. Considering that data informativeness serves as a critical component in all data-driven modeling techniques, an active approach to identifying the most informative data has the potential to further advance state-of-the-art data-driven modeling. Experimental design provides such a systematical approach for the active acquisition of data, which has been demonstrated effective in various types of problems, such as the estimation of some unknown model parameters~\cite{atkinson_design_1968, atkinson_optimum_1992, kirk_experimental_2009, seltman_experimental_2012, steiert_experimental_2012}. However, classical experimental design via solving an optimization problem is limited by its propensity to fall into local optima when applied to nonlinear models and its inability to fully incorporate prior knowledge~\cite{ryan_review_2016}. 

%Bayesian methodologies for optimal experimental design (Bayesian OED) have become more prominent in the literature due to their ability to describe parameter distributions from a probabilistic perspective, their flexibility in accommodating various types of data and models, and their inherent framework for learning \cite{ryan_review_2016,lindley_bayesian_1972,rainforth_modern_2023}. Sequential, or adaptive designs, have become increasingly popular in the Bayesian design literature as they provide flexible and efficient designs rather than static ones \cite{whitehead_bayesian_1995,cavagnaro_adaptive_2010,drovandi_sequential_2013}. Various novel strategies and techniques have been developed to overcome the computational challenges for Bayesian optimal design problems \cite{ivanova_implicit_2021,foster_deep_2021,shen_bayesian_2023}. 

Bayesian experimental design (BED) addresses these limitations by utilizing a probabilistic framework that incorporates prior information and updates parameter distributions as new data are acquired~\cite{ryan_review_2016, jones_bayes_2016}. This approach not only enhances the robustness of the design and data acquisition process but also provides a more flexible and adaptive framework that can efficiently handle complex, nonlinear models \cite{huan_simulation-based_2013,rainforth_modern_2023}. Foundational work \cite{raiffa_applied_2000,degroot_optimal_2005} introduced Bayesian decision theory into the field of experimental design, emphasizing the importance of selecting a utility function that appropriately reflects the purpose of the experiment, which is critical for identifying optimal designs \cite{lindley_bayesian_1972}, and various popular criteria used to find optimal designs were summarized in~\cite{chaloner_bayesian_1995}. The growing popularity of sequential or adaptive designs in Bayesian design literature highlights their flexibility and efficiency over static designs \cite{whitehead_bayesian_1995,cavagnaro_adaptive_2010,drovandi_sequential_2013}. Numerous novel strategies and techniques have been developed to overcome the computational challenges for Bayesian optimal design problems~\cite{ivanova_implicit_2021,foster_deep_2021,huan_gradient-based_2014,shen_bayesian_2023}, and a comprehensive review of Bayesian OED for its formulations and computations can be found in~\cite{huan2024optimal}.

%\textcolor{red}{Add a paragraph to introduce sequential Bayesian OED.}

Sequential Bayesian experimental design (sBED) has emerged as a significant methodology in experimental design, leveraging the results of preceding experiments to inform subsequent decisions \cite{muller_simulation-based_2007,von_toussaint_bayesian_2011,huan_numerical_2015}. It incorporates adaptive decision-making and feedback, making it highly effective in iterative experimentation. Among sBED methods, there are two main streams: greedy (or myopic) identification of the optimal design for the current situation using only the information obtained so far \cite{box_sequential_1992, drovandi_sequential_2014,kleinegesse_sequential_2020}, while policy-based approaches considers all future experiments when making each design decision  \cite{shen_bayesian_2023,huan_sequential_2016}. Consequently, the policy-based approaches are typically finite-horizon, which requires the total number of experiments to be predetermined. In the context of active online learning, where information and system states are continuously updated and the scale of the experiment cannot be pre-specified, the greedy approach tends to be more flexible.

% In Ivanova et al. (2021); Foster et al. (2021), the authors introduced a policy-based method for performing adaptive experimentation 


However, nearly all BED methods suffer from model discrepancy, a phenomenon that is prevalent in real-world scenarios \cite{kennedy_bayesian_2001}. The model discrepancy primarily introduces errors in the likelihood calculation of BED, leading to the following consequences: (i) introduction of bias in parameter estimation; (ii) continuous generation of similar designs and low-quality datasets, which present significant challenges in BED practice \cite{grunwald_inconsistency_2017,brynjarsdottir_learning_2014,catanach_metrics_2023}. In the traditional BED framework, two primary categories of approaches are employed to address the issue of model discrepancy. The first category involves augmenting the model with correction terms that contain some unknown parameters to be inferred \cite{feng_optimal_2015, surer_sequential_2024}. However, this approach substantially increases the dimensionality of the parameter space, especially when large neural networks are employed to construct those correction terms, thereby introducing significant computational challenges. The second category entails proposing a set of candidate models and either evaluating the robustness of the discrepancy-mitigating designs using new criteria \cite{catanach_metrics_2023}, or selecting the most plausible model through model discrimination \cite{donckels_anticipatory_2009, streif_optimal_2014}. Model discrimination can also be integrated into the parameter inference process, resulting in a multi-objective optimization problem for experimental design. This category of methods is constrained by the necessity of ensuring that the candidate model space includes the true model, a requirement that is often challenging and costly to fulfill.


%\textcolor{red}{Add a paragraph to introduce model discrepancy in general.}

%However, Bayesian optimal experimental design requires us to know the form of the dynamical equations governing the physical process. If our understanding of the system's dynamical equations, called the epistemic model, is flawed, the inferred parameters will be biased. This is because almost all Bayesian optimal experimental design methods necessitate the computation of likelihoods.\cite{walker_bayesian_2016}\cite{walter_identification_nodate}\cite{cook_optimal_2008}\cite{drovandi_sequential_2014}\cite{ryan_review_2016} \cite{prangle_bayesian_2023}. When calculating the likelihood, we generate outputs using the epistemic model and the considered parameter values to compare these outputs with actual observed data to assess the probability of observing the data under those parameters. If there is a discrepancy between the form of the dynamical equations in the epistemic model and the actual physical modelâ€”such as the addition or omission of certain termsâ€”the outputs generated by the epistemic model may deviate significantly from the measured data, resulting in severe distortion of the parameter estimates. It is common to have only partial knowledge of a physical system, resulting in an epistemic model with errors, which presents challenges in Bayesian optimal experimental design practice.


%In the traditional Bayesian optimal experimental design framework, there are two approaches to address this issue. First, we can augment the cognitive model with numerous alternative terms and include their coefficients as parameters to be inferred \cite{shen_bayesian_2023}, ensuring that the epistemic model space encompasses the true model. In subsequent parameter inference, erroneously added terms will be estimated as zero. However, this approach significantly increases the dimensionality of the parameter space, leading to computational challenges. Second, model discrimination can be incorporated into the current parameter inference problem \cite{donckels_anticipatory_2009}\cite{streif_optimal_2014}, forming a multi-objective optimization problem for experimental design. Nevertheless, this approach also substantially increases the complexity of the problem. Additionally, both methods require specifying one or more forms of the dynamical equations that include the true model. For complex dynamical systems, this assumption is often difficult to satisfy or highly costly.

To efficiently handle the model discrepancy in the context of BED, we propose a hybrid framework that integrates online learning for correcting the model discrepancy with sequential BED methods. An optimization-based method is adopted to gradually update a neural-network-based correction term that characterizes the model discrepancy, using the online data provided by each stage of sequential BED. It is worth noting that the neural-network-based correction term can potentially demand a high-dimensional parameter space, and the proposed approach avoids dealing with high-dimensional distributions of the unknown parameters, which poses a key challenge to Bayesian inference. At each stage of sequential BED, the proposed hybrid framework first employs BED for the parameters of the existing model and then performs optimization for the neural-network-based correction term. On one hand, each stage of sequential BED identifies optimal designs and the corresponding data to update the correction term. On the other hand, the updated correction term at each stage gradually mitigates the model discrepancy in BED.

To ensure the informativeness of data for correcting the model discrepancy, we introduce an ensemble-based indicator that approximates the information gain from a given dataset for the calibration of the model discrepancy, based on the ensemble Kalman inversion (EKI)~\cite{iglesias_ensemble_2013, kovachki_ensemble_2019}. More specifically, we propose an efficient and robust indicator to assess the data informativeness under the Gaussian assumption, which allows us to use the initial and updated ensembles from EKI to approximate the information gain for a candidate design and the corresponding measurement data. This process is more computationally feasible than those more accurate Bayesian approaches such as Markov chain Monte Carlo (MCMC), while still effectively quantifying the data informativeness and avoiding the calibration of model discrepancy on less informative or even misleading data, which could lead to a biased solution.


The key highlights of our work are as follows.
\begin{itemize}
    \item We propose a hybrid framework for efficient nonlinear BED and validate its performance in a numerical example with a low-dimensional parametric model error, for which full BED is still feasible.
    \item We further demonstrate that the hybrid framework can efficiently deal with structural errors characterized by high-dimensional parameters in the existing model, for which fully BED becomes impractical.
    \item We establish an ensemble-based indicator to approximate the information gain and improve the efficiency and robustness of the hybrid framework in the active learning of model discrepancy. %We show the effectiveness of the proposed hybrid framework in two general scenarios: (i) the existing model is implemented in an auto-differentiable way, or (ii) the Jacobian of the existing model can be obtained.
\end{itemize}


This paper is organized as follows. Section \ref{sec: Methodology} introduces the general formulation of our method. Section \ref{sec: Numerical Results} provides a comprehensive study of a contaminant source inversion problem in a convection-diffusion field to demonstrate the performance of the proposed method. Section~\ref{sec:discussions} presents discussions of potential future extensions. Finally, Section \ref{Conclusion} concludes the paper.

%\textcolor{blue}{Two schematic diagrams are presented here for consideration. The second one adds more details and the workflow about the model correction. Neither diagram explicitly shows the iterative frame. But I am not sure if this is necessary.}

%\textcolor{red}{The schematic figure is not clear enough. This is a figure for general readers, so ask Chuanqi to see if there are any components that he could not follow.}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{schematic.png}
    \caption{Schematic diagram of the hybrid framework for sequential BED and active learning of model discrepancy. The prior and posterior distributions refer to the probability distribution of the parameter $\theta_\mathcal{G}$ in Eq.~\eqref{eq:modeled_system}.}
    \label{fig:graphic_abstract}
\end{figure}




\section{Methodology}\label{sec: Methodology}
The model discrepancy is ubiquitous in the modeling and simulation of complex dynamical systems and poses a key challenge to the performance of Bayesian optimal experimental design. Unlike the unknown parameters of an existing physics-based model that are often in a low-dimensional vector space, the model discrepancy is in infinite-dimensional function space, for which the approximation approaches (e.g., based on neural networks) can involve unknown coefficients in a high-dimensional vector space and thus make full Bayesian approach such as MCMC practically infeasible. The general goal of this work is to leverage the optimal data identified initially for interested physical parameters by the standard BED via an ensemble-based method and to efficiently learn the unknown coefficients of the neural network that approximates the model discrepancy. 

In this work, the true system can be written in a general form:
\begin{equation}
\label{eq:true_system}
\begin{aligned}
    \frac{\partial \mathbf{u}}{\partial t} = \mathcal{G}^\dagger(\mathbf{u}; \boldsymbol{\theta}^\dagger),
\end{aligned}
\end{equation}
where the system state $\mathbf{u}(\mathbf{z}, t)$ is often a spatiotemporal field for many engineering applications, and $\mathcal{G}^\dagger$ denotes the true dynamics. If the detailed form of true dynamics is known, the BED can be used straightforwardly to estimate the unknown parameters $\boldsymbol{\theta}^\dagger$. In practice, the form of true dynamics is often unknown for many complex dynamical systems. Assuming that a physics-based model $\mathcal{G}(\mathbf{u};\boldsymbol{\theta}_\mathcal{G})$ exists to approximate the true dynamics $\mathcal{G}^\dagger$, we focus on the learning of model discrepancy $\mathcal{G}^\dagger - \mathcal{G}$, which is characterized by a neural network $\textrm{NN}(\mathbf{u};\boldsymbol{\theta}_\textrm{NN})$ and leads to the modeled system:

\begin{equation}
\label{eq:modeled_system}
    \frac{\partial \mathbf{u}}{\partial t}=\mathcal{G}(\mathbf{u};\boldsymbol{\theta}_\mathcal{G})+\textrm{NN}(\mathbf{u};\boldsymbol{\theta}_\textrm{NN}).
\end{equation}

Using a neural-network-based model to characterize the model discrepancy has recently been explored with different choices of neural networks, e.g., neural operators~\cite{chen2025neural} and diffusion models~\cite{dong2024data}, with a more general review by~\cite{wu2024learning}. In the context of BED, the design $\mathbf{d}$ corresponds to the spatiotemporal coordinate $(\mathbf{z}, t)$. The measurement $\mathbf{y}$ is the observation of variable $\mathbf{u}$ that represents the state of the physical system at the design $\mathbf{d}$, i.e., $\mathbf{y} = \mathbf{u}(\mathbf{d}) + \boldsymbol\eta$, where $\boldsymbol\eta \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol\Gamma)$ represents the measurement noise which is assumed to be Gaussian in this work when observing the state from the true system.

\subsection{Bayesian Experimental Design}
\label{sec:OED}

The Bayesian experimental design~\cite{rainforth_modern_2023,huan2024optimal} provides a general framework to systematically seek the optimal design by solving the optimization problem: 
\begin{equation}
\label{eq:oed_opt}
    \begin{aligned}
        \mathbf{d}^* &= \argmax_{\mathbf{d}\in\mathcal{D}} \mathbb{E}[U(\boldsymbol{\theta},\mathbf{y},\mathbf{d})]\\
        &= \argmax_{\mathbf{d}\in\mathcal{D}} \int_\mathcal{Y} \int_{\boldsymbol{\Theta}} U(\boldsymbol{\theta},\mathbf{y},\mathbf{d}) p(\boldsymbol{\theta},\mathbf{y}|\mathbf{d}) \mathrm{d}\boldsymbol{\theta}\mathrm{d}\mathbf{y}\\
        &= \argmax_{\mathbf{d}\in\mathcal{D}} \int_\mathcal{Y} \int_{\boldsymbol{\Theta}} U(\boldsymbol{\theta},\mathbf{y}, \mathbf{d}) p(\mathbf{y}|\boldsymbol{\theta},\mathbf{d})p(\boldsymbol{\theta}|\mathbf{d}) \mathrm{d}\boldsymbol{\theta}\mathrm{d}\mathbf{y},
    \end{aligned}
\end{equation}
where $\mathbf{y} \in \mathcal{Y} \in \mathbb{R}^{d_{\mathbf{y}}}$ is data from the design $\mathbf{d} \in \mathcal{D}$, $\boldsymbol{\theta} \in \boldsymbol{\Theta}$ denotes the target parameters, and $U$ is the utility function taking the inputs of $\mathbf{y}$ and $\boldsymbol{\theta}$ and returning a real value. The optimal design $\mathbf{d}^*$ is obtained by maximizing the expected utility function $\mathbb{E}[U(\boldsymbol{\theta},\mathbf{y},\mathbf{d})]$ over the design space $\mathcal{D}$. The term $p(\boldsymbol{\theta},\mathbf{y}|\mathbf{d})$ is the joint conditional distribution of data and parameters. $p(\mathbf{y}|\boldsymbol{\theta},\mathbf{d})$ is the likelihood. One thing that should be noted is that $p(\boldsymbol{\theta}|\mathbf{d})$ is the prior distribution of parameters, which is often assumed to be independent of the design $\mathbf{d}$: $p(\boldsymbol{\theta}|\mathbf{d}) = p(\boldsymbol{\theta})$. % $p(\mathbf{y}|\mathbf{d})$ is the evidence,  and $p(\boldsymbol{\theta}|\mathbf{y},\mathbf{d})$ is the posterior distribution.

%The optimization problem in Eq.~\eqref{eq:oed_opt} can be solved by gradient-based methods with the gradient being calculated as:
%\begin{equation}
%\label{eq:oed_grad}
%    \begin{aligned}
%        \frac{\mathrm{d} \mathbb{E}[U(\boldsymbol{\theta},\mathbf{y},\mathbf{d})]}{\mathrm{d} \mathbf{d}} &= \int_\mathcal{Y} \int_{\boldsymbol\Theta}\frac{\partial}{\partial \mathbf{d}} \left[ U(\boldsymbol{\theta},\mathbf{y},\mathbf{d}) p(\mathbf{y},\boldsymbol\theta | \mathbf{d})\right] \mathrm{d}\mathbf{y}\mathrm{d}\boldsymbol\theta.
%    \end{aligned}
%\end{equation}

%Eq.~\eqref{eq:oed_grad} is valid with mild assumptions~\cite{folland_real_1999}, i.e., the partial derivative of $U(\boldsymbol{\theta},\mathbf{y},\mathbf{d})p(\mathbf{y},\boldsymbol\theta | \mathbf{d})$ with respect to $\mathbf{d}$ is continuous and there exists an integrable function that bounds the partial derivative.

The utility function $U(\boldsymbol{\theta},\mathbf{y},\mathbf{d})$ can be regarded as the reward obtained from the data $\mathbf{y}$ for the corresponding design $\mathbf{d}$. This work employs Kullback-Leibler divergence between the posterior and the prior distribution of parameters $\boldsymbol{\theta}$:
\begin{equation}
    \begin{aligned}
        U(\boldsymbol{\theta},\mathbf{y},\mathbf{d}) &= D_{\textrm{KL}} \bigl(p(\boldsymbol\theta|\mathbf{y},\mathbf{d})\| p(\boldsymbol\theta)\bigr)\\
        &=\mathbb{E}_{\boldsymbol\theta|\mathbf{d},\mathbf{y}}(\log p(\boldsymbol{\theta}|\mathbf{y},\mathbf{d}) - \log p(\boldsymbol{\theta}))\\
        &=\int_{\boldsymbol{\Theta}} p(\boldsymbol{\theta}|\mathbf{y},\mathbf{d}) \log(\frac{p(\boldsymbol{\theta}|\mathbf{y},\mathbf{d})}{p(\boldsymbol{\theta})})\mathrm{d}\boldsymbol{\theta},
    \end{aligned}
    \label{eq: kld utility}
\end{equation}
where $p(\boldsymbol\theta|\mathbf{y},\mathbf{d})$ is the posterior distribution of $\boldsymbol\theta$ given a design $\mathbf{d}$ and the data $\mathbf{y}$. Note that data $\mathbf{y}$ could be either from the actual experimental measurement or the numerical model simulation. To identify an optimal design without performing any experiments, expected information gain (EIG,~\cite{lindley_measure_1956}) is employed to integrate the information gain over all possible predicted data:
\begin{equation}
\label{eq:EIG}
    \begin{aligned}
        \mathbb{E}[U(\boldsymbol{\theta},\mathbf{y},\mathbf{d})] &= \text{EIG} (\mathbf{d}) \\
        &=\mathbb{E}_{\mathbf{y} | \mathbf{d}} [D_{\textrm{KL}}(p(\boldsymbol\theta|\mathbf{y},\mathbf{d})\| p(\boldsymbol\theta))]\\
  &=\int_{\mathcal{Y}}D_{\textrm{KL}}(p(\boldsymbol\theta|\mathbf{y},\mathbf{d})\| p(\boldsymbol\theta)) p(\mathbf{y}|\mathbf{d})\mathrm{d}\mathbf{y}\\      %&=\int_{\mathcal{Y}}D_{\textrm{KL}}(p(\boldsymbol\theta|\mathbf{y},\mathbf{d})\| p(\boldsymbol\theta)) p(\mathbf{y}|\mathbf{d})\mathrm{d}\mathbf{y} \\
        %&=\int_{\mathcal{Y}}\left(\int_{\boldsymbol{\Theta}} p(\boldsymbol{\theta}|\mathbf{y},\mathbf{d}) \log(\frac{p(\boldsymbol{\theta}|\mathbf{y},\mathbf{d})}{p(\boldsymbol{\theta})})\mathrm{d}\boldsymbol{\theta}\right) p(\mathbf{y}|\mathbf{d})\mathrm{d}\mathbf{y}.
    \end{aligned}
\end{equation}
where $p(\mathbf{y}|\mathbf{d}):=\mathbb{E}_{\boldsymbol{\theta}}[p(\mathbf{y}|\boldsymbol{\theta},\mathbf{d})]$ is the distribution of the predicted data among all possible parameter values given a certain design. It should be noted that designs given by maximizing EIG solely depend on prior parameter belief \cite{zemplenyi_bayesian_2021} and an appropriate model \cite{rainforth_modern_2023}. Although this utility still works for the weak model discrepancy cases \cite{feng_optimal_2015}, the stronger model discrepancy will likely lead to correlated biases in likelihoods and non-negligible impacts on the EIG, thus resulting in designs of poor quality \cite{brynjarsdottir_learning_2014, catanach_metrics_2023}. This could be alleviated by introducing some information from the true system, i.e. actual measurements, as preliminary data~\cite{waldron_closed-loop_2019,wang_measure_2024}.  

%The EIG describes the information gained from an experiment as the expected KLD from the posterior to the prior. It can be equivalently thought of as the mutual information between $\mathbf{y}$ and $\boldsymbol{\theta}$ given $\mathbf{d}$, or the expected reduction in predictive uncertainty from observing $\mathbf{y}$  \cite{borth_total_1975}\cite{rainforth_modern_2023}.

Beyond each belief update, we mainly focus on the scenario of sequential BED \cite{shen_bayesian_2023}, which requires sequentially performing experiments to update the design. With the aggregated history information of optimal design and the corresponding measured data denoted as $I_i = [\mathbf{y}_1, \mathbf{d}_1, \mathbf{y}_2, \mathbf{d}_2, \allowbreak \cdots, \mathbf{y}_i, \mathbf{d}_i]$, the Bayesian updating process in the sequential BED at $i$-th stage can be written as:
% \begin{equation}p(\boldsymbol\theta|I_{i})=\frac{p(\mathbf{y}_{i}|\boldsymbol\theta,\mathbf{d}_{i})p(\boldsymbol\theta|I_{i-1})}{p(\mathbf{y}_{i}|\mathbf{d}_{i})},
% \end{equation}


\begin{equation}p(\boldsymbol\theta|I_{i})=\frac{p(\mathbf{y}_{i}|\boldsymbol\theta,\mathbf{d}_{i}, I_{i-1})p(\boldsymbol\theta|I_{i-1})}{p(\mathbf{y}_{i}|\mathbf{d}_{i})},
\end{equation}
where $p(\boldsymbol\theta|I_i)$ is the posterior distribution at $i$-th stage and $p(\mathbf{y}_{i}|\boldsymbol\theta,\mathbf{d}_{i}, I_{i-1})$ is the likelihood based on data $\mathbf{y}_{i}$, design $\mathbf{d}_{i}$ and aggregated information $I_{i-1}$. If using $p(\boldsymbol\theta|I_{i-1})$ and $p(\boldsymbol\theta|I_{i})$ to replace the prior distribution $p(\boldsymbol\theta)$ and the posterior one $p(\boldsymbol\theta|\mathbf{y},\mathbf{d})$, we can define the EIG at $i$-th stage and denote it as $\mathbb{E}[U(\boldsymbol{\theta},\mathbf{y},\mathbf{d})]_i$. In this work, a greedy strategy that maximizes $\mathbb{E}[U(\boldsymbol{\theta},\mathbf{y},\mathbf{d})]$ at every stage has been taken:
\begin{equation}
    \label{eq:optimal_design_i}
    \mathbf{d}_{i}= \argmax_{\mathbf{d} \in \mathcal{D}}\mathbb{E}[U(\boldsymbol{\theta},\mathbf{y},\mathbf{d})]_i.
\end{equation}
%The $\mathbf{d}_{i+1}$ can be regarded as the design that brings the most information based on current vision.


% Three options of OED can be formalized to estimate the parameters: $\{ \boldsymbol{\theta}_\mathcal{G}, \boldsymbol{\theta}_\text{NN} \}$ 
% \begin{itemize}
% \item Define $\boldsymbol{\theta}_\text{OED}:=\{\boldsymbol{\theta}_\mathcal{G},\boldsymbol{\theta}_\text{NN}\}$ for Bayesian OED, which leads to a very high-dimensional parameter space and thus demands a large computational cost for Bayesian OED.
% \item Define $\boldsymbol{\theta}_\text{OED}:=\{\boldsymbol{\theta}_\mathcal{G},\boldsymbol{\theta}_\text{NN}\}$ for classical OED, which is equivalent to a corresponding optimization problem and may fall into a local minimum.
% \item Define $\boldsymbol{\theta}_\text{OED}:=\{\boldsymbol{\theta}_\mathcal{G}\}$ for Bayesian OED and estimate $\boldsymbol{\theta}_\text{NN}$ via optimization techniques.
% \end{itemize}

% In this work, we focus on the third option above and propose a hybrid optimization algorithm that iteratively performs Bayesian OED for $\boldsymbol{\theta}_\mathcal{G}$ and gradient-based optimization for $\boldsymbol{\theta}_\text{NN}$. Compared to the other two options, the iterative algorithm in this work has the following advantages:
% \begin{itemize}
% \item The Bayesian OED for $\boldsymbol{\theta}_\mathcal{G}$ can leverage prior information of those parameters that carry physical meanings.
% \item The optimization of $\boldsymbol{\theta}_\text{NN}$ avoids the full Bayesian approach for neural network parameters and significantly reduces the computational cost.
% \item The iterative updates of Bayesian OED for $\boldsymbol{\theta}_\mathcal{G}$ introduces perturbations to the optimization stages of $\boldsymbol{\theta}_\text{NN}$, making it less prone to falling into a local minimum.
% \end{itemize}


\subsection{Model Discrepancy Correction}
\label{sec: Model error correction}

The presence of model discrepancy, i.e., the difference between $\mathcal{G}^\dagger$ in Eq.~\eqref{eq:true_system} and $\mathcal{G}$ in Eq.~\eqref{eq:modeled_system}, would inevitably introduce bias in the estimated parameters $\boldsymbol{\theta}_\mathcal{G}$ and thus motivate the study of quantifying the model discrepancy $\mathcal{G}^\dagger-\mathcal{G}$ for BED~\cite{feng_optimal_2015}. In this work, we investigate the use of neural networks to characterize the model discrepancy as shown in Eq.~\eqref{eq:modeled_system}, for which the key challenge is that the unknown parameters $\boldsymbol{\theta}_\text{NN}$ are often high-dimensional to fully exploit the representation power of neural networks and thus pose a computational challenge to Bayesian inference. Therefore, a common practice is to calibrate the model discrepancy offline with empirically chosen data. However, offline calibration relies heavily on the quality and availability of pre-selected data. In this work, we focus on the active online learning of model discrepancy parameters $\boldsymbol{\theta}_\text{NN}$, which exploits the optimal data obtained via sequential BED to estimate physics-based model parameters $\boldsymbol{\theta}_\mathcal{G}$ in Eq.~\eqref{eq:modeled_system}. 

Considering that $\boldsymbol{\theta}_\mathcal{G}$ is correlated with the model discrepancy, the optimal data for $\boldsymbol{\theta}_\mathcal{G}$ obtained via sequential BED is likely to be still informative in the calibration of the model discrepancy, which can be efficiently handled by solving an optimization problem. Therefore, this work employs an iterative approach to actively gather informative data and to estimate the unknown parameters $\boldsymbol{\theta}_\mathcal{G}$ and $\boldsymbol{\theta}_\text{NN}$ in Eq.~\eqref{eq:modeled_system}. 

More specifically, the iterative approach at $i$-th stage starts with solving a BED problem for $\boldsymbol{\theta}_\mathcal{G}$ based on the current estimation of $\boldsymbol{\theta}_\text{NN}$ to obtain the optimal design $\mathbf{d}$ in Eq.~\eqref{eq:optimal_design_i}. With the fixed design $\mathbf{d}$ and the neural network parameters $\boldsymbol{\theta}_\text{NN}$, we further obtain the maximum a posterior (MAP) point $\boldsymbol\theta_\mathcal{G}^*$:

\begin{equation}
    \label{eq:selet 1 theta}
    \boldsymbol\theta_\mathcal{G}^*=\argmax_{\boldsymbol\theta_\mathcal{G}}
 \{ p(\mathbf{y}|\boldsymbol\theta_\mathcal{G},\mathbf{d};\boldsymbol\theta_\text{NN})p(\boldsymbol\theta_\mathcal{G};\boldsymbol\theta_\text{NN})  \}.
\end{equation}

With the MAP estimation of $\boldsymbol\theta_\mathcal{G}^*$ further being fixed, the objective function $L(\boldsymbol\theta_\text{NN};\ \boldsymbol\theta_\mathcal{G}^*,\mathbf{y},\mathbf{d})$ is defined by the likelihood function $p(\mathbf{y}|\boldsymbol\theta_\mathcal{G}^*,\mathbf{d};\boldsymbol\theta_\text{NN})$ , and the updated estimation of $\boldsymbol\theta_\text{NN}$ can be obtained via:

\begin{equation}
    \label{eq:optimal_theta_NN}
    \boldsymbol\theta_\text{NN}^*=\argmax_{\boldsymbol\theta_\text{NN}} L(\boldsymbol\theta_\text{NN};\boldsymbol\theta_\mathcal{G}^*,\mathbf{y},\mathbf{d}).
\end{equation}

We omit the stage index $i$ in this section for simplicity in notation. In summary, there are three key steps at each stage of the hybrid framework illustrated in Fig.~\ref{fig:graphic_abstract}: (i) performing BED to obtain optimal design $\mathbf{d}$, (ii) obtaining the MAP estimation $\boldsymbol\theta_\mathcal{G}^*$, and (iii) updating the estimation of $\boldsymbol{\theta}_{\text{NN}}$. A sketch of proof for the existence of a globally optimal solution is presented in~\ref{appendx proof for obj in model correction}.

In practice, we sample $m$ points for the estimation of $\boldsymbol\theta_\mathcal{G}^*$ to enhance the robustness of the proposed algorithm:
\begin{equation}
    \{\boldsymbol\theta_\mathcal{G}^{(1)},\boldsymbol\theta_\mathcal{G}^{(2)},...,\boldsymbol\theta_\mathcal{G}^{(m)}\}=\operatorname{arg\,top}_m p(\mathbf{y}|\boldsymbol\theta_\mathcal{G},\mathbf{d};\boldsymbol\theta_\text{NN})p(\boldsymbol\theta_\mathcal{G};\boldsymbol\theta_\text{NN}),\label{eq:selet m theta}
\end{equation}
where $\operatorname{arg\,top}_m$ denotes the process that identifies the set of parameters corresponding to the top $m$ largest values of a function, and the $\boldsymbol\theta_\mathcal{G}^*$ in Eq.~\eqref{eq:optimal_theta_NN} is approximated by those samples (more related to posterior mean) :
\begin{equation}
    \boldsymbol\theta_\mathcal{G}^* \approx \frac{1}{m}\sum_{i=1}^m \boldsymbol\theta_\mathcal{G}^{(i)}.
\end{equation}
%The method for finding the argument corresponding to the maximum of a function aligns with the evaluation approaches for likelihood and posterior distributions. If grid search is employed, sorting can be performed. This approach is feasible under the reduced parameter space in our framework. Other sampling-based or gradient-based methods can also be considered. The objective function can be evaluated at the average value of the selected $m$ points as an ensemble mean to introduce perturbations:


The optimization problem in Eq.~\eqref{eq:optimal_theta_NN} can be efficiently solved via gradient descent methods. The gradient of the objective function in Eq.~\eqref{eq:optimal_theta_NN} with respect to the neural network parameters is written as:

\begin{equation}
    \frac{\mathrm{d} \text{Obj}}{\mathrm{d} \boldsymbol\theta_\text{NN}} = \frac{\partial \text{Obj}(\mathbf{u})}{\partial \mathbf{u}} \frac{\mathrm{d} \mathbf{u}}{\mathrm{d} \boldsymbol\theta_\text{NN}}.
\end{equation}

It should be noted that $\mathrm{d} \mathbf{u}/\mathrm{d} \boldsymbol\theta_\text{NN}$ is often expensive to evaluate directly when $\mathbf{u}$ and $\boldsymbol\theta_\text{NN}$ are both high-dimensional. Therefore, efficient methods have been developed to obtain $\mathrm{d} \text{Obj}/\mathrm{d} \boldsymbol\theta_\text{NN}$ such as automatic differentiation~\cite{griewank_automatic_1988,frostig_compiling_nodate} or adjoint methods~\cite{hilbert_methods_1985,givoli_tutorial_2021}. 

As automatic differentiation is essentially a discretized version of the adjoint method, we only briefly introduce the adjoint method here. To avoid directly calculating $\mathrm{d} \mathbf{u}/\mathrm{d} \boldsymbol\theta_\text{NN}$, an adjoint state $\lambda(t)$ is defined such that $\lambda(t)^\top=\partial L / \partial \mathbf{u}(t)$. For the modeled system in Eq.~\eqref{eq:modeled_system}, the adjoint state in time range $[t_0,t_1]$ can be obtained by backwardly solving:

\begin{equation}
\left\{
    \begin{array}{l}
        \begin{aligned}
            \frac{\mathrm{d}\lambda(t)}{\mathrm{d}t} &= -\left( \frac{\partial (\mathcal{G}+\text{NN})}{\partial \mathbf{u}} \right)^\top \lambda(t)\\
            \lambda^\top(t_1) &= \frac{\partial L}{\partial \mathbf{u}(t_1)}
        \end{aligned}
    \end{array}
\right.,
\end{equation}
where $\partial \text{NN}/\partial \mathbf{u}$ can be obtained directly via the backpropagation of the neural network $\text{NN}$ and $\partial \mathcal{G}/\partial \mathbf{u}$ is the Jacobian of the physics-based model $\mathcal{G}$. In practice, the Jacobian can be derived analytically or approximated by the coefficient matrix from the numerical solver, which often formulates a linear system. With the adjoint state obtained, the gradient $\mathrm{d} L /\mathrm{d} \boldsymbol\theta_\text{NN}$ can then be calculated as follows:

\begin{equation}
    \label{eq:adjoint_grad}
    \begin{aligned}
        \frac{\mathrm{d} L}{\mathrm{d} \boldsymbol\theta_\text{NN}}&=\int_{t_0}^{t_1} \lambda^\top(t)\frac{\partial (\mathcal{G}+\text{NN})}{\partial \boldsymbol\theta_\text{NN}} dt\\
        &=\underbrace{
    \int_{t_0}^{t_1} \lambda^\top(t)\frac{\partial \mathcal{G}}{\partial \boldsymbol\theta_\text{NN}} dt
}_{\mathclap{=0}}+\int_{t_0}^{t_1} \lambda^\top(t)\frac{\partial \text{NN}}{\partial \boldsymbol\theta_\text{NN}} dt.
    \end{aligned}
\end{equation}
%where the former term is equal to 0 because function $\mathcal{G}$
%is irrelevant to network parameters and the $\frac{\partial \textrm{NN}}{\partial \boldsymbol\theta_\text{NN}}$ in the latter term can be directly obtained from the neural network. 
%An illustration of this method is provided in section \ref{sec: node}.


\subsection{Ensemble-based Indicator for Data Informativeness} 
\label{sec:ensemble_indicator}

The key idea behind the iterative approach in Section~\ref{sec: Model error correction} for the calibration of unknown parameters $\boldsymbol\theta_\text{NN}$ is that $\boldsymbol{\theta}_\mathcal{G}$ and $\boldsymbol\theta_\text{NN}$ are correlated and thus the data from an optimal design for $\boldsymbol{\theta}_\mathcal{G}$ is assumed to be informative in calibrating $\boldsymbol\theta_\text{NN}$. The numerical examples of this work confirm that such an assumption is largely valid, while occasionally the data could be less informative and sometimes even misleading for the calibration of $\boldsymbol\theta_\text{NN}$. 

In this work, we propose an ensemble-based indicator that can quantify the data informativeness for the calibration of $\boldsymbol\theta_\text{NN}$. The general concept is to employ an ensemble Kalman method to efficiently approximate the posterior distribution and the utility function involved in Eq.~\eqref{eq: kld utility} and Eq.~\eqref{eq:EIG}, which account for the main computational challenge to standard BED methods when $\boldsymbol{\theta}$ is high-dimensional.

% \textcolor{blue}{
% Conclusion: The title could be replaced by ensemble-based Bayesian utility functions.

% Reason: EIG is a category of Bayesian utility function based on information theroy. There could be Bayesian utility functions that are not based on information theroy, such that:

% 1, Posterior Covariance Matrix;
% 2, Quadratic Loss;
% 3, Prediction Variance-Based Utilities;
% 4, Cost-Minimization Utilities.

% However, all Bayesian utility functionsâ€”whether information-theoretic or notâ€”can be approximated as long as prior and posterior distributions are available. This means ensemble-based methods are indeed applicable for calculating any Bayesian utility function, not just EIG.
% }

% The gradient-based method can optimize the parameters $\boldsymbol{\theta}_{\textrm{NN}}$ based on the measurement from designs. However, it can not assess the informativeness and effectiveness of the measurement which can allow us to exclude ineffective designs. Employing another Bayesian OED to identify an informative design for the model correction term poses significant computational challenges, especially because the dimension of parameters from a neural-network-based model is usually very high. To address this problem, we introduce ensemble Kalman inversion (EKI) to update the parameter $\boldsymbol{\theta}_{\textrm{NN}}$ in the model correction term and use the ensemble-based EIG to evaluate the informativeness of this design.


More specifically, ensemble Kalman inversion (EKI) is employed to derive the indicator that quantifies the data informativeness for the calibration of $\boldsymbol{\theta}_\text{NN}$. EKI was developed to solve a Bayesian inverse problem in the general form:
\begin{equation}
    \mathbf{y}=G(\boldsymbol\theta)+\boldsymbol\eta,
\end{equation}
where $G$ denotes a forward map from unknown parameters $\boldsymbol{\theta}$ to the data $\mathbf{y}$, and $\boldsymbol\eta \sim \mathcal{N}(\mathbf{0},\boldsymbol\Gamma)$ denotes the data measurement noises and is often assumed to be with a zero-mean Gaussian distribution. In this work, the forward map $G$ corresponds to solving the modeled system in Eq.~\eqref{eq:modeled_system} and observing the solution according to a given design $\mathbf{d}$, and $\boldsymbol{\theta}$ corresponds to the unknown parameters $\boldsymbol\theta_\text{NN}$ of the model discrepancy term.

% EKI is a derivative-free optimization method to estimate the unknown parameters $\boldsymbol\theta_\text{NN}$ from noisy data $\mathbf{y}$ by seeking the minimizer of the optimization problem:
% \begin{equation}
%     \min_{\boldsymbol\theta_\text{NN}} || \Sigma_{\boldsymbol\epsilon}^{-\frac{1}{2}}(\mathbf{y}-G(\boldsymbol\theta_\text{NN}))||^2.
% \end{equation}

% To adopt ensemble method, we first inject Gaussian-distributed noise to the parameters of the neural network in the baseline model $\boldsymbol\theta_\text{NN}^{(b)}$ to formalize the initial ensemble set $\{[\boldsymbol\theta_\text{NN}^{(j)}]_0\}^J_{j=1}$ with $[\boldsymbol\theta_\text{NN}^{(j)}]_0 = \boldsymbol\theta_\text{NN}^{(b)} + \boldsymbol\eta^{(j)}$. Here, $\boldsymbol\eta^{(j)} \sim \mathcal{N}(\mathbf{0}, c \mathbf{I}_{d_{\boldsymbol\theta}})$ are independently Gaussian distributed, where $c \in \mathbb{R}$ is a scalar, $\mathbf{I}_{d_{\boldsymbol\theta}} \in \mathbb{R}^{d_{\boldsymbol\theta} \times d_{\boldsymbol\theta}}$ is a identity matrix, and $d_{\boldsymbol\theta}$ is the dimension of $\boldsymbol\theta_\text{NN}$.

Given a design $\mathbf{d}$ and the corresponding data measurement $\mathbf{y}$, the EKI updating formula for the ensemble of parameters can be written as:
\begin{equation}
\boldsymbol\theta_{n+1}^{(j)}=\boldsymbol\theta_{n}^{(j)}+\Sigma^{\boldsymbol\theta \mathbf{g}}_n(\Sigma^{\mathbf{g}\mathbf{g}}_n+\Gamma)^{-1}(\mathbf{y}-\mathbf{g}^{(j)}_n),
\end{equation}
where $\mathbf{g}^{(j)}_n:=G(\boldsymbol\theta_{n}^{(j)})$, the index $n$ denotes the $n$-th EKI iteration, and the index $j$ indicates the $j$-th ensemble. The ensemble covariance matrices $\Sigma^{\boldsymbol\theta \mathbf{g}}_n$ and $\Sigma^{\mathbf{g}\mathbf{g}}_n$ can be calculated as:
\begin{equation}
    \begin{aligned}
    &\bar{\boldsymbol\theta}_n = \frac{1}{J}\sum_{j=1}^J \boldsymbol\theta_n^{(j)}, \quad \bar{\mathbf{g}}_n = \frac{1}{J}\sum_{j=1}^J \mathbf{g}^{(j)}_n,  \\
    &\Sigma^{\boldsymbol\theta \mathbf{g}}_n = \frac{1}{J-1} \sum_{j=1}^J \left(\bar{\boldsymbol\theta}_n - \boldsymbol\theta_n^{(j)}\right)\left(\bar{\mathbf{g}}_n  - \mathbf{g}^{(j)}_n \right)^\top ,   \\
    &\Sigma^{\mathbf{g}\mathbf{g}}_n = \frac{1}{J-1} \sum_{j=1}^J \left(\bar{\mathbf{g}}_n  - \mathbf{g}^{(j)}_n \right)\left( \bar{\mathbf{g}}_n  - \mathbf{g}^{(j)}_n \right)^\top.
    \label{eq: eki detials}
\end{aligned}
\end{equation}

With the initial ensemble $\{\boldsymbol\theta_0^{(j)}\}^J_{j=1}$ and the updated ensemble $\{\boldsymbol\theta_K^{(j)}\}^J_{j=1}$ after $K$ times of EKI iterations, we can approximate the KL divergence between the posterior and prior distributions of $\boldsymbol{\theta}$ by treating both ensembles as Gaussian:
\begin{equation}
\begin{gathered}
\label{eq:ensemble_KLD}
        D_{\textrm{KL}}(p(\boldsymbol{\theta}|\mathbf{y}) || p(\boldsymbol{\theta})  ) \approx \tilde{D}_{\textrm{KL}}(\{\boldsymbol\theta_K^{(j)}\}^J_{j=1} || \{\boldsymbol\theta_0^{(j)}\}^J_{j=1}) = \\ 
        \frac{1}{2} \left[ \text{tr} \left( \left( \Sigma^{\boldsymbol{\theta} \boldsymbol{\theta}}_K \right)^{-1} \Sigma^{\boldsymbol{\theta} \boldsymbol{\theta}}_0 \right) - d_{\boldsymbol\theta} + \ln \left( \frac{\det \Sigma^{\boldsymbol{\theta} \boldsymbol{\theta}}_K}{\det \Sigma^{\boldsymbol{\theta} \boldsymbol{\theta}}_0} \right)\right. 
        \left.+ \left(\bar{\boldsymbol{\theta}}_K - \bar{\boldsymbol{\theta}}_0 \right)^\top \left( \Sigma^{\boldsymbol{\theta} \boldsymbol{\theta}}_K \right)^{-1} \left(\bar{\boldsymbol{\theta}}_K - \bar{\boldsymbol{\theta}}_0 \right) \right],
\end{gathered}
\end{equation}
where $d_{\boldsymbol{\theta}}$ represents the dimension of $\boldsymbol{\theta}$. The ensemble covariance matrices $\Sigma^{\boldsymbol\theta \boldsymbol\theta}_0$ and $\Sigma^{\boldsymbol\theta \boldsymbol\theta}_K$ can be calculated as:
\begin{align}
    %&\left(\bar{\boldsymbol\theta}_{NN}\right)_K = \frac{1}{J}\sum_{j=1}^J \left(\boldsymbol\theta_\text{NN}^{(j)}\right)_K, \\
    &\Sigma^{\boldsymbol\theta \boldsymbol\theta}_n = \frac{1}{J-1} \sum_{j=1}^J \left(\bar{\boldsymbol\theta}_n - \boldsymbol\theta_n^{(j)}\right)\left(\bar{\boldsymbol\theta}_n - \boldsymbol\theta_n^{(j)}\right)^\top.
\end{align}

The ensemble-based approximation of KLD in Eq.~\eqref{eq:ensemble_KLD} can be used to evaluate the informativeness of the data $\mathbf{y}$ from a design $\mathbf{d}$. In this work, we focus on applying the approximated KLD for the high-dimensional parameters $\boldsymbol{\theta}_\text{NN}$ in Eq.~\eqref{eq:modeled_system} and determine whether to perform the calibration of the model discrepancy based on the optimal design and the corresponding measurements from the standard BED of the relatively low-dimensional parameters $\boldsymbol{\theta}_\mathcal{G}$. In practice, a few iterations of EKI updating are sufficient to evaluate the data informativeness, which can effectively avoid performing calibration of the model discrepancies on less informative or even misleading data that could further lead to larger model discrepancies and more biases in the estimation of $\boldsymbol{\theta}_\mathcal{G}$ in Eq.~\eqref{eq:modeled_system}. More detailed algorithm to exploit the ensemble-based approximation of KLD to quantify the data informativeness is presented in~\ref{sec:algorithm} for the iterative approach introduced in Section~\ref{sec: Model error correction}.

%In practice, we propose two strategies to guide the decision regarding the utility of the design after attaining the approximate information gain: (i)) establishing a KLD threshold based on prior knowledge, such as the mean KLD from the designs in the initial stages, and (ii) comparing the current design with another design and choosing the one with the higher KLD. The updated design is then used in the model error correction phase.


\section{Numerical Results}
\label{sec: Numerical Results}

To demonstrate the performance of our proposed method, we study the example of source inversion of a contaminant in the convection-diffusion field, which is a classical test example for BED and has been previously studied in \cite{shen_bayesian_2023}.  
The general goal of this inverse problem is to take concentration measurements in a flow field governed by a convection-diffusion equation and then infer the plume source location. More specifically, the contaminant concentration $\mathbf{u}$ at two-dimensional spatial location $\mathbf{z} = \{z_x, z_y\}$ and time $t$ is governed by the following equation: 
%\textcolor{red}{(Why you have $z$ and $\mathbf{z}$ at the same time? Please ensure consistency of the writing)} \textcolor{blue}{Detailed statement has been added below.}

\begin{equation}
    \frac{\partial \mathbf{u}(\mathbf{z},t;\boldsymbol\theta)}{\partial t}=\nabla^2\mathbf{u}-\mathbf{v}(t) \cdot \nabla \mathbf{u}+S(\mathbf{z},t;\boldsymbol\theta),~~~\mathbf{z} \in [z_L,z_R]^2,~~t>0
    \label{eq:true_system_example},
\end{equation}
where $\mathbf{v}=\{v_x,v_y\} \in \mathbb{R}^2$ is a time-dependent convection velocity, $S$ denotes the source term with some parameters $\boldsymbol\theta$. In this work, the true system has an exponentially decay source term in the following form with the parameters $\boldsymbol\theta=\{\theta_x,\theta_y,\theta_h,\theta_s\} \in \mathbb{R}^4$:
\begin{equation}
    S(\mathbf{z},t;\boldsymbol\theta)=\frac{\theta_s}{2\pi\theta_h^2}\exp \left(-\frac{(\theta_x-z_x)^2+(\theta_y-z_y)^2}{2\theta_h^2} \right),
    \label{eq:true_source_term}
\end{equation}
where \(\theta_x\) and \(\theta_y\) denote the source location, \(\theta_h\) and \(\theta_s\) represent the source width and source strength. The initial condition is \(\mathbf{u}(\mathbf{z}, 0;\boldsymbol\theta) = \mathbf{0}\), and a homogeneous Neumann boundary condition is imposed for all sides of the square domain. %It should be noted that the notation of the concentration $\mathbf{u}(\mathbf{z},t;\boldsymbol{\theta})$ in Eq. \eqref{eq: true system 1} is consistent with the notation of model state $\mathbf{u}(\mathbf{z};\boldsymbol{\theta})$ in Section \ref{sec:OED}, where the general coordinate $\mathbf{z} = \{z, t\} = \{z_x, z_y, t\}$ is a vector that combines the spatial location $z = \{z_x, z_y\}$ and time $t$.

\sloppy
For the true system, the parameters in the source term are set as $\theta_s=2$ and $\theta_h=0.05$. Figure~\ref{PDE simulation results} presents the system state $\mathbf{u}$ calculated at $t=0.05,0.10,0.15,0.20,0.25$ time units when the source locates at $\theta_x = 0.25$ and $\theta_y = 0.25$, which illustrates how the source affects the concentration value at different locations across the domain through the convection and diffusion with the time evolution.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.97\linewidth]{c-d_field2.png}
    \caption{Concentration value at different times in a convection-diffusion field. The numerical simulation is performed in a larger domain ($[-2,3]^2$) and presented in a smaller one ($[0,1]^2$) to emphasize the areas close to the source location. Stages 1-5 correspond to $0.05,0.10,0.15,0.20,0.25$ time units.}
    \label{PDE simulation results}
\end{figure}

The design $\mathbf{d}=\{d_x,d_y,d_t\}$ in this problem refers to the spatiotemporal coordinate to measure the concentration value. More specifically, each design involves the measurement of just one point in the domain. The spatial coordinates of the initial design start at $\{d_x^0,d_y^0\}=(0.5,0.5)$ and will gradually move around to other locations. The differences of spatial coordinates between two consecutive designs (i.e., $d_x^{i+1} - d_x^{i}$ and $d_y^{i+1} - d_y^{i}$) are constrained by the interval $[-0.2, 0.2]^2$.  The temporal coordinates of the measurement are set with a fixed time step $\Delta t=0.05$ time unit. At each time step, the posterior distribution from the previous stage serves as the prior for a new stage of BED as illustrated in Fig.~\ref{fig:graphic_abstract}. The current optimal design also serves as the starting point for the next stage of BED.

In all numerical examples, the physics-based unknown parameters $\boldsymbol{\theta}_\mathcal{G}$ are the location $(\theta_x,\theta_y)$ of the source. The key motivations and conclusions of numerical results are summarized below:
\begin{itemize}
\item We first validate the performance of our approach in an example where the model error only exists as an incorrect value of the parameter $\theta_s$ set in the true source term of Eq.~\eqref{eq:true_source_term}, for which it is feasible to perform the full BED for the joint distribution of unknown parameters $\boldsymbol{\theta}_\mathcal{G}$ and $\theta_s$. The comparison of the estimated model error parameter to the full BED approach confirms the effectiveness of the proposed iterative approach of calibrating the model error. Detailed results can be found in Section~\ref{sec:Correct parametric error}.
\item We study a more challenging example where the model error exists as an incorrect knowledge of the function form for the source term, and such a model discrepancy is characterized by a neural network, which leads to high-dimensional unknown parameters $\boldsymbol{\theta}_\text{NN}$. In practice, the joint distribution of $\boldsymbol{\theta}_\mathcal{G}$ and $\boldsymbol{\theta}_\text{NN}$ is high-dimensional and the full BED would become expensive or even infeasible in such a challenging setup. We demonstrate that the proposed iterative approach provides an efficient and robust correction of model discrepancy and leads to a less biased estimation of $\boldsymbol{\theta}_\mathcal{G}$. Detailed results can be found in Section~\ref{sec:Correct functional error}.
%\item We show the comparison of results obtained from an auto-differentiable solver and a traditional solver with its Jacobian available. The comparison demonstrates that the proposed hybrid OED approach is compatible with either modern auto-differentiable solvers or classical solvers. Detailed results can be found in Section~\ref{sec: node}.
\item We validated the performance of ensemble-based indicator for data informativeness in both of the aforementioned examples. The comparison of indicators computed from different datasets demonstrates that the indicator can effectively identify more informative data for correcting model discrepancy and serves as a much less expensive option to the full Bayesian approach. Detailed results can be found in both Section~\ref{sec:Correct parametric error} and Section~\ref{sec:Correct functional error}.
\end{itemize}


%\textcolor{red}{Your original writings about the case settings are too verbose. On the other hand, the key information and messages are not clearly delivered.}

\subsection{Validation of Iterative Approach for Parametric Model Error}\label{sec:Correct parametric error}
%\subsection{Correct parametric error}\label{sec:Correct parametric error}

In this section, the performance of our proposed iterative approach is validated by an example with parametric model error. The setup of this example is designed in such a way that the full BED remains feasible. Compared with the results of full BED approach, we demonstrate that the proposed iterative approach can efficiently and robustly calibrate the parametric model error. More specifically, we set up an example with parametric model error where the true form of the source function in Eq.~\eqref{eq:true_source_term} is known but the value of $\theta_s$ is set incorrectly, e.g., due to the lack of knowledge on the strength of the source term. In addition, the source location $(\theta_x, \theta_y)$ are unknown and to be determined by BED in the presence of parametric model error $\theta_s$. The true values are $\theta_x=0.45$, $\theta_y=0.25$, and $\theta_s=2$. The advection velocity is known and set as $v_x=v_y=20t$.

For standard sequential BED, the estimation of those unknown parameters considers a joint probability distribution of $\{\theta_x, \theta_y, \theta_s\}$. For our proposed approach, the joint distribution of $\{\theta_x, \theta_y\}$ is handled by sequential BED, while the estimation of $\theta_s$ is achieved via the gradient-based optimization described in Section~\ref{sec: Model error correction}. It is worth noting that no neural network is used in this example, while the existing model $\mathcal{G}$ depends on $\theta_s$, with which the gradient $\mathrm{d} L/\mathrm{d} \theta_s$ can be derived in a similar way as illustrated in Eq.~\eqref{eq:adjoint_grad}.



\subsubsection{Standard Approach on Parametric Model Error}
We first consider joint probability distribution of $\{\theta_x,\theta_y,\theta_s\}$ and perform a five-stage standard sequential BED as the benchmark results. The five stages correspond to 0.05, 0.10, 0.15, 0.20, and 0.25 time units. For the posterior distribution at each stage, we first present the marginal distribution of $\theta_s$, followed by the two-dimensional conditional distribution of $\theta_x$ and $\theta_y$, conditioned on the value of $\theta_s$ corresponding to the highest marginal probability density. The results in Fig.~\ref{fig:posterior of a six-step 3D OED} demonstrate that the standard approach successfully identifies the true values of the parameters. For $\theta_s$, a peak of probability density gradually forms around the true value of 2 in its marginal distribution, indicating increased confidence in the estimation. In the two-dimensional distribution of $\theta_x$ and $\theta_y$, the high probability region also aligns well with the true source location.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{standard_oed_with_kld.png}
  \caption{Results of learning parametric model error via standard sequential BED. The top row shows the marginal distribution of $\theta_s$ at each stage, and the bottom row presents the two-dimensional conditional distribution of $\theta_x$ and $\theta_y$, conditioned on the value of $\theta_s$ with the highest marginal probability density.}
  \label{fig:posterior of a six-step 3D OED}
\end{figure}


%We further compare the choice of utility function between KLD and TFIM in Figs.~\ref{fig:posterior of a six-step 3D OED with KLD} and \ref{fig:posterior of a six-step 3D OED with TFIM}, respectively. At the $5$-th stage, the posterior distribution of $\theta_s$ using KLD as the utility function better centers around the true value $2$, while the results using TFIM as the utility function still show noticeable bias in the posterior distribution of $\theta_s$ compared to its true value. Therefore, we focus on the use of KLD as the utility function for the rest of this work.

\subsubsection{Hybrid Approach on Parametric Model Error}

In the proposed hybrid approach, we only consider the posterior probability distribution of $\{\theta_x, \theta_y\}$ conditioned on the current stage estimation of model error in the sequential BED, while handling the calibration of model error via a gradient-based optimization introduced in Section~\ref{sec: Model error correction}, thereby restricting the computational cost to a lower-dimensional probability distribution. 

At the first stage of sequential BED, we employ a uniform prior distribution of $\{\theta_x, \theta_y\}$ over the range $[0,1]^2$, discretized into a $51\times 51$ grid of points, and set an initial value of $\theta_s = 3$. In each stage, BED is performed to infer the posterior distribution of $\{\theta_x, \theta_y\}$, and the optimal design is determined accordingly based on the information gain introduced in Section~\ref{sec:OED}. The optimal design subsequently provides additional information that facilitates the calibration of the model error $\theta_s$. In terms of the gradient-based optimization of model error, we have tested both automatic differentiation and adjoint method to acquire the gradient of $\mathrm{d} L/\mathrm{d} \theta_s$ and achieved comparable results. For simplicity and clarity, we only show the results from the automatic differentiation approach, where we have developed a numerical solver based on JAX-CFD~\cite{jax2018github, Kochkov2021-ML-CFD} to facilitate the extraction of gradient information.

Figure~\ref{fig: posterior of learning parameter error} shows the evolution of the posterior distribution for $\{\theta_x, \theta_y\}$ across five stages. The high probability regions gradually converge to the true source location, suggesting an effective correction of parametric model error $\theta_s$. The comparison between Figs~\ref{fig:posterior of a six-step 3D OED} and \ref{fig: posterior of learning parameter error} confirms that the proposed hybrid approach achieves similar results to standard BED for the joint distribution of $\{\theta_x, \theta_y,\theta_s\}$ but with lower computational cost.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{simple_case.png}
  \caption{Posterior results of OED parameter $\{\theta_x,\theta_y\}$ via the hybrid approach on parametric error. The $\{\theta_x,\theta_y\}$ space is $[0,1]^2$ as before, with a zoomed-in view $[0.2,0.6]\times[0.1,0.5]$ to highlight detailed behaviors.}
  \label{fig: posterior of learning parameter error}
\end{figure}

We further investigate the results of corrected parametric model error $\theta_s$. As shown in Fig.~\ref{Results of learning parameter value}, the proposed hybrid approach identifies the true value of $\theta^\dagger_s = 2$. It is worth noting that the optimization process of $\theta_s$ tends to converge to an incorrect value between iterations 450 and 600 (stage 4), mainly due to the poor data quality associated with the design at that stage for model error correction. To address this issue, we employ the approximated KLD defined in Eq.~\eqref{eq:ensemble_KLD} to evaluate the data informativeness. The poor-quality design at stage 4 in Fig.~\ref{Results of learning parameter value} not only renders a bias in the corrected value for the parametric model error $\theta_s$ but also leads to a slower rate of variance reduction during EKI iterations in Fig.~\ref{Results of EKI in learning parameter error}, which implies a smaller approximated KLD between the prior and posterior ensemble of $\theta_s$. The comparison in Fig.~\ref{Results of EKI in learning parameter error} confirms that the performance of the ensemble-based approximated KLD as an indicator to evaluate data informativeness for the calibration of parametric model error.
\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{theta_s_trajactory.png}
    \caption{Original model correction}
  \label{Results of learning parameter value}
  \end{subfigure}%
  \begin{subfigure}[b]{0.6\textwidth}
    \centering
    \includegraphics[width=0.95\linewidth]{EKI_simple_case.png}
    \caption{EKI updating}
  \label{Results of EKI in learning parameter error}
  \end{subfigure}
  \caption{Results of parametric model error calibration via the hybrid approach and the ensemble-based indicator of data informativeness. Panel (a) represents the model error calibration process at five stages, with each stage having 150 iterations of gradient-based optimization. Panel (b) presents the ensemble-based error parameter results updated by two different designs at stage 4.}
  \label{parameter Results of learning parameter error}
\end{figure}


\subsection{Hybrid Approach on Structural Errors}
\label{sec:Correct functional error}

In this section, we focus on the scenario that structural error exists in the modeled system of Eq.~\eqref{eq:modeled_system}, i.e., $\mathcal{G}^\dagger - \mathcal{G}$ is non-negligible. The structural model error is characterized by a neural-network-based model as shown in Eq.~\eqref{eq:modeled_system} with parameters $\boldsymbol{\theta}_\text{NN}$ in a high-dimensional space. This scenario presents a challenge for the BED methods, which often become prohibitively expensive or even infeasible due to the computational cost of handling high-dimensional parameter spaces. Our goal here is to demonstrate the efficacy of the proposed method in providing an efficient and robust correction of structural error based on the optimal designs and the corresponding data from BED for the relatively low-dimensional parameters $\boldsymbol{\theta}_\mathcal{G}$.

The modeled system is in the same form of Eq.~\eqref{eq:true_system_example}, with the source term defined by:
\begin{equation}
    S(\mathbf{z},t;\bm\theta)=\frac{3\theta_s}{\pi\left(\frac{(\theta_x-z_x)^2+(\theta_y-z_y)^2}{2\theta_h^2}+2\theta_h^2\right)},
    \label{eq: c sourse}
\end{equation}
which is different from the exponentially decaying source term as defined in Eq.~\eqref{eq:true_source_term} for the true system. Without addressing this model structural error, the inference of source location through standard BED yields biased results. In this work, a neural network $\text{NN}(z_x,z_y,\theta_x,\theta_y;\boldsymbol{\theta_\text{NN}})$ is employed to characterize the model structural error. More specifically, we utilize a fully connected neural network consisting of 37 parameters, a configuration that introduces a higher-dimensional parameter space than the example of parametric model error in Section~\ref{sec:Correct parametric error}. 
%This neural-network-based structural error model is calibrated with data identified by BED in each stage except for stage 1, due to the unconcentrated posterior distribution $p(\boldsymbol{\theta}_\mathcal{G})$ and potentially biased MAP $\boldsymbol{\theta}_\mathcal{G}^*=\{\theta_x^*,\theta_y^*\}$. It is worth noting that the optimal design and data from BED could be only a single design and corresponding data point, as dictated by the principle of BED. For the approach of collecting preliminary data in Section \ref{sec:OED}, while data on searching trajectory could also be utilized for model correction, we observed from numerical experiments that using only the single point performs better than using all of them.This is because when attempting to match multiple points simultaneously, the accuracy at any single point may be lower than when focusing on just that point. In scenarios with limited data, the most critical factor for updating the posterior in the next stage is the accuracy of the field near the design identified by the upcoming BED. Given that the starting point of the design search in the next stage is the endpoint of the current stage, ensuring high accuracy at the endpoint of the current stage is more beneficial for subsequent design search and posterior updates.


We employ a uniform prior distribution over the range $[0,1]^2$ for $\{\theta_x, \theta_y\}$, discretized into a $51\times 51$ grid, with the true value of the source location set at $\{0.25,0.25\}$. The values of $\{\theta_s^\dagger, \theta_h^\dagger\}$ are set as $\{2, 0.05\}$ in both the true system and the modeled one. The advection velocity is known and set as $v_x=v_y=50t$.
%We would iteratively infer the probability distribution of $\{\theta_x, \theta_y\}$ through BED and then update the neural network parameter $\boldsymbol{\theta_\text{NN}}$ by gradient descent in model correction. 

We first present the results of standard sequential BED for physics-based parameters updated without correcting the structural error. It is expected that model discrepancy introduces bias in both the model forecast and likelihood values, resulting in an incorrect posterior distribution, i.e., the high probability area does not cover the true source location in Fig.~\ref{fig:posterior_DP_1}. Fig.~\ref{fig:posterior_DP_2} shows that the posterior distribution can be improved by applying the proposed hybrid framework to correct the model error, and the high probability areas gradually converge to the true source location from stage 1 to stage 3. The deviation of high probability areas from the true source location in stages 4 and 5 is mainly caused by the uninformative design and data used for model error correction at stage 3. These design and data, optimized for physical parameters at stage 3, introduce biases into the calibrated model, which propagate into the Bayesian updating of physical parameters in subsequent stages. %More specifically, the solution field of the modeled system updated using the stage 3 design, demonstrates noticeable differences from the true system in Fig.~\ref{fig: predict field}, indicating the poor quality of the data and thus leading to increased biases in posterior distributions of the following stages.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[width=0.95\linewidth]{NN_case_no_correction.png}
    \caption{With $\boldsymbol\theta_\text{NN}$ frozen}
    \label{fig:posterior_DP_1}
  \end{subfigure}%
  \vspace{10pt}
  \begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[width=0.95\linewidth]{NN_case_poor_design.png}
    \caption{Update $\boldsymbol\theta_\text{NN}$}
    \label{fig:posterior_DP_2}
  \end{subfigure}%
  \caption{Posterior distributions of inferred source location $\{\theta_x,\theta_y\}$ based on (a) a randomly initialized structural error term and (b) the calibrated structural error term.}
  \label{fig: correct network error}
\end{figure}

Figure~\ref{fig: enhanced correct network error} shows that a more informative design and the corresponding measurement data (adopted at stage 3) for the model error correction significantly improve the posterior distributions of inferred source location at stages 4 and 5. We present the results using the ensemble-based approximated KLD to assess the data informativeness of a given design in Fig.~\ref{parameter Results of learning structural error}. Two designs are selected for comparison: (i) the design identified by BED for posterior estimation at stage 3, represented by the orange dot in Fig.~\ref{fig: enhanced correct network error}, and (ii) a modified design represented by the blue triangle in Fig.~\ref{parameter Results of learning structural error}. With the untrained model at stage 3 as a baseline, we first generate an ensemble of its parameters and apply EKI to update the ensemble for 10 iterations using the two designs and corresponding measurement data. The ensemble-based approximated KLD between the updated and initial ensembles at each EKI iteration in Fig.~\ref{Results of EKI in learning structural error} shows a faster increase with the good design, which confirms that the ensemble-based indicator proposed in Section~\ref{sec:ensemble_indicator} is effective of assessing data informativeness. The model error updated by the good design also shows less deviation from the true discrepancy in Fig.~\ref{Results of calibrating neural network}. Considering that the measurement data at a few design points are not sufficient to correct the model error term across the whole domain, some remaining differences in Fig.~\ref{Results of calibrating neural network} between the true source term and the modeled one are expected. Even with those differences in the modeled source term, especially at a smaller distance from the source location, the posterior distributions in Fig.~\ref{fig: enhanced correct network error} still demonstrate a good estimation of the inferred source location.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[width=0.95\linewidth]{NN_case_good_design.png}
  \end{subfigure}%
  \caption{Posterior distributions of inferred source location $\{\theta_x,\theta_y\}$ based on the calibrated structural error using more informative data at stage 3. The blue triangular indicates the better design for structural error correction.}
  \label{fig: enhanced correct network error}
\end{figure}


\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.6\textwidth}
    \centering
    \includegraphics[width=0.95\linewidth]{combined_network_shape.png}
    \caption{Network performance}
  \label{Results of calibrating neural network}
  \end{subfigure}%
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \includegraphics[width=0.76\linewidth]{KLD_of_parameter_of_NN_case.png}
    \caption{EKI results}
  \label{Results of EKI in learning structural error}
  \end{subfigure}
  \caption{Results of calibrating the structural error via the hybrid approach. Panel (a) presents the comparison of the true source term and the modeled ones. Panel (b) presents the ensemble-based approximated KLD for the model error calibration with good and poor designs.}
  \label{parameter Results of learning structural error}
\end{figure}

With the structural model errors calibrated by the good and poor designs, we further investigate the performance of the solution field. More specifically, we solve for the modeled systems and compare the solutions with the true solution field. To quantitatively assess the performance of the solution fields, we also employ the mean squared error (MSE) and relative error (RE) of a model solution field $\mathbf{u}$ against the true solution field $\mathbf{u}^\dagger$:
\begin{equation}
    \begin{aligned}
        \text{MSE}&=\frac{1}{N}\sum_{z_x,z_y} (\mathbf{u}(z_x,z_y) - \mathbf{u}^\dagger(z_x,z_y))^2,\\
        \text{RE}&=\frac{\sum_{z_x,z_y} |\mathbf{u}(z_x,z_y) - \mathbf{u}^\dagger(z_x,z_y)|}{\sum_{z_x,z_y}  |\mathbf{u}^\dagger(z_x,z_y)| },
    \end{aligned}
\end{equation}
where the $N_z$ represents the total number of discretization points in the solution field.


Figure~\ref{fig: predict field} and Table~\ref{tab: quantitative} present the comparison between the solutions from the modeled systems and the true one. Both the good design and the poor one lead to improvements in the agreement with the true solution field over the baseline model. However, the good design achieves a better agreement with the true field, as evidenced by the lower deviation in the contour of Fig.~\ref{fig: predict field} and the smaller MSE and RE values in Table~\ref{tab: quantitative}. This improvement confirms a more effective correction of model discrepancy, which also explains the improved performance of inferred source location in Fig.~\ref{fig: enhanced correct network error}. 


\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{predict_field_with_updated_nn.png}
    \caption{Predicted fields}
  \end{subfigure}%
  \vspace{10pt}
  \begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[width=0.7\linewidth]{design_comparison.png}
    \caption{Deviation between the predicted and true fields}
  \end{subfigure}%
  \caption{Solution fields of the true system and the modeled ones. Panel (a) shows a visual comparison of the solutions fields. Panel (b) presents the mismatches between the true solution field and the solution fields from three different modeled systems.}
  \label{fig: predict field}
\end{figure}

\begin{table}[H]
    \centering
    \caption{Mean squared error and relative error of the solution fields based on the modeled systems.}
    \begin{tabular}{c|c|c|c}
        \hline
        & Good design & Poor design & Baseline model \\ \hline
        MSE & 0.005 & 0.032 & 0.142 \\ \hline
        RE & 0.297 & 0.665 & 1.553 \\ \hline
    \end{tabular}
    \label{tab: quantitative}
\end{table}

%We conduct a more detailed examination of the training effectiveness of the network by comparing the performance of the trained neural network to the initial state in Fig. \ref{fig:network performance}. In this figure, the x-axis represents the distance between the measuring point and the source, which serves as the input for the neural networks and the forcing term. The y-axis represents the outputs. The exponential green dotted line represents the forcing term in the true model, the reciprocal orange dashed line indicates the epistemic forcing term, and the red and blue dot-dash line shows the forcing term corrected by the neural network in the epistemic model. The significant difference between the green dotted line and the orange dashed line highlights the distinct decay forms of an exponential source versus a reciprocal one.

%We show that we have successfully achieved our objective of enabling the neural network to capture the differences between these two types of sources. In the initial state of the neural network shown as the red dot-dash line, the untrained network can improve the epistemic model to a certain extent. After the seven-step learning process, the blue solid line in the grey area demonstrates that the trained network can capture the behavior of the central part of the source term, which is crucial for source inference. Since most optimized designs are close to the source center, input data and training focus more on this region. Although the network primarily learns the central part, it is already sufficient to assist the Bayesian process in making a reasonable inference of the parameter of interest. It is reasonable to assume that the neural network can better approach the desired state with additional learning steps or more advanced learning techniques. 

%\textcolor{red}{What are the input and output variables? Write them as xlabel and ylabel of your figure. Also, change "Exponent" to "True source function", "Reciprocal" to "Baseline model", "updated" to "trained". For the third and fourth lines in the legend, put "Baseline model" before "NN".}\\
%\textcolor{red}{How did you determine the grey area?} \\
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.58\linewidth]{combined network shape.png}
%    \caption{Comparison of the true source function and three models, including the baseline model that has a structural error, the baseline model with a randomly initialized neural network (NN), and the baseline model with the trained NN. The grey area marks the crucial region for Bayesian OED in the input domain of the neural network.}
%    \label{fig: combined network performance}
%\end{figure}

%\subsection{Hybrid approach on structural errors with traditional solvers}\label{sec: node}

%In Section~\ref{sec:Correct functional error}, an auto-differentiable solver based on JAX-CFD~\cite{} is developed to demonstrate the performance of the proposed hybrid framework for efficient sequential Bayesian OED with structural errors. It is worth noting that the proposed framework can also work with traditional numerical solvers which are not auto-differentiable if Jacobian matrix $\partial \mathcal{G} / \partial \mathbf{u}$ is available. For the same numerical example studied in Section~\ref{sec:Correct functional error}, the Jacobian matrix of physics-based model $\mathcal{G}$ can be explicitly derived, and more details of the derivation are presented in~\ref{sec:Jacobian_matrix}.

%With the Jacobian matrix being calculated, we can evaluate $\mathrm{d} \text{Obj} / \mathrm{d} \boldsymbol{\theta}_\text{NN}$ based on the adjoint method described in Section \ref{sec: node}. More specifically, we present the results with two different methods of calculating the Jacobian matrix, including (i) a second-order finite difference scheme, and (ii) numerical differentiation based on the results of the traditional solver. The results of these two methods are compared with the ones of differentiable programming in Fig.~\ref{fig:gradient_comparison}. It can be seen that the results of the calculated gradient from the adjoint methods only show slight differences compared to the results from differentiable programming.

%\textcolor{red}{Change "manual derive" to "numerical differentiation". Change "auto differentiation method" to "Differentiable programming". Capitalize the first letter of each line.}\\
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.5\linewidth]{gradient.png}
%    \caption{Results of gradient $\mathrm{d} \text{Obj} / \mathrm{d} \boldsymbol{\theta}_\text{NN}$ from adjoint methods and differentiable programming.}
%    \label{fig:gradient_comparison}
%\end{figure}

%We further perform a seven-step sequential Bayesian OED with structural error correction, based on the gradient $\mathrm{d} \text{Obj} / \mathrm{d} \boldsymbol{\theta}_\text{NN}$ calculated by an adjoint method. The results are presented in Fig.~\ref{fig:posterior_adjoint_method}. Compared to the results from differentiable programming in Fig.~\ref{fig:posterior_DP_3}, the results from the adjoint method show comparable performance of inferring the true source location, even though the posterior distribution still has a small shift from the true source location at the $7$-th stage, which is mainly attributed to the small differences in the approximated Jacobian matrix in Fig.~\ref{fig:gradient_comparison}. Nevertheless, the results from the adjoint method in Fig.~\ref{fig:posterior_adjoint_method} show a clear improvement compared to the results in Fig.~\ref{fig:posterior_DP_3}, where the neural-network-based correction term is randomly initialized and not optimized, which confirms the effectiveness of the proposed hybrid framework when applied with traditional solvers.

%\textcolor{red}{The result here is based on finite differencing or numerical differentiation?}\\
%\begin{figure}[H]
%  \begin{subfigure}[b]{1\textwidth}
%    \centering
%    \includegraphics[width=0.8\linewidth]{less fig adjoint method oed.png}
%    \caption*{}
%  \end{subfigure}%
%  \vspace{-10pt}
%  \caption{Posterior of source location estimated by the hybrid approach applied to a traditional solver with structural errors.}
%  \label{fig:posterior_adjoint_method}
%\end{figure}

\section{Discussions}
\label{sec:discussions}

This work adopts a Bayesian framework rooted in information theory while aligning more closely with traditional experimental design due to the reliance on measured data. Specifically, we conduct preliminary experiments with several proposed designs, evaluate the information gained from each, and use this information to optimize for an improved design. While the likelihood and posterior updates rely on an inaccurate model, the results presented in Section~\ref{sec: Numerical Results} demonstrate that this approach yields highly informative designs with respect to effectively enabling model correction despite discrepancies. The optimization problem can be practically addressed by proposing several candidate designs and selecting or interpolating the most informative one. Alternatively, it can be solved using either gradient-based methods or derivative-free optimization techniques. It is worth noting that the numerical results of Section~\ref{sec: Numerical Results} only employ the measurement data of the optimal design point for model error correction, instead of the measurement data on a trajectory of points that lead to the optimal design point. If the whole trajectory of measurement data is used for model error correction, the performance of the subsequent posterior distribution estimation for the physics-based model parameters could be slightly worse, which seems to be counter-intuitive and is mainly due to the direct contribution of the optimal design point in the subsequent posterior distribution estimation. This accounts for the main reason that having a better match of measurement data on the optimal design point is more favorable than an overall good match of measurement data on the whole trajectory of searching the optimal design point.
%The decision not to use all preliminary data for belief updates stems from the potential risks posed by model discrepancy. Frequent updates using an inaccurate model can lead to beliefs deviating further from the truth. To mitigate this, only the most informative data is selected and used for a single belief update, ensuring a more reliable adjustment while minimizing the impact of model errors.

In the numerical results of Section~\ref{sec: Numerical Results}, we mainly focused on the scenario that allows some experimental measurements during the iterative stages of sequential experimental design. We have also tested the scenario that solely depends on the numerical model simulations, and the comparisons indicate that both predicted and measured data are effective within our framework. The use of simulation data yields less satisfactory performances and demands further technical refinement. The reason for this performance gap is that the EIG of a given design can differ significantly from the actual information gain it provides. When the model is correct, this difference is primarily determined by the accuracy of the prior information. However, in the presence of model discrepancy, errors in the likelihood function further amplify this difference, leading to a scenario where the design identified based on EIG may yield an incorrect information gain. The biased likelihood function is utilized twice in the computation of basic version EIG: (i) to calculate the predicted distribution of measurements, and (ii) to update the posterior distribution after obtaining a specific prediction. By using real measurements, the first step can be bypassed, thereby mitigating the impact of model inaccuracies. Consequently, the computed information gain represents a more reliable metric of information provided by the data. To demonstrate the strength of the hybrid correction framework, we focus on using measurement data in our design process and leave the scenario that solely depends on numerical model simulations to approximate EIG for future work.

\section{Conclusion}\label{Conclusion}
In this work, we propose an efficient hybrid approach to actively learning the model discrepancy of a digital twin based on the data from the sequential BED. We first validate the performance of the proposed method in a classical numerical example governed by a convection-diffusion equation, for which full BED is still feasible. The results confirm that the proposed method can effectively calibrate the low-dimensional parametric model error based on the optimal designs from the BED. The proposed method is then further studied in the same numerical example with a high-dimensional structural model error, for which full BED is not practical anymore. We observe that most of the optimal designs from the BED focus on the low-dimensional physics-based parameters can still be informative for calibrating the model discrepancy, while some of those designs could gradually lead to a larger model discrepancy. Therefore, we propose an ensemble-based approximation of information gain to assess the data informativeness and to enhance the active learning model discrepancy. The results show that the proposed method is efficient and robust to active learning of the high-dimensional model discrepancy. With the promising results of the hybrid approach demonstrated in this work, some future directions are summarized below:
\begin{itemize}
    \item Using the ensemble-based approximation of information gain to identify the optimal designs and informative data that are directly applicable to the calibration of model discrepancy.
    \item Extending the ensemble-based approximation of information gain to handle non-Gaussian prior and posterior distributions.
\end{itemize}

\section*{Acknowledgments}
H.Y., C.C., and J.W. are supported by the University of Wisconsin-Madison, Office of the Vice Chancellor for Research and Graduate Education with funding from the Wisconsin Alumni Research Foundation.


\section*{Data Availability}
The data that support the findings of this study are available from the corresponding author upon reasonable request.
  
\bibliographystyle{unsrt}
\bibliography{references}

\clearpage
\appendix
\section{Algorithm}

A detailed algorithm for sequential BED with model discrepancy
correction and ensemble-based indicator has been summarized in Algorithm~\ref{alg:ActiveLearning}.

\label{sec:algorithm}
\begin{algorithm}[H]
\caption{Active Learning of Model Discrepancy with BED}
\label{alg:ActiveLearning}
\begin{algorithmic}
    \For {$i=1,2,..., N$} \Comment{Iterating stage}
        \State $\mathbf{d} \gets \argmax_{\mathbf{d} \in \mathcal{D}}\mathbb{E}[U(\boldsymbol{\theta}_{\mathcal{G}},\mathbf{y},\mathbf{d}; \boldsymbol{\theta}_{\textrm{NN}})]$ \Comment{$\boldsymbol{\theta}_{\textrm{NN}}$ is fixed}
        \State $\mathbf{y} \gets \mathbf{u}(\mathbf{d}) + \boldsymbol{\epsilon}$ \Comment{Measurement at optimal design} 
        \State $\boldsymbol\theta_\mathcal{G} \gets \boldsymbol\theta_\mathcal{G}|\mathbf{y}$ \Comment{Bayesian update}
        \State $\boldsymbol\theta_\mathcal{G}^* \gets \argmax_{\boldsymbol\theta_\mathcal{G}} p(\boldsymbol{\theta}_\mathcal{G};\boldsymbol\theta_\text{NN})
        $ \Comment{MAP}
        \State $D_{\textrm{KL}}(p(\boldsymbol{\theta}_{\textrm{NN}}|\mathbf{y}) || p(\boldsymbol{\theta}_{\textrm{NN}}) ) \gets$ \text{EKI} \Comment{Ensemble-based Indicator}
        \If { $D_{\textrm{KL}}$ is large } \Comment{If $\mathbf{y}$ is informative to $\boldsymbol\theta_\text{NN}$}
            \State $\boldsymbol\theta_\text{NN} \gets \argmax_{\boldsymbol\theta_\text{NN}} L(\boldsymbol\theta_\text{NN};\boldsymbol\theta_\mathcal{G}^*,\mathbf{y},\mathbf{d})$ \Comment{$\boldsymbol{\theta}_{\mathcal{G}}^*$ is fixed}
        \EndIf

    \EndFor
    
\end{algorithmic}
\end{algorithm}


\section{Theoretical global maximum}
\label{appendx proof for obj in model correction}

The model discrepancy $\mathcal{G}^\dagger - \mathcal{G}$  which is characterized by a neural network $\textrm{NN}(\mathbf{u}; \boldsymbol{\theta}_{\textrm{NN}})$ in Eq.~\eqref{eq:modeled_system} can be calibrated by solving the optimization problem in Eq.~\eqref{eq:optimal_theta_NN} where the objective function $L(\boldsymbol\theta_\text{NN};\ \boldsymbol\theta_\mathcal{G}^*,\mathbf{y},\mathbf{d})$ is defined as $p(\mathbf{y}|\boldsymbol\theta_\mathcal{G}^*,\mathbf{d};\boldsymbol\theta_\text{NN})$ which is the likelihood function of $\boldsymbol{\theta}_{\textrm{NN}}$. The solution obtained by maximizing the likelihood function is the optimal solution for the modeled system to approximate the true system. The universal approximation property of neural networks guarantees the existence of a solution to this equation. We further demonstrate why this objective function leads to the solution.

The likelihood function is written as follows: 
\begin{equation}
\begin{aligned}
    p(\mathbf{y} |\boldsymbol\theta^*_{\mathcal{G}},\mathbf{d};\boldsymbol\theta_\text{NN}) = \frac{1}{\sqrt{|2\pi \boldsymbol{\Sigma}_{\boldsymbol{\epsilon}}|}}\exp\left(-\frac{1}{2}  \boldsymbol{\Sigma}_{\boldsymbol{\epsilon}}^{-1}\left\Vert\mathbf{y} - \mathbf{u}(\mathbf{d};\boldsymbol\theta^*_{\mathcal{G}},\boldsymbol\theta_\text{NN}) \right\Vert^2_2 \right) 
\end{aligned}
\end{equation}
where $\mathbf{y}=\mathbf{u}^\dagger(\mathbf{d};\boldsymbol\theta^\dagger)+\boldsymbol{\epsilon}$ and $\mathbf{u}^\dagger$ indicates the true field from system.

Maximizing this likelihood function is equivalent to minimizing the negative log-likelihood, which can further be reformulated as minimizing the weighted mean squared error between $\mathbf{y}$ and $\mathbf{u}(\mathbf{d}; \boldsymbol{\theta}_\mathcal{G}^*, \boldsymbol{\theta}_\text{NN})$. Given sufficient data:
\begin{itemize}
    \item The measurement error $\boldsymbol{\epsilon}$ can be averaged out to enable the predictive field $\mathbf{y}$ to approximate the true field $\mathbf{u}^\dagger(\mathbf{d}; \boldsymbol{\theta}^\dagger)$;
    \item The pointwise alignment between $\mathbf{y}$ and $\mathbf{u}(\mathbf{d}; \boldsymbol{\theta}_\mathcal{G}^*, \boldsymbol{\theta}_\text{NN})$ ensures that the true field $\mathbf{u}^\dagger$ aligns with the simulated field $\mathbf{u}$.
\end{itemize}

This alignment at the field level further ensures the dynamic consistency between the true system $\mathcal{G}^\dagger$ and the modeled system $\mathcal{G} + \textrm{NN}$:
\[
\mathcal{G}^\dagger(\mathbf{u}; \boldsymbol{\theta}^\dagger) = \mathcal{G}(\mathbf{u}; \boldsymbol{\theta}_\mathcal{G}^*) + \textrm{NN}(\mathbf{u}; \boldsymbol{\theta}_\text{NN}).
\]

With $\boldsymbol{\theta}_\mathcal{G}^*$ approximating $\boldsymbol{\theta}_\mathcal{G}^\dagger$, a result progressively provided by Bayesian optimal experimental design (OED)\cite{lindley_bayesian_1972,sebastiani_maximum_2000}, the above alignment equation can represent the equivalence between Eq.~\eqref{eq:true_system} and Eq.~\eqref{eq:modeled_system}. The universal approximation property of neural networks guarantees the existence of a solution, which corresponds to the optimized $\boldsymbol{\theta}_{\text{NN}}^*$.

However, in the settings of traditional BED, $\boldsymbol{\theta}_{\text{NN}}^*$ can not fully capture the model discrepancy $\mathcal{G}^\dagger - \mathcal{G}$ when $\boldsymbol{\theta}_\mathcal{G}^*$ does not approximate $\boldsymbol{\theta}_\mathcal{G}^\dagger$. Through the method proposed in this paper including the iterative process of BED and model correction, both $\boldsymbol{\theta}_\mathcal{G}^*$ and $\boldsymbol{\theta}_{\text{NN}}^*$ can progressively converge to their respective true values.


Therefore, the choice of likelihood function as the objective to train the modeled system can capture the model discrepancy under the following conditions:  
\begin{itemize}
    \item Multiple experiments can average measure error $\boldsymbol{\epsilon}$ out;
    \item $\boldsymbol\theta_\mathcal{G}^\dagger$ can be approximated by $\boldsymbol\theta^*_\mathcal{G}$ from the posterior of $\Theta_\text{OED}$ by BED;
    \item The alignment of the fields ensures that the modeled system $\mathcal{G}+\text{NN}$ captures the dynamics of the true system $\mathcal{G}^\dagger$.
\end{itemize}




\section{Detailed derivation of the Jacobian matrix}
\label{sec:Jacobian_matrix}

We follow the PDE in Eq. \eqref{eq: c sourse} but assume the PDE solver is not auto-differentiable. To facilitate the adjoint method for the calculation of $\mathrm{d} \text{Obj}/\mathrm{d} \boldsymbol{\theta}_\text{NN}$, we first compute the Jacobian matrix $\partial (\mathcal{G}+\textrm{NN})/\partial \mathbf{u}$ using the functional derivative method \cite{courant_methods_1954}\cite{han_theoretical_2009} from the analytical PDE. In addition, we introduce other potential methods for obtaining the Jacobian matrix at the end of this section and present the results of the adjoint method using these different Jacobians in the next section.

The scalar field of model state $\mathbf{u} \in \mathbb{R}^{N_\mathbf{u} \times N_\mathbf{u}}$ in the PDE Eq. \eqref{eq: c sourse} is represented as a flattened vector $\mathbf{u} \in \mathbb{R}^{N_\mathbf{u}^2}$ and its corresponding Jacobian matrix is of shape $N_\mathbf{u}^2 \times N_\mathbf{u}^2$. We start by considering the $i$th element of the flattened $\mathbf{u}$:
\begin{equation}
    \frac{\partial \mathbf{u}_i(\mathbf{z};\boldsymbol\theta)}{\partial t}=\mathcal{G}_i(\mathbf{u};\boldsymbol\theta)+\textrm{NN}_i(\mathbf{z};\boldsymbol\theta).
\end{equation}

The functional derivative of $\mathcal{G}_i+\textrm{NN}_i$ with respect to $\mathbf{u}$ is

\begin{equation}
    \begin{aligned}
    \sum_{\mathbf{z} \in \mathcal{Z}} \frac{\delta (\mathcal{G}_{i}+\textrm{NN}_i)}{\delta \mathbf{u}}(\mathbf{z})\eta(\mathbf{z}) &= \left. \frac{\mathrm{d}\mathcal{G}(\mathbf{u} + h\eta)_{i}}{\mathrm{d}h} \right|_{h=0}+\underset{\mathclap{=0~\text{(the network is not explicit function of $\mathbf{u})$}}}{\underbrace{\left. \frac{\mathrm{d}\textrm{NN}(\mathbf{u} + h\eta)_{i}}{\mathrm{d}h} \right|_{h=0}}}\\
    &= \left. \frac{\mathrm{d}}{\mathrm{d}h} \left\{ \nabla^2(\mathbf{u} + h\eta(\mathbf{z})) -v(t)\cdot\nabla (\mathbf{u} + h\eta(\mathbf{z})) \right\}_{i} \right|_{h=0} \\&~~~~\text{(the forcing terms are not explicit function of $\mathbf{u})$}\\
    &= \left.\left\{\nabla^2 \eta(\mathbf{z}) -v(t)\cdot\nabla \eta(\mathbf{z})\right\}_{i}\right|_{h=0} \\
    &= \left\{\left[\nabla^2  -v(t)\cdot\nabla \right]\eta(\mathbf{z})\right\}_{i} \\
    &=\sum_{\mathbf{z} \in \mathbf{Z}}e_i^T\cdot\left[\nabla^2  -v(t)\cdot\nabla \right]\eta(\mathbf{z}).
    \end{aligned}\label{eq:functional derivative defination}
\end{equation}
where $e_i$ represents the standard basis vector in the $i$-th direction, used to extract the $i$-th component of the vector field. 

It should be noted that in Eq. \eqref{eq: c sourse}, network term $\textrm{NN}$ is not a function of model state $\mathbf{u}$ so we directly cancel the term $ \mathrm{d}\mathcal{G}(\mathbf{u} + h\eta)_{i}/\mathrm{d}h$ in Eq. \eqref{eq:functional derivative defination}. If in other cases the network term is a function of model state $\mathbf{u}$, this term can be easily obtained by the auto-differentiability of neural networks.

The functional derivative result  $\frac{\delta \mathcal{G}_i+\textrm{NN}_i}{\delta \mathbf{u}}(z)=e_i^T\cdot\left[\nabla^2  -v(t)\cdot\nabla \right]$ forms the $i$-th row of the Jacobian matrix.  The specific value for discretized numerical tests depends on the employed finite difference form. 

Considering the 2D central difference of $\nabla^2_h$ ($h$ denotes the discretized form) on the flatten scalar field $\mathbf{u}\in \mathbb{R}^{N_\mathbf{u}^2}$, the $i$-th element can be expressed as:
\begin{equation}
    (\nabla^2_h \mathbf{u})_{i} \approx \frac{1}{h^2}(u_{(i+N_\mathbf{u})}+u_{(i-N_\mathbf{u})}+u_{(i+1)}+u_{(i-1)}-4u_{i})=\frac{1}{h^2}\mathbf{A}_i \cdot \mathbf{u},
\end{equation}
where $\mathbf{A}_i \in \mathbb{R}^{N_\mathbf{u}^2}$ is

\begin{equation}
    (\mathbf{A}_i)_j=\left\{\begin{array}{rl}
         1,&  j=i\pm1,i\pm N_\mathbf{u})\\
         -4,& j=i\\
         0,& \text{otherwise}
    \end{array}\right.
\end{equation}

Following this formula, the whole Jacobian matrix $\mathbf{A}$ of Laplace operator $\nabla^2_h$ can be expressed as:
\begin{equation}
    \mathbf{A} =\frac{1}{h^2} \left[\begin{matrix}
        \mathbf{T} & \mathbf{I}& \mathbf{0} & \cdots & \mathbf{0}\\
        \mathbf{I} & \mathbf{T}& \mathbf{I} & \cdots & \mathbf{0}\\
        \mathbf{0} & \mathbf{I}& \mathbf{T} & \cdots & \mathbf{0}\\
        \vdots & \vdots& \vdots & \ddots & \mathbf{I}\\
        \mathbf{0}& \mathbf{0}& \mathbf{0} & \mathbf{I} & \mathbf{T}
    \end{matrix}\right]_{N_\mathbf{u}^2\times N_\mathbf{u}^2},
\end{equation}
where $\mathbf{A} \in \mathbb{R}^{N_\mathbf{u}^2\times N_\mathbf{u}^2}$ is composed of $N_\mathbf{u} \times N_\mathbf{u}$ blocks, specifically $\mathbf{I}$, $\mathbf{0}$, and $\mathbf{T}$, all of which are matrices in $\mathbb{R}^{N_\mathbf{u} \times N_\mathbf{u}}$. In detail, $\mathbf{I}$ is the $N_\mathbf{u}\times N_\mathbf{u}$ identity matrix, $\mathbf{0}$ is the $N_\mathbf{u}\times N_\mathbf{u}$ zero matrix, and $\mathbf{T}$ is
\begin{equation}
    \mathbf{T}=\left[\begin{matrix}
        -4&1&0&\cdots&0\\
        1&-4&1&\cdots&0\\
        0&1&-4&\cdots&0\\
        \vdots&\vdots&\vdots&\ddots&1\\
        0&0&0&1&-4\\
    \end{matrix}
    \right]_{N_\mathbf{u}\times N_\mathbf{u}}.
\end{equation}

The above difference scheme does not include specific modifications at the boundaries. Any necessary adjustments to the boundary points depend on the specific configuration of the computational case and the boundary conditions being applied.

For the same reason, the Jacobian matrix $\mathbf{B}$ of advection term $v(t)\cdot \nabla$ in 2D central difference form is
\begin{equation}
    \mathbf{B} =\frac{1}{2h} \left[\begin{matrix}
        \mathbf{X} & \mathbf{Y}& \mathbf{0} & \cdots & \mathbf{0}\\
        -\mathbf{Y} & \mathbf{X}& \mathbf{Y} & \cdots & \mathbf{0}\\
        \mathbf{0} & -\mathbf{Y}& \mathbf{X} & \cdots & \mathbf{0}\\
        \vdots & \vdots& \vdots & \ddots & \mathbf{Y}\\
        \mathbf{0}& \mathbf{0}& \mathbf{0} & -\mathbf{Y} & \mathbf{X}
    \end{matrix}\right]_{N_\mathbf{u}^2\times N_\mathbf{u}^2},
\end{equation}
where $\mathbf{B} \in \mathbb{R}^{N_\mathbf{u}^2\times N_\mathbf{u}^2}$ is composed of $N_\mathbf{u} \times N_\mathbf{u}$ blocks, specifically $\mathbf{X}$, $\mathbf{Y}$, and $\mathbf{0}$, all of which are matrices in $\mathbb{R}^{N_\mathbf{u} \times N_\mathbf{u}}$. In detail:
\begin{equation}
     \mathbf{X}=v_x\left[\begin{matrix}
        0&1&0&\cdots&0\\
        -1&0&1&\cdots&0\\
        0&-1&0&\cdots&0\\
        \vdots&\vdots&\vdots&\ddots&1\\
        0&0&0&-1&0\\
    \end{matrix}
    \right]_{N_\mathbf{u}\times N_\mathbf{u}},  \quad  \mathbf{Y}=v_y\mathbf{I},\quad v(t)=(v_x,v_y) .
\end{equation}

The current form is derived under the numerical case where $v(t)$ is spatially independent as discussed in this paper. More complex form of the Jacobian matrix if $v(t)$ depends on spatial position could refer to any numerical method textbook.

The whole desired Jacobian matrix for Eq.~\eqref{eq:true_system_example} equals to $\mathbf{A}-\mathbf{B}$. Higher-order difference forms primarily alter the values of specific elements and those in their immediate vicinity. However, the sum of these vectors should invariably equal one. The specific finite difference scheme to pick should align with the forms adopted by the PDE solver.

%If higher-order difference schemes exhibit non-linear properties, such as the van-Leer scheme, directly extracting the Jacobian matrix from the solver could be an alternative method. However, if the solver is difficult to access or modify, the last resort is to calculate the gradient directly using numerical differentiation on the results of the PDE solver.

\end{document}