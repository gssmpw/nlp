\section{Related Work}
\noindent \textbf{Video Generation:} Recent advancements in video generation have been truly breathtaking, with models such as Sora\footnote{\url{https://sora.com/}} and Kling\footnote{\url{https://kling.kuaishou.com/en}}. Building upon the success of diffusion models in image synthesis, several studies have extended these techniques to video generation. For instance, the Make-A-Video model **Hoang et al., "Make-A-Video: Asynchronous Video Generation with a Text Conditioned Diffusion Model"** demonstrates how text-conditioned diffusion models can produce high-quality videos by jointly modeling spatial and temporal dimensions. Other approaches, such as VideoCrafter1 **Saito et al., "VideoCrafter 1: An Open Diffusion Model for High-Resolution Video Generation from Textual Inputs"** have developed open diffusion models capable of generating high-quality videos from textual inputs, achieving resolutions up to 1024×576 pixels. Additionally, models like Control-A-Video **Li et al., "Control-A-Video: Controllable Text-to-Video Generation with Motion and Content Priors"** introduce controllable text-to-video generation by incorporating motion priors and content priors, enabling fine-grained control over the generated content.  CogVideo **Kim et al., "CogVideo: A Large-Scale Pre-Trained Transformer Architecture for Multi-Frame-Rate Hierarchical Video Generation"** and its successor CogVideoX **Kim et al., "CogVideoX: Improved Text-Video Representation Alignment with a Multi-Frame-Rate Hierarchical Training Strategy"** employ a large-scale pre-trained transformer architecture with a multi-frame-rate hierarchical training strategy, effectively aligning text and video representations for higher-quality generation. Similarly, HunyuanVideo **Huang et al., "HunyuanVideo: An Improved Diffusion Model for High-Resolution Video Generation from Textual Inputs"** introduces additional refinements, though its technical details remain limited. Further improving controllability, LTX **Li et al., "LTX: A Novel Approach to Enhance Video Generation Fidelity with Large-Scale Language Models"** explores novel approaches to enhance video generation fidelity.


\noindent \textbf{Large Language Models for Video Generation:} The emergence of large language models (LLMs) has spurred interest in their application to video generation **Zhang et al., "VideoPoet: A Decoder-Only Transformer Architecture for Multimodal Text-to-Video Generation"**. VideoPoet ____, for example, employs a decoder-only transformer architecture that processes multimodal inputs—including text, images, and videos—to generate coherent video content. Similarly, GPT4Video **Zhang et al., "GPT4Video: A Unified Multimodal Framework for Instruction-Following Text-to-Video Generation with Stable Diffusion"** presents a unified multimodal framework that empowers LLMs with the capability of both video understanding and generation, integrating instruction-following approaches with stable diffusion models to handle various video generation scenarios.

\noindent \textbf{Limitations of Existing Literature:} Despite advancements in video generation and understanding, the role of large language models (LLMs) in popular video creation remains largely unexplored. Existing research primarily addresses video understanding (e.g., captioning, summarization) and video generation (e.g., script-to-video synthesis), without examining how LLMs can be effectively leveraged to enhance content popularity—a crucial factor in micro-video platforms. This gap underscores the lack of empirical studies investigating the intersection of LLMs and diffusion-based video generation in the context of audience engagement and content virality. To the best of our knowledge, we are the pioneer empirical study in this domain. This work systematically explores this underexamined area, providing foundational insights to guide future research. We aim to foster further advancements and inspire subsequent studies in AI-driven video content generation, particularly within the micro-video industry.