% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib")
@inproceedings{yuan2023go,
  title={Where to go next for recommender systems? id-vs. modality-based recommender models revisited},
  author={Yuan, Zheng and Yuan, Fajie and Song, Yu and Li, Youhua and Fu, Junchen and Yang, Fei and Pan, Yunzhu and Ni, Yongxin},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2639--2649},
  year={2023}
}
@inproceedings{fu2024exploring,
  title={Exploring adapter-based transfer learning for recommender systems: Empirical studies and practical insights},
  author={Fu, Junchen and Yuan, Fajie and Song, Yu and Yuan, Zheng and Cheng, Mingyue and Cheng, Shenghui and Zhang, Jiaqi and Wang, Jie and Pan, Yunzhu},
  booktitle={Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
  pages={208--217},
  year={2024}
}
@inproceedings{fu2024iisan,
  title={IISAN: Efficiently adapting multimodal representation for sequential recommendation with decoupled PEFT},
  author={Fu, Junchen and Ge, Xuri and Xin, Xin and Karatzoglou, Alexandros and Arapakis, Ioannis and Wang, Jie and Jose, Joemon M},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={687--697},
  year={2024}
}
@article{chivileva2024dataset,
  title={A dataset of text prompts, videos and video quality metrics from generative text-to-video AI models},
  author={Chivileva, Iya and Lynch, Philip and Ward, Tom{\'a}s E and Smeaton, Alan F},
  journal={Data in Brief},
  volume={54},
  pages={110514},
  year={2024},
  publisher={Elsevier}
}
@inproceedings{jiang2023text2performer,
  title={Text2performer: Text-driven human video generation},
  author={Jiang, Yuming and Yang, Shuai and Koh, Tong Liang and Wu, Wayne and Loy, Chen Change and Liu, Ziwei},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={22747--22757},
  year={2023}
}
@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948
        
        
        
        
        
        },
  year={2025}
}
@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115
        
        
        
        
        
        },
  year={2024}
}
@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437
        
        
        
        
        
        },
  year={2024}
}
@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276
        
        
        
        
        
        },
  year={2024}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971
        
        
        
        
        
        
        
        },
  year={2023}
}
@article{lu2024mufm,
  title={MUFM: A Mamba-Enhanced Feedback Model for Micro Video Popularity Prediction},
  author={Lu, Jiacheng and Xiao, Mingyuan and Wang, Weijian and Du, Yuxin and Cui, Yi and Zhao, Jingnan and Hua, Cheng},
  journal={arXiv preprint arXiv:2411.15455
        
        
        
        
        
        
        
        },
  year={2024}
}
@article{cho2024amps,
  title={AMPS: Predicting popularity of short-form videos using multi-modal attention mechanisms in social media marketing environments},
  author={Cho, Minhwa and Jeong, Dahye and Park, Eunil},
  journal={Journal of Retailing and Consumer Services},
  volume={78},
  pages={103778},
  year={2024},
  publisher={Elsevier}
}
@article{zhou2024survey,
  title={A survey on generative ai and llm for video generation, understanding, and streaming},
  author={Zhou, Pengyuan and Wang, Lin and Liu, Zhi and Hao, Yanbin and Hui, Pan and Tarkoma, Sasu and Kangasharju, Jussi},
  journal={arXiv preprint arXiv:2404.16038
        
        
        
        
        
        
        
        },
  year={2024}
}
@article{zhang2024interactivevideo,
  title={InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions},
  author={Zhang, Yiyuan and Kang, Yuhao and Zhang, Zhixin and Ding, Xiaohan and Zhao, Sanyuan and Yue, Xiangyu},
  journal={arXiv preprint arXiv:2402.03040
        
        
        
        
        
        
        
        },
  year={2024}
}
@article{bianchi2024well,
  title={How well can llms negotiate? negotiationarena platform and analysis},
  author={Bianchi, Federico and Chia, Patrick John and Yuksekgonul, Mert and Tagliabue, Jacopo and Jurafsky, Dan and Zou, James},
  journal={arXiv preprint arXiv:2402.05863
        
        
        
        
        
        },
  year={2024}
}
@article{zheng2024cheating,
  title={Cheating automatic llm benchmarks: Null models achieve high win rates},
  author={Zheng, Xiaosen and Pang, Tianyu and Du, Chao and Liu, Qian and Jiang, Jing and Lin, Min},
  journal={arXiv preprint arXiv:2410.07137
        
        
        
        
        
        
        
        
        
        },
  year={2024}
}
@article{gao2024bayesian,
  title={Bayesian calibration of win rate estimation with llm evaluators},
  author={Gao, Yicheng and Xu, Gonghan and Wang, Zhe and Cohan, Arman},
  journal={arXiv preprint arXiv:2411.04424
        
        
        
        },
  year={2024}
}
@article{liu2024sora,
  title={Sora: A review on background, technology, limitations, and opportunities of large vision models},
  author={Liu, Yixin and Zhang, Kai and Li, Yuan and Yan, Zhiling and Gao, Chujie and Chen, Ruoxi and Yuan, Zhengqing and Huang, Yue and Sun, Hanchi and Gao, Jianfeng and others},
  journal={arXiv preprint arXiv:2402.17177
        
        
        
        
        
        },
  year={2024}
}
@inproceedings{ran2022revenue,
  title={Revenue and User Traffic Maximization in Mobile Short-Video Advertising},
  author={Ran, Dezhi and Zheng, Weiqiang and Li, Yunqi and Bian, Kaigui and Zhang, Jie and Deng, Xiaotie},
  booktitle={AAMAS'22: Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages={1092--1100},
  year={2022},
  organization={Association for Computing Machinery}
}
@article{guo2024survey,
  title={A survey of micro-video analysis},
  author={Guo, Jie and Gong, Rui and Ma, Yuling and Liu, Meng and Xi, Xiaoming and Nie, Xiushan and Yin, Yilong},
  journal={Multimedia Tools and Applications},
  volume={83},
  number={11},
  pages={32191--32212},
  year={2024},
  publisher={Springer}
}
@article{ouyang2023llm,
  title={LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code Generation},
  author={Ouyang, Shuyin and Zhang, Jie M and Harman, Mark and Wang, Meng},
  journal={arXiv preprint arXiv:2308.02828
        
        
        
        
        
        },
  year={2023}
}
@article{cai2023low,
  title={Low-code llm: Visual programming over llms},
  author={Cai, Yuzhe and Mao, Shaoguang and Wu, Wenshan and Wang, Zehua and Liang, Yaobo and Ge, Tao and Wu, Chenfei and You, Wang and Song, Ting and Xia, Yan and others},
  journal={arXiv preprint arXiv:2304.08103
        
        
        
        },
  volume={2},
  year={2023},
  publisher={Apr}
}
@inproceedings{zhang2024mathverse,
  title={Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?},
  author={Zhang, Renrui and Jiang, Dongzhi and Zhang, Yichi and Lin, Haokun and Guo, Ziyu and Qiu, Pengshuo and Zhou, Aojun and Lu, Pan and Chang, Kai-Wei and Qiao, Yu and others},
  booktitle={European Conference on Computer Vision},
  pages={169--186},
  year={2024},
  organization={Springer}
}
@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300
        
        
        
        
        
        },
  year={2024}
}
@article{jin2024comprehensive,
  title={A comprehensive survey on process-oriented automatic text summarization with exploration of llm-based methods},
  author={Jin, Hanlei and Zhang, Yang and Meng, Dan and Wang, Jun and Tan, Jinghua},
  journal={arXiv preprint arXiv:2403.02901
        
        
        
        
        
        
        
        },
  year={2024}
}
@article{yang2024cogvideox,
  title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2024}
}
@article{hong2022cogvideo,
  title={CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers},
  author={Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},
  journal={arXiv preprint arXiv:2205.15868
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{HaCohen2024LTXVideo,
  title={LTX-Video: Realtime Video Latent Diffusion},
  author={HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and Panet, Poriya and Weissbuch, Sapir and Kulikov, Victor and Bitterman, Yaki and Melumian, Zeev and Bibi, Ofir},
  journal={arXiv preprint arXiv:2501.00103},
  year={2024}
}
@article{kong2024hunyuanvideo,
  title={Hunyuanvideo: A systematic framework for large video generative models},
  author={Kong, Weijie and Tian, Qi and Zhang, Zijian and Min, Rox and Dai, Zuozhuo and Zhou, Jin and Xiong, Jiangfeng and Li, Xin and Wu, Bo and Zhang, Jianwei and others},
  journal={arXiv preprint arXiv:2412.03603
        
        
        
        
        
        
        
        
        
        },
  year={2024}
}

@article{fu2024efficient,
  title={Efficient and Effective Adaptation of Multimodal Foundation Models in Sequential Recommendation},
  author={Fu, Junchen and Ge, Xuri and Xin, Xin and Karatzoglou, Alexandros and Arapakis, Ioannis and Zheng, Kaiwen and Ni, Yongxin and Jose, Joemon M},
  journal={arXiv preprint arXiv:2411.02992
        
        
        
        
        
        
        
        
        
        },
  year={2024}
}
@inproceedings{wang2024gpt4video,
  title={Gpt4video: A unified multimodal large language model for lnstruction-followed understanding and safety-aware generation},
  author={Wang, Zhanyu and Wang, Longyue and Zhao, Zhen and Wu, Minghao and Lyu, Chenyang and Li, Huayang and Cai, Deng and Zhou, Luping and Shi, Shuming and Tu, Zhaopeng},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={3907--3916},
  year={2024}
}
@article{maaz2023video,
  title={Video-chatgpt: Towards detailed video understanding via large vision and language models},
  author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
  journal={arXiv preprint arXiv:2306.05424
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}
@inproceedings{hong2023large,
  title={Large language models are frame-level directors for zero-shot text-to-video generation},
  author={Hong, Susung and Seo, Junyoung and Shin, Heeseong and Hong, Sunghwan and Kim, Seungryong},
  booktitle={First Workshop on Controllable Video Generation@ ICML24},
  year={2023}
}

@inproceedings{zhao2023learning,
  title={Learning video representations from large language models},
  author={Zhao, Yue and Misra, Ishan and Kr{\"a}henb{\"u}hl, Philipp and Girdhar, Rohit},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6586--6597},
  year={2023}
}
@article{kondratyuk2023videopoet,
  title={Videopoet: A large language model for zero-shot video generation},
  author={Kondratyuk, Dan and Yu, Lijun and Gu, Xiuye and Lezama, Jos{\'e} and Huang, Jonathan and Schindler, Grant and Hornung, Rachel and Birodkar, Vighnesh and Yan, Jimmy and Chiu, Ming-Chang and others},
  journal={arXiv preprint arXiv:2312.14125
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}
@article{chen2023control,
  title={Control-a-video: Controllable text-to-video generation with diffusion models},
  author={Chen, Weifeng and Ji, Yatai and Wu, Jie and Wu, Hefeng and Xie, Pan and Li, Jiashi and Xia, Xin and Xiao, Xuefeng and Lin, Liang},
  journal={arXiv preprint arXiv:2305.13840
        
        
        
        
        
        
        
        },
  year={2023}
}
@article{chen2023videocrafter1,
  title={Videocrafter1: Open diffusion models for high-quality video generation},
  author={Chen, Haoxin and Xia, Menghan and He, Yingqing and Zhang, Yong and Cun, Xiaodong and Yang, Shaoshu and Xing, Jinbo and Liu, Yaofang and Chen, Qifeng and Wang, Xintao and others},
  journal={arXiv preprint arXiv:2310.19512
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{singer2022make,
  title={Make-a-video: Text-to-video generation without text-video data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  journal={arXiv preprint arXiv:2209.14792
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@inproceedings{zhong2024predicting,
  title={Predicting Micro-video Popularity via Multi-modal Retrieval Augmentation},
  author={Zhong, Ting and Lang, Jian and Zhang, Yifan and Cheng, Zhangtao and Zhang, Kunpeng and Zhou, Fan},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2579--2583},
  year={2024}
}
@article{ni2023content,
  title={A content-driven micro-video recommendation dataset at scale},
  author={Ni, Yongxin and Cheng, Yu and Liu, Xiangyan and Fu, Junchen and Li, Youhua and He, Xiangnan and Zhang, Yongfeng and Yuan, Fajie},
  journal={arXiv preprint arXiv:2309.15379
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{wang2023generative,
  title={Generative recommendation: Towards next-generation recommender paradigm},
  author={Wang, Wenjie and Lin, Xinyu and Feng, Fuli and He, Xiangnan and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2304.03516
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243
        
        
        
        
        
        
        
        ",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
