% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.


\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs,multicol,multirow,authblk}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{colortbl}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}



\title{
    \makebox[1pt][l]{\includegraphics[width=0.4cm]{figs/popcorn_icon.png}} %
    \,\,\, LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation
}

\author{
  \textbf{Junchen Fu\textsuperscript{1}},
  \textbf{Xuri Ge\textsuperscript{2}}\thanks{Corresponding author.},
  \textbf{Kaiwen Zheng\textsuperscript{1}}, 
  \textbf{Ioannis Arapakis\textsuperscript{3}}, 
  \textbf{Xin Xin\textsuperscript{2}}, 
  \textbf{Joemon M. Jose\textsuperscript{1}} \\
  \textsuperscript{1}University of Glasgow, United Kingdom \\
  \textsuperscript{2}Shandong University, China \\
  \textsuperscript{3}Telefónica Scientific Research, Spain \\
\small \texttt{\{j.fu.3, k.zheng.1\}@research.gla.ac.uk}\\
\small \texttt{\{xuri.ge, xinxin\}@sdu.edu.cn} \\
\small \texttt{arapakis.ioannis@gmail.com}, \texttt{joemon.jose@glasgow.ac.uk} 
}

\begin{document}
\maketitle
\begin{abstract}
Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold significant commercial value. The rise of high-quality AI-generated content has spurred interest in AI-driven micro-video creation. However, despite the advanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek in text generation and reasoning, their potential to assist the creation of popular micro-videos remains largely unexplored.

In this paper, we conduct an empirical study on \underline{LLM}-assisted \underline{pop}ular mi\underline{c}ro-vide\underline{o} gene\underline{r}atio\underline{n} (LLMPopcorn\footnote{We selected popcorn as the icon for this paper because it symbolizes leisure and entertainment. This aligns with this study on leveraging LLMs as assistants for generating popular micro-videos, which are often consumed during leisure time.}). Specifically, we investigate the following research questions: \textit{(i) 
How can LLMs be effectively utilized to assist popular micro-video generation?
(ii) To what extent can prompt-based enhancements optimize the LLM-generated content for higher popularity?
(iii) How well do various LLMs and video generators perform in the popular micro-video generation task?
} By exploring these questions, we show that advanced LLMs like DeepSeek-V3 enable micro-video generation to achieve popularity comparable to human-created content. Prompt enhancements further boost popularity, and benchmarking highlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and HunyuanVideo lead in video generation. This pioneering work advances AI-assisted micro-video creation, uncovering new research opportunities. We will release the code and datasets to support future studies.
\end{abstract}





\section{Introduction}
Micro-videos  (or short videos)  have emerged as a fundamental element of the digital economy, representing a multi-billion-dollar industry \cite{guo2024survey}. They have become an integral part of daily life for people worldwide, providing substantial commercial value for social media platforms and content creators. Popular content creators can receive significant revenue through their content \cite{ran2022revenue}, which underscores the ever-growing influence of micro-videos in modern society.

Despite their widespread popularity and financial impact, producing popular micro-video content remains a costly and labor-intensive process. Professional filming, scripting, and editing require significant time and resources, which not all creators or businesses can afford. %Motivated by these challenges, we investigate AI-based solutions that can streamline micro-video creation. 
Driven by these challenges, we explore simplifying the micro-video creation process using AI solutions, with a particular focus on generating popular micro-videos.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/fig1.pdf}
    \caption{An overview of the LLMPopcorn pipeline. The numbered steps indicate the sequential implementation of the pipeline. Given a user query as input, the pipeline generates video titles and micro-videos, followed by an evaluation process.}
    \label{fig:pipeline}
\end{figure*}


On one hand, the rapid advancement of Large Language Models (LLMs) like ChatGPT has unlocked new possibilities for content generation, demonstrating strong capabilities in tasks such as document summarization \cite{jin2024comprehensive}, programming \cite{cai2023low,ouyang2023llm}, mathematical reasoning \cite{shao2024deepseekmath,zhang2024mathverse} and recommendation~\cite{yuan2023go,fu2024iisan,fu2024efficient,fu2024exploring}. On the other hand, video generation models powered by diffusion techniques and neural rendering are transforming creative content production, enabling high-quality synthesis for applications in film production \cite{liu2024sora} and interactive media \cite{zhang2024interactivevideo}.
Significant breakthroughs in LLMs and video generation models have prompted many studies \cite{zhou2024survey} to integrate LLM to facilitate the capability of automatic video generation. 
For instance, VideoLLMs like VideoPoet~\cite{kondratyuk2023videopoet} and GPT4Video~\cite{wang2024gpt4video} leverage LLMs and video generation models for multimodal video understanding and generation.




Although LLMs can assist in video generation, existing studies have neglected the ability of different LLMs to assist in the generation of ``\textbf{popular}'' micro-videos. The main reason for this is that existing studies focus on the quality of video generation, such as high resolution, long duration, etc., but neglect that popularity of video generation does not only depend on these. Recently, many studies \cite{cho2024amps,zhong2024predicting,lu2024mufm} have started to focus on and propose the latest micro-video popularity prediction models, which comprehensively integrate micro-video popularity factors. For instance, the Microlens dataset \citep{ni2023content}, which includes raw videos and popularity metrics, along with pre-trained short video popularity prediction models \citep{zhong2024predicting}, provides a strong basis for video popularity prediction.
However, how mainstream LLMs can be utilised to facilitate popular micro-video generation, especially in effectively measuring the high-popularity of generated videos, remains largely unexamined by the research community.


Driven by the above insights, we explore the research question of \textit{whether LLMs can assist in the \textbf{popular} micro-video creation.} To address this research question systematically, 
we propose and investigate three following Research Questions (RQs):

\begin{itemize} \item \textbf{RQ1:} \emph{ 
How can LLMs be effectively utilized to assist popular micro-video generation?
} We address this by developing LLMPopcorn, a comprehensive popular micro-video creation pipeline that leverages LLMs for generating prompts tailored to popular video production. To introduce video popularity assessment in the pipeline, we employ a state-of-the-art offline popularity evaluator to quantify the popularity of generated videos. We compare three mainstream LLMs in our LLMPopcorn pipeline and evaluate their performance alongside human-created videos.



\item \textbf{RQ2:} \emph{To what extent can prompt-based enhancements optimize the content generated by LLMs for achieving greater popularity?
}  
To address the question, we propose and explore a new \textbf{Prompt Enhancement} that leverages Retrieval-Augmented Generation (RAG) and chain-of-thought (CoT) prompting. This approach is inspired by the video creation process typical of human content creators.

\item \textbf{RQ3:} \emph{How do various LLMs and video generators perform on popular micro-video generation?} 
Five state-of-the-art LLMs and three video generation models under the LLMPopcorn pipeline fully compare the performance of different models.
\end{itemize}

By investigating these research questions, this study represents an initial exploration of this emerging domain, providing key insights into AI-assisted micro-video creation. 
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/fig2.pdf}
    \caption{An overview of the Prompt Enhancement (PE) process. PE enables the LLM reviewing relevant micro-videos from the database and engaging in chain-of-thought reasoning.}
    \label{fig:PE_process}
    \vspace{-0.1in}
\end{figure*}




\begin{table*}[htbp]
    \centering
    \small
    \renewcommand{\arraystretch}{0.1}
    \scalebox{0.9}{
    \begin{tabular}{p{2cm}p{6.2cm}p{6.2cm}}
        \toprule
        \textbf{Category} & \textbf{Abstract} & \textbf{Concrete} \\
        \midrule
        \textbf{Anime} &
        Summarize the top 5 trending anime this season with a brief analysis. \newline
        Create a video exploring how anime portrays futuristic cities. \newline
        Generate a video analyzing the symbolism in anime opening themes. &
        Create a video showcasing Goku’s most iconic transformations in Dragon Ball. \newline
        Animate a battle between Levi Ackerman and a Titan in Attack on Titan. \newline
        Make a montage of Naruto’s Rasengan evolution throughout the series. \\
        \midrule
        
        \textbf{Delicacy} &
        Create a video about the history and evolution of a popular street food. \newline
        Showcase 5 unique ways desserts are prepared across different cultures. \newline
        Compare how breakfast is eaten in various countries. &
        Generate a video showcasing the preparation of wagyu beef steak. \newline
        Create a video about assembling a colorful bento box. \newline
        Show the step-by-step process of making matcha tiramisu. \\
        \midrule
        
        \textbf{Daily Sharing} &
        Create a video showcasing life hacks that simplify daily routines. \newline
        Generate a video on organizing spaces for maximum efficiency. \newline
        Explore tips for creating a morning routine that boosts productivity. &
        Generate a video showing the most creative DIY desk setups for students. \newline
        Create a video of organizing a messy kitchen cabinet with clever hacks. \newline
        Highlight a daily skincare routine featuring Korean beauty products. \\
        \midrule

        \textbf{Film and TV} &
        Generate a video ranking iconic moments in cinematic history. \newline
        Explore how directors use symbolism in lighting and shadows. \newline
        Create a video comparing film adaptations of popular novels. &
        Generate a video ranking the top five Marvel Cinematic Universe villains. \newline
        Create a video comparing the different Batmen from movie history. \newline
        Highlight the most iconic scenes in The Lord of the Rings trilogy. \\
        \midrule

        \textbf{Game} &
        Create a video analyzing how games portray complex moral choices. \newline
        Generate a list of the most innovative game mechanics in recent years. \newline
        Discuss how multiplayer games foster social interactions. &
        Generate a video showing the top 10 sniper rifles in Call of Duty history. \newline
        Create a video ranking the most popular Minecraft mods. \newline
        Highlight the funniest glitches in Skyrim. \\
        \bottomrule
    \end{tabular}
    }
    % \vspace{-0.1in}
    \caption{User prompt examples for abstract and concrete video generation tasks. }
    \label{tab:dataset_abstract_concrete}
    \vspace{-0.2in}
\end{table*}



\section{The Proposed \textit{LLMPopcorn}}

\subsection{Problem Formulation}

We consider a problem in which a user provides an input prompt \(x \in \mathcal{X}\). An LLM processes this prompt and generates both a video title and a video prompt, which we denote by \(f(x) = (t, p)\), where \(t\) belongs to the set of titles \(\mathcal{T}\) and \(p\) belongs to the set of video prompts \(\mathcal{P}\). Next, a pre-trained video generation model \(g\) uses the generated prompt to produce an AI-created micro-video \(v \in \mathcal{V}\), that is, \(v = g(p)\). An offline video popularity predictor \(h\) then assigns a popularity score \(s \in \mathbb{R}\) to the generated video, so that \(s = h(v,t)\). Overall, the entire pipeline can be represented as the composite function: 
\[
F(x) = h\bigl(g(f(x))\bigr).
\]

The objective is to optimize the LLM function \(f\) (with \(g\) and \(h\) fixed) to maximize the expected popularity score of the generated videos. Formally, we wish to achieve:
\[
\max_{f \in \mathcal{F}} \; \mathbb{E}_{x \sim \mathcal{X}} \left[ h\bigl(g(f(x))\bigr) \right],
\]
where \(\mathcal{F}\) denotes the set of all possible configurations of the LLM. This formulation captures the aim of leveraging LLMs to enhance the creation of popular AI-generated micro-videos.

\subsection{Pipeline}
 Based on the problem formulation, we propose a new \textbf{LLMPopcorn} pipeline in Figure \ref{fig:pipeline} 
 %\textcolor{red}{wrong Fig reference - please check!}
 that begins by constructing user prompts, categorized into concrete and abstract prompts, which are constructed by rules into inputs for an LLM. We demonstrate the basic prompt template in Appendix Figure \ref{fig:BP_template}. The LLM serves as a video generation assistant, producing \textit{video titles} and \textit{video generation prompts} based on these inputs. These video generation prompts are then fed into a video generator to create corresponding videos. The generated videos are combined with their titles to form final short videos, which are subsequently evaluated using a pre-trained video popularity prediction model to assess their potential impact. This pipeline streamlines the process of AI-driven video generation and evaluation, enabling automatic popular micro-video content creation.

\subsection{Prompt Enhancement}
We demonstrate the Prompt Enhancement process in Figure \ref{fig:PE_process}, which enhances the input prompt for LLM inspired by human content creators.

Formally, let \(x \in \mathcal{X}\) be the user prompt (e.g., "Create a video showcasing Goku’s  most iconic transformations in Dragon Ball."). An embedding function \(f_{\text{embed}}: \mathcal{X} \to \mathbb{R}^d\) maps \(x\) to its vector representation:
\[
\mathbf{x} = f_{\text{embed}}(x).
\]
Given a predefined set of topic tags \(\mathcal{T}\) (e.g., Food, Anime, Movie) with corresponding embeddings, a matching algorithm based on cosine similarity is used to select the most relevant tags:
\[
\mathcal{T}_x = \operatorname{arg\,max}_{t \in \mathcal{T}} sim\bigl(\mathbf{x}, f_{\text{embed}}(t)\bigr),
\]
where \(sim(\cdot,\cdot)\) denotes the cosine similarity. For each selected tag \(t \in \mathcal{T}_x\), the system retrieves Top-K associated video samples from the test set of the Microlens dataset\footnote{Since MMRA is trained on the training set of the Microlens dataset, we exclusively use the test set to prevent data leakage.} and partitions them into positive examples \(S^+_t\) and negative examples \(S^-_t\) based on popularity. Finally, an enhancement function constructs the new prompt:
\[
x' = \operatorname{Enhance}\Bigl(x,\ \bigcup_{t \in \mathcal{T}_x} \{S^+_t,\, S^-_t\}\Bigr),
\]
which is then used to guide the video generation. The refined examples of the enhanced prompt templates are presented in Appendix Figure \ref{fig:PE_template}.


\section{Experiment Setup}
\subsection{Dataset}
\label{sec:dataset}
To enhance comprehensiveness, we designed two types of user prompts as inputs for the LLM: \textbf{Concrete} and \textbf{Abstract}. The user prompts are primarily derived from five key categories—Anime, Delicacy, Daily Sharing, Film \& Television, and Gaming. We reference the Microlens dataset \cite{ni2023content}, as it is the largest available micro-video dataset. As detailed in Table \ref{tab:dataset_abstract_concrete}, concrete prompts include specific elements that provide clear details, while abstract prompts describe videos in a more general and high-level manner. Specifically, we utilized ChatGPT-4o\footnote{\url{https://chatgpt.com/}} to generate two corresponding user prompt datasets, each containing \textit{one hundred} prompts\footnote{Due to the high cost of video generation, overall two hundred prompts are similar size to existing video generation datasets.\cite{jiang2023text2performer,chivileva2024dataset}}. These datasets are further divided into the aforementioned five categories, with each category comprising twenty prompts. This approach ensures a balanced and diverse set of prompts across all categories, catering to both specific and broad input styles for the LLM. 
%\textcolor{red}{How many chatgpt generated prompts in our database? I couldn't find this}

\subsection{Evaluation}




In this study, we evaluate the popularity of the generated videos using the state-of-the-art micro-video popularity prediction model, MMRA \cite{zhong2024predicting}. Following \citet{zhong2024predicting}, we define popularity as the number of comments. For robustness, we primarily use the median popularity score as the key metric. Following \citet{zhong2024predicting}, we define popularity as the number of comments and use the median for robustness. This metric is greater than 0, with higher values indicating that the model generates prompts leading to greater median engagement, suggesting a higher potential for videos to achieve high engagement on the platform.  In model-wise comparison sections such as Section \ref{sec:PE} and Section \ref{sec:Bench}, we primarily use Win-Rate to compare the performance of different models in predicting popularity. Specifically, given a user prompt, we generate micro-videos using both Model A and Model B and assess which model achieves higher popularity based on evaluation models.
This pairwise comparison approach is widely adopted in the existing literature \cite{zheng2024cheating,gao2024bayesian,bianchi2024well}. More experiment details are in Apendix \ref{sec:implementation}.



\section{LLM Pipeline for Popular Micro-videos Generation (RQ1)}
To ensure a comprehensive evaluation, we consider three large language models (LLMs)—Llama‐3.3‐70B~\cite{touvron2023llama}, ChatGPT‐4o~\cite{hurst2024gpt}, and DeepSeek‐V3~\cite{liu2024deepseek}—while setting CogVideoX~\cite{yang2024cogvideox} as the default video generation model. We utilize both concrete and abstract user prompt datasets as described in Section \ref{sec:dataset}. Additionally, to better contextualize the popularity values of videos generated by the LLMPopcorn pipeline, we compare them with human-created videos from the Microlens dataset \cite{ni2023content}. Since directly producing videos based on prompts would be prohibitively expensive, we approximate this comparison by matching each user prompt to the most semantically similar human-created video title using an embedding model (all-MiniLM-L12-v2). This approach mitigates the influence of the prompt's thematic bias on video popularity, allowing for a closer examination of how the popularity of LLMPopcorn-generated videos compares to real human-created content. 




As demonstrated in Table \ref{tab:popularity_medians}, compared to the Microlens human-created dataset, which achieves median popularity scores of 0.44 for concrete prompts and 0.71 for abstract prompts, the AI-generated content by LLMPopcorn demonstrates competitive performance, with certain models even surpassing human benchmarks using DeepSeek-V3.  Among the evaluated models, DeepSeek-V3 stands out as the most robust and stable performer, achieving the highest median scores of 0.56 for concrete prompts and 0.73 for abstract prompts, highlighting its capability to generate content with strong engagement potential. However, while these models achieve performance on par with human-created videos on average, their absolute values remain relatively low. We hypothesize that this may stem from the limitations of current video generation models
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{LLMPopcorn}            & \textbf{Concrete} & \textbf{Abstract} \\ \midrule
\textbf{Llama-3.3-70B}  & 0.41              & 0.57              \\
\textbf{DeepSeek-V3}    & \textbf{0.56}              & \textbf{0.73}              \\
\textbf{ChatGPT-4o}     & 0.52              & 0.49      \\
\midrule

\midrule
\textbf{Human Video} & \textbf{Concrete-M} & \textbf{Abstract-M}\\
\midrule
\textbf{Microlens}     & 0.44              & 0.71     
\\ \bottomrule
\end{tabular}
\caption{Comparison of popularity medians for video generation tasks (using CogVideoX-5b as the video generator). Human-created videos are included for reference. The popularity predictor provides predicted values of Human Crafted video for comparison. Concrete-M and Abstract-M are the most closely matched human videos to the same user prompt in the concrete and abstract datasets, respectively.}
\label{tab:popularity_medians}
\vspace{-0.1in}
\end{table}
, which struggle to fully capture and convey the complexity of the given descriptions \cite{liu2024sora}.

\begin{table*}[t]
    \centering
    \renewcommand{\arraystretch}{0.8}
    \begin{tabular}{l|ccc|ccc}
        \toprule
        \multirow{2}{*}{Category} & \multicolumn{3}{c|}{Abstract} & \multicolumn{3}{c}{Concrete} \\
        \cline{2-7}
        & Llama & DeepSeek & ChatGPT & Llama & DeepSeek & ChatGPT \\
        \midrule
        Anime                        & 0.54  & \textbf{0.92}  & 0.63  & 0.37  & \textbf{0.45}  & 0.39  \\
        Daily Sharing                & \textbf{0.59 } & 0.36  & 0.23  & 0.28  & \textbf{0.69}  & 0.61  \\
        Delicacy                     & 0.48  & \textbf{1.12}  & 0.64  & \textbf{0.66}  & 0.59  & 1.24  \\
        Film and Television Works    & 0.68  &\textbf{1.02}  & 0.82  & 0.46  & \textbf{0.66}  & 0.35  \\
        Game                         & 0.38  & 0.49  & \textbf{0.55}  & 0.35  & 0.30  & \textbf{0.56}  \\
        \bottomrule
    \end{tabular}
         % \vspace{-0.1in}
        \caption{Comparison of popularity across different categories and LLMs.}
        \vspace{-0.1in}
    \label{tab:comparison_categories}
\end{table*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/popularity_distributions_6plots.pdf}
    \caption{Predicted popularity distributions for different LLMs. Evaluated using three LLMs: Llama-3.3-70B, DeepSeek-V3, and ChatGPT-4o. Concrete and Abstract represent the types of user prompt 
    datasets.}
    \label{fig:popularity_distribution}
    \vspace{-0.1in}
\end{figure*}



The predicted popularity distributions are presented in Figure \ref{fig:popularity_distribution}.
For abstract prompts, the distributions of Llama-3.3-70B and ChatGPT-4o are highly concentrated near a popularity score of 0, with occasional outliers achieving higher scores. These rare successes highlight the overall instability of these models. In contrast, DeepSeek-V3 exhibits a smoother and more evenly distributed spread, with a higher proportion of videos falling into mid-to-high popularity ranges. However, in concrete datasets, the differences among the three models become negligible. This finding is further supported by DeepSeek-V3's highest median score in abstract datasets, while its median score in concrete prompts remains comparable to the others. 

Breaking down the analysis by category (Table \ref{tab:comparison_categories}), DeepSeek-V3 achieves the highest predicted popularity scores across most categories in the abstract dataset. Notably, it significantly outperforms other models in the Anime and Film and Television Works categories, while also demonstrating strong performance in Delicacy. However, its dominance is less pronounced in the Daily Sharing and Game categories, where its advantage is more limited.

\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}llc@{}}

\toprule
\textbf{LLM}            & \textbf{Task}    & \textbf{Win Rate (\%)} \\ \midrule
\multirow{2}{*}{\textbf{Llama-3.3-70B}}  & Concrete         & \textbf{56}                     \\
                        & Abstract         & \textbf{52}                     \\ \midrule
\multirow{2}{*}{\textbf{DeepSeek-V3}}  & Concrete         & \textbf{55}                     \\
                        & Abstract         & 49                     \\ \midrule
\multirow{2}{*}{\textbf{ChatGPT-4o}}    & Concrete         & \textbf{51}                     \\
                        & Abstract         & \textbf{53}                    \\ \bottomrule
\end{tabular}
\caption{Comparison of popularity win rates across different LLMs.}
\label{tab:win_rates_LLM}
\vspace{-0.15in}
\end{table}

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}llc@{}}

\toprule
\textbf{Video Generators}            & \textbf{Task}    & \textbf{Win Rate (\%)} \\ \midrule
% \multirow{2}{*}{\textbf{CogVideoX-5b}}  & Concrete         & \textbf{55}                     \\
%                         & Abstract         & 49                    \\ \midrule
\multirow{2}{*}{\textbf{LTX}}  & Concrete         & \textbf{60}                     \\
                        & Abstract         & \textbf{61}                     \\ \midrule
\multirow{2}{*}{\textbf{Hunyuan}}    & Concrete         & 48                     \\
                        & Abstract         & \textbf{52}                  \\ \bottomrule
\end{tabular}
\caption{Comparison of popularity win rates across different video generation models.}
\vspace{-0.15in}
\label{tab:win_rates_VG}
\end{table}



\textbf{(Answer to RQ1)} LLMs facilitate micro-video creation by structuring descriptions and guiding Video generation models, with their effectiveness assessed using a pre-trained offline popularity predictor. While the absolute values remain modest, the median popularity of videos generated by state-of-the-art LLMs matches or even exceeds that of average human-created videos in our evaluation. It suggests that the proposed method has considerable potential for popular short video generation.  These findings highlight promising directions for enhancing AI-generated video engagement.






\section{Prompt-based Popularity Enhancement (RQ2)}
\label{sec:PE}




The comparative results for Prompt Enhancement (PE) versus the basic LLMPopcorn prompt are summarized in Tables~\ref{tab:win_rates_LLM} and~\ref{tab:win_rates_VG}, using win rates as the evaluation metric. All win rates presented are relative values, derived from pairwise comparisons between PE and the basic prompt across different models and scenarios. Since these values are model-specific and do not represent absolute performance, they should not be used for direct cross-model comparisons.








Table~\ref{tab:win_rates_LLM} evaluates the PE’s effectiveness across three LLMs for both concrete and abstract video creation tasks. Notably, with Llama-3.3-70B, PE achieves a 56\% win rate for concrete tasks and 52\% for abstract tasks, surpassing basic LLMPopcorn. DeepSeek-V3 delivers competitive gains (55\% on concrete tasks), but its relatively low abstract-task win rate (49\%). In contrast, ChatGPT-4o exhibits balanced improvements, achieving a 53\% win rate on abstract tasks, indicating strong compatibility with PE’s human-like creative process. 




Table~\ref{tab:win_rates_VG} further validates the impact of PE on two more video generation models. LTX exhibits the most significant improvements, achieving win rates of 60\% on concrete tasks and 61\% on abstract tasks. Hunyuan shows a slight performance gain on the Abstract dataset but performs marginally worse on the Concrete dataset. Overall, these results indicate that PE effectively enhances LLM-generated prompts for popular video generation models, despite some fluctuations. 



\begin{table*}[h!]
\centering
\renewcommand{\arraystretch}{0.8}
\scalebox{0.9}{
\begin{tabular}{@{}l cccccc@{}}
\toprule
\multirow{2}{*}{\textbf{LLM}} & \multirow{2}{*}{\textbf{Model Comparison}} 
             & \multicolumn{2}{c}{\textbf{Concrete Dataset (\%)}} 
             & \multicolumn{2}{c}{\textbf{Abstract Dataset (\%)}} \\ 
\cmidrule(r){3-4} \cmidrule(r){5-6}
&                           
& \textbf{BP WR} 
& \textbf{PE WR} 
& \textbf{BP WR} 
& \textbf{PE WR} \\ 
\midrule

%----------------------------------
% 1) Llama-3.3-70B
\multirow{4}{*}{\textbf{Llama-3.3-70B}} 
  & vs. DeepSeek-V3   
     & \cellcolor{red!30}48 & \cellcolor{red!30}48 
     & \cellcolor{red!30}43 & \cellcolor{yellow!30}50 \\

  & vs. ChatGPT-4o  
     & \cellcolor{red!30}49 & \cellcolor{green!30}51 
     & \cellcolor{red!30}49 & \cellcolor{green!30}53 \\

  & vs. DeepSeek-R1    
     & \cellcolor{red!30}46 & \cellcolor{red!30}40 
     & \cellcolor{red!30}45 & \cellcolor{green!30}53 \\

  & vs. Qwen-2.5-72B  
     & \cellcolor{green!30}51 & \cellcolor{green!30}56 
     & \cellcolor{yellow!30}50 & \cellcolor{green!30}55 \\
\midrule

%----------------------------------
% 2) DeepSeek-V3
\multirow{4}{*}{\textbf{DeepSeek-V3}} 
  & vs. Llama-3.3-70B 
     & \cellcolor{green!30}52 & \cellcolor{green!30}52 
     & \cellcolor{green!30}57 & \cellcolor{yellow!30}50 \\

  & vs. ChatGPT-4o   
     & \cellcolor{green!30}52 & \cellcolor{green!30}55 
     & \cellcolor{green!30}52 & \cellcolor{green!30}60 \\

  & vs. DeepSeek-R1    
     & \cellcolor{green!30}51 & \cellcolor{red!30}44 
     & \cellcolor{yellow!30}50 & \cellcolor{green!30}58 \\

  & vs. Qwen-2.5-72B  
     & \cellcolor{yellow!30}50 & \cellcolor{green!30}64 
     & \cellcolor{green!30}55 & \cellcolor{red!30}49 \\
\midrule

%----------------------------------
% 3) ChatGPT-4.0
\multirow{4}{*}{\textbf{ChatGPT-4o}} 
  & vs. Llama-3.3-70B 
     & \cellcolor{green!30}51 & \cellcolor{red!30}49 
     & \cellcolor{green!30}51 & \cellcolor{red!30}46 \\

  & vs. DeepSeek-V3   
     & \cellcolor{red!30}48 & \cellcolor{red!30}45 
     & \cellcolor{red!30}48 & \cellcolor{red!30}40 \\

  & vs. DeepSeek-R1    
     & \cellcolor{red!30}47 & \cellcolor{red!30}43 
     & \cellcolor{red!30}46 & \cellcolor{red!30}48 \\

  & vs. Qwen-2.5-72B  
     & \cellcolor{green!30}52 & \cellcolor{green!30}54 
     & \cellcolor{red!30}49 & \cellcolor{red!30}44 \\
\midrule

%----------------------------------
% 4) DeepSeek-R1
\multirow{4}{*}{\textbf{DeepSeek-R1}} 
  & vs. Llama-3.3-70B 
     & \cellcolor{green!30}54 & \cellcolor{green!30}60 
     & \cellcolor{green!30}55 & \cellcolor{red!30}47 \\

  & vs. DeepSeek-V3   
     & \cellcolor{red!30}49 & \cellcolor{green!30}56 
     & \cellcolor{yellow!30}50 & \cellcolor{red!30}42 \\

  & vs. ChatGPT-4o   
     & \cellcolor{green!30}53 & \cellcolor{green!30}57 
     & \cellcolor{green!30}54 & \cellcolor{green!30}52 \\

  & vs. Qwen-2.5-72B  
     & \cellcolor{green!30}56 & \cellcolor{green!30}66 
     & \cellcolor{green!30}55 & \cellcolor{green!30}51 \\
\midrule

%----------------------------------
% 5) Qwen-2.5-72B
\multirow{4}{*}{\textbf{Qwen-2.5-72B}} 
  & vs. Llama-3.3-70B 
     & \cellcolor{red!30}49 & \cellcolor{red!30}44 
     & \cellcolor{yellow!30}50 & \cellcolor{red!30}45 \\

  & vs. DeepSeek-V3   
     & \cellcolor{yellow!30}50 & \cellcolor{red!30}36 
     & \cellcolor{red!30}45 & \cellcolor{green!30}51 \\

  & vs. ChatGPT-4o  
     & \cellcolor{red!30}48 & \cellcolor{red!30}46 
     & \cellcolor{green!30}51 & \cellcolor{green!30}56 \\

  & vs. DeepSeek-R1    
     & \cellcolor{red!30}44 & \cellcolor{red!30}34 
     & \cellcolor{red!30}45 & \cellcolor{red!30}49 \\
\bottomrule
\end{tabular}
}
\caption{Different LLMs' Win Rate comparisons across concrete and abstract datasets.
Green indicates cases where the Video Generation model outperforms, red signifies losses, and yellow represents ties. BP denotes the basic prompt, WR represents the win rate, and PE refers to prompt enhancement.}
\label{tab:win_rates_llms_benchmark}
\end{table*}

\textbf{(Answer to RQ2)} We demonstrate the overall effectiveness of the proposed prompt enhancement, confirming its efficacy.



\begin{table*}[h!]
\centering
\renewcommand{\arraystretch}{0.8}
\scalebox{0.9}{
\begin{tabular}{@{}lcccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Video Generators}} & \multirow{2}{*}{\textbf{Model Comparison}} & \multicolumn{2}{c}{\textbf{Concrete Dataset (\%)}} & \multicolumn{2}{c}{\textbf{Abstract Dataset (\%)}} \\ 
\cmidrule(r){3-4} \cmidrule(r){5-6}
&                           & BP WR & PE WR & BP WR & PE WR \\ \midrule
\multirow{2}{*}{\textbf{CogVideoX-5b}} 
  & vs. LTX-Video   & \cellcolor{red!30}49 & \cellcolor{red!30}38 & \cellcolor{red!30}42 & \cellcolor{red!30}39 \\ 
  & vs. HunyuanVideo & \cellcolor{red!30}44 & \cellcolor{red!30}38 & \cellcolor{red!30}47 & \cellcolor{yellow!30}50 \\ \midrule
\multirow{2}{*}{\textbf{LTX-Video}} 
  & vs. CogVideoX-5b & \cellcolor{green!30}51 & \cellcolor{green!30}62 & \cellcolor{green!30}58 & \cellcolor{green!30}61 \\ 
  & vs. HunyuanVideo   & \cellcolor{red!30}48 & \cellcolor{green!30}52 & \cellcolor{red!30}49 & \cellcolor{green!30}60 \\ \midrule
\multirow{2}{*}{\textbf{HunyuanVideo}} 
  & vs. CogVideoX-5b & \cellcolor{green!30}56 & \cellcolor{green!30}62 & \cellcolor{green!30}53 & \cellcolor{yellow!30}50 \\ 
  & vs. LTX-Video & \cellcolor{green!30}52 & \cellcolor{red!30}48 & \cellcolor{green!30}51 & \cellcolor{red!30}40 \\ \bottomrule
\end{tabular}
}
\caption{Different video generation models' Win Rate comparisons across concrete and abstract Datasets.}
\label{tab:win_rates_vgs_benchmark}
\vspace{-0.1in}
\end{table*}

\section{Benchmarking Different Models (RQ3)}
\label{sec:Bench}
In this study, we employed five state-of-the-art large language models (LLMs): Llama-3.3-70B \cite{touvron2023llama}, Qwen-2.5-72B \cite{yang2024qwen2}, and ChatGPT-4o \cite{hurst2024gpt}. Additionally, we utilized the API versions of DeepSeek-V3 \cite{liu2024deepseek} and DeepSeek-R1 \cite{guo2025deepseek} for evaluation. For video generation, we incorporated three widely recognized open-source models: CogVideoX-5B \cite{yang2024cogvideox}, LTX-Video \cite{HaCohen2024LTXVideo}, and HunyuanVideo \cite{kong2024hunyuanvideo}. Furthermore, for the open-source LLMs, we adopted the instruction-tuned versions.

Table~\ref{tab:win_rates_llms_benchmark} demonstrates that DeepSeek models (V3 and R1) dominate most comparisons, achieving the highest number of wins. Notably, DeepSeek-R1 reaches a peak BP WR of 56\% and PE WR of 66\%. Meanwhile, Llama-3.3-70B stands out in PE WR for the Abstract dataset, reaching up to 56\%, indicating strong adaptation to enhanced prompts in abstract reasoning. In contrast, ChatGPT-4o and Qwen-2.5-72B generally perform the worst, with Qwen-2.5-72B registering the lowest PE WR (34\%) against DeepSeek-R1. These results highlight DeepSeek’s overall superiority, Llama-3.3-70B’s specific strength in abstract PE tasks, and the weaker performance of ChatGPT-4o and Qwen-2.5-72B.

Table \ref{tab:win_rates_vgs_benchmark} indicates that CogVideoX-5b generally underperforms compared to the other two models. In contrast, both LTX-Video and HunyuanVideo achieve win rates exceeding 50\% in many pairwise comparisons against CogVideoX-5b on both the Concrete and Abstract datasets, demonstrating their relatively stronger performance. Notably, both models attain a maximum PE WR of 62\% against CogVideoX-5b on the Concrete dataset. The marginal performance difference between LTX-Video and HunyuanVideo suggests that they handle diverse prompts at a comparably high level.



\textbf{(Answer to RQ3)} We evaluated five cutting-edge LLMs and three state-of-the-art video-generation models using the proposed pipeline. The results indicate that \textbf{DeepSeek-V3} and \textbf{DeepSeek-R1} rank as the top LLMs overall, consistently outperforming or tying other models in both Concrete and Abstract scenarios. For video generation, \textbf{LTX-Video} and \textbf{HunyuanVideo} deliver robust outputs with only minor gaps in win rates across different prompt types, underscoring their competitive reliability in diverse content creation tasks.

\section{Related Work}
\noindent \textbf{Video Generation:} Recent advancements in video generation have been truly breathtaking, with models such as Sora\footnote{\url{https://sora.com/}} and Kling\footnote{\url{https://kling.kuaishou.com/en}}. Building upon the success of diffusion models in image synthesis, several studies have extended these techniques to video generation. For instance, the Make-A-Video model \cite{singer2022make} demonstrates how text-conditioned diffusion models can produce high-quality videos by jointly modeling spatial and temporal dimensions. Other approaches, such as VideoCrafter1 \cite{chen2023videocrafter1} have developed open diffusion models capable of generating high-quality videos from textual inputs, achieving resolutions up to 1024×576 pixels. Additionally, models like Control-A-Video \cite{chen2023control} introduce controllable text-to-video generation by incorporating motion priors and content priors, enabling fine-grained control over the generated content.  CogVideo \cite{hong2022cogvideo} and its successor CogVideoX \cite{yang2024cogvideox} employ a large-scale pre-trained transformer architecture with a multi-frame-rate hierarchical training strategy, effectively aligning text and video representations for higher-quality generation. Similarly, HunyuanVideo \cite{kong2024hunyuanvideo} introduces additional refinements, though its technical details remain limited. Further improving controllability, LTX \cite{HaCohen2024LTXVideo} explores novel approaches to enhance video generation fidelity.


\noindent \textbf{Large Language Models for Video Generation:} The emergence of large language models (LLMs) has spurred interest in their application to video generation \cite{zhao2023learning,hong2023large,maaz2023video}. VideoPoet \cite{kondratyuk2023videopoet}, for example, employs a decoder-only transformer architecture that processes multimodal inputs—including text, images, and videos—to generate coherent video content. Similarly, GPT4Video \cite{wang2024gpt4video} presents a unified multimodal framework that empowers LLMs with the capability of both video understanding and generation, integrating instruction-following approaches with stable diffusion models to handle various video generation scenarios.

\noindent \textbf{Limitations of Existing Literature:} Despite advancements in video generation and understanding, the role of large language models (LLMs) in popular video creation remains largely unexplored. Existing research primarily addresses video understanding (e.g., captioning, summarization) and video generation (e.g., script-to-video synthesis), without examining how LLMs can be effectively leveraged to enhance content popularity—a crucial factor in micro-video platforms. This gap underscores the lack of empirical studies investigating the intersection of LLMs and diffusion-based video generation in the context of audience engagement and content virality. To the best of our knowledge, we are the pioneer empirical study in this domain. This work systematically explores this underexamined area, providing foundational insights to guide future research. We aim to foster further advancements and inspire subsequent studies in AI-driven video content generation, particularly within the micro-video industry.



\section{Conclusion}
This empirical study demonstrates that the proposed LLMPopcorn for micro-video generation assisted by state-of-the-art LLMs performs slightly better than ordinary human-created videos, evaluated with an offline popularity predictor.
These results highlight the potential of this approach. To further enhance the LLMPopcorn pipeline, we introduced prompt enhancement (PE), which exhibited strong adaptability across multiple LLMs and video-generation models. Among the tested models, DeepSeek-V3 and DeepSeek-R1 consistently performed best as LLMs, while LTX-Video and HunyuanVideo produced reliable outputs across diverse prompts. 

In future work, we plan to explore reinforcement learning and fine-tuning techniques to better align prompts with contextual cues, ultimately improving video quality and engagement. This study pioneers popularity-driven micro-video generation, establishing a strong foundation for future research and innovation.


\section{Limitations}
Despite the promising results, this study has several limitations, which also suggest potential directions for future research in this area:

\begin{enumerate}
    \item  While some videos generated by LLMPopcorn using state-of-the-art LLMs exceed the median popularity of human-created content, their overall absolute scores remain modest. Nevertheless, LLMPopcorn is still able to match the average performance of human-generated videos, highlighting its potential for further refinement in aligning content with audience preferences.
    
    \item We did not conduct real-world user studies to evaluate engagement with generated videos. Although the offline popularity predictor offers insights, integrating actual audience interactions (e.g., views, likes) would better assess content effectiveness.  

    \item Current state-of-the-art video generation models, despite advancements, struggle to accurately interpret complex prompts, often losing critical details and producing oversimplified or incomplete outputs.

    \item  This study does not fine-tune the LLM or video generation models, which could further enhance video quality and engagement. Incorporating popularity-based signals into video model adaptation may improve content relevance and audience resonance, potentially increasing the chances of generating viral videos.
    \item While AI-generated videos offer significant advantages, they also present risks of misuse, such as spam content flooding social media platforms and the exploitation of recommender systems. However, most video platforms impose restrictions on account registration and daily upload limits to mitigate such risks. 
 

\end{enumerate}



We recognize that this study has several limitations, largely due to the limited existing literature in this field. However, as an initial exploration, it provides a foundation for future research. More importantly, these limitations represent opportunities for further advancements, requiring a collective effort from the broader research community. Addressing these challenges will play a crucial role in driving progress in AI-assisted popular micro-video creation.



\section{Ethical Considerations}
This research follows ethical guidelines. The Microlens dataset is publicly available, and all user prompts were generated by ChatGPT-4o, without human involvement or personal data. Evaluation was conducted using pre-trained models via open-source implementations or APIs, without human annotation. We strive to ensure transparency and compliance with ethical standards.

\section*{Acknowledgements}
We sincerely appreciate Dr. Alexandros Karatzoglou for his invaluable guidance, which has played a crucial role in shaping the direction of this research. His insightful advice and expertise have provided significant support throughout the development of this work.

\bibliography{custom}

\clearpage
\appendix

\section{Appendix}
\label{sec:appendix}
\subsection{Implementation details}
\label{sec:implementation}
In this paper, we utilized five state-of-the-art LLMs, including two open-source models, Llama-3.3-70B and Qwen-2.5-72B, and three API-based models, DeepSeek-V3, DeepSeek-R1, and ChatGPT-4o. For efficiency, we apply 4-bit quantization to the open-source LLMs. Additionally, we incorporated three widely-used open-source video generation models: CogVideoX-5B, LTX-Video, and HunyuanVideo. All model weights were obtained from the HuggingFace platform\footnote{\url{https://huggingface.co/}} to ensure consistency and reproducibility. Notably, all open-source LLMs used in this study were instruction-tuned versions. We ensured reproducibility by fixing the random seed for opensourced models, allowing for consistent results. We adopt Faiss\footnote{\url{https://github.com/facebookresearch/faiss}} as library for matching and indexing.


For the prompt enhancement related to RQ2, we search the retrieval-augmented generation (RAG) size from [10, 50, 100]. 
%We set the ratio of positive to negative samples at 9:1. 
In terms of the embedding model, we adopt "all-MiniLM-L12-v2"\footnote{\url{https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2}}. The video generation prompt database we adopted is from \cite{fu2024efficient}, which contains existing video titles and video captions paired with their popularity. All experiments are conducted on one H100 80G GPU. 


\subsection{License Statement}
\label{sec:license}
All the open-source models used in this study adhere to their respective licenses and usage terms. Specifically, Llama-3.3-70B follows Meta's custom research license, which allows research and commercial use under specified conditions. Qwen-2.5-72B, developed by Alibaba, is released under the Tongyi Qianwen Open License, which permits research use while imposing restrictions on certain commercial applications. Similarly, the open-source video generation models, CogVideoX-5B, LTX-Video, and HunyuanVideo, are available under their respective open-access agreements, ensuring transparency and reproducibility in academic research.

For API-based models, including DeepSeek-V3, DeepSeek-R1, and ChatGPT-4o, we accessed them through their official APIs in compliance with the respective providers' terms of use. DeepSeek models are governed by DeepSeek’s API usage policy, which allows research use while enforcing rate limits and responsible AI usage guidelines. This paper was written with ChatGPT's assistance in compliance with OpenAI’s terms of service. The "all-MiniLM-L12-v2" embedding model is publicly available on HuggingFace under the Apache 2.0 License, allowing unrestricted research and commercial use. 

By strictly following these licensing terms, we ensure that our research maintains compliance with ethical AI deployment standards, respects intellectual property rights, and upholds reproducibility for future studies.


\begin{table}
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{RAG Num} & \textbf{Concrete} & \textbf{Abstract} \\
        \midrule
        LLMPopcorn & 0.52 & 0.95\\
        + PE-RAG Num = 10 & \textbf{0.92} & 0.94 \\
        + PE-RAG Num = 50 & 0.72 & 0.87 \\
        + PE-RAG Num = 100 & 0.84 & \textbf{0.99} \\
        \bottomrule
    \end{tabular}
    \caption{Median predicted popularity for ablation study.}
    \label{tab:abl_rag}
    
\end{table}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/Baseline_Abstract_Videos.pdf}
    \caption{Video examples from LLMPopcorn for a concrete user query.}
    \label{fig:LLMPopcorn_concrete}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/Baseline_Concrete_Videos.pdf}
    \caption{Video examples from LLMPopcorn for an abstract user query.}
    \label{fig:LLMPopcorn_abstract}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/PE_Abstract_Videos.pdf}
    \caption{Video examples from LLMPopcorn with the Prompt Enhancement for an concrete user query.}
    \label{fig:PE_concrete}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/PE_Concrete_Videos.pdf}
    \caption{Video examples from LLMPopcorn with the Prompt Enhancement for an abstract user query.}
    \label{fig:PE_abstract}
\end{figure*}




\subsection{Ablation Study for the Prompt Enhancement}
In this section, we analyze the impact of RAG numbers on prompt enhancement. Using DeepSeek-V3 with LTX as the default LLM and video generator, Table \ref{tab:abl_rag} shows that the Concrete dataset benefits more from RAG, while the Abstract dataset requires a larger scale for improvement.

For the Abstract dataset, LLMPopcorn already performs well (0.95), with only PE-RAG Num = 100 providing a notable boost (0.99). In contrast, the Concrete dataset starts lower (0.52) but becomes more effective with RAG, achieving 0.92 with PE-RAG Num = 10. This suggests that RAG may be more impactful for Concrete tasks, while Abstract tasks improve gradually with larger enhancements.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/prompt_examples.pdf}
    \vspace{-5em}
    \caption{An overview of the Generated Video Titles and Video Prompt by LLMPopcorn.}
    \label{fig:prompt_examples}
\end{figure*}




\subsection{Video and Prompt Examples}
To evaluate and compare different video generation models, we apply two prompts across three models, as depicted in Figures \ref{fig:LLMPopcorn_concrete}, \ref{fig:LLMPopcorn_abstract}, \ref{fig:PE_concrete}, and \ref{fig:PE_abstract}. In these figures, the first row corresponds to videos generated by CogVideoX-5b, the second row by HunyuanVideo, and the third row by LTX-Video.  The first prompt is abstract: "Create a video about the history and evolution of a popular street food." The second is a concrete prompt: "Create a video explaining the world-building in Avatar (2009)." 
%Additionally, we showcase the generated video titles and corresponding videos in Figure [X].

\subsection{Efficiency}
We present Table \ref{tab:process-efficiency} and Table \ref{tab:llmpopcorn_efficiency}, which report the efficiency of three key processes: retrieval for PE, LLM inference for LLMPopcorn and PE, video generation, and evaluation. Efficiency is measured in terms of VRAM usage and inference time. All evaluations are conducted on a single H100 GPU.

\begin{table}
    \centering

    \begin{tabular}{l l r r}
    \toprule
    & & \textbf{VRAM} & \textbf{Time} \\
    \midrule
    \multicolumn{2}{l}{\textbf{Retrieval Process}} 
     & 896 & 12\,s/query \\
    \midrule
    \multicolumn{4}{l}{\textbf{Video Generation}} \\
    \hspace{2em} & CogVideoX & 28,788 & 170\,s/video \\
    \hspace{2em} & Hunyuan & 12,016 & 54\,s/video \\
    \hspace{2em} & LTX & 27,100 & 20\,s/video \\
    \midrule
    \multicolumn{2}{l}{\textbf{Evaluation Process}}& 3,000 & 4\,s/video \\
    \bottomrule
    \end{tabular}
        \caption{Efficiency of the pipeline procedures. We denote Time as the inference time (measured in seconds, denoted as $s$), and VRAM is measured in megabytes (MB).}
    \label{tab:process-efficiency}
\end{table}



\begin{table*}
    \centering
    \begin{tabular}{llcc}
        \toprule
        \textbf{LLM} & \textbf{Pipeline} & \textbf{VRAM (MB)} & \textbf{Inference Time (s/query)} \\
        \midrule
        \multirow{4}{*}{Qwen-2.5-72B} 
        & LLMPopcorn & 44,246 & 8 \\ 
        & +PE-RAG Num = 10 & 47,468 & 28 \\ 
        & +PE-RAG Num = 50 & 53,504 & 62 \\ 
        & +PE-RAG Num = 100 & 67,192 & 90 \\ 
        \midrule
        \multirow{4}{*}{LLaMA-3.3-70B} 
        & LLMPopcorn & 42,240 & 9 \\ 
        & +PE-RAG Num = 10 & 43,948 & 48 \\ 
        & +PE-RAG Num = 50 & 49,662 & 75 \\ 
        & +PE-RAG Num = 100 & 56,842 & 86 \\ 
        \bottomrule
    \end{tabular}
    \caption{Inference efficiency test results for LLMPopcorn on Qwen-2.5-72B and LLaMA-3.3-70B}
    \label{tab:llmpopcorn_efficiency}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/BP_template.pdf}
    \caption{An overview of the basic prompt template for LLMPopcorn.}
    \label{fig:BP_template}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/PE_template.pdf}
    \caption{An overview of the Prompt Enhancement template.}
    \label{fig:PE_template}
\end{figure*}


\end{document}



\section{Ablation Studies}

\section{Case Studies}

\section{Conclusion}



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}



This is an appendix.

\end{document}
