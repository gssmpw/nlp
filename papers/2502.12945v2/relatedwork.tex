\section{Related Work}
\noindent \textbf{Video Generation:} Recent advancements in video generation have been truly breathtaking, with models such as Sora\footnote{\url{https://sora.com/}} and Kling\footnote{\url{https://kling.kuaishou.com/en}}. Building upon the success of diffusion models in image synthesis, several studies have extended these techniques to video generation. For instance, the Make-A-Video model \cite{singer2022make} demonstrates how text-conditioned diffusion models can produce high-quality videos by jointly modeling spatial and temporal dimensions. Other approaches, such as VideoCrafter1 \cite{chen2023videocrafter1} have developed open diffusion models capable of generating high-quality videos from textual inputs, achieving resolutions up to 1024×576 pixels. Additionally, models like Control-A-Video \cite{chen2023control} introduce controllable text-to-video generation by incorporating motion priors and content priors, enabling fine-grained control over the generated content.  CogVideo \cite{hong2022cogvideo} and its successor CogVideoX \cite{yang2024cogvideox} employ a large-scale pre-trained transformer architecture with a multi-frame-rate hierarchical training strategy, effectively aligning text and video representations for higher-quality generation. Similarly, HunyuanVideo \cite{kong2024hunyuanvideo} introduces additional refinements, though its technical details remain limited. Further improving controllability, LTX \cite{HaCohen2024LTXVideo} explores novel approaches to enhance video generation fidelity.


\noindent \textbf{Large Language Models for Video Generation:} The emergence of large language models (LLMs) has spurred interest in their application to video generation \cite{zhao2023learning,hong2023large,maaz2023video}. VideoPoet \cite{kondratyuk2023videopoet}, for example, employs a decoder-only transformer architecture that processes multimodal inputs—including text, images, and videos—to generate coherent video content. Similarly, GPT4Video \cite{wang2024gpt4video} presents a unified multimodal framework that empowers LLMs with the capability of both video understanding and generation, integrating instruction-following approaches with stable diffusion models to handle various video generation scenarios.

\noindent \textbf{Limitations of Existing Literature:} Despite advancements in video generation and understanding, the role of large language models (LLMs) in popular video creation remains largely unexplored. Existing research primarily addresses video understanding (e.g., captioning, summarization) and video generation (e.g., script-to-video synthesis), without examining how LLMs can be effectively leveraged to enhance content popularity—a crucial factor in micro-video platforms. This gap underscores the lack of empirical studies investigating the intersection of LLMs and diffusion-based video generation in the context of audience engagement and content virality. To the best of our knowledge, we are the pioneer empirical study in this domain. This work systematically explores this underexamined area, providing foundational insights to guide future research. We aim to foster further advancements and inspire subsequent studies in AI-driven video content generation, particularly within the micro-video industry.