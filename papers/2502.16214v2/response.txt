\section{Related Work}
\subsection{Driver Saliency Prediction}
% In the field of driver saliency prediction, Tawari et al. improved gaze estimation by collecting first-person driving perspectives using Google Glass **Tawari, "Real-Time Eye Tracking for Driver Distraction Detection"**. Deng et al. proposed a bottom-up saliency detection model that predicts driver gaze points through both low-level and high-level features  **Deng, "Driver Gaze Estimation Using Bottom-Up Saliency Detection"**. Traditional models often focus on static driving scenes and lack dynamic understanding of real driving conditions. Alletto et al. released the DR(eye)VE dataset  **Alletto, "DR(eye)VE: A Dataset for Driver Gaze Prediction in Various Weather Conditions"**, which includes a variety of weather and driving conditions, enhancing the understanding of driver attention processes. Xia et al. constructed the BDD-A dataset  **Xia, "BDD-A: An Emergency Braking Events Dataset for Driver Attention Analysis"**, which includes emergency braking events, and introduced the Human Weighted Sampling (HWS) method for predicting driver attention. Fang et al. published the DADA-2000 dataset  **Fang, "DADA-2000: A Multi-Scenario Dataset for Driver Attention Prediction"**, covering normal driving and accident scenarios, and proposed a multi-path semantic-guided attention fusion network. Deng et al. released the TrafficGaze dataset  **Deng, "TrafficGaze: A Comprehensive Clear-Weather Dataset for Driver Gaze Estimation"**, providing richer driving scene data. Tian and Deng et al. developed the DrFixD-rainy/night dataset  **Tian, Deng, "DrFixD-rainy/night: A Complex Weather Conditions Dataset for Driver Attention Prediction"**, addressing the research gap in complex weather conditions. These datasets and methods continue to drive advancements in the field of driver gaze prediction.
Previous works have made notable advances in driver saliency prediction, primarily focusing on inherent visual features or semantic information from image segmentation and optical flow. Tawari et al. pioneered first-person gaze estimation using Google Glass  **Tawari, "Real-Time Eye Tracking for Driver Distraction Detection"**, while Deng et al. proposed a bottom-up saliency model combining low and high-level features  **Deng, "Driver Gaze Estimation Using Bottom-Up Saliency Detection"**, though limited by traditional machine learning's feature extraction capabilities.

As research progressed, numerous datasets and deep learning algorithms emerged to address conventional limitations. The DR(eye)VE dataset by Alletto et al. covers various driving conditions but lacks scene diversity and semantic richness  **Alletto, "DR(eye)VE: A Dataset for Driver Gaze Prediction in Various Weather Conditions"**. The BDDA dataset introduced by Xia et al. enriches the field with urban driving scenarios and emergency events  **Xia, "BDD-A: An Emergency Braking Events Dataset for Driver Attention Analysis"**. Fang et al. presented DADA-2000 dataset covering normal and accident scenarios with a semantic-guided attention fusion network, though limited to collision scenes and segmentation-based semantics  **Fang, "DADA-2000: A Multi-Scenario Dataset for Driver Attention Prediction"**. The TrafficGaze dataset by Deng et al. offers comprehensive clear-weather data with a lightweight CNN framework  **Deng, "TrafficGaze: A Comprehensive Clear-Weather Dataset for Driver Gaze Estimation"**, while the DrFixD-rainy/night dataset specifically addresses adverse weather conditions  **Tian, Deng, "DrFixD-rainy/night: A Complex Weather Conditions Dataset for Driver Attention Prediction"**. Nevertheless, these approaches rely on scene features or segmentation-based semantics, lacking scene understanding. Brishtel et al. demonstrated correlations in gaze patterns across driving modes  **Brishtel, "Gaze Correlation Across Driving Modes"**. Vozniak et al. successfully incorporated semantic danger cues in attention prediction  **Vozniak, "Semantic Danger Cues in Attention Prediction"**, though obtaining annotated semantic data remains challenging.

In summary, while substantial research has developed numerous models for predicting driver attention, there remains a gap in utilizing basic semantic guidance of driving scenes for driver saliency prediction. Therefore, this paper employs the CLIP  **Radford, "Learning Transferable Visual Models"** model to extract semantic information from driving scenes. To validate the effectiveness of our proposed method in diverse and dynamic driving scenarios, we conduct experiments across datasets with different weather conditions (TrafficGaze, DrFixD-rainy) and complex semantic information (BDDA).

\subsection{Downstream Tasks}
\subsubsection{Salient Object Detection.} This is an important task in computer vision aimed at detecting the most prominent object regions in an image. In the context of traffic driving scenes, drivers often automatically filter out objects unrelated to the current driving task. Therefore, some existing works use driver attention allocation as prior knowledge to detect prominent or key objects  **Du, "Attention-Based Driving Event Dataset and Model"**. This approach does not detect all objects in the driving scene but focuses on those most relevant to the current driving task, thereby reducing redundant information.
\subsubsection{Drive Event Recognition.} In the field of intelligent transportation systems, detecting driving events by analyzing driving scene information is one of the key tasks in preventing traffic accidents. Driver visual attention helps identify information relevant to the current driving task while suppressing irrelevant information. Du et al.  **Du, "Attention-Based Driving Event Dataset and Model"** established an attention-based driving event dataset (ADED) and proposed a driver attention-guided model that uses driver attention as guidance to better recognize events that cause shifts in driver attention.

In conclusion, we believe that the semantic information of the driving scene is beneficial in identifying the driverâ€™s attention most aligned with the current driving task. Inspired by visual cognition, we have developed an attention prediction model driven by the semantic information of the current driving scene, integrating this semantic information at the deepest level of image feature extraction to guide the driver's attention effectively.
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{model_view_crc_f.pdf}
    \caption{Overview of the proposed SalM$^2$ network. (a) shows the overall network framework, which includes two branches: a ``Bottom-up" branch and a ``Top-down" branch. (b) illustrates the principle of the self-attention mechanism. (c) illustrates the principle of our proposed cross-modal attention mechanism.}
    \label{fig:model_overview}
\end{figure*}