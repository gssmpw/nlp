\section{Related Work}
\subsection{Driver Saliency Prediction}
% In the field of driver saliency prediction, Tawari et al. improved gaze estimation by collecting first-person driving perspectives using Google Glass \cite{tawari2014attention}. Deng et al. proposed a bottom-up saliency detection model that predicts driver gaze points through both low-level and high-level features  \cite{deng2017learning}. Traditional models often focus on static driving scenes and lack dynamic understanding of real driving conditions. Alletto et al. released the DR(eye)VE dataset  \cite{alletto2016dr}, which includes a variety of weather and driving conditions, enhancing the understanding of driver attention processes. Xia et al. constructed the BDD-A dataset \cite{xia2019predicting}, which includes emergency braking events, and introduced the Human Weighted Sampling (HWS) method for predicting driver attention. Fang et al. published the DADA-2000 dataset \cite{fang2019dada}, covering normal driving and accident scenarios, and proposed a multi-path semantic-guided attention fusion network. Deng et al. released the TrafficGaze dataset  \cite{deng2019drivers}, providing richer driving scene data. Tian and Deng et al. developed the DrFixD-rainy/night dataset \cite{tian2022driving,deng2023driving}, addressing the research gap in complex weather conditions. These datasets and methods continue to drive advancements in the field of driver gaze prediction.
Previous works have made notable advances in driver saliency prediction, primarily focusing on inherent visual features or semantic information from image segmentation and optical flow. Tawari et al. pioneered first-person gaze estimation using Google Glass \cite{tawari2014attention}, while Deng et al. proposed a bottom-up saliency model combining low and high-level features \cite{deng2017learning}, though limited by traditional machine learning's feature extraction capabilities.

As research progressed, numerous datasets and deep learning algorithms emerged to address conventional limitations. The DR(eye)VE dataset by Alletto et al. covers various driving conditions but lacks scene diversity and semantic richness \cite{alletto2016dr}. The BDDA dataset introduced by Xia et al. enriches the field with urban driving scenarios and emergency events \cite{xia2019predicting}. Fang et al. presented DADA-2000 dataset covering normal and accident scenarios with a semantic-guided attention fusion network, though limited to collision scenes and segmentation-based semantics \cite{fang2019dada}. The TrafficGaze dataset by Deng et al. offers comprehensive clear-weather data with a lightweight CNN framework \cite{deng2019drivers}, while the DrFixD-rainy/night dataset specifically addresses adverse weather conditions \cite{tian2022driving,deng2023driving}. Nevertheless, these approaches rely on scene features or segmentation-based semantics, lacking scene understanding. Brishtel et al. demonstrated correlations in gaze patterns across driving modes \cite{9945234}. Vozniak et al. successfully incorporated semantic danger cues in attention prediction \cite{vozniak2023context}, though obtaining annotated semantic data remains challenging.

In summary, while substantial research has developed numerous models for predicting driver attention, there remains a gap in utilizing basic semantic guidance of driving scenes for driver saliency prediction. Therefore, this paper employs the CLIP \cite{radford2021learningtransferablevisualmodels} model to extract semantic information from driving scenes. To validate the effectiveness of our proposed method in diverse and dynamic driving scenarios, we conduct experiments across datasets with different weather conditions (TrafficGaze, DrFixD-rainy) and complex semantic information (BDDA).

\subsection{Downstream Tasks}
\subsubsection{Salient Object Detection.} This is an important task in computer vision aimed at detecting the most prominent object regions in an image. In the context of traffic driving scenes, drivers often automatically filter out objects unrelated to the current driving task. Therefore, some existing works use driver attention allocation as prior knowledge to detect prominent or key objects \cite{qin2022id,shi2023fixated}. This approach does not detect all objects in the driving scene but focuses on those most relevant to the current driving task, thereby reducing redundant information.
\subsubsection{Drive Event Recognition.} In the field of intelligent transportation systems, detecting driving events by analyzing driving scene information is one of the key tasks in preventing traffic accidents. Driver visual attention helps identify information relevant to the current driving task while suppressing irrelevant information. Du et al. \cite{du2023causes} established an attention-based driving event dataset (ADED) and proposed a driver attention-guided model that uses driver attention as guidance to better recognize events that cause shifts in driver attention.

In conclusion, we believe that the semantic information of the driving scene is beneficial in identifying the driverâ€™s attention most aligned with the current driving task. Inspired by visual cognition, we have developed an attention prediction model driven by the semantic information of the current driving scene, integrating this semantic information at the deepest level of image feature extraction to guide the driver's attention effectively.
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{model_view_crc_f.pdf}
    \caption{Overview of the proposed SalM$^2$ network. (a) shows the overall network framework, which includes two branches: a ``Bottom-up" branch and a ``Top-down" branch. (b) illustrates the principle of the self-attention mechanism. (c) illustrates the principle of our proposed cross-modal attention mechanism.}
    \label{fig:model_overview}
\end{figure*}