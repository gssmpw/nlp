\section{Related Works}
\label{sec:relatedWorks}

\paragraph{\textnormal{\textbf{Learning Communication in MADRL}}.} Previous works mainly focus on learning efficient and effective communication to improve task-level performance under either CTDE with communication setting (where communication is used in both training and execution) \cite{Wang2020NDQ,Das2019TarMAC}, or DTDE with communication setting \cite{Iqbal2019MAAC,Liu2020G2ANet}. In CTDE with communication, existing research works utilize either a shared Q-function (e.g., DIAL \cite{Foerster2016Comm}), centralized but factorized Q-functions (e.g., NDQ \cite{Wang2020NDQ}, TMC \cite{Zhang2020TMC}, MAIC \cite{Yuan2022MAIC}, MASIA \cite{Guan2023Aggregation}, and T2MAC \cite{Sun2024T2MAC}), or a joint value function (e.g., TarMAC \cite{Das2019TarMAC}, GAMCL \cite{Mao2020GatedACML}, I2C \cite{Ding2020I2C}, and TEM \cite{Guo2023Email}) to enable efficient training of communication architectures. Compared to CTDE with communication, communication in the DTDE setting is under-explored. The existing works mainly rely on actor-critic methods \cite{Jiang2018ATOC,Liu2020G2ANet,Niu2021MAGIC,Chen2024RGMComm}. When communication is allowed between individual critics not actors (i.e., the DCCDA setting), learning communication relies on MAAC \cite{Iqbal2019MAAC} and its variant GAAC \cite{Liu2020G2ANet}. In MAAC, individual agents aggregate and integrate encoded information from other agents into local Q-functions. GAAC proposes to incorporate graph neural networks in the critic (Q) network to effectively combine information from neighboring agents. In addition to DCCDA methods, there are specific DTDE methods that allow communication among individual actors. Concretely, ATOC \cite{Jiang2018ATOC} enables communication in actors and uses non-communicating critics for training. MAGIC \cite{Niu2021MAGIC} allows each agent's critic and actor to share the same neural network components, including the communication architecture. IMAC \cite{Wang2020IMAC} and RGMComm \cite{Chen2024RGMComm} use a setup where each agent integrates encoded messages into its policies and incorporates the actions and observations of all other agents into individual critics, implicitly assuming full observability. In contrast, DCCDA methods do not involve message sharing during execution but learning what or when to communicate local information during training. As a result, DCCDA methods allow decentralized execution without communication and benefit from communication during training.

\paragraph{\textnormal{\textbf{Variance Reduction in MADRL}}.} Variance reduction is an essential topic in MADRL \cite{Tucker2018Baseline,Kuba2021Setting}. Previous works have built a theoretical analysis of the variance in policy gradients without considering learning communication. Lyu et al. \cite{Lyu2021Contrasting,Lyu2023Centralized} theoretically contrast policy gradients under CTDE and DTDE without communication settings and claim that the uncertainty of other agents' observations and actions appeared in centralized Q-functions can increase the variance in policy gradients. One of the most successfully applied and extensively studied methods to reduce variance is known as the \textit{baseline} technique \cite{Wu2018Baseline,Foerster2018COMA,Kuba2021Setting}. Specifically, Wu et al. \cite{Wu2018Baseline} utilizes an action-dependent baseline to consider the influence of different dimensions of actions. Foerster et al. \cite{Foerster2018COMA} introduces a counterfactual baseline that marginalizes out a single agent’s action, while keeping the other agents’ actions fixed. More recently, Kuba et al. \cite{Kuba2021Setting} mathematically analyze the variance of policy gradients under CTDE and quantify how agents contribute to the total variance in Markov games, where states are available to agents. They propose a state-based baseline technique to achieve minimal variance when estimating policy gradients under CTDE. In summary, existing baseline techniques consider the source of variance from the uncertainty in states or actions. In contrast, our baseline technique considers the source of variance from the uncertainty in messages based on partial information. To the best of our knowledge, this is the first work to study variance in policy gradients considering learning communication in decentralized MADRL and partially observable environments, and propose a baseline technique to decrease such variance.