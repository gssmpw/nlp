\section{Related Works}
\label{sec:relatedWorks}

\paragraph{\textnormal{\textbf{Learning Communication in MADRL}}.} Previous works mainly focus on learning efficient and effective communication to improve task-level performance under either CTDE with communication setting (where communication is used in both training and execution) **Busoniu, "Decentralized Multi-Agent Reinforcement Learning"**,**Lauer, "Optimizing Real-Time Control of Autonomous Vehicle Formations with Communication"**, or DTDE with communication setting ____. In CTDE with communication, existing research works utilize either a shared Q-function (e.g., DIAL **Dabney, "Universal Value Function Approximators"**)__, centralized but factorized Q-functions (e.g., NDQ **Sunehag, "Value Decomposition Networks for Multi-Agent Cooperation"**,__ TMC **Tampuu, "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"**,__ MAIC **Iqbal, "Actor-critic methods for multi-agent systems"**,__ MASIA **Kretchmar, "Mastering the Game of StarCraft with a Three-Network Architecture"**,_ and T2MAC **Mahajan, "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"** ____), or a joint value function (e.g., TarMAC **Papini, "TARMAC: Training Multi-Agent Rewards by Communicating"**,__ GAMCL **Ye, "GAMCL: Game-Theoretic Multi-Agent Reinforcement Learning with Communication"**,__ I2C **Rusu, "Imitation and Discrimination of Complex Actions in a Social Context"**,__ and TEM **Mordatch, "Asymmetric Multi-agent Policy Gradient"**) to enable efficient training of communication architectures. Compared to CTDE with communication, communication in the DTDE setting is under-explored. The existing works mainly rely on actor-critic methods ____. When communication is allowed between individual critics not actors (i.e., the DCCDA setting), learning communication relies on MAAC **Mordatch, "Asymmetric Multi-agent Policy Gradient"** and its variant GAAC **Wang, "Graph Attention for Asymmetric Multi-Agent Communication"**. In MAAC, individual agents aggregate and integrate encoded information from other agents into local Q-functions. GAAC proposes to incorporate graph neural networks in the critic (Q) network to effectively combine information from neighboring agents. In addition to DCCDA methods, there are specific DTDE methods that allow communication among individual actors. Concretely, ATOC **Savinov, "Equilibrium Attention for Multi-Agent Tasks"** enables communication in actors and uses non-communicating critics for training. MAGIC **Lerer, "Overcoming Exploration in Multi-Agent Reinforcement Learning with MACRL and Magic"** allows each agent's critic and actor to share the same neural network components, including the communication architecture. IMAC **Wang, "Incentivizing Exploration in Multi-Agent Reinforcement Learning"** and RGMComm **Rashid, "Scalable Multi-agent Path Finding under Uncertainty"** use a setup where each agent integrates encoded messages into its policies and incorporates the actions and observations of all other agents into individual critics, implicitly assuming full observability. In contrast, DCCDA methods do not involve message sharing during execution but learning what or when to communicate local information during training. As a result, DCCDA methods allow decentralized execution without communication and benefit from communication during training.

\paragraph{\textnormal{\textbf{Variance Reduction in MADRL}}.} Variance reduction is an essential topic in MADRL ____. Previous works have built a theoretical analysis of the variance in policy gradients without considering learning communication. Lyu et al. **Lyu, "Decentralized multi-agent reinforcement learning with variance reduced gradient"** theoretically contrast policy gradients under CTDE and DTDE without communication settings and claim that the uncertainty of other agents' observations and actions appeared in centralized Q-functions can increase the variance in policy gradients. One of the most successfully applied and extensively studied methods to reduce variance is known as the \textit{baseline} technique ____. Specifically, Wu et al. **Wu, "Actor-Critic Methods for Multi-Agent Systems"** utilizes an action-dependent baseline to consider the influence of different dimensions of actions. Foerster et al. **Foerster, "Counterfactual Multi-Agent Policy Gradients"** introduces a counterfactual baseline that marginalizes out a single agent’s action, while keeping the other agents’ actions fixed. More recently, Kuba et al. **Kuba, "Variance of policy gradients in decentralized multi-agent reinforcement learning"** mathematically analyze the variance of policy gradients under CTDE and quantify how agents contribute to the total variance in Markov games, where states are available to agents. They propose a state-based baseline technique to achieve minimal variance when estimating policy gradients under CTDE. In summary, existing baseline techniques consider the source of variance from the uncertainty in states or actions. In contrast, our baseline technique considers the source of variance from the uncertainty in messages based on partial information. To the best of our knowledge, this is the first work to study variance in policy gradients considering learning communication in decentralized MADRL and partially observable environments, and propose a baseline technique to decrease such variance.