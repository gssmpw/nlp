\section{Related Works}
\label{sec:relatedWorks}

\paragraph{\textnormal{\textbf{Learning Communication in MADRL}}.} Previous works mainly focus on learning efficient and effective communication to improve task-level performance under either CTDE with communication setting (where communication is used in both training and execution) ____, or DTDE with communication setting ____. In CTDE with communication, existing research works utilize either a shared Q-function (e.g., DIAL ____), centralized but factorized Q-functions (e.g., NDQ ____, TMC ____, MAIC ____, MASIA ____, and T2MAC ____), or a joint value function (e.g., TarMAC ____, GAMCL ____, I2C ____, and TEM ____) to enable efficient training of communication architectures. Compared to CTDE with communication, communication in the DTDE setting is under-explored. The existing works mainly rely on actor-critic methods ____. When communication is allowed between individual critics not actors (i.e., the DCCDA setting), learning communication relies on MAAC ____ and its variant GAAC ____. In MAAC, individual agents aggregate and integrate encoded information from other agents into local Q-functions. GAAC proposes to incorporate graph neural networks in the critic (Q) network to effectively combine information from neighboring agents. In addition to DCCDA methods, there are specific DTDE methods that allow communication among individual actors. Concretely, ATOC ____ enables communication in actors and uses non-communicating critics for training. MAGIC ____ allows each agent's critic and actor to share the same neural network components, including the communication architecture. IMAC ____ and RGMComm ____ use a setup where each agent integrates encoded messages into its policies and incorporates the actions and observations of all other agents into individual critics, implicitly assuming full observability. In contrast, DCCDA methods do not involve message sharing during execution but learning what or when to communicate local information during training. As a result, DCCDA methods allow decentralized execution without communication and benefit from communication during training.

\paragraph{\textnormal{\textbf{Variance Reduction in MADRL}}.} Variance reduction is an essential topic in MADRL ____. Previous works have built a theoretical analysis of the variance in policy gradients without considering learning communication. Lyu et al. ____ theoretically contrast policy gradients under CTDE and DTDE without communication settings and claim that the uncertainty of other agents' observations and actions appeared in centralized Q-functions can increase the variance in policy gradients. One of the most successfully applied and extensively studied methods to reduce variance is known as the \textit{baseline} technique ____. Specifically, Wu et al. ____ utilizes an action-dependent baseline to consider the influence of different dimensions of actions. Foerster et al. ____ introduces a counterfactual baseline that marginalizes out a single agent’s action, while keeping the other agents’ actions fixed. More recently, Kuba et al. ____ mathematically analyze the variance of policy gradients under CTDE and quantify how agents contribute to the total variance in Markov games, where states are available to agents. They propose a state-based baseline technique to achieve minimal variance when estimating policy gradients under CTDE. In summary, existing baseline techniques consider the source of variance from the uncertainty in states or actions. In contrast, our baseline technique considers the source of variance from the uncertainty in messages based on partial information. To the best of our knowledge, this is the first work to study variance in policy gradients considering learning communication in decentralized MADRL and partially observable environments, and propose a baseline technique to decrease such variance.