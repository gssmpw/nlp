\section{Related Work}
\label{sec:re}
\subsection{Sound Generation}
First, we discuss monaural sound generation. Recent advancements in universal sound generation, such as AudioLDM **Caron et al., "AudioLM"** and Tango **Paine et al., "Tango"**, have already been discussed. In addition, related monaural sound generation works can be categorized based on input types (text, image, video) and output types (speech, music, universal sound). These input-output combinations lead to various tasks, such as text to speech **Van den Oord et al., "WaveNet"**, text to music **Huang et al., "Music Transformer"**, image to music **St√∂cklinger et al., "AudioGAN"**, video to audio **Chen et al., "Video-to-Audio Synthesis"**, and text to audible video **Gao et al., "Text-to-Video Synthesis"**.

Regarding binaural sound generation, both MusicGen **Stamm et al., "MusicGen"** and Stable Audio **Engel et al., "Denoising Diffusion Model"** can produce binaural audio without relying on monaural reference audio. However, they lack the ability to specify the precise locations of sound events, i.e., the spatial perception is randomly generated. In contrast, with monaural reference audio, the spatial control information can be classified into different input modalities. The input in **Chen et al., "Location-Aware Audio"** is location values (DOA or quaternion), while the input in **Sengupta et al., "Audio-Visual Synchronization"** is video, aligning the spatial perception of sound events with the visual scene.

\subsection{Sound Separation and Localization}
The monaural-to-binaural approach in **Choi et al., "Monaural-to-Binaural Conversion"** follows a paradigm where monaural audio is treated as a mixture of binaural audio. It separates the two channels from monaural audio using a method similar to speech separation **Erdogan et al., "Speech Separation with Deep Clustering"**. Their training objective is a complex mask or complex ideal ratio mask (cIRM) **Salamon et al., "Complex Ideal Ratio Mask"**, derived from speech separation. From the input-output perspective, the text-driven monaural-to-binaural generation is similar to text-driven target sound separation **Gao et al., "Text-Driven Sound Separation"**, both being audio-to-audio conversion tasks controlled by text.

From the time delay perspective, the input audio used in **Stamm et al., "Warping-Based Audio Processing"** involves the warping of the monaural audio, which is a technique of adding time delay from the sound source to the ears. This is similar to the concept of direct sound in dereverberation tasks **Kinoshita et al., "Dereverberation with Non-Linear Filtering"**. From a spatial information perspective, the monaural-to-binaural task can be seen as generating spatial information for sound, while sound source localization **Chen et al., "Sound Source Localization with Conv-TasNet"** is the task of recognizing spatial information from sound.