\section{Experiments}
\label{sec:experiments}


\begin{table*}[ht]
\centering
\caption{\textbf{Top1 accuracy of zero-shot image classification on the OOD benchmark} when using the default hard prompt.
The results of CoCoOp are obtained from the TPT paper, while others are reproduced with our code. 
The best results under zero-shot conditions are highlighted in \textbf{bold}. Performance improvements over the zero-shot CLIP-ViT-B/16 are indicated with an upward blue arrow {\textcolor{blue}{($\uparrow$blue)}} and a downward red arrow {\textcolor{red}{($\downarrow$red)}}.}
\label{tab:imagenets_result}
\begin{adjustbox}{width=0.9\textwidth}
\begin{tabular}{lccccccc}
\toprule
\rowcolor{gray!10} \textbf{Method} & \textbf{ImageNet} & \textbf{ImageNet-A} & \textbf{ImageNet-V2} & \textbf{ImageNet-R} & \textbf{ImageNet-Sketch} & \textbf{Average} & \textbf{OOD Avg.} \\
\midrule
CLIP-ViT-B/16 & 66.71 & 47.80 & 60.63 & 73.99 & 46.15 & 59.06 & 57.14 \\
\midrule
CoOp \citep{zhou2022learning} & 71.75 & 50.13 & 64.51 & 75.28 & 47.92 & 61.92 & 59.46 \\
CoCoOp \citep{zhou2022conditional} & 71.02 & 50.63 & 64.07 & 76.18 & 48.75 & 62.13 & 59.91 \\
\midrule
TPT \citep{shu2022test} & 69.02 & 54.73 & 63.70 & 77.15 & 47.99 & 62.52 & 60.89 \\ 
C-TPT \citep{yoon2024c} & 68.50 & 51.60 & 62.70 & 76.00 & 47.90 & 61.34 & 59.55 \\
MTA \citep{zanella2024test} & 69.23 & 56.87 & 63.67 & 76.88 & 48.54 & 63.04 & 61.49 \\
\midrule
\nameie & 64.26 & 56.31 & 59.70 & 75.89 & 47.65 & 60.76 & 59.89 \\ 
\rowcolor{blue!10} \textbf{\namemem} (Ours) & 69.21\textsubscript{\textcolor{blue}{($\uparrow$2.49)}} & \textbf{60.57}\textsubscript{\textcolor{blue}{($\uparrow$12.77)}} & 64.28\textsubscript{\textcolor{blue}{($\uparrow$3.65)}} & 77.53\textsubscript{\textcolor{blue}{($\uparrow$3.54)}} & 48.73\textsubscript{\textcolor{blue}{($\uparrow$2.57)}} & 64.06\textsubscript{\textcolor{blue}{($\uparrow$5.01)}} & 62.78\textsubscript{\textcolor{blue}{($\uparrow$5.64)}} \\
\rowcolor{blue!5} \textbf{\namemae} (Ours) & 66.27\textsubscript{\textcolor{red}{($\downarrow$0.45)}} & 52.55\textsubscript{\textcolor{blue}{($\uparrow$4.75)}} & 60.87\textsubscript{\textcolor{blue}{($\uparrow$0.24)}} & 75.57\textsubscript{\textcolor{blue}{($\uparrow$1.58)}} & 47.01\textsubscript{\textcolor{blue}{($\uparrow$0.85)}} & 60.45\textsubscript{\textcolor{blue}{($\uparrow$1.39)}} & 59.00\textsubscript{\textcolor{blue}{($\uparrow$1.86)}} \\
\rowcolor{blue!15} \textbf{\name} (Ours) & \textbf{69.40}\textsubscript{\textcolor{blue}{($\uparrow$2.68)}} & 60.52\textsubscript{\textcolor{blue}{($\uparrow$12.72)}} & \textbf{64.43}\textsubscript{\textcolor{blue}{($\uparrow$3.80)}} & \textbf{77.84}\textsubscript{\textcolor{blue}{($\uparrow$3.85)}} & \textbf{48.94}\textsubscript{\textcolor{blue}{($\uparrow$2.79)}} & \textbf{64.23}\textsubscript{\textcolor{blue}{($\uparrow$5.17)}} & \textbf{62.93}\textsubscript{\textcolor{blue}{($\uparrow$5.79)}} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

This section reports benchmark results for zero-shot image classification, comparing our proposed approach with previous methods.
Following prior work \citep{shu2022test,feng2023diverse,karmanov2024efficient},  we evaluate out-of-distribution (OOD) performance on 4 datasets derived from ImageNet and fine-grained (FG) classification on 10 datasets spanning various categories.

\subsection{Experimental setup}
\label{dataset}

\textbf{Datasets.}\hspace{5mm}
For the OOD benchmark, we use ImageNet \citep{deng2009imagenet} and its variants including ImageNet-A \citep{hendrycks2021natural}, ImageNet-V2 \citep{recht2019imagenet}, ImageNet-R \citep{hendrycks2021many}, and ImageNet-Sketch \citep{wang2019learning}, to evaluate robustness against 4 out-of-distribution patterns derived from ImageNet.
Additionally, we evaluate 10 different datasets for the fine-grained benchmark. These datasets encompass a wide range of categories, including plants and animals (Flower102 \citep{nilsback2008automated}, OxfordPets \citep{parkhi2012cats}), scene recognition (SUN397 \citep{xiao2010sun}), textures (DTD \citep{cimpoi2014describing}), food (Food101 \citep{bossard2014food}), transportation (StanfordCars \citep{krause20133d}, Aircraft \citep{maji2013fine}), human actions (UCF101 \citep{soomro2012ucf101}), satellite images (EuroSAT \citep{helber2019eurosat}), and general objects (Caltech101 \citep{li2022caltech}). This benchmark assesses the applicability of our TTT method across a diverse range of categories.
% The details of each dataset are provided in Appendix \ref{sec:datasets}.
\newline

\noindent\textbf{Baselines.}\hspace{5mm}
We compare our method with the baseline CLIP-ViT-B/16 and few-shot learning methods — CoOp \citep{zhou2022learning}, CoCoOp \citep{zhou2022conditional} — as well as existing test-time prompt tuning methods — TPT \citep{shu2022test} and C-TPT \citep{yoon2024c}.
Additionally, we compare \nameie, which tunes the image encoder parameters without relying on LoRA, and MTA \citep{zanella2024test}, which operates without backpropagation.
For a fair comparison, we focus on methods that do not rely on external models or cached data.
\newline

\noindent\textbf{Implementation details.}\hspace{5mm}
We adopt the pre-trained CLIP-ViT-B/16 as the common backbone architecture.
In text prompt tuning methods, the number of trainable text tokens is set to 4, with initial weights based on the prompt ``a photo of a''.
We prepare three versions as precomputed text prompts: the default hard prompt ``a photo of a'' to match the initialization commonly used in text prompt tuning, an ensemble of 80 different hand-crafted prompts \cite{radford2021learning}, and CoOp \citep{zhou2022learning}.
The weights of CoOp are pre-trained on ImageNet with 16 shots and 4 tokens.
LoRA is applied exclusively to the transformer architecture in layers 11 and 12 of the image encoder with a rank of 16 targeting the key, query, value, and output matrices.
The LoRA scale $\gamma$ is set to 12 for the OOD benchmark and 2 for the fine-grained benchmark.
Matrix A of LoRA is initialized using Kaiming-uniform \citep{he2015delving}, while matrix B is initialized to zero.
For the test-time loss variants of our method, \textbf{\namemem} uses the MEM loss only, \textbf{\namemae} uses the MAE loss only, and \textbf{\name} combines the two losses with the weights set to $\lambda_1 = 1$ and $\lambda_2 = 16$.
We optimize the LoRA parameters in a single step, using the AdamW optimizer \citep{loshchilov2017decoupled} with a learning rate of 0.001 and a weight decay value of 0.2.
For \nameie, we use the same optimizer and loss settings as in \textbf{\name} for a fair comparison, directly tuning the parameters of the key, query, value, and output matrices in the transformer layers 11 and 12 of the image encoder.
Data augmentation follows TPT by expanding a single instance into a 64-batch using random resized crops, including the original instance.
Additionally, the top 10\% of high-confidence samples from the batch of 64 are selected to compute the test losses and filter out noisy views.
All the experiments are conducted using a single NVIDIA RTX 3090 GPU with 24GB of memory.


\subsection{Results}
\begin{table*}[h]
\centering
\caption{\textbf{Top-1 accuracy of zero-shot image classification on the fine-grained benchmark} using the default hard prompt. CoCoOp is from the TPT paper, while others are reproduced with our code. The best results under zero-shot conditions are in \textbf{bold}. Performance improvements over zero-shot CLIP-ViT-B/16 are indicated with an upward blue arrow {\textcolor{blue}{($\uparrow$blue)}} and a downward red arrow {\textcolor{red}{($\downarrow$red)}}.}
\label{tab:fg_result}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lccccccccccc}
\toprule
\rowcolor{gray!10} \textbf{Method}  & \textbf{Flower102} & \textbf{DTD} & \textbf{Pets} & \textbf{Cars} & \textbf{UCF101} & \textbf{Caltech} & \textbf{Food101} & \textbf{SUN397} & \textbf{Aircraft} & \textbf{EuroSAT} & \textbf{FG Avg.} \\
\midrule
CLIP-ViT-B/16 & 67.40 & 44.39 & 88.25 & 65.51 & 65.24 & 93.31 & 83.64 & 62.56 & 23.91 & 42.22 & 63.64 \\
\midrule
CoOp \citep{zhou2022learning} & 68.30 & 42.34 & 89.35 & 63.30 & 67.19 & 92.85 & 83.72 & 64.53 & 19.96 & 40.19 & 63.17 \\
CoCoOp \citep{zhou2022conditional} & 70.85 & 45.45 & 90.46 & 64.90 & 68.44 & 93.79 & 83.97 & 66.89 & 22.29 & 39.23 & 64.63 \\
\midrule
TPT \citep{shu2022test} & 68.98 & 45.92 & 87.27 & 67.02 & \textbf{68.99} & 93.55 & \textbf{85.00} & \textbf{65.11} & 23.76 & 43.44 & 64.91 \\ 
C-TPT \citep{yoon2024c} & \textbf{69.67} & 44.80 & 88.47 & 65.97 & 65.27 & 93.35 & 83.23 & 64.28 & 23.97 & 42.21 & 64.12 \\
MTA \citep{zanella2024test} & 68.29 & 45.33 & 88.17 & \textbf{68.08} & 68.07 & \textbf{94.12} & 84.88 & 64.72 & 25.38 & 40.91 & 64.79 \\
\midrule
\nameie & 59.03 & 42.91 & 83.81 & 66.60 & 67.57 & 88.64 & 82.11 & 62.71 & 23.67 & 36.67 & 61.37 \\

\rowcolor{blue!10}\textbf{\namemem} (Ours) & 67.60\textsubscript{\textcolor{blue}{($\uparrow$0.20)}} & \textbf{46.04}\textsubscript{\textcolor{blue}{($\uparrow$1.65)}} & 87.11\textsubscript{\textcolor{red}{($\downarrow$1.14)}} & 67.81\textsubscript{\textcolor{blue}{($\uparrow$2.30)}} & 68.38\textsubscript{\textcolor{blue}{($\uparrow$3.15)}} & 93.59\textsubscript{\textcolor{blue}{($\uparrow$0.28)}} & 84.83\textsubscript{\textcolor{blue}{($\uparrow$1.19)}} & 64.61\textsubscript{\textcolor{blue}{($\uparrow$2.05)}} & 25.68\textsubscript{\textcolor{blue}{($\uparrow$1.77)}} & 39.27\textsubscript{\textcolor{red}{($\downarrow$2.95)}} & 64.49\textsubscript{\textcolor{blue}{($\uparrow$0.85)}} \\

\rowcolor{blue!5}\textbf{\namemae} (Ours) & 68.33\textsubscript{\textcolor{blue}{($\uparrow$0.93)}} & 45.21\textsubscript{\textcolor{blue}{($\uparrow$0.83)}} & \textbf{88.72}\textsubscript{\textcolor{blue}{($\uparrow$0.46)}} & 66.94\textsubscript{\textcolor{blue}{($\uparrow$1.43)}} & 66.35\textsubscript{\textcolor{blue}{($\uparrow$1.11)}} & 93.71\textsubscript{\textcolor{blue}{($\uparrow$0.41)}} & 84.39\textsubscript{\textcolor{blue}{($\uparrow$0.75)}} & 63.63\textsubscript{\textcolor{blue}{($\uparrow$1.07)}} & 25.38\textsubscript{\textcolor{blue}{($\uparrow$1.47)}} & \textbf{44.52}\textsubscript{\textcolor{blue}{($\uparrow$2.30)}} & 64.72\textsubscript{\textcolor{blue}{($\uparrow$1.08)}} \\

\rowcolor{blue!15}\textbf{\name} (Ours) & 67.88\textsubscript{\textcolor{blue}{($\uparrow$0.49)}} & 45.86\textsubscript{\textcolor{blue}{($\uparrow$1.48)}} & 87.63\textsubscript{\textcolor{red}{($\downarrow$0.63)}} & 67.72\textsubscript{\textcolor{blue}{($\uparrow$2.20)}} & 68.38\textsubscript{\textcolor{blue}{($\uparrow$3.15)}} & 93.83\textsubscript{\textcolor{blue}{($\uparrow$0.53)}} & 84.99\textsubscript{\textcolor{blue}{($\uparrow$1.35)}} & 64.59\textsubscript{\textcolor{blue}{($\uparrow$2.03)}} & \textbf{25.92}\textsubscript{\textcolor{blue}{($\uparrow$2.01)}} & 43.23\textsubscript{\textcolor{blue}{($\uparrow$1.01)}} & \textbf{65.00}\textsubscript{\textcolor{blue}{($\uparrow$1.36)}} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


\begin{table}[t]
\centering
\caption{\textbf{Generalization in prompts.} Performance differences from each single hard prompt are indicated with an upward blue arrow {\textcolor{blue}{($\uparrow$blue)}} and a downward red arrow {\textcolor{red}{($\downarrow$red)}}.}
\label{tab:prompt}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{lccc}
\toprule
\rowcolor{gray!10} \textbf{Method} & \textbf{ImageNet} & \textbf{OOD Average} & \textbf{FG Average} \\
\midrule
CLIP-ViT-B/16 + Ensemble & 68.31\textsubscript{\textcolor{blue}{($\uparrow$1.59)}} & 59.52\textsubscript{\textcolor{blue}{($\uparrow$2.38)}} & 64.68\textsubscript{\textcolor{blue}{($\uparrow$1.04)}} \\
TPT + Ensemble & 67.21\textsubscript{\textcolor{red}{($\downarrow$1.81)}} & 59.93\textsubscript{\textcolor{red}{($\downarrow$0.96)}} & 63.37\textsubscript{\textcolor{red}{($\downarrow$1.54)}} \\ 
\rowcolor{blue!10}\textbf{\name + Ensemble} (Ours) & \textbf{70.67}\textsubscript{\textcolor{blue}{($\uparrow$1.27)}} & \textbf{64.99}\textsubscript{\textcolor{blue}{($\uparrow$2.06)}} & \textbf{65.95}\textsubscript{\textcolor{blue}{($\uparrow$0.95)}} \\
\midrule
CLIP-ViT-B/16 + CoOp & 71.75\textsubscript{\textcolor{blue}{($\uparrow$5.03)}} & 59.46\textsubscript{\textcolor{blue}{($\uparrow$2.32)}} & 63.17\textsubscript{\textcolor{red}{($\downarrow$0.47)}}\\
TPT + CoOp & 73.63\textsubscript{\textcolor{blue}{($\uparrow$4.62)}} & 62.98\textsubscript{\textcolor{blue}{($\uparrow$2.09)}} & 63.95\textsubscript{\textcolor{red}{($\downarrow$0.95)}}\\
\rowcolor{blue!10}\textbf{\name + CoOp} (Ours) & \textbf{74.03}\textsubscript{\textcolor{blue}{($\uparrow$4.63)}} & \textbf{64.35}\textsubscript{\textcolor{blue}{($\uparrow$1.42)}} & \textbf{64.00}\textsubscript{\textcolor{red}{($\downarrow$1.01)}}\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\textbf{Zero-shot classification.}\hspace{5mm}
In \cref{tab:imagenets_result} and \cref{tab:fg_result}, \name\ enhances the zero-shot generalization capacity of CLIP-ViT-B/16, outperforming both few-shot learning and text prompt tuning methods on average across the two benchmark evaluations. Notably, it significantly surpasses existing methods on the OOD benchmark, demonstrating that it is a highly effective approach for adapting to various domains.
% \name\ improves the zero-shot generalization capacity of CLIP-ViT-B/16, outperforming both the few-shot learning and the text prompt tuning methods on average across the two benchmark evaluations, demonstrating that it is a highly versatile approach capable of adapting to various domains.
\name\ achieves state-of-the-art performance among methods that do not rely on domain knowledge, additional models, or cache.
Despite \nameie\ failing to adapt effectively to the fine-grained benchmark, \name\ consistently delivers significantly better results, showing improvements of 3.04\% in OOD and 3.63\% in fine-grained averages compared to \nameie.
This confirms that LoRA tuning is effective for adapting to domain gaps in VLMs, helping to prevent forgetting and making it a suitable approach for TTT.

In the comparison of the two test-time losses, \namemem\ and \namemae, both methods show performance improvements over the baseline.
In \cref{tab:fg_result}, \namemae\ (\ie, MAE loss) achieves comparable or better results than \namemem\ (\ie, MEM loss) across a wide range of category domains in the fine-grained benchmark, particularly demonstrating the high versatility of MAE as a test-time loss in VLMs.
Although our use of MAE loss involves only an image encoder without a decoder, it demonstrates that restoring global features of masked images contributes to understanding image context in VLMs.
Combining the two losses further enhances both versatility and performance, surpassing TPT in the fine-grained benchmark.
For example, a comparison of TPT with \namemem\ on EuroSAT highlights the importance of tuning the text prompt when using only the MEM loss, but the combination with the MAE loss helps overcome this weakness.
\newline


\noindent\textbf{Generalization in prompts.}\hspace{5mm}
\cref{tab:prompt} shows the generalization performance when using either the ensemble of text prompts or CoOp.
TPT is designed to initialize a single hard prompt, making it not straightforward to combine with the ensemble of prompts.
In our implementation of TPT, we initialize the prompt using the average of embeddings from the text prompts.
By combining our method with the ensemble, we achieve performance improvements comparable to those of CLIP-ViT-B/16 with the ensemble, significantly surpassing the state-of-the-art in text prompt tuning while maintaining zero-shot conditions.
Even when combined with CoOp, a few-shot learning approach, \name\ shows a similar trend to CLIP-ViT-B/16 + CoOp, indicating that our method improves performance independently of the text prompt.
Our method focuses solely on tuning the parameters within the image encoder, allowing for greater flexibility in designing text prompts, an advantage in practical applications where the ability to freely choose prompts is beneficial \citep{gu2023systematic}.
\newline

\noindent\textbf{Calibration.}\hspace{5mm}
As shown in \cref{fig:calib_telora_m}, $\mathcal{L}_{\text{MEM}}$, commonly used as a test-time loss in VLMs, is known to induce overconfidence \citep{guo2017calibration,yoon2024c}, where the model's predicted confidence exceeds its actual accuracy.
Calibration is quantified using Expected Calibration Error (ECE) \citep{naeini2015obtaining}, which measures alignment between predicted probabilities and actual outcomes.
\cref{tab:calib} compares the ECE for our loss functions, TPT, and C-TPT.
Since \namemae\ (\ie, MAE loss) is not explicitly designed based on confidence, it retains its original calibration properties.
\namemae\ achieves ECE performance comparable to or better than C-TPT, without any specific efforts for calibration, even though C-TPT is designed to improve the calibration performance of TPT.
From a TTT perspective, the MAE loss demonstrates advantages not only in generalization across various domains but also in calibration, which is essential given the critical role of prediction uncertainty in real-world applications such as healthcare diagnostics \citep{wang2022medclip,liu2023clip} and autonomous vehicles \citep{dorbala2022clip,khandelwal2022simple}.
\newline

\noindent\textbf{Efficiency.}\hspace{5mm} 
Runtime and memory consumption, shown in \cref{fig:runtime} and \cref{fig:memory}, demonstrate the efficiency of our approach compared to TPT.
Our method precomputes text features and eliminates the need for a text encoder during TTT.
This reduces model size and shortens forward pass times both before and after optimization compared to TPT where text encoding often serves as a bottleneck.
Although \name\ has a substantially larger number of trainable parameters than TPT, it limits LoRA application to the deeper layers of the image encoder, reducing memory usage during backpropagation.
Notably, \namemae\ significantly reduces memory usage during backpropagation, primarily by calculating the loss from only 10\% of the augmented images and masking half of the tokens.
Consequently, it requires minimal additional resources even when the MAE loss is included in the total loss.
\name\ achieves higher efficiency than TPT while delivering significantly superior performance.
Based on these results, \name\ is adaptable to a wide range of domains and applications, including streaming data processing \citep{wang2023test,azimi2022self}, from high-stakes environments \citep{wang2022medclip,liu2023clip,dorbala2022clip,khandelwal2022simple} to memory-constrained edge devices \citep{cai2020tinytl,song2023ecotta}.


\begin{table}[t]
\centering
\caption{\textbf{Expected Calibration Error} ($\downarrow$).}
\label{tab:calib}
\begin{adjustbox}{width=0.8\linewidth}
\begin{tabular}{lccc}
\toprule
\rowcolor{gray!10} \textbf{Method} & \textbf{ImageNet} & \textbf{OOD Average} & \textbf{FG Average} \\
\midrule
CLIP-ViT-B/16 & 1.93 & 4.80 & 4.53 \\
\midrule
TPT & 10.61 & 12.08 & 11.71 \\
C-TPT & 3.11 & 5.38 & 5.29 \\
\midrule
\textbf{\namemem} & 20.32 & 22.76 & 19.73 \\
\rowcolor{blue!10} \textbf{\namemae} & 2.97 & 5.59 & 4.80 \\
\textbf{\name} & 14.04 & 16.49 & 12.75 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{figure}[t]
  \centering
    \begin{minipage}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figure/calib/CLIP-ViT-B-16.pdf}
        \subcaption{CLIP-ViT-B/16}
        \label{fig:calib_baseline}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figure/calib/LoRA-TTT-M.pdf}
        \subcaption{$\mathcal{L}_{\text{MEM}}$}
        \label{fig:calib_telora_m}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figure/calib/LoRA-TTT-A.pdf}
        \subcaption{$\mathcal{L}_{\text{MAE}}$}
        \label{fig:calib_telora_a}
    \end{minipage} 
    \caption{\textbf{Comparison of calibration performance on the Cars dataset.} The MAE loss can improve performance while preserving the baseline model's output characteristics.}
    \label{fig:calib}
\end{figure}


\begin{comment}
\noindent\textbf{Efficiency.}\hspace{5mm} 
Runtime and memory consumption, shown in \cref{fig:runtime} and \cref{fig:memory}, demonstrate the efficiency of our approach compared to TPT.
Our method precomputes text features and eliminates the need for a text encoder during TTT.
This reduces model size and shortens forward pass times both before and after optimization compared to TPT where text encoding often serves as a bottleneck.
Although \name\ has a substantially larger number of trainable parameters than TPT, it limits LoRA application to the deeper layers of the image encoder, reducing memory usage during backpropagation.
Notably, \namemae\ significantly reduces memory usage during backpropagation, primarily by calculating the loss from only 10\% of the augmented images and masking half of the tokens.
Consequently, it requires minimal additional resources even when the MAE loss is included in the total loss.
\name\ achieves higher efficiency than TPT while delivering significantly superior performance.
Based on these results, \name\ is adaptable to a wide range of domains and applications, including streaming data processing \citep{wang2023test,azimi2022self}, from high-stakes environments \citep{wang2022medclip,liu2023clip,dorbala2022clip,khandelwal2022simple} to memory-constrained edge devices \citep{cai2020tinytl,song2023ecotta}.
\end{comment}

\begin{figure}[t]
  \centering
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figure/efficiency/runtime.pdf}
        \subcaption{Runtime}
        \label{fig:runtime}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figure/efficiency/memory.pdf}
        \subcaption{Memory Consumption}
        \label{fig:memory}
    \end{minipage}
    \caption{\textbf{TTT efficiency in ImageNet evaluation.} The efficiency of TPT heavily depends on the number of classes and input text tokens. To optimize TPT, the number of tokens is set to 20, including the 4 learnable tokens, matching the length of the longest class name in the dataset.}
    \label{fig:efficiency}
\end{figure}