\section{Related Work}
\label{sec:related_works}

\noindent\textbf{Test-Time Training (TTT)} allows models to adapt to distribution shifts between training and test data during inference through dynamic parameter updates \citep{liang2024comprehensive,wang2024search,chen2022contrastive}.
The challenges in this area lie in designing an effective test-time objective without labels and developing an efficient system suitable for real-world deployment.
For example, TENT \citep{wang2020tent} tunes batch normalization statistics at test time using entropy loss; however, this approach requires batch processing rather than instance-level processing, making it challenging to handle sequential data in real-time.
Sun \etal~\cite{sun2020test} and Gandelsman \etal~\cite{gandelsman2022test} update the image encoder by introducing auxiliary tasks and applying self-supervision; however, these methods require fine-tuning the model with auxiliary tasks beforehand for TTT.
% For VLMs, methods that primarily tune text prompts \citep{zhou2022learning,zhou2022conditional}, visual prompts \citep{jia2022visual,bahng2022exploring}, or both \citep{khattak2023maple} have been proposed to address distribution shift, owing to their simplicity and effectiveness.
For VLMs, TPT \citep{shu2022test} focuses on optimizing a text prompt at test time, valued for its simplicity and effectiveness.
It demonstrates that augmenting a single test instance and calculating marginal entropy minimization \citep{zhang2022memo} serves as an effective loss for VLMs.
DiffTPT \citep{feng2023diverse} utilizes stable diffusion to enhance data augmentation quality, while C-TPT \citep{yoon2024c} is a technique that calibrates TPT to improve reliability.
RLCF \citep{zhao2023test} tunes the image encoder and demonstrates that CLIP-ViT-B can achieve performance comparable to CLIP-ViT-L but requires CLIP-ViT-L as a feedback source, which poses challenges in memory-constrained environments.
Our method requires no external resources and remains feasible even in closed memory-constrained environments such as edge devices.
\newline

\noindent\textbf{Application of Low-rank adaptation (LoRA)} aims to achieve efficient fine-tuning of large models with vast numbers of parameters in memory-constrained environments by introducing trainable low-rank matrices into each layer of the Transformer architecture, allowing the pre-trained parameters to remain frozen \citep{hu2021lora,han2024parameter,xin2024parameter}.
MeLo \citep{zhu2024melo} demonstrates that applying LoRA to vision transformers  (ViT) for downstream medical image diagnosis achieves comparable performance to fully fine-tuned ViT models while significantly reducing memory consumption.
CLIP-LoRA \citep{zanella2024low} demonstrate significant performance improvements in few-shot learning by applying LoRA to the vision encoder of CLIP.
However, CLIP-LoRA requires a few labeled samples from the target downstream task.
To the best of our knowledge, no approach has applied LoRA for TTT in VLMs. 
