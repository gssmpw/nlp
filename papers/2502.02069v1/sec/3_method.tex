\section{Method}

\begin{figure*}[!htbp]
\centering
\includegraphics[width=1.0\textwidth]{figure/method/method.pdf}
\caption{\textbf{\name} for zero-shot image classification. Our method applies LoRA to the layers of the image encoder in VLMs. \name\ updates the LoRA parameters using MEM loss and MAE loss, calculated from the top 10\% of high-confidence augmented views. This approach allows adaptation to domain shifts with low memory consumption while maintaining generalization ability.}
\label{fig:method}
\end{figure*}

\subsection{Preliminaries}

\textbf{Contrastive Language-Image Pre-training (CLIP).}\hspace{5mm}
Pre-training VLMs is typically conducted with specific vision-language objectives that facilitate learning image-text correspondences from extensive image and text datasets \citep{radford2021learning,yu2022coca,yao2021filip}.
CLIP \citep{radford2021learning} consists of an image encoder $g$ and a text encoder $f$, pre-trained using a contrastive loss to align the embeddings from both encoders in a shared feature space.
After pre-training, CLIP exhibits strong zero-shot capabilities across various downstream tasks \citep{radford2021learning,minderer2022simple,wang2021actionclip,li2022language}.
For zero-shot image classification with CLIP, input text prompts are created by inserting each of the K-class category labels $\sY = {y_1, y_2, \dots, y_K}$ into a prompt prefix $\vp$ (e.g., $\vp_i$ = "a photo of a $y_i$").
% it creates input text prompts by adding each of the K-class category labels $\sY = \{y_1, y_2, \dots, y_K\}$ into a prompt prefix $\vp$ (\eg, $\vp_i$ =``a photo of a $y_i$'').
The image $X$ and the text input $\vp_i$ representing the $i$-th class are encoded into $\vv=g(X)$ and $\vt_i=f(\vp_i)$ by their respective encoders. The classification score for the $i$-th class of image $X$ is then calculated by
\begin{equation}
p(y_i\mid X) = \frac{\exp(\text{cos}(\vt_i \cdot \vv) / \tau)}{\sum_{i=1}^{K} \exp(\text{cos}(\vt_i \cdot \vv) / \tau)}
\end{equation}
in zero-shot fashion, where $\tau$ is the temperature parameter and $\text{cos}(\cdot, \cdot)$ represents the cosine similarity.
\newline

\noindent\textbf{TPT} \citep{shu2022test} makes the text prompt prefix trainable $\vp \in \mathbb{R}^{L \times D}$ instead of using pre-determined, hand-crafted prompt, where $L$ represents the number of tokens and $D$ is the embedding size of the text encoder.
TPT optimizes the learnable text prompt using backpropagation with Marginal Entropy Minimization (MEM) loss \citep{zhang2022memo} from each test instance.
A test image instance undergoes data augmentation $N$ times $\{X_n\}_{n=1}^{N}$, and the top $N_p$ samples with the highest confidence are selected to calculate the MEM loss, filtering out noisy augmented views.
The entropy of the predicted probability distribution over the K-classes is calculated by
\begin{equation}
% \mathcal{L}_{\text{MEM}} = H\left(\bar{p}(\cdot \mid X)\right) = -\sum_{i=1}^{K} \bar{p}(y_i \mid X) \log(\bar{p}(y_i \mid X)),
\mathcal{L}_{\text{MEM}} = -\sum_{i=1}^{K} \bar{p}(y_i \mid X) \log(\bar{p}(y_i \mid X)),
\end{equation}
where
$\bar{p}(\cdot \mid X) = \frac{1}{N_p} \sum_{n=1}^{N_p} p(\cdot \mid X_n)$.
MEM loss has become the de facto standard in modern TTT for VLMs. \citep{farina2024frustratingly}.
\newline

\noindent\textbf{LoRA} \citep{hu2021lora} represents the incremental adjustment of pre-trained weights by injecting two small matrices, based on the concept of the intrinsic rank of downstream domain shifts \citep{aghajanyan2020intrinsic}.
Given that $\mW_0 \in \mathbb{R}^{d_1 \times d_2}$ is a pre-trained weight matrix in the network, with $\vx$ as the input and $\displaystyle \vh$ as the hidden state, the forward pass after applying LoRA is given by:
\begin{equation}
\label{eq:lora}
\vh = \mW_0 \vx + \Delta \mW \vx =  \mW_0 \vx + \gamma\mB\mA\vx,
\end{equation}
where $\mA \in \mathbb{R}^{r \times d_2}$ and $\mB \in \mathbb{R}^{d_1 \times r}$ are the two small matrices introduced by LoRA.
$\gamma$ is a constant scale hyper-parameter that determines the contribution of LoRA in the subsequent layers. The intrinsic rank $r$ is typically much smaller than $d_1$ and $d_2$. Therefore, by freezing the pre-trained matrix and updating only the parameters of the $\mA$ and $\mB$ matrices during fine-tuning, the number of parameters that need to be updated can be significantly reduced. In the original LoRA paper, LoRA is applied to the key, query, value, and output matrices on the attention matrices of transformer-based models. 

\subsection{\name}

\textbf{Application of LoRA for TTT.}\hspace{5mm}
Focusing on the current mainstream method, TPT, our approach shifts the target of parameter updates from the text prompt to the image encoder, while leveraging the MEM loss.
However, directly updating the entire image encoder is expected to result in excessive memory consumption and domain-specific behaviors that lose the out-of-distribution generalization and robustness of foundation models \citep{wortsman2022robust, kumar2022fine}. % this sentence comes from TPT paper
As shown in \cref{fig:method}, inspired by the effectiveness of LoRA in the large language model field, we apply LoRA to layers of the image encoder in VLMs, updating only the LoRA parameters during test time.
The original LoRA paper shows that the change in weights during model adaptation has a low intrinsic rank and we hypothesize that LoRA can adapt to unique domain-specific features of each instance even with unlabeled data similar to fine-tuning in downstream tasks.
By applying LoRA, we adjust only a small number of parameters without altering the original well pre-trained weights in VLMs.
This approach allows individual adaptation to each test instance while preserving the strong zero-shot capability of the original VLMs and reducing memory consumption during backpropagation.
Furthermore, \name\ applies LoRA exclusively to the vision encoder of VLMs, rendering the text encoder unnecessary during TTT.
This allows the model to be tuned independently of specific text prompts.
Following the approach of Episodic TTT \citep{wang2020tent,shu2022test,zhao2023test}, we update parameters using only a single test instance and reset them afterward, ensuring a certain level of robustness against sequence data.
\newline

\noindent\textbf{Masked Image Reconstruction for VLMs.}\hspace{5mm}
\name\ is not limited by the type of loss function.
We are able to leverage a self-supervised reconstruction loss based on masked autoencoders (MAE) \citep{he2022masked, gandelsman2022test}, owing to the benefits of parameterizing the image encoder.
\name\ differs from conventional TTT methods based on MAE \citep{gandelsman2022test,wang2023test} and offers an efficient solution suitable for TTT, as it requires neither an image decoder nor fine-tuning of the model prior to TTT.
% differs from conventional TTT methods based on MAE \citep{gandelsman2022test,wang2023test} and offers an efficient solution suitable for TTT, as it does not require an image decoder. % ?
\name\ takes both augmented images and their randomly masked versions as input to the image encoder and calculates the mean squared error of only the encoded class tokens as the loss. These images are selected from the top 10\% of views with the highest confidence, similar to TPT.
We optimize the following loss:
\begin{equation}
\mathcal{L}_{\text{MAE}} = \text{MSE}(g(X)_{\text{cls}}, g(\text{mask}(X))_{\text{cls}}),
\end{equation}
where $\text{MSE}(\cdot, \cdot)$ represents the mean squared error between the encoded class tokens of the masked and unmasked images, $\text{mask}()$ randomly masks out majority of the input image patches (\eg, 50\%). 
This loss encourages the model to reconstruct the original global features by leveraging the remaining visual clues, enhancing visual understanding to better support downstream tasks.
The total loss can be expressed as
$
\mathcal{L} = \lambda_1\mathcal{L}_{\text{MEM}} + \lambda_2\mathcal{L}_{\text{MAE}},
$ where $\lambda_1$ and $\lambda_2$ are coefficients that balance the two losses.