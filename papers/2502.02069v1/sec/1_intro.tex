\section{Introduction}
\label{sec:intro}

\begin{figure}[ht]
    \centering
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figure/method/text_tta_mem.pdf}
        \subcaption{Test-time Prompt Tuning \citep{shu2022test}}
        \label{fig:tpt}
    \end{minipage}
    
    \vspace{1em} % 縦のスペース調整

    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figure/method/image_tta_mem.pdf}
        \subcaption{\name\ (Ours)}
        \label{fig:image_tta}
    \end{minipage}
    
    \caption{\textbf{Comparison of our proposed \name\ and Test-time Prompt Tuning (TPT).} (a) TPT optimizes the learnable text prompt via backpropagation, which results in high memory consumption, long runtime, and non-interchangeable text prompts. (b) \name\ requires only the image encoder and tunes only the LoRA parameters, demonstrating high performance while minimizing memory consumption and ensuring faster runtime. Additionally, it offers the flexibility of interchangeable text prompts.} 
    \label{fig:concept}
\end{figure}
% \jx{but change text prompt also requires re-training LoRA?}

% recent VLMs and their issue
Recent advancements in large-scale Vision-Language Models (VLMs) such as CLIP \citep{radford2021learning} and ALIGN \citep{jia2021scaling} have enabled impressive zero-shot generalization across diverse tasks, leveraging extensive pre-training on noisy, web-scale image-text pairs \citep{sharma2018conceptual, changpinyo2021conceptual, thomee2016yfcc100m}.
However, VLMs often struggle to maintain robust performance under domain shifts \citep{shu2023clipood, xiao2024any}, a common challenge in real-world applications.

The original CLIP paper \citep{radford2021learning} has demonstrated that the choice of text prompts significantly impacts zero-shot image classification performance.
As a result, the tedious work of pre-engineering text prompts for each downstream task has become increasingly important \citep{gu2023systematic,zhu2023prompt,bulat2023lasp}.
Initial works \citep{zhou2022learning,zhou2022conditional} address the burden of prompt engineering by proposing prompt fine-tuning, which directly learns text prompts using a small amount of labeled data from the target domain.
Recently, Test-Time Training (TTT) methods that adapt models during inference without requiring labeled data have gained significant attention \citep{boudiaf2022parameter,chen2022contrastive,wang2020tent}.
Although several TTT methods for VLMs aimed at achieving zero-shot generalization have been proposed \citep{shu2022test,sui2024just,yoon2024c,metzen2023autoclip}, most rely on text prompt tuning, as it is simple, requires minimal modifications, and supports black-box adaptation—an advantage for VLMs with intellectual property concerns \citep{zhang2024vision}.
% TPT issues
However, text prompt tuning often requires intensive computation \citep{karmanov2024efficient, zanella2024test}, incurs high memory consumption, and offers limited flexibility in prompt selection \citep{zanella2024test}.
For example, \cref{fig:tpt} illustrates TPT \citep{shu2022test}, a pioneering text prompt tuning method, where both the text encoder and image encoder are executed for each individual test instance, and the input text prompt — computationally intensive for backpropagation — is optimized from a fixed text prompt initialization during test time.
In practice, TTT is frequently deployed in environments with memory-constrained edge devices \citep{cai2020tinytl,song2023ecotta} and in scenarios requiring real-time data processing \citep{wang2023test,azimi2022self} or flexible prompt combinations, highlighting the practical challenges of text prompt tuning.
% Text prompt tuning is particularly inefficient for TTT, as it is often used in environments involving memory-constrained edge devices \citep{cai2020tinytl,song2023ecotta}, requires real-time processing of streaming data \citep{wang2023test,azimi2022self}, and makes it difficult to flexibly combine various prompts depending on the situation.

% Our motivation
The challenges of current TTT methods for VLMs lead to a simple question: \textit{can we improve vision representation by directly tuning the image encoder, without relying on text prompt tuning, given that downstream tasks focus on vision tasks such as image classification, segmentation, and object detection?}
Our hypothesis is that focusing on the image encoder can result in more effective and efficient adaptation in vision tasks.
However, since VLMs are pre-trained on web-scale datasets, directly tuning the parameters of the image encoder on a test instance can degrade its generalization ability and lead to catastrophic forgetting \citep{wortsman2022robust,kumar2022fine}.
Inspired by recent advancements in Parameter Efficient Fine-Tuning (PEFT) \citep{han2024parameter, xu2023parameter, ding2023parameter} for large models,
% Brief explanation of the method
we propose a \textbf{Lo}w-\textbf{Ra}nk \textbf{T}est-\textbf{T}ime \textbf{T}raining (\name) that applies Low-Rank Adaptation (LoRA) \citep{hu2021lora} to the image encoder of VLMs.
LoRA is proposed as one of the most extensively studied PEFT methods, effectively fine-tuning low-rank matrices connected in parallel to the fully connected matrices of each transformer layer, enabling the model to be tuned while preventing forgetting and minimizing memory consumption.

% explanation of the method
% \name\ offers a simple and easily implementable alternative to previous TTT methods, achieving exceptionally high performance and enhanced calibration, along with strong generalization to text prompts.
As shown in \cref{fig:image_tta}, \name\ takes a novel approach from text prompt tuning by replacing the updated parameters from text prompts with LoRA parameters using entropy loss \citep{zhang2022memo,shu2022test}.
Tuning only the LoRA parameters with a single instance during test time allows us to maintain a simple architecture and preserve the initial generalization ability, while adapting to the domain-specific features of the single instance.
\name\ focuses exclusively on vision-side parameters, eliminating the need for the text encoder during test time by precomputing and storing text features.
This reduces TTT runtime and significantly lowers memory consumption by avoiding memory-intensive text prompt tuning.
The method also generalizes well to a wide range of text prompts without relying on specific prompt designs.
% MAE utilization
In addition, focusing on the recent effectiveness of utilizing masked image modeling for TTT \citep{gandelsman2022test,wang2023test,liu2024continual}, we introduce a highly efficient reconstruction loss suited for TTT.
This loss addresses the issue of overconfidence inherent in entropy loss \citep{guo2017calibration,yoon2024c}, making it applicable in high-stakes environments \citep{wang2022medclip,liu2023clip,dorbala2022clip,khandelwal2022simple} where reliable model outputs are crucial.
By combining the entropy and reconstruction loss, our method ensures zero-shot generalization across various domains and class categories without incurring computational overhead or additional memory consumption. Our contributions can be summarized as follows:
\newline
\begin{itemize}
    \item We propose a \textbf{Lo}w-\textbf{Ra}nk \textbf{T}est-\textbf{T}ime \textbf{T}raining (\name), which applies LoRA to the image encoder of VLMs. \name\ efficiently and effectively enhances zero-shot generalization capabilities without relying on teacher models or caching, making it well-suited for real-time processing on memory-constrained edge devices or in high-stakes environments.
    \item We introduce an efficient reconstruction loss suited for TTT, which can be combined with entropy loss. It demonstrates excellent calibration performance and can be easily adapted to real-world applications.
    \item We conduct comprehensive evaluations and comparisons with existing TTT techniques for VLMs across 15 datasets from two benchmarks, achieving state-of-the-art performance by simply replacing the text prompt with LoRA for the target parameters and combining two types of losses.
\end{itemize}