\begin{abstract}

The rapid advancements in vision-language models (VLMs), such as CLIP, have intensified the need to address distribution shifts between training and testing datasets.
Although prior Test-Time Training (TTT) techniques for VLMs have demonstrated robust performance, they predominantly rely on tuning text prompts, a process that demands substantial computational resources and is heavily dependent on entropy-based loss.
In this paper, we propose \name, a novel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively to the image encoder of VLMs.
By introducing LoRA and updating only its parameters during test time, our method offers a simple yet effective TTT approach, retaining the model's initial generalization capability while achieving substantial performance gains with minimal memory and runtime overhead.
Additionally, we introduce a highly efficient reconstruction loss tailored for TTT.
Our method can adapt to diverse domains by combining these two losses, without increasing memory consumption or runtime.
Extensive experiments on two benchmarks, covering 15 datasets, demonstrate that our method improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of 5.79\% on the OOD benchmark and 1.36\% on the fine-grained benchmark, efficiently surpassing test-time prompt tuning, without relying on any external models or cache.

\begin{comment}

The rapid advancements in vision-language models (VLMs), such as CLIP, have intensified the need to address distribution shifts between training and testing datasets.
Although prior Test-Time Training (TTT) techniques for VLMs have demonstrated robust performance, they predominantly rely on tuning text prompts, a process that demands substantial computational resources and heavily depends on entropy-based loss.
In this paper, we propose \name, a novel TTT method leveraging Low-Rank Adaptation (LoRA) applied exclusively to the image encoder of VLMs.
By introducing LoRA and updating only its parameters during test time, our method provides a simple TTT approach that retains the model's initial generalization capability while achieving substantial performance gains with minimal memory and runtime overhead.
Additionally, we introduce a highly efficient reconstruction loss tailored for TTT.
Our method can adapt to various domains by combining the two losses, without increasing memory consumption and runtime.
Extensive experiments on two benchmarks consisting of 15 datasets demonstrate that our method improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of 5.79\% on the OOD benchmark and 1.36\% on the fine-grained benchmark, efficiently surpassing test-time prompt tuning, without relying on any external models or cache.

The remarkable recent development of vision-language models (VLMs) such as CLIP has heightened interest in addressing distribution shifts between training and testing data.
Although prior TTA studies for VLMs have demonstrated solid performance, most methods predominantly rely on tuning text prompts which requires intensive computation and calculating the entropy-based loss.
In contrast, we propose \name , a novel approach that enables the model to learn domain-specific features effectively and efficiently by simply applying Low-Rank Adaptation (LoRA) to the image encoder.
Updating only the LoRA parameters during test time allows our approach to maintain a simple architecture and preserve the initial generalization ability, while delivering significant performance improvements and minimizing memory consumption and runtime increase.
In addition, we introduce a highly efficient reconstruction loss suited for VLMs. Our method can adapt to various domains by combining the two losses, without increasing memory consumption and runtime.
Extensive experiments on two benchmarks consisting of 15 datasets demonstrate that our method improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of 5.79\% on the OOD benchmark and 1.36\% on the fine-grained benchmark, efficiently surpassing the recently proposed test-time prompt tuning, without relying on any external models or cache.
\end{comment}

\end{abstract}