\section{Ablation Study}
\label{sec:ablation}

\subsection{How to apply LoRA for TTT}
\label{abs:lora}
In this section, we explore the utilization of LoRA for TTT.
We investigate the key factors for effectively applying LoRA, including: (1) determining the optimal layers and the extent of LoRA application within the transformer model, (2) understanding the relationship between the appropriate rank and scale, and (3) selecting the attention matrices for tuning.
\newline

\begin{table}[t]
\centering
\caption{\textbf{Layers for LoRA application.}}
\label{tab:layer}
\begin{adjustbox}{width=0.8\linewidth}
\begin{tabular}{c|ccc}
\toprule
\rowcolor{gray!10} \textbf{LoRA Layer} & \textbf{ImageNet} & \textbf{OOD Average} & \textbf{FG Average} \\
\midrule
12 & \textbf{69.59} & 62.65 & 64.68 \\
\rowcolor{blue!10} 11-12 & 69.40 & \textbf{62.93} & \textbf{65.00} \\
9-12 & 68.97 & 62.86 & 64.73 \\
5-8 & 66.88 & 61.34 & 64.83 \\
1-4 & 67.99 & 60.69 & 64.56 \\
All & 68.12 & 62.54 & 64.62 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}


\begin{figure}[t]
    \centering
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figure/ablation/lora_rank_and_scale.pdf}
        \subcaption{LoRA rank and scale}
        \label{fig:lora_rank_scale}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figure/ablation/lora_rank_and_matrix.pdf}
        \subcaption{Attention matrices}
        \label{fig:lora_rank_matrix}
    \end{minipage}
    \caption{\textbf{Impact of LoRA application design.} The average top-1 accuracy on the fine-grained benchmark is shown, with LoRA applied to layers 11 and 12 of the image encoder.}
    \label{fig:lora}
\end{figure}

\noindent\textbf{Which layers should we apply LoRA to?}\hspace{5mm}
\cref{tab:layer} presents the zero-shot classification performance when LoRA is applied to specific layers of the image encoder in CLIP-ViT-B/16.
Our results indicate that applying LoRA to deeper layers is more effective than to shallower ones, aligning with trends observed in fine-tuning language models \citep{zhang2023adalora}.
Additionally, applying LoRA to more layers does not necessarily improve performance.
Limiting its application to the 11th and 12th layers not only outperforms applying it across all layers in terms of performance but also reduces memory consumption and runtime, making our approach more efficient for TTT.
\newline

\noindent\textbf{LoRA rank and scale.}\hspace{5mm}
As shown in ~\cref{fig:lora_rank_scale}, increasing the rank does not directly lead to performance gains.
Each rank has an optimal scale, and as the rank increases, the corresponding optimal scale tends to decrease.
When the rank is small (\eg, rank 4), performance remains stable across different scales, reducing the need for extensive hyperparameter tuning.
\newline

\noindent\textbf{LoRA rank and attention matrices.}\hspace{5mm}
We investigate the optimal application of LoRA to different attention matrices in CLIP-ViT-B/16.
In \cref{fig:lora_rank_matrix}, we observe that applying LoRA to $\mW_{\text{v}}$ at the same rank achieves the best results among the 4 matrices ($\mW_{\text{o}}$, $\mW_{\text{v}}$, $\mW_{\text{q}}$, and $\mW_{\text{k}}$).
This trend aligns with previous research \citep{zhang2023adalora,zanella2024low}, even in the context of TTT.
% Additionally, a trend is observed wherein $\mW_{\text{vq}}$ generally outperforms $\mW_{\text{kq}}$.
Given the same total number of parameters, applying LoRA to $\mW_{\text{kvqo}}$ shows little difference in performance compared to applying it to $\mW_{\text{vq}}$ or $\mW_{\text{kq}}$.

\subsection{Masking strategy}
\label{sec:masking}
In masked image modeling, the mask strategy plays a crucial role \citep{hondru2024masked,gao2024mimic}.
We examine the effects of the masking ratio, the confidence selection cutoff, the use of an image decoder, and the impact of reconstruction targets.
We use a randomly initialized transformer-based decoder with 8 layers, 16 heads, and a 768 embedding size, without prior fine-tuning to ensure a fair evaluation.
This decoder allows us to incorporate the pixel-wise reconstruction loss proposed in TTT methods based on MAE \citep{gandelsman2022test, wang2023test}.

As shown in \cref{tab:mask}, while the masking ratio does not significantly affect the overall performance, we choose a default masking ratio of 50\% as it strikes a good balance between performance and computational efficiency.
As proposed in TPT, selecting and masking the top 10\% of augmented images with the lowest entropy yields better performance than masking all 64 images (\ie, applying a cutoff of 1), with an improvement of over 1\% observed in the OOD average.
The 10\% cutoff not only improves performance but also enhances the computational efficiency of TTT by calculating the loss on only one-tenth of the images.
Furthermore, reconstructing the class token is more effective than reconstructing masked visual tokens or image pixels using the decoder.
This supports the hypothesis that improving zero-shot image classification performance in VLMs relies more on aligning high-level semantics than on capturing fine-grained features.
% This supports the hypothesis that reconstructing global features enhances visual understanding, thereby improving zero-shot image classification performance in VLMs.
%\citep{gao2024mimic}

\begin{table}[t]
\centering
\caption{\textbf{Masking strategy.} The LoRA scale $\gamma$ is set to 2 for both benchmarks. Performance differences from zero-shot CLIP-ViT-B/16 are shown with a blue {\textcolor{blue}{($\uparrow$)}} or red {\textcolor{red}{($\downarrow$)}} arrow.}
\label{tab:mask}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{cccc|ccc}
\toprule
\rowcolor{gray!10} \textbf{Reconstruction} & \textbf{Mask Ratio} & \textbf{Cutoff} & \textbf{Decoder} & \textbf{ImageNet} & \textbf{OOD Average} & \textbf{FG Average} \\
\midrule
\multirow{6}{*}{Class token} 
 & 0.25 & 0.1 &  & 67.65\textsubscript{\textcolor{blue}{($\uparrow$0.94)}} & \textbf{58.94}\textsubscript{\textcolor{blue}{($\uparrow$1.80)}} & 64.57\textsubscript{\textcolor{blue}{($\uparrow$0.92)}} \\
 & \cellcolor{blue!10}0.5 & \cellcolor{blue!10}0.1 & \cellcolor{blue!10} & \cellcolor{blue!10}\textbf{67.78}\textsubscript{\textcolor{blue}{($\uparrow$1.07)}} & \cellcolor{blue!10}58.85\textsubscript{\textcolor{blue}{($\uparrow$1.71)}} & \cellcolor{blue!10}\textbf{64.72}\textsubscript{\textcolor{blue}{($\uparrow$1.08)}} \\
 & 0.75 & 0.1 & & 67.48\textsubscript{\textcolor{blue}{($\uparrow$0.76)}} & 57.87\textsubscript{\textcolor{blue}{($\uparrow$0.72)}} & 64.35\textsubscript{\textcolor{blue}{($\uparrow$0.71)}} \\
 & 0.5 & 0.5 & & 67.52\textsubscript{\textcolor{blue}{($\uparrow$0.80)}} & 58.30\textsubscript{\textcolor{blue}{($\uparrow$1.16)}} & 64.49\textsubscript{\textcolor{blue}{($\uparrow$0.84)}} \\
 & 0.5 & 1 & & 67.20\textsubscript{\textcolor{blue}{($\uparrow$0.48)}} & 57.79\textsubscript{\textcolor{blue}{($\uparrow$0.65)}} & 64.34\textsubscript{\textcolor{blue}{($\uparrow$0.69)}} \\
 & 0.5 & 0.1 & \ding{51} & 67.27\textsubscript{\textcolor{blue}{($\uparrow$0.55)}} & 58.28\textsubscript{\textcolor{blue}{($\uparrow$1.14)}} & 64.14\textsubscript{\textcolor{blue}{($\uparrow$0.50)}} \\
\midrule
Visual tokens & 0.5 & 0.1 & & 66.89\textsubscript{\textcolor{blue}{($\uparrow$0.17)}} & 57.46\textsubscript{\textcolor{blue}{($\uparrow$0.32)}} & 63.79\textsubscript{\textcolor{blue}{($\uparrow$0.15)}} \\
\midrule
Image pixel & 0.5 & 0.1 & \ding{51} & 66.67\textsubscript{\textcolor{red}{($\downarrow$0.05)}} & 57.05\textsubscript{\textcolor{red}{($\downarrow$0.10)}} & 63.50\textsubscript{\textcolor{red}{($\downarrow$0.14)}} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\subsection{Initialization of LoRA weights}
LoRA demonstrates high effectiveness and efficiency for TTT, even when initialized with random weights.
In this section, we explore the performance gains achieved by fine-tuning the LoRA weights before TTT.
We prepare a third dataset, CC3M \citep{sharma2018conceptual}, for LoRA initialization and train only the LoRA weights using the same contrastive loss as in CLIP pre-training \citep{radford2021learning} with image-text pairs.
We employ Adam with a learning rate of $1\text{e-}6$ and a weight decay of 0.05 for optimization, performing one epoch of training with a batch size of 64.

As shown in \cref{fig:init}, LoRA initialization using 21k randomly sampled image-text pairs from CC3M (\ie, only 1\% of the total CC3M dataset) improves performance by more than 1\% on the fine-grained benchmark and by 0.6\% on the OOD benchmark.
Furthermore, TTT consistently improves performance on both the benchmarks, regardless of the LoRA initialization.
% On the OOD benchmark, it exceeds the Kaiming initialization by more than 1\%, demonstrating the effectiveness of LoRA fine-tuning. 
Our experiments demonstrate that fine-tuning LoRA with a small amount of data shows the potential to enhance its performance.
While adhering to the constraints of not leveraging domain-specific information or a teacher model, LoRA fine-tuning delivers significant performance improvements in TTT, establishing it as an effective approach for future applications of LoRA in TTT.

\begin{comment}
\begin{table}[t]
\centering
\caption{\textbf{LoRA weight initialization}}
\label{tab:init}
\begin{adjustbox}{width=0.8\linewidth}
\begin{tabular}{ccc|ccc}
\toprule
Initialization & Data Size & TTT & ImageNet & OOD Average & FG Average \\
\midrule
Kaiming & 0 & & 66.72 & 57.14 & 63.64 \\
Kaiming & 0 & \ding{51} & 69.31 & 61.21 & 65.00 \\
\midrule
CC3M & 21 $\mathrm{k}$ & & 67.92 & 57.74 & 64.80 \\
CC3M & 21 $\mathrm{k}$ & \ding{51} & 70.52 & 62.57 & 65.64 \\
CC3M & 110 $\mathrm{k}$ & & 67.56 & 57.21 & 64.92 \\
CC3M & 110 $\mathrm{k}$ & \ding{51} & 70.24 & 62.16 & 65.94 \\
CC3M & 230 $\mathrm{k}$ & & 66.96 & 56.25 & 64.68 \\
CC3M & 230 $\mathrm{k}$ & \ding{51} & 69.78 & 61.44 & 65.76 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}
\end{comment}

\begin{figure}[t]
  \centering
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figure/ablation/lora_finetuning_ood.pdf}
        \subcaption{OOD Average}
        \label{fig:init_ood}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figure/ablation/lora_finetuning_fg_ttt.pdf}
        \subcaption{Fine-Graind Average}
        \label{fig:init_fg}
    \end{minipage}
    \caption{\textbf{Impact of LoRA weight initialization} by data size and comparison with TTT.}
    \label{fig:init}
\end{figure}
