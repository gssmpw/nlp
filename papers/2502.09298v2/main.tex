\documentclass{article}


\usepackage{PRIMEarxiv}
\usepackage[numbers]{natbib}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
%\fancyhead[LO]{Running Title for Header}
%\fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}

% my packages
\usepackage{amsmath,bm}
\usepackage{enumerate,enumitem}
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\usepackage{multirow,multicol}
\usepackage{tikz}
\usetikzlibrary{positioning, shapes, decorations.markings, decorations.pathreplacing, decorations.pathmorphing, arrows, calc, matrix}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
%\input{main.bbl}

  
%% Title
\title{Convex Is Back: \\ Solving Belief MDPs With Convexity-Informed Deep Reinforcement Learning}



\author{
  Daniel Koutas \\
  ERA group \\
  TU Munich \\
  Munich, Germany\\
  \texttt{daniel.koutas@tum.de} \\
  %% examples of more authors
  \And
  Daniel Hettegger \\
  AIR chair \\
  TU Munich \\
  Munich, Germany\\
  \texttt{daniel.hettegger@tum.de} \\
  \And
  Kostas G. Papakonstantinou \\
  College of Engineering \\
  Penn State University \\
  Pennsylvania, USA\\
  \texttt{kpapakon@psu.edu} \\
  \And
  Daniel Straub \\
  ERA group \\
  TU Munich \\
  Munich, Germany\\
  \texttt{straub@tum.de} \\
}


\begin{document}
\maketitle

\vspace{-1em}
\begin{abstract}
We present a novel method for Deep Reinforcement Learning (DRL), incorporating the convex property of the value function over the belief space in Partially Observable Markov Decision Processes (POMDPs). We introduce hard- and soft-enforced convexity as two different approaches, and compare their performance against standard DRL on two well-known POMDP environments, namely the Tiger and FieldVisionRockSample problems. Our findings show that including the convexity feature can substantially increase performance of the agents, as well as increase robustness over the hyperparameter space, especially when testing on out-of-distribution domains. The source code for this work can be found at \url{https://github.com/Dakout/Convex_DRL}.
\end{abstract}


\section{Introduction}
\label{sec:Introduction}
%

Markov Decision Processes (MDPs) have become the standard formalism for solving sequential decision making problems \citep{van2012rlmdp}. For applications in which perfect observability cannot be assumed, MDPs can be extended to model the probabilistic decision process as a Partially Observable MDP (POMDP), which provides an efficient framework for optimal decision making under uncertainty \citep{kochenderfer2015decision,kaelbling1998planning,shani2013survey,braziunas2003pomdp,walraven2019point,oliehoek2008optimal,cassandra1994acting}. 

Classical dynamic programming (DP) and reinforcement learning RL are the two established solution methods for MDP and POMDPs. RL approaches are used to overcome the curse of dimensionality of DP methods, and additionally do not require a model of the environment \citep{kochenderfer2015decision}. Neural network (NN) based deep RL (DRL) applied to MDPs has been particularly successful, even in high dimensional problem settings \citep[e.g.,][]{mnih2013playingatari,mnih2015human,silver2016mastering,silver2018general}. Solving POMDPs is a more difficult task, but one approach is to directly handle the noisy observation-action history (or a variant thereof) \citep{koutas2024investigation,hettegger,giacomo}. In cases, where no environment model is available, this is the only option. If one has an environment model, then beliefs, i.e., a probability distribution over all system states, can be computed. Using beliefs can be computationally beneficial \citep{giacomo,koutas2024investigation}; however, most current approaches do not use DRL with beliefs. In this work, we propose to extend belief-based DRL solutions for POMDPs, by taking into account the theoretical property that the optimal value function is convex over the belief space. We hypothesize that introducing this informative property in the training process enables faster learning, and leads to better performance in out-of-distribution (OOD) domains.

We propose two approaches for convexity-enforcement of the value function in the training process, namely soft-enforced and hard-enforced convexity. 
The performance of convexity-informed DRL is examined for both convexity approaches applied to the Dueling Q-Network \citep{wang2016dueling} value-based architecture. Two benchmarks are used for performance comparison, which are the classic \emph{Tiger} \citep{kaelbling1998planning} and \emph{FieldVisionRockSample} (FVRS) \citep{ross2007aemsfvrs} problems. We show that when trained with the convexity modification, the involved NNs have stronger generalization performance compared to the standard training schedule and, in some cases, better training performance.

\textbf{Related work} \newline
Learning convex functions has a rich history in machine learning. However, the techniques developed in literature deal with explicitly defined target functions, e.g., including Karush-Kuhn-Tucker conditions of quadratic programs as differentiable layers \cite{amos2017optnet}, 
log-likelihoods in conditional random fields \cite{zheng2015rnncrf}, or energy functions in structured prediction energy networks \cite{belanger2016spen}. These methods are not applicable to DRL, where the value function is implicitly defined through the Bellman equation, and are thus not similar to our contribution.  

Regarding hard-enforced convex NNs for MDP problems, the closest work to this paper is a combination of Amos et al. \citep{amos2017inputconvexnns} and Sivaprasad et al. \citep{sivaprasad2021curious}. Amos et al. \citep{amos2017inputconvexnns} introduce fully and partially input convex neural networks, where the architectures include required skip connections at every layer. The focus of their work lies more on performing inference on the final convex NN to find the optimal input values. Their reference to the application on DRL focuses merely on perfectly observable and continuous action problems. They represent the negative Q-function with an input convex NN, and subsequently select the optimal action as a convex optimization problem over the NN output. However, they do not consider partially observable environments, as is the focus of this work. Sivaprasad et al. \citep{sivaprasad2021curious} define conditions to achieve input-output convexity with fully connected NNs without the need for skip connections, and convolutional architectures; however, they do not consider DRL implementations. 

Regarding soft-enforcement of convexity in NNs for MDPs or POMDPs, we are not aware of any related work. 

%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
%
%
\section{Belief MDP}
\label{sec:Belief_MDP}
Markov Decision Processes (MDP) provide an efficient framework for finding optimal solutions in sequential decision making problems, where the environment $E$ is stochastic, the consequence of the agent's actions is probabilistic, and the state of the environment is known \citep{puterman2014markov}. Partially observable MDPs (POMDPs) provide a natural extension, where the true environment state is not perfectly known; instead the agent receives imperfect observations \citep{kochenderfer2015decision}. The POMDP can be formulated as an MDP by replacing the system states $s_t \in \sS$ with the corresponding belief $b(s_t) = p(s_t \mid o_{1:t},a_{1:t-1})$, where $o_t \in \sO$ and $a_t \in \sA$ denote the received observation and action performed by the agent, respectively. As new information is available to the agent, the new belief states can be obtained with Bayesian updating \citep{kochenderfer2015decision,andriotis2019managing}:
%
\begin{equation}
\begin{aligned}
\label{eq_belief_update}
    b(s_{t+1})&=p(s_{t+1} \mid o_{t+1},a_t,\vect{b}_t) \\ %b'(s'_i) 
    &= \frac{O\left(o_{t+1} \mid s_{t+1}, a_t \right)}{p(o_{t+1} \vert \vect{b}_t,a_t)}
    %& \propto O\left(o_{t+1} \mid s_{t+1}, a_t \right)
    \sum_{s_t \in \sS} T\left(s_{t+1} \mid s_t, a_t\right) b(s_t),
\end{aligned}
\end{equation}
%
where $O$ and $T$ represent the observation and state transition probabilities, respectively, $p(o_{t+1} \vert \vect{b}_t,a_t)$ is the normalizing constant, and the belief vector $\vect{b}_t$ of
length $\abs{\sS}$ represents the collection of beliefs
$b(s_t) ~\forall s \in \sS$ \citep{andriotis2019managing}. 

Based on the chosen action and the underlying true state of the environment, the agent receives a reward $r_t \in \sR$ determined by the reward function $r(s_t, a_t)$, and the expected reward in a certain belief state can be obtained by $r(\vect{b}_t, a_t) = \sum_{s_t \in S} r(s_t,a_t)b(s_t)$. The decision-making rule, mapping beliefs to actions, is called policy $\pi(\vect{b}_t)$ \footnote{We limit this work to deterministic policies, but an extension to stochastic policies is possible}. 
%
The total expected discounted reward, or \emph{value function}, $V^{\pi}(\vect{b}_t)$, starting from $\vect{b}$ at time $t$ until the horizon $h$ under the policy $\pi$ is defined as \citep{walraven2019point, andriotis2019managing}:
%
%
\begin{equation}
\begin{aligned}
    \label{eq_value_function}
    V^{\pi}(\vect{b}_t) &= \E_{s_k\sim T,a_k\sim\pi, o_k\sim O}\left[\sum_{k=t}^h \gamma^{k-t} r(\vect{b}_k, a_k) \right],
\end{aligned}
\end{equation}
%
where $\gamma<1$ defines a discount factor. 
Similarly to \Eqref{eq_value_function}, one can define an \emph{action-value} function, $Q^{\pi}(\vect{b}_t, a_t)$, which defines the value of taking action $a$ at belief $\vect{b}$ at time $t$ until the horizon $h$ under the policy $\pi$:
%
\begin{equation}
\begin{aligned}
    \label{eq_qvalue_function}
    Q^{\pi}(\vect{b}_t, a_t)
    & =  \E_{s_k\sim T,a_k\sim\pi, o_k\sim O}\left[\sum_{k=t}^h \gamma^{k-t} r(\vect{b}_k, a_k) ~\bigg|~ a_t\right].
\end{aligned}
\end{equation}
%
The relationship between Equations \ref{eq_value_function} and \ref{eq_qvalue_function} is $V^{\pi}(\vect{b}_t) = Q^{\pi}(\vect{b}_t, \pi(\vect{b}_t))$. Having the Q-values, the agent's policy can be easily extracted by:
%
\begin{equation}
    \label{eq_policy}
    \pi(\vect{b}_t) = \argmax_{a_t \in \sA} Q^{\pi}(\vect{b}_t,a_t).
\end{equation}
%
The goal of the agent is to find the policy which maximizes the expected sum of discounted rewards. This optimal policy $\pi^*$ maximizes the value and action-values at every time $t$:
%
\begin{equation}
    \label{eq_opt_value_function}
    V^{*}(\vect{b}_t) := \left. V^{\pi}(\vect{b}_t)\right|_{\pi=\pi^*} = \max_{\pi} V^{\pi}(\vect{b}_t)
\end{equation}
%
%
\begin{equation}
    \label{eq_opt_qvalue_function}
    Q^{*}(\vect{b}_t, a_t) := \left. Q^{\pi}(\vect{b}_t, a_t)\right|_{\pi=\pi^*} = \max_{\pi} Q^{\pi}(\vect{b}_t, a_t),
\end{equation}
%
where $Q^{*}$ satisfies the recursive Bellman optimality condition \citep{cassandra1994acting}:% \citep{braziunas2003pomdp}:
%
\begin{equation}
\begin{aligned}
    \label{eq_opt_qvalue_function_bellman}
    Q^{*}(\vect{b}_t, a_t)
    & =  r(\vect{b}_t, a_t) + \gamma \sum \limits_{\vect{b}_{t+1} \in \sB} p(\vect{b}_{t+1} \vert \vect{b}_t,a_t) \max \limits_{a_{t+1} \in \sA} Q^{*}(\vect{b}_{t+1}, a_{t+1}).
\end{aligned}
\end{equation}
%
%
The focus of this work revolves around an important property of the optimal POMDP value function, namely that it should be convex over the belief space \citep{kochenderfer2015decision}. The different methods of enforcing this property are discussed in \Secref{sec:Convexity} and the application of the approaches to the Dueling architecture is discussed in \Secref{subsec:Convexity_informed_DRL}.

%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
%
%
%
\section{Deep Reinforcement Learning}
\label{sec:DRL}
%
One can use the recursive formulation in \Eqref{eq_opt_qvalue_function_bellman} to find the function $Q^*$, from which the optimal policy can then be extracted. Dynamic programming variants, such as, e.g., value or policy iteration, perform the optimization by iterating over all possible combinations of (belief) states, actions and observations \citep{kochenderfer2015decision}. However, due to the super-exponential growth in the value function complexity \citep{hauskrecht2000value}, this approach is not feasible for larger state and action spaces. By contrast, Neural Networks as universal function approximators \citep{hanin2019universal} have proven to be effective even in large state and action spaces, which has motivated their use for approximating $V,~Q,$ or $\pi$, in what is known as Deep Reinforcement Learning (DRL) \citep[e.g.,][]{andriotis2019managing,vinyals2019grandmaster}. 

One of the most prominent architectures for DRL are Deep Q-Networks (DQNs) \citep{mnih2013playingatari}, where the recursive \Eqref{eq_opt_qvalue_function_bellman} is reformulated into the temporal difference (TD) mean-squared error (MSE) loss function:
%
\begin{equation}
    \label{eq:MSE_td}
    MSE_t = \frac{1}{n_t} \sum_{i=1}^{n_t} \left|y^{(i)}-\Tilde{y}^{(i)} \right|^2
\end{equation}
%
where $y^{(i)}$ denotes an output sample of the NN and $~\Tilde{}~$ always denotes the counterpart of the target NN, whose weights get updated periodically \citep{mnih2015human}:
%
\begin{align}
    \label{eq:Q_output_NN}
    y^{(i)} &= Q(\vect{b}_t^{(i)}, a_t^{(i)} \mid \theta) \\
    \label{eq:Q_output_NN_target}
    \tilde{y}^{(i)} &= r_t^{(i)} + \gamma \max \limits_{a_{t+1}^{(i)} \in \sA} \Tilde{Q}(\vect{b}_{t+1}^{(i)}, a_{t+1}^{(i)} \mid \Tilde{\theta}).
\end{align}
%
Note that the Q-values in Equations \ref{eq:Q_output_NN} and \ref{eq:Q_output_NN_target} are DRL approximations of the optimal Q-function in \Eqref{eq_opt_qvalue_function}, and the TD-MSE in \Eqref{eq:MSE_td} is a sample-based approximation of the Bellman condition in \Eqref{eq_opt_qvalue_function_bellman}. The assumption is that, given enough samples and training, the DRL approximation should converge to the optimal solution.

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
%
%
%
\section{Convex Neural Networks}
\label{sec:Convexity}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Convexity conditions for multi-dimensional functions}
\label{subsec:types_of_convexity}
%
%In this section we revise the conditions for convexity for multi-dimensional functions.

%
A function $f : \mathbb{R}^n \longrightarrow \mathbb{R}$ is convex if its domain is a convex set and for all $\vu, \vv$ in its domain, and all $t \in [0, 1]$, we have \citep{ahmadi2016theory}:
\begin{equation}
\label{eq:point_based_convexity}
    f(t \vu + (1 - t ) \vv ) \leq t f(\vu) + (1 - t)f(\vv).
\end{equation}
%
If $f$ is differentiable, one can define an alternative condition of convexity, which is equivalent to \Eqref{eq:point_based_convexity} \citep{ahmadi2016theory}:
%
\begin{equation}
    \label{eq:gradient_based_convexity}
    f(\vu)+\nabla_\vu f(\vu)^T(\vv-\vu) \leq f(\vv), \text { for all } \vu, \vv \in \operatorname{dom}(f).
\end{equation}
%
If $f$ is twice differentiable, then one can define yet another condition of convexity which is equivalent to Equations \ref{eq:point_based_convexity} and \ref{eq:gradient_based_convexity} %\Eqref{eq:point_based_convexity,eq:gradient_based_convexity}
\citep{ahmadi2016theory}:
%
\begin{equation}
    \label{eq:hessian_based_convexity}
    0 \preceq \mH (f)(\vu) = \nabla^2_\vu f(\vu) , \text { for all } \vu \in \operatorname{dom}(f),
\end{equation}
%
i.e., the Hessian matrix $\mH ( f) (\vu)$ must be positive semi-definite.
\Eqref{eq:point_based_convexity} defines convexity in terms of the function value at different points, whereas Equations \ref{eq:gradient_based_convexity} and \ref{eq:hessian_based_convexity}
%\Eqref{eq:gradient_based_convexity,eq:hessian_based_convexity}
define convexity via the first and second derivatives.

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Hard-enforced convexity}
\label{subsec:hard-enforced_convexity}
%
The first obvious choice to satisfy convexity in the value function is to use an NN architecture which guarantees convexity. Considering a multi-layer perceptron of $k$ layers, where $h_i^{(l)}$ denotes the $i-$th neuron output in the $l-$th layer, then for an input $\vx\in \mathbb{R}^d$, $h_i^{(l)}$ is defined as \citep{sivaprasad2021curious}:
%
\begin{equation}
\label{eq:hidden_layer_output}
    h_i^{(l)}=\phi\left(\sum_j W_{i,j}^{(l)} h_j^{l-1}+b_i^{(l)}\right),
\end{equation}
%
with weights $W_{i,j}^{(l)}$, bias  $b_i^{(l)}$ and activation function $\phi(x)$. Further, $h_j^{(0)}=x_j(j=1, \ldots, d)$ and $h_j^{(k+1)}=y_j$ ($j^{\text {th }}$ NN output). Only two conditions are needed to ensure convexity of the final output y with
respect to the input x, namely \citep{amos2017inputconvexnns,sivaprasad2021curious}:
%
\begin{enumerate}[label=\roman*), ref=\roman*]
\item \label{item:nonnegative_weights} $0 \leq W_{i,j}^{(2: k+1)}$,
\item \label{item:phi_convex_and_nonnegative} $\phi$ \text{ is convex and a non-decreasing function.}
\end{enumerate}
%

Condition \ref{item:phi_convex_and_nonnegative}) can be achieved by using, e.g., Leaky Rectified Linear Unit (LReLU) \citep{maas2013rectifierlrelu}, Parametric ReLU (PReLU) \citep{he2015delvingprelu} or Exponential LU (ELU) \citep{clevert2015fastelu} activation functions. 

Condition \ref{item:nonnegative_weights}) needs to be enforced during the training process, e.g., by clipping negative weights to zero, taking absolute of weights, exponentiation of negative weights, or shifting negative weights after each iteration \citep{sivaprasad2021curious}. As this enforcement from outside interferes with the weight updates, and hence potentially hinders training, we also investigate the approach of soft-enforced convexity. Note that other, more complex approaches exist to enforce convexity, e.g., by representing weights as separate NN layers with absolute activation functions \citep{rashid2020monotonicqmix}. However, this would change the underlying architecture and complicate comparability between the approaches, which is why other enforcement options are left for future research. 


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Soft-enforced convexity}
\label{subsec:soft-enforced_convexity}
%
Soft-enforced convexity mimics the idea of soft-enforced differential equations in Physics-Informed Neural Networks (PINNs) \citep{raissi2019pinn}. The core idea is to add a second term to the temporal loss function in \Eqref{eq:MSE_td}, which penalizes deviations of the target function from one of the convexity criteria outlined in \Secref{subsec:types_of_convexity}. Suppose we use the MSE loss function also for the second term, then the total MSE loss function is:
%
\begin{equation}
    \label{eq:mse_total_loss}
    MSE = MSE_t + c \cdot MSE_c,
\end{equation}
%
where the constant $c$ defines the relative weight of the TD- and convexity loss terms.

%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
\section{Methodology}
\label{sec:Methodology}


\subsection{Convexity-informed DRL}
\label{subsec:Convexity_informed_DRL}
The NN architecture used throughout this work is the Dueling architecture \citep{wang2016dueling} depicted in \Figref{fig:dueling_architecture}.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
    \def\layerdist{0.81}
    \def\belheight{1.1}
    \def\belwidth{0.25*\belheight}
    \def\fcheight{3}
    
    \def\lettersep{0.2}
    \def\bndline{very thick}
    \def\arrthick{thick}
    \def\mergethick{0.1}
    
    \def\widthfac{1.39}
    %% nodes 
    % 0 node
    \coordinate (b) at (0,0);
    % first belief layer
    \node[rectangle, draw, fill=black, \bndline, minimum width = \belwidth cm, minimum height = \belheight cm, align=center, %label=above: $\vect{b}$
    ] (bel) at (b) {};
    % first fc layers
    \node[rectangle, draw, \bndline, minimum width = \belwidth cm, minimum height = \fcheight cm, right=\layerdist cm of bel, align=center, fill=gray] (fc1) {}; 
    \node[rectangle, draw, \bndline, minimum width = \belwidth cm, minimum height = \fcheight cm, right=\layerdist cm of fc1, align=center, fill=gray] (fc2) {};
    % arrows
    \draw[-latex, \arrthick] (bel) -- (fc1);
    \draw[-latex, \arrthick] (fc1) -- (fc2);
    % advantage & value fc
    \node[rectangle, draw, fill=black!30!cyan, \bndline, minimum width = \belwidth cm, minimum height = 0.5*\fcheight cm, right=\layerdist cm of fc2, yshift=0.33*\fcheight cm, align=center] (fcv) {}; 
    \node[rectangle, draw, fill=black!30!green, \bndline, minimum width = \belwidth cm, minimum height = 0.5*\fcheight cm, right=\layerdist cm of fc2, yshift=-0.33*\fcheight cm, align=center] (fca) {};
    % advantage & value
    \node[rectangle, draw, fill=black!30!cyan, \bndline, minimum width = 0.25*\belheight cm, minimum height = 0.25*\belheight cm, right=\layerdist cm of fcv, align=center, 
    %label=north: $V$
    ] (val) {};
    \node[rectangle, draw, fill=black!30!green, \bndline, minimum width = 0.25*\belheight cm, minimum height = \belheight cm,
    right=\layerdist cm of fca, align=center, 
    %label=south: $A$
    ] (adv) {};
    % arrows
    \draw[-latex, \arrthick] (fc2) -- (fcv);
    \draw[-latex, \arrthick] (fc2) -- (fca);
    \draw[-latex, \arrthick] (fca) -- (adv);
    \draw[-latex, \arrthick] (fcv) -- (val);
    % q-values
    \coordinate[right=3.7*\layerdist cm of fc2] (m);
    \node[rectangle, draw, fill=black!10!red, \bndline, minimum width = 0.25*\belheight cm, minimum height = \belheight cm, right=\layerdist cm of m, align=center,
    %label=north: $Q$
    ] (qval) {};
    % arrows
    \draw[line width=\mergethick cm, brown] (adv) -- (m);
    \draw[line width=\mergethick cm, brown] (val) -- (m);
    \draw[line width=\mergethick cm, brown] (m) -- (qval);
    \end{tikzpicture}
    %\captionsetup{width=\textwidth}
    \caption{Dueling network architecture, with the belief input (black), dense layers (gray), value stream (cyan), advantage stream (green) and Q-value output (red). Arrows indicate dense weights and the brown lines indicate computation without weights; adapted from \citep{wang2016dueling}.}
    \label{fig:dueling_architecture}
\end{figure}
%
\textbf{Hard-enforcement} of the value-convexity with respect to the belief input is performed by adjusting the weights according to condition i) in \Secref{subsec:hard-enforced_convexity} in the shared layers (gray), as well as the value stream (cyan) in \Figref{fig:dueling_architecture}. Condition ii) in \Secref{subsec:hard-enforced_convexity} is met by an appropriate choice of the activation function, which is kept identical for all layers.

\textbf{Soft-enforcement} of the value-convexity is performed by adding a second loss term according to \Eqref{eq:mse_total_loss}. Depending on the choice of the convexity criterion, namely 
point-based (p) in \Eqref{eq:point_based_convexity}, or gradient-based (g) in \Eqref{eq:gradient_based_convexity}, $MSE_c$ takes the form of:
%
\begin{equation}
    \label{eq:MSE_point_convexity}
    MSE_c^p = \frac{1}{n_c} \sum_{i=1}^{n_c} \max\left\{0,
    f(t^{(i)} \vu^{(i)} + (1 - t^{(i)}) \vv^{(i)} ) - t^{(i)} f(\vu^{(i)}) - (1 - t^{(i)})f(\vv^{(i)})\right\}^2
\end{equation}
%
%
\begin{equation}
    \label{eq:MSE_grad_convexity}
    MSE_c^g = \frac{1}{n_c} \sum_{i=1}^{n_c} \max\left\{0,
    f(\vu^{(i)})+\nabla_\vu f(\vu^{(i)})^T(\vu^{(i)}-\vv^{(i)}) - f(\vv^{(i)})\right\}^2.
    %f\left(b_{c,1}^i, b_{c,1}^{i}\right)^2
\end{equation}
%
The hessian-based condition in \Eqref{eq:hessian_based_convexity} cannot be translated into a loss function in a straightforward manner. A matrix $\mM$ is positive semi-definite (psd) if
%
\begin{equation}
    \label{eq:mat_pos_semi_def}
    \vx^T \mM \vx \geq 0 \quad \forall \vx \in \mathbb{R}^n.
\end{equation}
%
For 1D inputs, this is not a problem and the condition reduces to $\frac{d^2}{du^2}f(u)\geq0$ and the convex loss takes the form of
%
\begin{equation}
    \label{eq:MSE_hess_convexity_1D}
    MSE_c^{h,1D} = \frac{1}{n_c} \sum_{i=1}^{n_c} \max\left\{0,
    -\frac{d^2}{{du^{(i)}}^{2}} f(u_i)\right\}^2.
    %f\left(b_{c,1}^i, b_{c,1}^{i}\right)^2
\end{equation}
%
By contrast, for multidimensional inputs, \Eqref{eq:mat_pos_semi_def} can be checked in a sample based manner:
%
\begin{equation}
    \label{eq:MSE_hess_convexity_nD}
    MSE_c^{h,nD} = \frac{1}{n_c} \sum_{i=1}^{n_c} \frac{1}{n_{psd}} \sum_{j=1}^{n_{psd}} \max\left\{0,
    -{\vx^{(j)}}^T\mH ( f)(\vu^{(i)})\vx^{(j)}\right\}^2.
\end{equation}
%

$\vu^{(i)}$ and $\vv^{(i)}$ in Equations \ref{eq:MSE_point_convexity}, \ref{eq:MSE_grad_convexity}, \ref{eq:MSE_hess_convexity_1D} and \ref{eq:MSE_hess_convexity_nD} denote points $i=1,...,n_c$ sampled from the belief space, for which the respective convexity condition is checked; $\vx^{(j)}, ~j=1,...,n_{psd}$ denote points sampled from $\mathbb{R}^n$ for which the psd condition in \Eqref{eq:mat_pos_semi_def} checked.   

Once the respective soft enforcement method is chosen, belief points $\vect{b}^{(i)}$ (corresponding to $\vu^{(i)}$) are sampled from the problem-specific belief space and, together with other inputs, propagated through the network to obtain their values $V(\vect{b}^{(i)})$ (corresponding to $f(\vu^{(i)})$). In this work, we employ the Dueling architecture, hence the values can be obtained by propagation through the shared layers (gray) and subsequently through the value stream (cyan) of the network in \Figref{fig:dueling_architecture}.


\subsection{Numerical investigations}
\label{subsec:Hypotheses}
%
With numerical experiments, we test if enforcing convexity in the value function can improve the performance of DRL. Specifically, we test the following two hypotheses:

\emph{H1}: Enforcing convexity enables the DRL agent to learn faster due to restriction of the search space.

\emph{H2}: Convexity-informed DRL performs better in out-of-distribution domains due to improved extrapolation of the value function.

The two hypotheses are tested with experiments on two classic problems, namely the \emph{Tiger} \citep{kaelbling1998planning} and the \emph{FieldVisionRockSample} (FVRS) \citep{ross2007aemsfvrs} environments. Detailed descriptions of these problems are given in Sections \ref{sec:app_tiger_problem} and \ref{sec:app_fvrs_problem}. 

To test \emph{H1}, we train the DRL agent with and without enforcing convexity for a fixed number of training steps. We then compare the performance of the individual agents. 

To test \emph{H2}, we evaluate the performance of the agents for belief points which are not included in the training distribution. We achieve this by testing on observation functions which differ from the one used to train the agent. 
For the Tiger problem, we simply change the constant Tiger observation accuracy. For FVRS, the observation accuracy $p_{obs}$ for a rock depends on its Euclidian distance $d$ to the agent. The default (def) observation function is given as $p_{obs}^{def}(d) = 0.5+2^{-1-d/d_0}$, where the constant $d_0=(n-1)\sqrt{2}/4$ is chosen depending on the grid size $n$. To evaluate the agent on OOD data, we define the heaviside (heavi) function $p_{obs}^{heavi}(d) = 1$ for $d \leq d_0$ else $0.5$, where $d_0=1$. Furthermore, we also define a set of constant (const) observation functions  $p_{obs}^{const}(d) = c$, which do not depend on the distance between rock and agent. We use $c \in \{0.5, 0.6, ..., 1.0\}$. 

To allow for a fair comparison between the DRL with and without enforcing convexity we fix the hyperparameter optimization procedure beforehand. This prevents bias inflicted during the optimization (e.g., amount of time invested, number of samples, amount of steps). The detailed procedure used is reported in \Secref{sec:app_training_specifications}. After the hyperparameter search is conducted, we perform two separate investigations. Firstly, we evaluate the performance of each method over all hyperparameter samples, yielding a rough estimate of the robustness/sensitivity of each method with respect to changing hyperparameters. This approach is not customary in Machine Learning, which is why the corresponding results are only reported in Section \ref{sec:app_robustness_over_hyperparameters}. Secondly, we take the best hyperparameters of each search, and evaluate them over a certain number of runs. The results of this conventional approach are reported in the main text in Section \ref{sec:Results}.



The final policy of each agent is evaluated with a Monte Carlo (MC) approximation of the expected sum of discounted rewards $\hat{\rs}_r$. To evaluate the performance of each method, we employ boxplots, comprising the median, interquartile range, and maximum performance, which allows a more complete interpretation of the obtained results.  


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
%
\section{Results}
\label{sec:Results}
%
% 

%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
%
%
%

%
\subsection{Computation time}
\label{subsec:computation_time}
%
All computations were performed on an Nvidia Tesla V100 GPU with 16GB RAM. For the Tiger problem, training for 5,000 steps and evaluating the policy with $2 \cdot 10^5$ MC samples took around 5 minutes. For FVRS, training for 50,000 steps and evaluating the policy with $10^4$ MC samples took approximately 60 minutes.


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Tiger}
\label{subsec:Tiger}
%
%
We test the Tiger problem for all versions of enforced convexity (hard, point, grad and hess) as well as for the standard approach without enforced convexity. 

A visualization of the convexity violation of the standard DRL approach, as well as the corresponding value function correction of our proposed methods is shown in Section \ref{subsec:app_Tiger}.

We perform a hyperparameter search for  all convexity methods for various observation accuracies $p_{obs}=\{0.6, 0.8, 0.9, 1.0\}$. 
The evaluation of our hypotheses over the whole hyperparameter search is outlined in Section \ref{subsec:app_Tiger}. 

The test of $H1$ for the best hyperparameters yields no difference between the standard DRL approach and our proposed methods. This is due to the simplicity of the problem, where the majority of agents finds the optimal policy in the given amount of steps; thus, the mean performance over multiple seeds is close to the optimal performance with only little variation. 

%
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/tiger_crossval_box_with_max.png}
    \caption{Boxplots (color-coded) over all optimal agents of a hyperparameter search with 200 runs for each convexity method. An optimal agent is one which has reached the optimal policy in the given amount of training steps, and the number of optimal agents was for grad: 178, hard: 68, hess: 69, None: 193, point: 183. The agents have been trained on $p_{obs}=1.0$ and cross-evaluated on $p_{obs}=\{0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}$ with $10^5$ MC samples. Each boxplot includes the median as a blue horizontal line, interquartile range (IQR) as an opaque colored box, as well as the $1.5\cdot$IQR distances from the respective quartiles as whiskers; the maximum achieved value is marked with a colored hollow circle, other outliers are not visualized to avoid cluttering.
    }
    \label{fig:tiger_best_agents_cross_rewards}
\end{figure}
%

To test $H2$ for the best hyperparameters, we train the agents on a specific observation accuracy and cross-evaluate their performance on different observation accuracies. Since again, the majority of the hyperparameters converge to the optimal solution, we perform the cross-evaluation over all optimal agents. We observe that for agents trained on $p_{obs}=\{0.6, 0.8, 0.9\}$ there was no noticeable difference in the cross-evaluation performance between the individual convexity methods. Our explanation for this is that the optimal policy is characterized by two transition points from the action \emph{'listen'} to \emph{'open-left'/'open-right'}. Since these transition points lie very close to each other for $p_{obs} \in (0.5, 1.0)$ the methods do not have to perform a large extrapolation, which leads to almost identical results. For $p_{obs}=1.0$, however, the agent receives only the belief points $b=0.5$ and $b=1.0$ during training; thus, the agent does not know the location of the policy transition. For this case, the results are shown in Figure \ref{fig:tiger_best_agents_cross_rewards}, where the performance on the originally trained observation accuracy is the same for all agents (cf. $H1$), but the performance on $\{0.6, 0.8, 0.9\}$, in a distributional sense, is noticeably worse for the plain DRL approach compared to all convexity-enforced methods. This is reflected by lower medians (blue lines) and/or worse interquartile ranges. This shows that a well-behaved extrapolation of the value function can lead to better performance in out-of-distribution domains. We note that the max over all optimal agents is still the same for all methods. We suspect that this is again due to the simplicity of the Tiger problem.


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{FVRS}
\label{subsec:FVRS}
%
%

For FVRS, we do not consider the hard-enforced approach, because the value function is convex with regard to the belief inputs but not with regard to the position inputs. However, enforcing convexity of the neural network for only a subset of the inputs is not straightforward, hence we choose to leave this for further research. Moreover, we also do not consider the hessian approach to soft-enforced convexity. Computing second derivatives is simply too time-intensive for larger problems and one would choose other optimization algorithms (e.g., Newton) over gradient descent if second derivatives were available. 

Furthermore, for FVRS we use the LReLU activation functions, as we noticed that during training when the method converges to a stable policy (e.g., always go left), the weights of the hidden layers become high and negative. This ensures that the output is always -1 after passing through multiple ELU layers which yields the same Q-value for every possible combination of inputs. As a result, the method is stuck in this local minimum. To avoid this saturation, we switch to LReLU activation functions for the FVRS problem.

Moreover, we do hyperparameter searches for all convexity methods for the default (def) and heaviside (heavi) observation functions. We report the performances on the originally trained environment as well as the cross-evaluations on other observation functions in the same figures. The results over all samples are reported in Section \ref{subsec:app_FVRS}, whereas the performances of the hyperparameters over 10 runs for are shown in Figures \ref{fig:fvrs_def_cross_rewards_seed_eval} and \ref{fig:fvrs_heavi_cross_rewards_seed_eval}, respectively. When trained on the default setting (Figure \ref{fig:fvrs_def_cross_rewards_seed_eval}), both convexity approaches perform better than standard DRL, both on the original and all OOD domains. On the other hand, when trained on the heaviside observation function, the gradient-based approach emerges as the single clear winner over all observation functions. 

%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
%
%
%
\section{Conclusion and future work}
\label{sec:Conclusion}
%
In this work, we propose to extend DRL by enforcing the belief-convexity of the value function in the training process.
%To the best of our knowledge it is the first work towards a more theory-driven DRL approach.
We have shown that convexity-enforced DRL can yield notable improvements compared to the standard approach, such as better robustness over the hyperparameter space, as well as better mean performance of the best hyperparameters. Our approach performs particularly well when trained on edge case problems ($p_{obs}=1.0$ for Tiger and $p_{obs}=$ heavi for FVRS) and applying the policy to the standard problem formulation counterpart. This suggests that a well-behaved extrapolation of the value function leads to better policies when encountering OOD-data.

Based on the results in this work, we recommend the usage of the gradient-based enforcement, as it was better or at least equally good compared to the standard and point-based approach in every investigated setting.  

Several new empirical results are presented in this paper, yet there are still numerous open questions to be addressed. In particular, the largest performance gains of the convexity-enforced methods have been observed when training on edge cases, and extrapolating to the standard settings. This strongly suggests that these methods can improve performance when large extrapolations are needed. Hence, we anticipate particularly promising future research directions to be for cases where value extrapolations are required, i.e., in low data regimes, and higher dimensional problems. On the other hand, sampling-based techniques, as our soft-enforced methods, can face scalability challenges. Further investigations of the application to high-dimensional belief spaces are needed to fully grasp the potential of our proposed approaches.


Other potential research directions could be the investigation of convexity-informed DRL using actor-critic architectures, where the target directly is the value function, or the application of these methods to continuous-state POMDPs. Further developments of the convexity injection, e.g., heuristics for choosing an optimal value for $c$ in \Eqref{eq:mse_total_loss}, dynamic $c$ adjustments along the lines of LR-schedules, or including convexity loss every $k$ training steps to speed up the training process, can potentially lead to further improvements and training stability. 

%
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/fvrs_def_seed_eval_1std.png}
    \caption{Best agents (color-coded) evaluated for 10 runs for each convexity method. The agents have been trained on the \textbf{default} observation function and are cross-evaluated on the heaviside (heavi) and a set of $p_{obs}=\{0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}$ constant observation functions with $10^4$ MC samples. The figure shows the respective reward means (solid horizontal line) as well as $\pm$ 1 standard deviation (transparent bars).}
    \label{fig:fvrs_def_cross_rewards_seed_eval}
\end{figure}
%


%
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/fvrs_heavi_seed_eval_1std.png}
    \caption{Best agents (color-coded) evaluated for 10 runs for each convexity method. The agents have been trained on the \textbf{heaviside} observation function and are cross-evaluated on the default (def) and a set of $p_{obs}=\{0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}$ constant observation functions with $10^4$ MC samples. The figure shows the respective reward means (solid horizontal line) as well as $\pm$ 1 standard deviation (transparent bars).}
    \label{fig:fvrs_heavi_cross_rewards_seed_eval}
\end{figure}
%



%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
%
%
%
%
\section*{Acknowledgements}
This work was supported by the German federal ministry for economic affairs and climate action (BMWK) through the project BIG-ROHU in the aviation research program LUFO VI-3 and by the TUM Georg Nemetschek Institute Artificial Intelligence for the Built World.


%\clearpage
\bibliographystyle{unsrt}
\bibliography{main}
%\input{main.bbl}
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
%
%
%
%
%\begin{appendices}
\newpage
\appendix
%\gdef\thefigure{\thesection.\arabic{figure}}
%\gdef\thetable{\thesection.\arabic{figure}}
\counterwithin{figure}{section}
\counterwithin{table}{section}

\label{appendix}

%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Convexity violation}
\label{sec:app_convexity_violation}
%
To check whether the convexity violation of the standard approach is prevalent during training, we plot the value function of 6 example agents 
trained on the Tiger problem with $p_{obs}=1.0$ in \ref{fig:Ng100vf}. Note that not all None-based value functions showed convexity violations, but the majority. 

To show that the convexity enforcement approaches proposed in this work mitigate the convexity violations, we also plot the value function of 6 example agents trained on the same setting, but now with gradient-based enforcement. The choice for gradient-based enforcement is arbitrary, all other proposed methods show similar convexity corrections.



%
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \caption{None}
        \includegraphics[width=\textwidth]{figs/N100vf.png} % Replace with your image path
        \label{fig:Nonevf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \caption{grad}
        \includegraphics[width=\textwidth]{figs/g100vf.png} % Replace with your image path
        \label{fig:gradvf}
    \end{subfigure}
    %
    \caption{Tiger value function plot over the belief space for 6 example agents trained without (a) and with gradient-based convexity enforcement (b) on $p_{obs}=1.0$
    .}
    \label{fig:Ng100vf}
\end{figure}
%

%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Robustness over hyperparameter space}
\label{sec:app_robustness_over_hyperparameters}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Tiger}
\label{subsec:app_Tiger}
%

To test $H1$ over all hyperparameters, we show their achieved reward distributions for observation accuracies $p_{obs}=\{0.6, 0.8, 0.9, 1.0\}$ in Figure \ref{fig:tiger_all_params_reward_comp}.
The figure shows that, in a distributional sense, 
the performance is significantly worse for the hard-enforced and hessian soft-enforced convexity. Our explanation for this is that training is harder with these convexity enforcements. For the hard enforcement, we suspect that the additional adjustment of the weights after the backpropagation step, which assures that the output of the NN is convex with respect to the input, interferes with the training and hence it is harder to find the optimal policy. For the hessian enforcement, we suspect that calculating third order derivatives is not as stable, which results in slower learning.  

%
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/tiger_reward_comp_box_with_max.png}
    \caption{Boxplots (color-coded) over all agents of a hyperparameter search with 200 runs for each convexity method trained on $p_{obs}=\{0.6, 0.8, 0.9, 1.0\}$, and evaluated with $10^5$ MC samples. Each boxplot includes the median as a blue horizontal line, interquartile range (IQR) as an opaque colored box, as well as the $1.5\cdot$IQR distances from the respective quartiles as whiskers; the maximum achieved value is marked with a colored hollow circle, other outliers are not visualized to avoid cluttering.
    }
    \label{fig:tiger_all_params_reward_comp}
\end{figure}
%



%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{FVRS}
\label{subsec:app_FVRS}
%
The results for all agents trained on the default, and cross-evaluated on the heaviside and constant observation functions, are shown in Figure \ref{fig:fvrs_all_def_cross_rewards}. The grad- and point-based methods perform better in all settings compared to the standard approach in terms of their medians and third quartiles ($Q_3$). Note however, that the point method partially shows high variation, which is reflected by low first quartiles ($Q_1$). Overall the grad method shows the best performance based on highest $Q_1-Q_3$ (highest maximum performance is investigated in Section \ref{subsec:FVRS}).

Even more pronounced is the difference when more extrapolation capacities are needed, i.e., when training on the heaviside and evaluating on default and constant observation functions. The results for this setting are shown in Figure \ref{fig:fvrs_all_heavi_cross_rewards}, where the best agents of the grad and point approaches clearly perform better than the None counterpart, both on the original and OOD domains. There does not seem to be a clear difference when comparing the medians of point and None; grad however emerges as the clear best over all observation functions. 
%
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/fvrs_def_all_cross_eval_box_with_max.png}
    \caption{Boxplots (color-coded) over all agents of a hyperparameter search with 150 runs for each convexity method. The agents have been trained on the \textbf{default} (def) observation function and are cross-evaluated on the heaviside (heavi) and a set of $p_{obs}=\{0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}$ constant observation functions with $10^4$ MC samples. Each boxplot includes the median as a blue horizontal line, interquartile range (IQR) as an opaque colored box, as well as the $1.5\cdot$IQR distances from the respective quartiles as whiskers; the maximum achieved value is marked with a colored hollow circle, other outliers are not visualized to avoid cluttering.}
    \label{fig:fvrs_all_def_cross_rewards}
\end{figure}
%


%
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/fvrs_heavi_all_cross_eval_box_with_max.png}
    \caption{Boxplots (color-coded) over all agents of a hyperparameter search with 150 runs for each convexity method. The agents have been trained on the \textbf{heaviside} (heavi) observation function and are cross-evaluated on the default (def) and a set of $p_{obs}=\{0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}$ constant observation functions with $10^4$ MC samples. Each boxplot includes the median as a blue horizontal line, interquartile range (IQR) as an opaque colored box, as well as the $1.5\cdot$IQR distances from the respective quartiles as whiskers; the maximum achieved value is marked with a colored hollow circle, other outliers are not visualized to avoid cluttering.}
    \label{fig:fvrs_all_heavi_cross_rewards}
\end{figure}
%

%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tiger Problem}
\label{sec:app_tiger_problem}

%
\subsection{Tiger description}
\label{subsec:app_tiger_description}
%
%
The Tiger problem \citep{kaelbling1998planning} consists of an agent standing in front of two doors. Behind one of the doors there is a tiger ($T$), behind the other there is no tiger ($\Bar{T}$), and the agent does not know where the tiger is. At each timestep the agent can decide whether he wants to open one of the doors with the actions \emph{'open-right' / 'open-left'} ($O^R$ / $O^L$) or perform a listening ($L$) action. By listening, the agent hears the tiger roaring behind its true location with probability $p_{obs} \in [0.5, 1.0]$. 

Opening the door with the tiger behind yields a reward of $r(T)=-100$, opening the other door incurs $r(\Bar{T})=10$. Listening on the other hand, costs $r(L)=-1$. After opening a door, the environment resets with a new random tiger location.  

The environment state of the Tiger problem can be fully described with a scalar value denoting, e.g., the belief of the tiger being behind the left door $b \in [0.0, 1.0]$. In general, the optimal policy depends on the horizon $h$, i.e., the number of times the game is played. For the special cases of $h=1$ and $h\rightarrow \infty$, the optimal policy collapses to a relatively simple one. It consists of listening at every timestep until the belief that the tiger is at a given door falls below a certain optimal belief threshold $b_{opt}$, or the belief surpasses $1-b_{opt}$. For $h=1$, the associated optimal belief threshold $b_{opt}^1$ can be obtained by considering only the immediate expected rewards. The agent should open the door if the associated expected reward is higher than the expected reward of listening:
%
\begin{equation}
\begin{split}
    \E\left[\rr(b,O^{R/L})\right] &> \E\left[\rr(b,L)\right]
    \\[0.3em]
    \Longleftrightarrow \quad 
    b(T)r(T) + (1-b(T))r(\Bar{T}) &> r(L) \\
    \Longleftrightarrow \quad 
    b(T) &< \frac{r(L) - r(\Bar{T})}{r(T) - r(\Bar{T})} = b_{opt}^1.
\end{split}
\end{equation}
%
With the aforementioned rewards, $b_{opt}^1 = 0.1$. 

For $h\rightarrow\infty$, the derivation of the thresholds is generally not trivial, as the discounted expected reward of future games has to be considered in addition to the immediate reward, and the threshold depends on the observation accuracy. Thus, we use the \emph{pomdp} package, which is an \textsf{R} implementation of the well-known original \emph{pomdp-solve} developed by Antony Cassandra \citep{pomdp-solve-cassandra,pomdp-r}. Instead of calculating the thresholds, we extract the optimal policy graphs together with the associated belief points and optimal expected rewards achieved with discount factor $\gamma=0.9$ for the investigated observation accuracy cases. Computing the KL-divergence of the optimal policy graphs and the agent policies gives a faster way of determining whether the optimal policy is reached than with extensive MC simulation.

In Sections \ref{subsec:app_uninformative_observations} and \ref{subsec:app_perfect_observations} some special cases resulting from this general optimal policy are outlined, for which the optimal rewards and Q-values can be calculated analytically.

%
\subsection{Uninformative observations}
\label{subsec:app_uninformative_observations}
%
When $p_{obs}=0.5$, listening does not yield an improvement of the initial belief (hence, the explored belief space reduces to a single point $b^S=0.5$). Here, the optimal action is to listen at every timestep and the optimal reward $r^{*}_{UI}$ and optimal Q-values $Q^{*}(b, a_i)$ depending on the horizon $h$ and discount factor $\gamma$ are:
%
\begin{align}
    r^{*}_{UO} &= -\sum_{t=0}^{h} \gamma^t = - \frac{1 - \gamma^{h+1}}{1 - \gamma} \stackrel{h \rightarrow \infty}{=} -\frac{1}{1-\gamma}
    \\[0.5em]
    Q^{*}(b, L) &= r^{*}_{UO}
    \\[0.5em]
    Q^{*}(b^S, O^L) &= 0.5 \cdot 10 + 0.5 \cdot (-100) - \sum_{t=1}^{h} \gamma^t 
    \notag \\ 
    &= -45 - \gamma \frac{1-\gamma^{h}}{1-\gamma}
    \stackrel{h \rightarrow \infty}{=} -45 -\frac{\gamma}{1-\gamma}
    \\[0.5em]
    Q^{*}(b^S, O^R) &= Q^{*}(b^S, O^L)
\end{align}
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
\subsection{Perfect observations}
\label{subsec:app_perfect_observations}
%
When $p_{obs}= 1.0$, listening once directly yields the position of the tiger with certainty. Here, the optimal policy is to listen first and then to open the door opposite to the perceived roar. The optimal reward $r^{*}_{PO}$ and optimal Q-values for belief states $b^S=0.5$, $b^L=1.0$, $b^R=0.0$ depending on the trajectory until horizon $h$ and discount factor $\gamma$ are given in Equations \ref{eq:r*_tiger_perfect_obs} and \ref{tab:Q*_tiger_perfect_obs}
%
\begin{equation}
\begin{split}
\label{eq:r*_tiger_perfect_obs}
    r^{*}_{PO} &= -1 + 10 \gamma - \cdots + 10 \gamma^h = -\sum_{t=0}^{\frac{h-1}{2}} \gamma^{2t} + 10 \sum_{t=0}^{\frac{h-1}{2}} \gamma^{2t+1} \\[0.3em]
    &= -\frac{1-\gamma^{h+1}}{1-\gamma^2} + 10\frac{1-\gamma^{h+2}}{1-\gamma^2} \stackrel{h \rightarrow \infty}{=} \frac{10\gamma -1}{1-\gamma^2}.
\end{split}
\end{equation}
%
%
\begin{table}
%\renewcommand{\arraystretch}{1.5}
\caption{Optimal Q-values for beliefs $b^S=0.5, ~b^L=1.0, ~b^R=0.0$ and actions $L$, $O^L$, $O^R$ for horizon $h\rightarrow\infty$ and $r^*_{PO}$ given in \Eqref{eq:r*_tiger_perfect_obs}. \newline}
    \label{tab:Q*_tiger_perfect_obs}
    \centering
    %\begin{tabular}{c || C{2.5cm} | C{2.5cm} | C{2.5cm}}
    \begin{tabular}{c | c  c  c}
    \toprule
    %&  \multicolumn{3}{c}{Action} \\
    %\cmidrule(l){2-4}
        & $L$ & $O^L$ & $O^R$ \\
    \hline
    $b^S$ & $r^{*}_{PO}$ & $-45 + \gamma r^{*}_{PO}$ & $-45 + \gamma r^{*}_{PO}$ \\
    $b^L$ & $r^{*}_{PO}$ & $-100 + \gamma r^{*}_{PO}$ & $10 + \gamma r^{*}_{PO}$ \\ 
    $b^R$ & $r^{*}_{PO}$ & $10 + \gamma r^{*}_{PO}$ & $-100 + \gamma r^{*}_{PO}$ \\
    \bottomrule
    \end{tabular}
\end{table}
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
\section{FVRS problem}
\label{sec:app_fvrs_problem}
%
\subsection{FVRS description}
\label{subsec:app_fvrs_description}
%
The FieldVisionRockSample \citep{smith2012heuristicrocksample,ross2007aemsfvrs} problem is defined by a tuple $(n,k)$, where $n$ defines the width of a square grid and $k$ the number of rocks distributed randomly in the grid. The state of each rock is either good (GR) or bad (BR). The actions available to the agent at each timestep are to move north (MN), south (MS), east (ME), west (MW), or to perform a sampling (C) action. The starting point of the agent is randomly sampled along the west boundary of the grid, and there is an exit zone situated along the east boundary of the grid. 

Upon reaching the exit zone the agent receives the reward $r_E=10$. When the agent is located in the same grid cell as a rock, it can sample it. If the rock is good, the agent receives $r_{GR}=10$; if it is bad, the agent receives $r_{BR}=-10$. Otherwise, performing the sampling action with no rock present incurs a reward of 0. In addition, we punish the agent if it wants to move out of the grid at the west, north and south boundary. These illegal moves are associated with a reward of $r_{IM}=-10$. Hence overall, in order to maximize the expected sum of discounted rewards, the task of the agent is to sample good rocks and to reach the exit zone in as few steps as possible. Throughout this work we use $(n,k)=(4,4)$.

%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
\subsection{Belief update}
\label{subsec:app_fvrs_belief_update}

The standard belief update after on observation is given in \Eqref{eq_belief_update}.
%
%
Firstly, the dependence of $O$ on action $a_t$ can be dropped, since the observations only depend on the state of the rocks and their distance to the robot. Furthermore, an action does not change the state of the rocks \footnote{To be precise, sampling a good rock does change its state to a bad rock; this is however implemented as an additional step after the belief update and hence can be ignored here}, thus the transition probabilities reduce to $T(s_{t+1},s_t) = \delta_{t+1,t}$, where $\delta_{t+1,t}$ denotes the Kronecker delta. Hence, the belief update can be simplified to
%
\begin{equation}
    \label{eq_app_belief_update_simplified}
    b(s_{t+1}) \propto O\left(o_{t+1} \mid s_{t+1}\right) b(s_t).
\end{equation}
%


%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
%
\subsection{Ignoring rocks policy}
\label{subsec:app_fvrs_ignoring_rocks}
The first obvious stationary policy is if the agent completely ignores the rocks and collects the exit reward as fast as possible. Thus, the policy consists of choosing the action \emph{"move east"} at every timestep the reward resulting from this policy is
%
\begin{equation}
\label{eq:r*_fvrs_ignoring_rocks}
    r^* = \gamma^{n-1}r_E.%,
\end{equation}
%
%
The optimal Q-values under the policy of only moving east are then independent of the rock positions as well as the belief about the rock states, i.e., they only depend on the on the position of the agent relative to the exit zone. To simplify, we can formulate an averaged optimal Q-value $\Tilde{Q}^*(A) = \mathbb{E}_B\left[Q^*(b,A)\right]$ by averaging over all grid positions.
%
\begin{equation}
    \Tilde{Q}^*(ME) = \frac{\gamma^0r_E + \gamma^1r_E + \dots + \gamma^{n-1}r_E }{n} = \frac{r_E}{n}\sum_{t=0}^{n-1}\gamma^t = \frac{1-\gamma^n}{1-\gamma}\frac{r_E}{n}.
\end{equation}
%
Likewise, for actions $MN,~MS,~MW$ we have to take into account the delay of exit reward as well as potential negative rewards incurred due to illegal moves at the borders. Similarly, for action $C$, we have to consider the delay of exit reward as well as the expected reward for performing action $C$ at the agent's location. Since, in our case, the rocks are equally likely to be in a good or a bad state($p(GR)=p(BR)$), the rewards for a good and bad rock have the same absolute value $r_{GR}=-r_{BR}$, and the reward for performing action $C$ when there is no rock is 0, the overall expected collection reward is zero:
%
\begin{equation}
\begin{split}
    \mathbb{E}\left[\rr(C)\right] &= \rr(C|R)p(R) + \rr(C|\Bar{R})p(\Bar{R}) \\ 
    &= \frac{k}{n^2}\left[p(GR)r_{GR} + p(BR)r_{BR}\right] + \frac{n^2-k}{n^2}\rr(C|\Bar{R}) \\
    &= 0.
\end{split}
\end{equation}
%
Hence, the averaged optimal Q-values for each action are
%
\begin{align}
    \Tilde{Q}^*(MN) & = \frac{r_E}{n}\sum_{t=1}^{n}\gamma^t + \frac{r_{IM}}{n} 
    %= \frac{1-\gamma^n}{1-\gamma}\frac{\gamma r_E}{n} + \frac{r_{IM}}{n}
    = \gamma \Tilde{Q}^*(ME) + \frac{r_{IM}}{n}\\ 
    \Tilde{Q}^*(MS) & = \Tilde{Q}^*(MN) \\
    \Tilde{Q}^*(MW) & = \frac{r_E}{n}\sum_{t=2}^{n+1}\gamma^t + \frac{r_{IM}}{n} 
    %= \frac{1-\gamma^n}{1-\gamma}\frac{\gamma^2 r_E}{n} + \frac{r_{IM}}{n} 
    = \gamma^2 \Tilde{Q}^*(ME) + \frac{r_{IM}}{n} \\
    \Tilde{Q}^*(C) &= \gamma \Tilde{Q}^*(ME).
\end{align}


%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
%
\subsection{Convenience collection policy}
\label{subsec:app_fvrs_convenience_collection}
%
Another stable policy with higher reward than completely ignoring the rocks would be to keep moving towards the exit zone at every timestep. However, if a good rock lies incidentally on the path of the agent, it is collected. This policy is always  better or equal than ignoring the rocks, when the rewards are selected as
%
\begin{equation}
\begin{split}
    \gamma^{n-1} r_E & \leq  \gamma^{n}r_E + \gamma^{n-1}r_{GR} 
    \quad 
    %= \gamma^{n-1}\left(r_{GR} + \gamma r_E \right) \\
    \longrightarrow \quad \left(1-\gamma \right)r_E \leq r_{GR},
\end{split}
\end{equation}
%
which is guaranteed by the initial setup. To calculate the analytical reward and Q-values resulting from this policy is not straightforward, but can be easily obtained with Monte Carlo simulation.

%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Training specifications}
\label{sec:app_training_specifications}

We first start with a large number of hyperparameters and a broad range (i.e., multiple orders of magnitude) of possible values. For each convexity method, we then sample this space of possible hyperparameters with a fixed number of samples, which we call training runs, and let the agents train for a fixed number of timesteps (no early stopping). The sampling of the hyperparameter space is conducted via via Bayesian hyperparameter tuning \citep{wandbbayes}, which empirically finds better hyperparameters compared to their more prominent counterparts, namely grid and random search \citep{bergstra2012random}. At the end of each run, we evaluate the policies of the respective agents. In the general case, we estimate the achieved expected sum of discounted rewards with Monte Carlo simulation. For the Tiger problem, an optimal solution is available with classical methods; hence, we can additionally use the Kullback-Leibner divergence to filter for agents which achieved the optimal policy (e.g., in Figure \ref{fig:tiger_best_agents_cross_rewards}).

Since the agents usually find the optimal solution when trained for a sufficiently large number of steps, we heavily restrict the number of available steps for each training run. A few training runs were used for an initial estimate of the speed of convergence and a fraction of that was used for the cutoff. Based on this small pre-analysis, the maximum number of training steps is chosen as 5,000 and 50,000 for the Tiger and FVRS problems, respectively. 

For the Tiger problem, we consider the infinite-horizon stationary policy. With a discount factor of $\gamma=0.9$, the rollout depth $d_T$ was chosen as 150, where the reward's contribution to the sum of discounted rewards is in the order of $10^{-5}$. On the other hand, for the FVRS problem, we search for the policy which maximizes the expected rewards over a game instance, i.e., from the starting point until the robot reaches the end zone. We restrict the maximum rollout depth to $d_{FVRS}=n^2\cdot k$. Hence, this defines a limit policy of visiting every grid cell and sampling every rock.



%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Neural network specifications}
\label{sec:app_nn_specs}
%
For this work, we fixed a number of network and optimizer parameters, the specifications can be found in Tables \ref{tab:app_env_independent_NN_and_opt_params} and \ref{tab:app_env_specific_NN_and_opt_params}. Unless otherwise specified, we use the PyTorch default values. The resulting total number of trainable weights for the Tiger and FVRS networks are given as 174 and 32,106, respectively.

On the other hand, a number of parameters are selected to be optimized. We choose the Bayesian optimization method with an MC approximation of the expected sum of discounted rewards evaluated at the end of each training run as the maximization target. We reduce the learning rate when the target metric stops improving according to the \emph{ReduceLROnPlateau} %\footnote{See, e.g., \url{https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html}} 
scheduler (LRS). For agent exploration we employ the $\epsilon$-greedy scheme. The list of optimizable parameters along with their distributions and environment-specific bounds is given in Table \ref{tab:app_optimizable_params}.

Regarding the choice of the weight $c$ in \Eqref{eq:mse_total_loss}, the approach we took was to first train agents without the convexity loss and then to evaluate their convexity losses with respect to a convexity measure of choice. $c$ is then chosen such that the TD-MSE and the average convexity MSE loss are roughly equal. Future work can investigate this topic further, for a more systematic and general handling of this parameter, potentially providing even better performance enhancements.

%
\begin{table}[H]
  \caption{Environment-independent fixed network and optimizer parameters \newline}
  \label{tab:app_env_independent_NN_and_opt_params}
  \centering
  \begin{tabular}{ll}
    \toprule
    Parameter     & Tiger/FVRS value \\
    \midrule
    Architecture type & Dueling \citep{wang2016dueling} \\
    Target update type & hard \\
    Target update period & 3 \\
    
    Batch size & 20 \\
    Rollout steps & 25 \\
    Discount factor $\gamma$ & 0.9 \\
    
    Optimizer & Adam \citep{kingma2014adam} \\
    AMSGRAD & Included \citep{reddi2019amsgrad} \\
    Minimum learning rate & $10^{-4}$ \\
    
    Initial exploration rate & $0.5$ \\
    \bottomrule
  \end{tabular}
\end{table}
%
%
\begin{table}[H]
  \caption{Environment-specific fixed network and optimizer parameters \newline}
  \label{tab:app_env_specific_NN_and_opt_params}
  \centering
  \begin{tabular}{lll}
    \toprule
    Parameter     & Tiger & FVRS \\
    \midrule
    \# input nodes & 1 & 3k+2 \\
    \# output nodes & 3 & 5 \\
    \# FC Layer & 2 & 3  \\
    FC layer width & 10 & 100 \\
    \# value layers & - & 1 \\
    Lalue layer width & - & 50 \\
    \# advantage layers & - & 1 \\
    Advantage layer width & - & 50 \\ 
    
    Activation function & ELU & LeakyReLU \\
    Activation func. par. & 1 (scale) & 0.03 (neg. slope) \\
    
    Max epochs & 5,000 & 50,000 \\
    Max \# frames & 100,000 & 1,000,000 \\
    \bottomrule
  \end{tabular}
\end{table}
%
%
\begin{table}[H]
  \caption{Optimizable parameters \newline}
  \label{tab:app_optimizable_params}
  \centering
  \begin{tabular}{llll}
    \toprule
    Parameter & Distribution & Tiger bounds & FVRS bounds \\
    \midrule
    Initial learning rate & log-uniform & $[e^{-4}, ~e^{-1}]$ & $[e^{-7}, ~e^{-3}]$ \\
    Replay buffer size & int-uniform & $[1, ~10^5]$ & $[1, ~10^6]$ \\
    \# epochs per rollout & int-uniform & $[1, ~25]$ & $[1, ~25]$ \\
    LRS factor & uniform & $[0.8, 1.0]$ & $[0.8, ~1.0]$ \\
    LRS patience & int-uniform & $[1, ~10^4]$ & $[1, ~5\cdot10^4]$ \\
    \# $\epsilon$ steps & int-uniform & $[1, ~10^4]$ & $[1, ~10^5]$ \\
    Final $\epsilon$ & uniform & $[0.001, ~0.5]$ & $[0.001, ~0.5]$ \\
    \bottomrule
  \end{tabular}
\end{table}
%


\end{document}