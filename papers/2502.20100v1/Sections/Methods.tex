\subsection{Denoising Diffusion Probabilistic Models and RePaint} \label{subsection: Denoising Diffusion Probabilistic Models and RePaint}

Denoising Diffusion Probabilistic Models (DDPMs) are a type of generative model that learns to approximate a data distribution by reversing a gradual, multi-step noise addition process. They were first introduced by Sohl-Dickstein et al. \cite{sohl2015deep}, and subsequently improved upon by Ho et al. \cite{ho2020denoising}, and Nichol and Dhariwal \cite{nichol2021improved}. The latter showed that diffusion models can outperform GAN based models for image synthesis \cite{dhariwal2021diffusion}. Diffusion models are also appealing as they do not suffer from the training difficulties often encountered with GANs \cite{esan2023generative}. \newline

%In the forward process during training, denoising diffusion probabilistic models (DDPMs) define a diffusion process that transforms an initial image $x_0$ into approximately white Gaussian noise $x_T \sim \mathcal{N}(0, I)$ over  $T$ timesteps. Each forward step in this process is defined by
%\[
%q(x_t \mid x_{t-1}) = \mathcal{N}\left(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I \right)
%\]
%In this equation, noise is added to $x_{t-1}$ using Gaussian noise with variance $\beta_t$ and the previous latent $x_{t-1}$ is scaled by $ \sqrt{1 - \beta_t} $ according to a variance schedule \cite{ho2020denoising}. \newline

%The learning goal in a DDPM is to approximate the the reverse process $q(x_{t-1} \mid x_t, x_0)$, which is another Gaussian of which the mean and variance that can be written in closed form \cite{ho2020denoising}. This reverse process is modeled by
%\[
%p_\theta(x_{t-1} \mid x_t) = \mathcal{N}\left(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)\right)
%\]
%of which the mean $\mu_\theta(x_t, t) $ and optionally the variance $ \Sigma_\theta(x_t, t)$ are estimated by a neural network, typically a U-Net \cite{ronneberger2015u}. The mean and variance can then be learned by optimizing the variational bound on the negative log likelihood, which boils down to minimizing the Kullback-Leibler (KL) divergence between the two Guassian distributions $q(x_{t-1} \mid x_{t},x_{0})$ and $p_\theta(x_{t-1} \mid x_t)$. Because the mean and variance of the target distribution have closed expressions, the KL divergence can be computed analytically using the predicted mean and variance, allowing for direct optimization via gradient descent \cite{ho2020denoising}. \newline

The learning methodology of the DDPM has been progressively refined. The DDPM of Ho et al. estimates the noise of the Gaussian target distributions and uses it to calculate the latent of the previous step in the reverse process of the diffusion model. The variance is kept constant with a hyperparameter \cite{ho2020denoising}. Nichol and Dhariwal introduced several improvements to the method of Ho et al. These most important improvements are estimating the variance in addition to the mean, improving the noise scheduler, and using importance sampling during training \cite{nichol2021improved}. This work uses the DDPM described by Nichol and Dhariwal \cite{dhariwal2021diffusion}. \newline




RePaint \cite{lugmayr2022repaint} is a method that use a DDPM to replace specific regions of an image, as controlled by a given mask. During inference, the method takes as input an image and a mask. After each step of the reverse diffusion process, the part of the generated image that falls outside of the mask is replaced by the corresponding parts of the input image, with appropriate noise added for that step in the diffusion process. This ensures that only the image regions inside the mask are synthesized, while those outside are left unchanged, and thus allows the unconditionally trained DDPM to be guided by the input image during inference.



\subsection{Training of the diffusion model}\label{subsection: Training of the DDPM}


The goal of the proposed method is to enrich the variety of annotated datasets, so the training dataset of the DDPM should be diverse enough. The CAMUS dataset is mostly diverse enough for this purpose. However, since the majority of sector angles in the acquisitions is around $75^\circ$, a DDPM trained on the CAMUS dataset struggles to generate images with a different sector angle. Therefore, we apply a preprocessing augmentation step that randomly narrows the sector angle by up to 20 degrees, removing pixels along the peripheral scan lines, and then stretches the cut sector back to $256\times256$ pixels, as illustrated in Fig.~\ref{fig: sector_width_aug}. Additionally, to preserve data variety while reducing the training dataset size, only every Nth frame is used, where N is a random number between 8 to 12 frames. This reduces the training set size while preserving the variation since consecutive frames are often very similar due to the high acquisition framerate. \newline








%All diffusion models in the remainder of this work use this augmentation step during training, except for the model used in subsection \ref{subsection: Evaluation of generated images}. \newline

\begin{figure}[h]
\centering
  \centering
  \includegraphics[trim={0cm 0cm 0cm 0 cm}, clip,width = 1\linewidth]{figures/sector_width_aug.png}
  \caption{Sector width preprocessing augmentation on CAMUS performed before training of the DDPM. This was done to handle the lack of sector width variation in the CAMUS dataset. The sector angle is first reduced and then the sector is stretched back to 256 by 256 pixels. This step increases the variation in sector widths in the RePaint training set. These augmentations are applied solely during the training of the diffusion model and are not part of the generative augmentations described below.}
  \label{fig: sector_width_aug}
\end{figure}

The generative model in this work is the RePaint model as described by Lugmayr et al \cite{lugmayr2022repaint}, where the training and sampling process of the DDPM is replaced by the improved denoising diffusion probabilistic model as described by Nichol and Dhariwal
\cite{nichol2021improved}. Table \ref{table: characteristics diff_unet} summarizes the technical details.


\begin{table}
\scriptsize
  \centering\caption{Key characteristics of the U-Net and its training setup in the diffusion model. The "number of channels" row refers to the number of channels at the first, bottom, and final convolution layers of the U-Net architecture. The "Residual blocks" row refers to the number of blocks per spatial resolution level. For more details, see Nichol and Dhariwal \cite{dhariwal2021diffusion, nichol2021improved}.}
  \renewcommand{\arraystretch}{1} % Adjust row spacing
  \begin{tabular}{m{86pt}|m{130pt}}
    \toprule
    Number of parameters & 44.1 million \\
    Input size & $256 \times 256$ \\
    Number of channels &  64$\downarrow$ 256 $\uparrow$ 64 \\ 
    Lowest resolution & 8 $\times$ 8\\ 
    Upsampling scheme & Nearest neighbor interpolation\\ 
    Downsampling scheme & Average pooling \\ 
    Normalization scheme & GroupNorm \\ 
    Batch Size & 64 \\
    Optimizer & Adam \\
    Learning rate & 1e-4\\ 
    Learning rate scheduler & None \\ 
    Activation & SiLU \\
    Residual blocks & 3 \\ 
    Training steps & 500k \\
    Self-attention & At resolutions 8 and 16, 4 heads\\
    Diffusion steps & 4000 \\
    Noise scheduler & Cosine \cite{nichol2021improved}\\
    Learn variance & Yes \\
    Loss & Mean squared error, corresponding to the $L_{simple}$ learning objective in \cite{dhariwal2021diffusion}. \\
    \bottomrule
  \end{tabular}
      \label{table: characteristics diff_unet}
\end{table}




\subsection{Generative augmentations} \label{subsection: Generative augmentations}

\begin{figure*}[h]
\centering
  \centering
  \includegraphics[trim={0.75cm 0.25cm 0.75cm 0.25cm}, clip,width = 1\linewidth]{figures/aug_process.drawio.pdf}
  \caption{Process of creating generative augmentations. First, the frame is transformed with the transformation described in section \ref{subsection: Generative augmentations}. Then, the pixels outside the original sector are turned black. Finally, the generative model repaints all black pixels re-creating a complete sector in the process. The green contour delineates the part that is kept from the original image.}
  \label{fig: aug_process}
\end{figure*}

To apply generative augmentations to a cardiac ultrasound image, the image is first transformed using random depth, tilt, width and translation transformations as described below and illustrated in Fig.~\ref{fig: generative_aug_examples}. Then the trained diffusion model is applied to synthesize pixels outside of original input image using the RePaint method. Fig.~\ref{fig: aug_process} shows this process. Each image is transformed and augmented five times. The augmented training dataset then contains the original image and the five augmented samples.


\begin{itemize}
    \item \textbf{Depth increase}: the depth of the original image is increases randomly by $\lambda=[0,150]$ pixels by adding black pixels at the bottom of the image and then resizing to $256\times256$.
    \item \textbf{Tilt variation}: the original image is rotated with a random angle around the sector tip by $\theta$ degrees, with $-30^{\circ}<\theta<30^{\circ}$.
    \item \textbf{Sector width adjustment}: the width of the original image is multiplied by a factor $\lambda$. If $\lambda>0$, the image gets stretched out horizontally and cropped back to $256\times256$. If $\lambda<0$, the image gets squeezed into the center. Here, $0.5<\lambda<1.5$. This augmentation is similar to the work of Gazda et al. \cite{gazda2024generative}. Resizing to $256\times256$ distorts the image.
    \item \textbf{Translation}: the original image is shifted by a vector with a random angle and length $\lambda$, with $0<\lambda<50$ pixels. The result is cropped back to $256\times256$ pixels.
    \item \textbf{Combination}: all of the above augmentations are applied with a 50\% chance.
\end{itemize}

%After the transformations described above, all non-black pixels that fall outside the original sector are turned black. The model then repaints all black pixels in the new image. Figure \ref{fig: aug_process} shows this process. Each image is augmented five times.




%and the border of pixels between the non-black and black pixels. The latter avoids border artifacts in the inpainted image.  

% TODO: add a figure showing this?



%The repaint mask, the area designated for inpainting, includes all black pixels with a slight extension to prevent border artifacts. %As a result, the RePaint model does not need to reconstruct the sector's original shape.


\subsection{Survey}

To evaluate the realism of the generated images, a survey was conducted with three groups of human evaluators. The first consisted of three senior cardiologists certified by the EACVI in transthoracic echocardiography, each with over 15 years of experience and more than 10,000 examinations. The second group included four clinical researchers. The last group consisted of three engineers specializing in cardiac ultrasound. Each participant was asked to distinguish real from synthetic images. The real images were sampled randomly from the CAMUS dataset. The synthetic images were generated by the DDPM trained on the CAMUS dataset. The participants were given 50 pairs of images and were told one of the images in each pair was synthetic. The participants than had to select the synthetic image and give an explanation for their selection. Additionally, 5 of the 50 pairs contained two real images without the knowledge of the participants. \newline


\subsection{Segmentation ablation study}

\begin{table}[ht]
\scriptsize
\centering
\caption{Key characteristics of the nnU-Net used for segmentation \cite{isensee2021nnu, nnUNetV2}. The "number of channels" row refers to the number of channels at the first, bottom, and final convolution layers of the U-Net architecture. The "Residual blocks" row refers to the number of blocks per spatial resolution level. The augmentations listed here are performed on top of the proposed generative augmentations. For more details, see Isensee et al. \cite{nnUNetV2}.}
\begin{tabular}{m{83pt}|m{130pt}}
    \toprule
    Number of parameters & 33.4 million \\
    Input size & $256 \times 256$ \\
    Number of channels & 32 $\downarrow$ 512 $\uparrow$ 32 \\ 
    Lowest resolution & $4 \times 4$ \\ 
    Upsampling scheme & Transposed convolutions \\ 
    Downsampling scheme & Strided
convolutions \\ 
    Normalization scheme & InstanceNorm \\ 
    Batch Size & 49 \\
    Optimizer & Adam \\
    Initial learning rate & 1e-2 \\ 
    Learning rate scheduler & Polynomial annealing \\ 
    Loss function & Dice \& cross-entropy \\ 
    Inter-layer activation & Leaky ReLU \\ 
    Final layer activation & Softmax \\ 
    Residual blocks & 2\\
    Epochs & 500 \\
    Deep supervision & At resolutions 128, 64, 32, and 16\\ 
    \\
    Augmentations & Rotations, scaling, Gaussian noise, Gaussian blur, brightness, contrast, simulation of low resolution, gamma correction, and mirroring. \\
    \bottomrule
\end{tabular}
\label{table: characteristics nnunet}
\end{table}

The goal of the ablation study is to evaluate how different types of generative augmentations, described in subsection \ref{subsection: Generative augmentations}, improve segmentation performance. We use the nnU-Net framework \cite{isensee2021nnu, nnUNetV2} as the segmentation model, applying its default configuration but skipping cross-validation. Instead, a single model is trained on the dataset splits defined in section \ref{section: Datasets}. Table \ref{table: characteristics nnunet} lists the key characteristics of the nnU-Net used. \newline


For each type of generative augmentation, we create a training and validation set that combines all the original images frames with five randomly augmented images generated from each original image. Additionally, we use an baseline augmentation that applies the same transformations as the combination augmentation but does not repaint the missing parts, leaving those areas black. This allows us to evaluate whether the repainting actually compared to just applying the transformation augmentations. The resulting sets are then used to train separate models using the nnU-Net framework. The nnU-Net framework applies its regular annotations listed in table \ref{table: characteristics nnunet} on top of the augmentations described here. We evaluate each model’s performance on the original test sets of both CAMUS and HUNT4 datasets.


\subsection{Clinical evaluation on HUNT4}

This experiment compares the automatic segmentation-based ejection fraction (EF) trained with different augmented datasets to the manually measured EF using the clinical EchoPAC software from GE HealthCare. The automated estimation of EF is the same procedure as in previous works \cite{smistad2020real, van2024towards} and follows the steps outlined by the clinical guidelines for manual EF estimation \cite{lang2015recommendations}:

\begin{enumerate}
    \item Use the timing network proposed by Fiorito et al. \cite{fiorito2018detection} to detect the ED and ES frames of each cardiac cycle for both the A2C and A4C recordings of the same patient. Thus, the view is manually labeled during acquisition, while the timing is obtained through deep learning.
    \item Use the segmentation network to segment all ED and ES frames.
    %\item Algorithmically extract the apex and base points of the LV. Appendix \ref{TODO} describes this algorithm.
    \item Use the modified Simpson method to calculate the LV volume in ED and ES using the A2C and A4C frames. Each A2C frame is combined with each A4C frame for each cardiac cycle and the results are averaged.
    \item Calculate $EF=\frac{{ED\:volume}-ES\:volume}{ED\:volume}$
    
\end{enumerate}

When the segmentation fails for all heart cycles, the algorithm can not extract an EF value and the exam is omitted from analysis, similar to our previous work \cite{van2024towards}. For a fair comparison between the models trained with and without generative augmentations, we only include exams for which both versions manage to extract an EF value. Our results include 1872 out of 1900 patients in the HUNT4 EF set, which corresponds to a feasibility of 98.5\%. \newline




\subsection{Real-time demo}

To visually demonstrate the differences between the model trained with and without generative augmentations with varying acquisition parameters, a real-time application was created using the FAST framework \cite{smistad2015fast}. The application shows the segmentation output of the segmentation model trained without generative augmentations and the model trained with the combination of all generative augmentations side by side in real-time while streaming from a GE HealthCare Vivid E95 scanner. Fig.~\ref{fig: real_time_demo} shows a screenshot of the application. The video is available at \url{https://doi.org/10.6084/m9.figshare.28219919}.
These clearly demonstrate an increased segmentation model robustness in terms of acquisition parameters such as depth, angle and LV positioning.


\begin{figure}[h]
\centering
  \centering
  \includegraphics[trim={0.2cm 0.2cm 0.2cm 0.2cm}, clip,width = 1\linewidth]{figures/screenshot_realtime_demo.pdf}
  \caption{Screenshot of the real-time demo application. The left side shows the segmentation of the segmentation model trained in the usual way. The right side shows the segmentation of the same model trained with the combination of all generative augmentations.}
  \label{fig: real_time_demo}
\end{figure}


