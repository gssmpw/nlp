


\section{Introduction}


Ischemic heart disease is the leading cause of death worldwide, accounting for 13\% of all global fatalities and is the fastest rising cause of death since the beginning of the century \cite{who_global_health_estimates_2024}. Ultrasound imaging, being cost-effective, safe, and real-time, is the most common technology used for evaluating heart function. Accurately delineating the left ventricle and myocardium directly or indirectly enables the extraction of clinical measurements of heart function, such as ejection fraction (EF) and global longitudinal strain (GLS). However, measurements are labor intensive and even for experienced cardiologists there is high operator-related variability, with a coefficient of variation between 6 and 11\% \cite{armstrong2015quality}. Automating the process of extracting clinical measures from the recordings reduces inter-observer variability and allows measurements over multiple heart cycles, as recommended in the guidelines \cite{lang2015recommendations, olaisen2024automatic, nyberg2024deep, salte2023deep}, without additional labor. \newline


Due to data protection regulations and the labor-intensive process of data curation and annotation, only a limited number of open datasets are available, and those that exist are of limited size. Although guidelines for tracing cardiac structures such as the endocardium exist \cite{lang2015recommendations}, the amount of noise and artifacts in ultrasound, as well as structures such as the trabeculae, make tracing of the true endocardial wall challenging.
Thus, if one would ask multiple experts to annotate the endocardial wall, it would result in multiple different annotations. This variability in annotation preference and inter-vendor differences means that combining datasets from different centers should be done with care. This is in stark contrast with the field of computer vision for natural images, where large, labeled datasets are common. 
The sparsity of annotated data with a consistent annotation protocol creates unique challenges for deep learning networks, particularly in terms of robustness and generalization \cite{chen2020deep}. \newline


Several works have explored using generative AI to improve segmentation in cardiac ultrasound. the idea is to generate images conditioned on segmentation masks to create image-label pairs for training segmentation models. Gilbert et al. \cite{gilbert2021generating} and Tiago et al. \cite{tiago2022data} use generative adversial networks (GANs) for image generation from anatomical models in 2D and 3D respectively. Stojanovski et al. \cite{stojanovski2023echo} developed a conditional Denoising Diffusion Probabilistic
Models (DDPMs) that generates images from segmentation masks. They apply basic transformations to the segmentation masks of an existing labeled dataset and then feed these to the conditional DDPM to generate new images. Jafari et al. \cite{jafari2020cardiac} and Tiago et al. \cite{tiago2023domain} address domain translation, using CycleGANs and adversarial diffusion models respectively. \newline

%Generative models can be trained either conditionally or unconditionally. Conditionally trained models take a label as an extra conditioning input and thus can only be trained on labeled dataset. While conditioning is a powerful method to control the output of the generative model, it has the inherent limitation that the generative model can only be trained on labeled data. \newline


Generative models trained with conditioning on a segmentation mask have the inherent limitation that they can only be trained on labeled data. In cardiac ultrasound, typically only a portion of the data is labeled with pixel-wise segmentation masks as the annotation process is labor intensive. \newline


In this work, we develop a method that augments a labeled cardiac segmentation dataset using an unconditional diffusion model. Our method has two unique advantages. First, since it uses an unconditionally trained DDPM, a dataset can be augmented using a generative model trained on an unlabeled dataset or a dataset with different annotation conventions. Second, since our model does only alter the surroundings of the segmentation masks, the most crucial parts of the image remain untouched. Thus, fine annotation subtleties and details in the original image-label pair are not affected by the generative model. This distinguishes our approach from the work of Kupyn and Rupprecht \cite{kupyn2025dataset} who repaint the semantic object itself. \newline
%Domain-specific design, makes our proposed generative augmentations tailored for the use in cardiac ultrasound. \newline

The proposed method is most effective for improving the performance of a segmentation model trained on a dataset with limited variation in terms of acquisition and positioning of the left ventricle (LV) in the image. More specifically, this work uses the HUNT4Echo \cite{Olaisen2023-fy} dataset which has annotations of high quality which are time-consuming to obtain. The recordings in this dataset are LV-focused: the recordings were obtained following the clinical guidelines and maximize the area of the ventricle in the image \cite{lang2015recommendations}. Thus, the dataset has limited variety in terms depth, sector width, and positioning of the ventricle in the image. This leads to poor performance of segmentation models trained on this dataset when tested on an external dataset with more variation like the public CAMUS \cite{leclerc2019deep} dataset. In this work, we explore whether generative augmentations can be used to enrich this limited but high-quality dataset so that the resulting models can generalize better to datasets with more variation. \newline 


The contributions of this paper are:
\begin{itemize}
    \item A method for creating realistic augmentations of cardiac ultrasound images using DDPMs.
    \item A blinded expert evaluation of the realism of fully generated images.
    \item An ablation study that evaluates the effect of the proposed augmentations on the segmentation accuracy.
    \item A clinical evaluation of the augmentations effect on automatic segmentation-based ejection fraction measurements.
\end{itemize}


%\section{Background}

%\subsection{Other use cases of generative AI in cardiac ultrasound}

%Generative AI has also been applied in cardiac ultrasound for tasks other than segmentation. Reynaud et al. \cite{reynaud2023feature} use a diffusion model conditioned on a single frame to generate the remaining of the video. In another work \cite{reynaud2022d}, they additionally condition on the ejection fraction to simulate how the recording would have looked if the patient had a different ejection fraction. Stevens et al. \cite{stevens2024sequential} generates complete images from a sample of scan lines. The unconditional DDPM is guided by the limited information of the scan lines using active diffusion subsampling \cite{nolan2024active}. \newline
%Phi et al. \cite{nguyen2024training} use an unconditional DDPM to generate image-segmentation label pairs, initiating the reverse diffusion process from a noisy sketch of the segmentation mask rather than pure noise. 



%Other approaches generate images conditioned on segmentation masks to create image-label pairs for training segmentation models. \newline
% TODO drag you gan method




%This work proposes a novel method for enhancing a labeled cardiac ultrasound dataset for segmentation using generative augmentations. More specifically, the proposed method increases the variation in the data by using a generative model to imitate variations in acquisition in terms of depth, tilt, with and translation. As generative model, we use an unconditional diffusion model that can be trained on unlabeled data.










