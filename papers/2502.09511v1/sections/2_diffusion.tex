\section{Formulations of Diffusion Models}\label{sec:diffusion}

This fundamental framework of diffusion models comprises two parts: the \textit{forward diffusion process} and the \textit{reverse generation process}, as shown in \cref{fig:diffusion}. In the forward process, the model progressively adds noise to real data, eventually approaching a simple prior distribution.
In the reverse process, the model learns to progressively restore the data distribution from noise.
The reverse process is typically parameterized using neural networks.

This framework can be formulated in various ways, such as the Denoising Diffusion Probabilistic Models~\citep{DDPM}, Score Matching with Langevin Dynamics~\citep{SMLD}, the generalized Stochastic Differential Equations~\citep{SDE}, and other variants.


\subsection{Denoising Diffusion Probabilistic Models (DDPMs)}

DDPM~\citep{Diffusion,DDPM} is a classical diffusion model that performs step-by-step denoising using a fixed noise schedule. It employs two Markov chains for the forward and reverse processes. 

Starting with the original noise-free data $\bx_0$, the forward process transforms it into a sequence of noisy data $\bx_1, \bx_2, \ldots, \bx_T$ using a forward transition kernel:
\begin{equation}
q(\bx_t|\bx_{t-1}) = \mathcal{N}\left(\bx_t; \sqrt{\alpha_t}\bx_{t-1}, (1-\alpha_t)\bI\right),
\end{equation}
where $\alpha_t \in (0, 1)$ for $t = 1, 2, ..., T$ are hyperparameters that define the noise ratio at each step. $\mathcal{N}(\bx; \boldsymbol{\mu}, \mathbf{\Sigma})$ denotes a Gaussian distribution with mean $\boldsymbol{\mu}$ and covariance $\mathbf{\Sigma}$. A useful property of this Gaussian transition kernel is that $\bx_t$ can be directly derived from $\bx_0$ by:
\begin{equation}
q(\bx_t|\bx_0) = \mathcal{N}\left(\bx_t; \sqrt{\bar{\alpha}_t}\bx_0, (1-\bar{\alpha}_t)\bI\right),
\label{eq:forward-property}
\end{equation}
where $\bar{\alpha}_t := \prod_{i=1}^t \alpha_i$. Thus, $\bx_t$ is given by $\bx_t = \sqrt{\bar{\alpha}_t}\bx_0 + \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}$, where $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \bI)$. Typically, we set $\bar{\alpha}_T \approx 0$ so that $q(\bx_T) \approx \mathcal{N}(\bx_T; \mathbf{0}, \bI)$, allowing the reverse diffusion process to start from a random Gaussian noise.

The reverse transition kernel is parameterized by the neural networks $\boldsymbol{\mu}_{\theta}$ and $\mathbf{\Sigma}_{\theta}$:
\begin{equation}
p_\theta(\bx_{t-1}|\bx_t) = \mathcal{N}\left(\bx_{t-1}; \boldsymbol{\mu}_\theta(\bx_t, t), \mathbf{\Sigma}_\theta(\bx_t, t)\right),
\label{eq:reverse}
\end{equation}
where $\theta$ denotes $p_{\theta}$'s learnable parameters. 
The goal is to maximize the likelihood of the training sample $\bx_0$ by optimizing $p_\theta(\bx_0)$. This is achieved by minimizing the variational lower bound of the negative log-likelihood $\mathbb{E}[-\log p_\theta(\bx_0)]$.

DDPM simplifies the covariance matrix $\mathbf{\Sigma}_\theta$ in \cref{eq:reverse} to a constant-scaled matrix $\tilde{\beta}_t \bI$, where $\tilde{\beta}_t=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}(1-\alpha_t)$ varies across each step to control noise. Additionally, the mean $\boldsymbol{\mu}$ in \cref{eq:reverse} is expressed as a function of a learnable noise term:
\begin{equation}
\boldsymbol{\mu}_\theta(\bx_t, t) = \frac{1}{\sqrt{\alpha_t}} \left(\bx_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\bx_t, t) \right),
\end{equation}
where $\boldsymbol{\epsilon}_\theta$ is a network that predicts noise $\boldsymbol{\epsilon}$ for $\bx_t$ and $t$.
According to the property in \cref{eq:forward-property} and discarding the weight, DDPM simplifies the objective function to:
\begin{equation}
\mathbb{E}_{t,\bx_0,\boldsymbol{\epsilon}} \left[\left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta \left( \sqrt{\bar{\alpha}_t} \bx_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}, t \right) \right\|^2 \right].
\end{equation}
Eventually, samples are generated by removing noise from $\bx_T \sim \mathcal{N}(\bx_T; \mathbf{0}, \bI)$. Specifically, for $t = T, T-1, ..., 1$,
\begin{equation}
\bx_{t-1} \leftarrow \frac{1}{\sqrt{\alpha_{t}}} (\bx_{t} - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\bx_{t}, t)) + \sigma_t \bz,
\end{equation}
where $\bz \sim \mathcal{N}(\mathbf{0}, \bI)$ for $t = T, ..., 2$, and $\bz = \mathbf{0}$ for $t = 1$.

DDPM has been widely applied in generating molecules. Considering that DDPM is designed based on continuous data space, it is more commonly used for generating continuous 3D molecular structures, such as in EDM~\citep{EDM} and GeoDiff~\citep{GeoDiff}. For discrete 2D molecular structures, the discrete version of DDPM, Discrete Denoising Diffusion Probabilistic Model (D3PM), is typically employed, as exemplified by DiGress~\citep{DiGress}.



\begin{figure*}[t]
    \centering
    \resizebox{0.91\linewidth}{!}{
    \includegraphics{figures/figure2.pdf}}
    \caption{Illustration of molecular diffusion models, showcasing the forward and reverse processes. The three primary formulations—DDPM, SMLD, and SDE—are presented. Molecules can be generated in 2D space, 3D space, or jointly in 2D and 3D spaces.} 
    \label{fig:diffusion}
    \vspace{-6pt}
\end{figure*}

\subsection{Score Matching with Langevin Dynamics (SMLDs)}

SMLD~\citep{SMLD}, also known as score-based generative model (SGM), uses score matching theory to learn the score function of the data distribution,
% (i.e., the gradient of the log probability density), 
and combines it with Langevin dynamics for sampling. It comprises two main components: score matching and annealed Langevin dynamics (ALD). ALD generates samples iteratively using Langevin Monte Carlo, relying on the Stein score of a density function $q(\bx)$, defined as $\nabla_\bx \log q(\bx)$. Since the true distribution $q(\bx)$ is often unknown, score matching~\citep{ScoreMatching} approximates the Stein score with a neural network.

For efficiency, variants of score matching, such as denoising score matching~\citep{DenoisingScoreMatching},
are often used in practice.
Denoising score matching processes observed data using the forward transition kernel $q(\bx_t|\bx_0) = \mathcal{N}(\bx_t; \bx_0, \sigma_t^2 \bI)$, where $\sigma_t^2$ is a seiries of increasing noise levels for $t = 1, \ldots, T$, and then jointly estimates the Stein scores for the noise density functions $q_{\sigma_1}(\bx), q_{\sigma_2}(\bx), ..., q_{\sigma_T}(\bx)$.
The Stein score is approximated by a neural network $\bs_\theta(\bx, t)$ with $\theta$ as its learnable parameters. Thus, the objective function is defined as follows: 
\begin{equation}
\mathbb{E}_{t, \bx_0, \bx_t} \left[ \left\| \bs_\theta(\bx_t, t) - \nabla_{\bx_t} \log q_{\sigma_t}(\bx_t) \right\|^2 \right]. 
\end{equation}
With the Gaussian assumption of the forward transition kernel, the objective function can be rewritten as a tractable version: 
\begin{equation}
\mathbb{E}_{t, \bx_0, \bx_t} \left[ \delta(t) \left\| \bs_\theta(\bx_t, t) + \frac{\bx_t - \bx_0}{\sigma_t^2} \right\|^2 \right], 
\end{equation}
where $\delta(t)$ is a positive weight that depends on the noise scale $\sigma_t$. Once the score-matching network $\bs_\theta$ is trained, the ALD algorithm is used for sampling. It begins with a sequence of increasing noise levels $\sigma_1, \ldots, \sigma_T$ and an starting point \(\bx_{T,0} \sim \mathcal{N}(\mathbf{0}, \bI)\). For \(t = T, T - 1, \ldots, 0\), \(\bx_t\) is updated through \(N\) iterations that compute the following steps:
\begin{equation}
\bx_{t,n} \leftarrow \bx_{t,n-1} + \frac{1}{2} \eta_t \bs_\theta \left( \bx_{t,n-1}, t \right) + \sqrt{\eta_t} \bz, 
\end{equation}
where $\bz \sim \mathcal{N}(\mathbf{0}, \bI)$, $n = 1, \dots, N$, and $\eta_t$ is the update step. After $N$ iterations, the resulting $\bx_{t,N}$ becomes the starting point for the next $N$ iterations. $\bx_{0,N}$ will be final sample. 

SMLD has also been applied to generating molecules, such as in MDM~\citep{MDM} and LDM-3DG~\citep{LDM-3DG}. Subsequent studies demonstrat that SMLD and DDPM are theoretically equivalent and can both be regarded as discretizations of the SDE introduced in the next subsection.


\subsection{Stochastic Differential Equations (SDEs)}

Both DDPM and SMLD rely on discrete processes, requiring careful design of diffusion steps. \cite{SDE} formulate the forward process as an SDE, extending the discrete methods to continuous time space. The reverse process is modeled as a time-reverse SDE, enabling sampling by solving it. Let $\bw$ and $\bar{\bw}$ denote a standard Wiener process and its time-reverse, with continuous diffusion time $t \in [0, T]$. A general SDE is:
\begin{equation}
d\bx = f(\bx, t)dt + g(t)d\bw,
\end{equation}
where $f(\bx, t)$ and $g(t)$ are the drift coefficient and the diffusion coefficient for the diffusion process, respectively. 

The corresponding time-reverse SDE is defined as:
\begin{equation}
d\bx = \left[ f(\bx, t) - g(t)^2 \nabla_\bx \log q_t(\bx) \right] dt + g(t)d\bar{\bw}.
\end{equation}
Sampling from the probability flow ordinary differential equation (ODE) has the same distribution as the time-reverse SDE:
\begin{equation}
d\bx = \left[ f(\bx, t) - \frac{1}{2} g(t)^2 \nabla_\bx \log q_t(\bx) \right] dt.
\end{equation}
Here  $\nabla_\bx \log q_t(\bx)$ is the Stein score of the marginal
distribution of $\bx_t$, which is unknown but can be learned with a similar method as in
SMLD with the objective function:
\begin{equation}
\mathbb{E}_{t, \bx_0, \bx_t} \left[ \delta(t) \left\| \bs_\theta(\bx_t, t) - \nabla_{\bx_t} \log q_{0t}(\bx_t | \bx_0) \right\|^2 \right].
\label{eq:sde-objective}
\end{equation}

DDPM and SMLD can be regarded as discretizations of two SDEs.
Recall that $\alpha_t$ is a defined in DDPM and $\sigma_t^2$ denotes the noise level in SMLD. The SDE corresponding to DDPM is known as variance preserving (VP) SDE, defined as:
\begin{equation}
d\bx = -\frac{1}{2} \alpha(t)\bx dt + \sqrt{\alpha(t)}d\bw,
\end{equation}
where $\alpha(\cdot)$ is a continuous function, and $\alpha\left(\frac{t}{T}\right) = T(1 - \alpha_t)$ as $T \to \infty$. For the forward process of SMLD, the associated SDE is known as variance exploding (VE) SDE, defined
as:
\begin{equation}
d\bx = \sqrt{\frac{d \left[ \sigma(t)^2 \right]}{dt}} d\bw,
\end{equation}
where $\sigma(\cdot)$ is a continuous function, and $\sigma\left(\frac{t}{T}\right) = \sigma_t$ as $T \to \infty$.
Inspired by VP SDE, sub-VP SDE is designed and
performs especially well on likelihoods, given by:
\begin{equation}
d\bx = -\frac{1}{2} \alpha(t)\bx dt + \sqrt{\alpha(t) \left( 1 - e^{-2 \int_0^t \alpha(s)ds} \right)} d\bw.
\end{equation}

The objective function in \cref{eq:sde-objective} involves a perturbation distribution $q_{0t}(\bx_t | \bx_0)$ that varies for
different SDEs (i.e., VP SDE, VE SDE, sub-VP SDE). 
After $\bs_\theta(\bx, t)$ is trained, samples can be generated by solving the time-reverse SDE or the probability flow ODE with techniques such as ALD.

Because SDEs provide a continuous and flexible formulation that allows for improved control over generation processes, they have gradually replaced discrete-time formulations like DDPM and SMLD in molecular generative tasks~\citep{GDSS,EEGSDE,JODO}.


\subsection{More Variants}

These three formulations establish the theoretical foundation of diffusion models and demonstrate excellent performance in generative tasks. Building on them, diffusion models have spawned many variants and extensions aimed at enhancing generation efficiency or expanding application scenarios.
For example, 
\textit{Discrete Denoising Diffusion Probabilistic Models (D3PMs)}~\citep{D3PM} extend the DDPM to discrete data space, such as text or graphs. 
\textit{Latent Diffusion Models (LDMs)}~\citep{LDM} perform the diffusion process in latent space, significantly reducing computational complexity while maintaining generation quality.
\textit{Consistency Models (CMs)}~\citep{ConsistencyModel} focus on learning a single-step mapping from noise to data, enabling fast and high-quality sampling while maintaining consistency with the underlying data distribution.
\textit{Diffusion Bridges (DBs)}~\citep{DSB,SB-FBSDE,DDBM} extend diffusion models for generative tasks that connect different distributions, enabling efficient generation from one distribution to another.

These formulations propose innovative solutions tailored to different tasks, driving the widespread application in multi-modal generative tasks such as image, text, video, and graph~\citep{DiffusionSurvey1,DiffusionGraphSurvey1,DiffusionGraphSurvey2}.