@article{VOUDOURIS1999469,
title = {Guided local search and its application to the traveling salesman problem},
journal = {European Journal of Operational Research},
volume = {113},
number = {2},
pages = {469-499},
year = {1999},
issn = {0377-2217},
doi = {https://doi.org/10.1016/S0377-2217(98)00099-X},
url = {https://www.sciencedirect.com/science/article/pii/S037722179800099X},
author = {Christos Voudouris and Edward Tsang},
keywords = {Heuristics, Combinatorial optimization, Traveling salesman, Guided local search, Tabu search},
abstract = {The Traveling Salesman Problem (TSP) is one of the most famous problems in combinatorial optimization. In this paper, we are going to examine how the techniques of Guided Local Search (GLS) and Fast Local Search (FLS) can be applied to the problem. GLS sits on top of local search heuristics and has as a main aim to guide these procedures in exploring efficiently and effectively the vast search spaces of combinatorial optimization problems. GLS can be combined with the neighborhood reduction scheme of FLS which significantly speeds up the operations of the algorithm. The combination of GLS and FLS with TSP local search heuristics of different eiciency and effectiveness is studied in an effort to determine the dependence of GLS on the underlying local search heuristic used. Comparisons are made with some of the best TSP heuristic algorithms and general optimization techniques which demonstrate the advantages of GLS over alternative heuristic approaches suggested for the problem.}
}
@article{holland1973genetic,
  title={Genetic algorithms and the optimal allocation of trials},
  author={Holland, John H},
  journal={SIAM journal on computing},
  volume={2},
  number={2},
  pages={88--105},
  year={1973},
  publisher={SIAM}
}
@article{dorigo1996ant,
  title={Ant system: optimization by a colony of cooperating agents},
  author={Dorigo, Marco and Maniezzo, Vittorio and Colorni, Alberto},
  journal={IEEE transactions on systems, man, and cybernetics, part b (cybernetics)},
  volume={26},
  number={1},
  pages={29--41},
  year={1996},
  publisher={Ieee}
}
@inproceedings{
wang2024promptagent,
title={PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization},
author={Xinyuan Wang and Chenxi Li and Zhen Wang and Fan Bai and Haotian Luo and Jiayou Zhang and Nebojsa Jojic and Eric Xing and Zhiting Hu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=22pyNMuIoa}
}
@inproceedings{
yang2024large,
title={Large Language Models as Optimizers},
author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V Le and Denny Zhou and Xinyun Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=Bb4VGOWELI}
}
@inproceedings{
liu2024large,
title={Large Language Models to Enhance Bayesian Optimization},
author={Tennison Liu and Nicol{\'a}s Astorga and Nabeel Seedat and Mihaela van der Schaar},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=OOxotBmGol}
}
@inproceedings{
ma2024llamoco,
title={{LL}aMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation},
author={Zeyuan Ma},
booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=EKCubxFdOs},
note={under review}
}
@article{article,
author = {Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M. and Dupont, Emilien and Ruiz, Francisco and Ellenberg, Jordan and Wang, Pengming and Fawzi, Omar and Kohli, Pushmeet and Fawzi, Alhussein},
year = {2023},
month = {12},
pages = {},
title = {Mathematical discoveries from program search with large language models},
volume = {625},
journal = {Nature},
doi = {10.1038/s41586-023-06924-6}
}
@inproceedings{
ye2024reevo,
title={ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution},
author={Haoran Ye and Jiarui Wang and Zhiguang Cao and Federico Berto and Chuanbo Hua and Haeyeon Kim and Jinkyoo Park and Guojie Song},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=483IPG0HWL}
}
@inproceedings{Liu2024EvolutionOH,
  title={Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model},
  author={Fei Liu and Xialiang Tong and Mingxuan Yuan and Xi Lin and Fu Luo and Zhenkun Wang and Zhichao Lu and Qingfu Zhang},
  booktitle={International Conference on Machine Learning},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:266755732}
}
@article{Mahowald2023DissociatingLA,
  title={Dissociating language and thought in large language models: a cognitive perspective},
  author={Kyle Mahowald and Anna A. Ivanova and Idan Asher Blank and Nancy G. Kanwisher and Joshua B. Tenenbaum and Evelina Fedorenko},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.06627},
  url={https://api.semanticscholar.org/CorpusID:255941592}
}
@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=_VjQlMeSB_J}
}
@article{Liu2023LLMPEL,
  title={LLM+P: Empowering Large Language Models with Optimal Planning Proficiency},
  author={B. Liu and Yuqian Jiang and Xiaohan Zhang and Qian Liu and Shiqi Zhang and Joydeep Biswas and Peter Stone},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.11477},
  url={https://api.semanticscholar.org/CorpusID:258298051}
}
@misc{huang2022innermonologueembodiedreasoning,
      title={Inner Monologue: Embodied Reasoning through Planning with Language Models}, 
      author={Wenlong Huang and Fei Xia and Ted Xiao and Harris Chan and Jacky Liang and Pete Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and Pierre Sermanet and Noah Brown and Tomas Jackson and Linda Luu and Sergey Levine and Karol Hausman and Brian Ichter},
      year={2022},
      eprint={2207.05608},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2207.05608}, 
}
@misc{shinn2023reflexionlanguageagentsverbal,
      title={Reflexion: Language Agents with Verbal Reinforcement Learning}, 
      author={Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
      year={2023},
      eprint={2303.11366},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2303.11366}, 
}
@inproceedings{10.1145/3657604.3662042,
author = {Kumar, Harsh and Xiao, Ruiwei and Lawson, Benjamin and Musabirov, Ilya and Shi, Jiakai and Wang, Xinyuan and Luo, Huayin and Williams, Joseph Jay and Rafferty, Anna N. and Stamper, John and Liut, Michael},
title = {Supporting Self-Reflection at Scale with Large Language Models: Insights from Randomized Field Experiments in Classrooms},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3662042},
doi = {10.1145/3657604.3662042},
abstract = {Self-reflection on learning experiences constitutes a fundamental cognitive process, essential for consolidating knowledge and enhancing learning efficacy. However, traditional methods to facilitate reflection often face challenges in personalization, immediacy of feedback, engagement, and scalability. Integration of Large Language Models (LLMs) into the reflection process could mitigate these limitations. In this paper, we conducted two randomized field experiments in undergraduate computer science courses to investigate the potential of LLMs to help students engage in post-lesson reflection. In the first experiment (N=145), students completed a take-home assignment with the support of an LLM assistant; half of these students were then provided access to an LLM designed to facilitate self-reflection. The results indicated that the students assigned to LLM-guided reflection reported somewhat increased self-confidence compared to peers in a no-reflection control and a non-significant trend towards higher scores on a later assessment. Thematic analysis of students' interactions with the LLM showed that the LLM often affirmed the student's understanding, expanded on the student's reflection, and prompted additional reflection; these behaviors suggest ways LLM-interaction might facilitate reflection. In the second experiment (N=112), we evaluated the impact of LLM-guided self-reflection against other scalable reflection methods, such as questionnaire-based activities and review of key lecture slides, after assignment. Our findings suggest that the students in the questionnaire and LLM-based reflection groups performed equally well and better than those who were only exposed to lecture slides, according to their scores on a proctored exam two weeks later on the same subject matter. These results underscore the utility of LLM-guided reflection and questionnaire-based activities in improving learning outcomes. Our work highlights that focusing solely on the accuracy of LLMs can overlook their potential to enhance metacognitive skills through practices such as self-reflection. We discuss the implications of our research for the learning-at-scale community, highlighting the potential of LLMs to enhance learning experiences through personalized, engaging, and scalable reflection practices.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {86–97},
numpages = {12},
keywords = {field experiments, human-ai collaboration, large language models, learning engineering, self-reflection},
location = {Atlanta, GA, USA},
series = {L@S '24}
}
@inproceedings{
welleck2023generating,
title={Generating Sequences by Learning to Self-Correct},
author={Sean Welleck and Ximing Lu and Peter West and Faeze Brahman and Tianxiao Shen and Daniel Khashabi and Yejin Choi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=hH36JeQZDaO}
}
@misc{paul2024refinerreasoningfeedbackintermediate,
      title={REFINER: Reasoning Feedback on Intermediate Representations}, 
      author={Debjit Paul and Mete Ismayilzada and Maxime Peyrard and Beatriz Borges and Antoine Bosselut and Robert West and Boi Faltings},
      year={2024},
      eprint={2304.01904},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.01904}, 
}
@incollection{Matai10,
author = {Rajesh Matai and Surya Singh and Murari Lal Mittal},
title = {Traveling Salesman Problem: an Overview of Applications, Formulations, and Solution Approaches},
booktitle = {Traveling Salesman Problem},
publisher = {IntechOpen},
address = {Rijeka},
year = {2010},
editor = {Donald Davendra},
chapter = {1},
doi = {10.5772/12909},
url = {https://doi.org/10.5772/12909}
}
@misc{kool2019attentionlearnsolverouting,
      title={Attention, Learn to Solve Routing Problems!}, 
      author={Wouter Kool and Herke van Hoof and Max Welling},
      year={2019},
      eprint={1803.08475},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1803.08475}, 
}
@misc{applegate2006concorde,
  title={Concorde TSP solver},
  author={Applegate, David and Bixby, Ribert and Chvatal, Vasek and Cook, William},
  year={2006}
}
@book{emmons2012flow,
  title={Flow shop scheduling: theoretical results, algorithms, and applications},
  author={Emmons, Hamilton and Vairaktarakis, George},
  volume={182},
  year={2012},
  publisher={Springer Science \& Business Media}
}
@article{pan2021deep,
  title={Deep reinforcement learning based optimization algorithm for permutation flow-shop scheduling},
  author={Pan, Zixiao and Wang, Ling and Wang, Jingjing and Lu, Jiawen},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  year={2021},
  publisher={IEEE}
}
@article{fernandez2014insertion,
  title={On insertion tie-breaking rules in heuristics for the permutation flowshop scheduling problem},
  author={Fernandez-Viagas, Victor and Framinan, Jose M},
  journal={Computers \& Operations Research},
  volume={45},
  pages={60--67},
  year={2014},
  publisher={Elsevier}
}
@article{nawaz1983heuristic,
  title={A heuristic algorithm for the m-machine, n-job flow-shop sequencing problem},
  author={Nawaz, Muhammad and Enscore Jr, E Emory and Ham, Inyong},
  journal={Omega},
  volume={11},
  number={1},
  pages={91--95},
  year={1983},
  publisher={Elsevier}
}
@article{campbell1970heuristic,
  title={A heuristic algorithm for the n job, m machine sequencing problem},
  author={Campbell, Herbert G and Dudek, Richard A and Smith, Milton L},
  journal={Management science},
  volume={16},
  number={10},
  pages={B--630},
  year={1970},
  publisher={INFORMS}
}
@article{gupta1971functional,
  title={A functional heuristic algorithm for the flowshop scheduling problem},
  author={Gupta, Jatinder ND},
  journal={Journal of the Operational Research Society},
  volume={22},
  pages={39--47},
  year={1971},
  publisher={Springer}
}
@article{luo2023neural,
  title={Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization},
  author={Luo, Fu and Lin, Xi and Liu, Fei and Zhang, Qingfu and Wang, Zhenkun},
  journal={arXiv preprint arXiv:2310.07985},
  year={2023}
}
@inproceedings{kwon2021matrix,
  title={Matrix encoding networks for neural combinatorial optimization},
  author={Kwon, Yeong-Dae and Choo, Jinho and Yoon, Iljoo and Park, Minah and Park, Duwon and Gwon, Youngjune},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}
@article{arnold2019knowledge,
  title={Knowledge-guided local search for the vehicle routing problem},
  author={Arnold, Florian and S{\"o}rensen, Kenneth},
  journal={Computers \& Operations Research},
  volume={105},
  pages={32--46},
  year={2019},
  publisher={Elsevier}
}
@misc{alsheddy2018guided,
  title={Guided Local Search.},
  author={Alsheddy, Abdullah and Voudouris, Christos and Tsang, Edward PK and Alhindi, Ahmad},
  year={2018}
}
@software{ortools,
  title = {OR-Tools},
  author = {Laurent Perron and Vincent Furnon},
    year= {2023}
}
@article{reinelt1991tsplib,
  title={TSPLIB—A traveling salesman problem library},
  author={Reinelt, Gerhard},
  journal={ORSA journal on computing},
  volume={3},
  number={4},
  pages={376--384},
  year={1991},
  publisher={INFORMS}
}
@article{taillard1993benchmarks,
  title={Benchmarks for basic scheduling problems},
  author={Taillard, Eric},
  journal={european journal of operational research},
  volume={64},
  number={2},
  pages={278--285},
  year={1993},
  publisher={Elsevier}
}
@inproceedings{cheng-etal-2022-improving,
    title = "Improving Multi-turn Emotional Support Dialogue Generation with Lookahead Strategy Planning",
    author = "Cheng, Yi  and
      Liu, Wenge  and
      Li, Wenjie  and
      Wang, Jiashuo  and
      Zhao, Ruihui  and
      Liu, Bang  and
      Liang, Xiaodan  and
      Zheng, Yefeng",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.195",
    doi = "10.18653/v1/2022.emnlp-main.195",
    pages = "3014--3026",
    abstract = "Providing Emotional Support (ES) to soothe people in emotional distress is an essential capability in social interactions. Most existing researches on building ES conversation systems only considered single-turn interactions with users, which was over-simplified. In comparison, multi-turn ES conversation systems can provide ES more effectively, but face several new technical challenges, including: (1) how to adopt appropriate support strategies to achieve the long-term dialogue goal of comforting the user{'}s emotion; (2) how to dynamically model the user{'}s state. In this paper, we propose a novel system MultiESC to address these issues. For strategy planning, drawing inspiration from the A* search algorithm, we propose lookahead heuristics to estimate the future user feedback after using particular strategies, which helps to select strategies that can lead to the best long-term effects. For user state modeling, MultiESC focuses on capturing users{'} subtle emotional expressions and understanding their emotion causes. Extensive experiments show that MultiESC significantly outperforms competitive baselines in both dialogue generation and strategy planning.",
}
@inproceedings{vath-etal-2023-conversational,
    title = "Conversational Tree Search: A New Hybrid Dialog Task",
    author = {V{\"a}th, Dirk  and
      Vanderlyn, Lindsey  and
      Vu, Ngoc Thang},
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.91",
    doi = "10.18653/v1/2023.eacl-main.91",
    pages = "1264--1280",
    abstract = "Conversational interfaces provide a flexible and easy way for users to seek information that may otherwise be difficult or inconvenient to obtain. However, existing interfaces generally fall into one of two categories: FAQs, where users must have a concrete question in order to retrieve a general answer, or dialogs, where users must follow a pre-defined path but may receive a personalized answer. In this paper, we introduce Conversational Tree Search (CTS) as a new task that bridges the gap between FAQ-style information retrieval and task-oriented dialog, allowing domain-experts to define dialog trees which can then be converted to an efficient dialog policy that learns only to ask the questions necessary to navigate a user to their goal. We collect a dataset for the travel reimbursement domain and demonstrate a baseline as well as a novel deep Reinforcement Learning architecture for this task. Our results show that the new architecture combines the positive aspects of both the FAQ and dialog system used in the baseline and achieves higher goal completion while skipping unnecessary questions.",
}
@inproceedings{
hao2023reasoning,
title={Reasoning with Language Model is Planning with World Model},
author={Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and Daisy Zhe Wang and Zhiting Hu},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=VTWWvYtF1R}
}
@inproceedings{ma2023neu_kopt,
    title={Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt},
    author={Ma, Yining and Cao, Zhiguang and Chee, Yeow Meng},
    booktitle = {Advances in Neural Information Processing Systems},
    year={2023}
}
@article{sui2023neuralgls,
  title={NeuralGLS: learning to guide local search with graph convolutional network for the traveling salesman problem},
  author={Sui, Jingyan and Ding, Shizhe and Xia, Boyang and Liu, Ruizhi and Bu, Dongbo},
  journal={Neural Computing and Applications},
  pages={1--20},
  year={2023},
  publisher={Springer}
}
@inproceedings{hudson2022_gnn_gls,
  title={Graph Neural Network Guided Local Search for the Traveling Salesperson Problem},
  author={Hudson, Benjamin and Li, Qingbiao and Malencia, Matthew and Prorok, Amanda},
  booktitle={International Conference on Learning Representations},
  year={2022}
}
@article{arnold2019_KGLS_VRP,
  title={Knowledge-guided local search for the vehicle routing problem},
  author={Arnold, Florian and S{\"o}rensen, Kenneth},
  journal={Computers \& Operations Research},
  volume={105},
  pages={32--46},
  year={2019},
  publisher={Elsevier}
}
@ARTICLE{10065588,
  author={Zhang, Fangfang and Mei, Yi and Nguyen, Su and Zhang, Mengjie},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Survey on Genetic Programming and Machine Learning Techniques for Heuristic Design in Job Shop Scheduling}, 
  year={2024},
  volume={28},
  number={1},
  pages={147-167},
  keywords={Job shop scheduling;Machine learning;Dynamic scheduling;Sequential analysis;Genetic programming;Schedules;Automatic learning;genetic programming (GP);job shop scheduling (JSS);machine learning;scheduling heuristics},
  doi={10.1109/TEVC.2023.3255246}}
@article{10.5555/3586589.3586778,
author = {Chen, Tianlong and Chen, Xiaohan and Chen, Wuyang and Wang, Zhangyang and Heaton, Howard and Liu, Jialin and Yin, Wotao},
title = {Learning to optimize: a primer and a benchmark},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {Learning to optimize (L2O) is an emerging approach that leverages machine learning to develop optimization methods, aiming at reducing the laborious iterations of hand engineering. It automates the design of an optimization method based on its performance on a set of training problems. This data-driven procedure generates methods that can efficiently solve problems similar to those in training. In sharp contrast, the typical and traditional designs of optimization methods are theory-driven, so they obtain performance guarantees over the classes of problems speci_ed by the theory. The difference makes L2O suitable for repeatedly solving a particular optimization problem over a specific distribution of data, while it typically fails on out-of-distribution problems. The practicality of L2O depends on the type of target optimization, the chosen architecture of the method to learn, and the training procedure. This new paradigm has motivated a community of researchers to explore L2O and report their findings.This article is poised to be the first comprehensive survey and benchmark of L2O for continuous optimization. We set up taxonomies, categorize existing works and research directions, present insights, and identify open challenges. We benchmarked many existing L2O approaches on a few representative optimization problems. For reproducible research and fair benchmarking purposes, we released our software implementation and data in the package Open-L2O.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {189},
numpages = {59},
keywords = {learning to optimize, meta learning, optimization, algorithm unrolling}
}
@inproceedings{Sttzle1998ApplyingIL,
  title={Applying iterated local search to the permutation ow shop problem},
  author={Thomas Sttzle},
  year={1998},
  url={https://api.semanticscholar.org/CorpusID:6885283}
}
@article{XING202457,
title = {Multi-sensor dynamic scheduling for defending UAV swarms with Fresnel zone under complex terrain},
journal = {ISA Transactions},
volume = {153},
pages = {57-69},
year = {2024},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2024.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S001905782400377X},
author = {Zehua Xing and Shengbo Hu and Ruxuan Ding and Tingting Yan and Xia Xiong and Xu Wei},
keywords = {Combinatorial optimization, Complex terrain, Defending UAV swarms, Fresnel zone, Sensor scheduling},
abstract = {The increasing role of unmanned aerial vehicle (UAV) swarms in modern warfare poses a significant challenge to ground and air defense systems. Considering complex terrain environments and multi-sensor resources including radar and photoelectric systems constraints, a novel multi-sensor dynamic scheduling algorithm is proposed in this paper. Firstly, a transmission model with Fresnel zone under complex terrain and sensor models for radar/photoelectric systems are established. Considering the constraints of 6 factors, such as pitch angle, array scanning angle and threat levels, a detection model is developed subsequently. Secondly, to meet the real-time requirements of ground and air defense systems, a fast calculation method for Fresnel zone clearance using adaptive buffer is achieved. Thirdly, an improved Hungarian algorithm is proposed to solve the combinatorial optimization problem of sensor scheduling. Finally, simulation experiments are conducted to evaluate the algorithm performance under different conditions. The results demonstrate that the proposed approach significantly reduces the sensor switching rate while achieving a high sensor-UAV matching rate and high-threat matching rate. Furthermore, the simulation results verify the effectiveness of the proposed algorithm when applied to multi-sensor scheduling for defending UAV swarms.}
}
@article{wang2021deep,
  title={Deep reinforcement learning for transportation network combinatorial optimization: A survey},
  author={Wang, Qi and Tang, Chunlei},
  journal={Knowledge-Based Systems},
  volume={233},
  pages={107526},
  year={2021},
  publisher={Elsevier}
}
@article{zhao2024combinatorial,
  title={A Combinatorial Optimization Analysis Method for Detecting Malicious Industrial Internet Attack Behaviors},
  author={Zhao, Kejing and Zhang, Zhiyong and Raymond Choo, Kim-Kwang and Zhang, Zhongya and Zhang, Tiantian},
  journal={ACM Transactions on Cyber-Physical Systems},
  volume={8},
  number={1},
  pages={1--20},
  year={2024},
  publisher={ACM New York, NY}
}
@article{witt2024ilp,
  title={ILP-based resource optimization realized by quantum annealing for optical wide-area communication networks—A framework for solving combinatorial problems of a real-world application by quantum annealing},
  author={Witt, Arthur and Kim, Jangho and K{\"o}rber, Christopher and Luu, Thomas},
  journal={Frontiers in Computer Science},
  volume={6},
  pages={1356983},
  year={2024},
  publisher={Frontiers Media SA}
}