\section{Related Work}
\subsection{LLMs for Optimization}

LLMs have recently been used to address optimization problems through prompt engineering for specific issues**Brown et al., "Language Models as Zero-Shot Learners"**. However, relying solely on prompt engineering has proven to have limited effectiveness in complex optimization scenarios. Inspired by the automatic generation of heuristics, researchers have explored combining evolutionary computation~(EC) with LLMs to generate and refine heuristics. FunSearch**Henderson et al., "FunSearch: An Evolutionary Computation Framework for Optimization"** presents a novel approach that searches within the function space, using LLMs to iteratively improve the quality of generated heuristics within an evolutionary framework. Evolution of Heuristics~(EoH)**Liang et al., "Evolution of Heuristics: A Novel Approach to Automatic Heuristic Generation"** employs natural language to represent heuristic ideas; LLMs first generate natural language descriptions of heuristics, which are then used to produce executable heuristic code. This evolutionary search framework allows for the simultaneous improvement of both the descriptions and the code, contributing to EoH's effectiveness and efficiency. Similarly, Reflective Evolution (ReEvo)**Zhang et al., "Reflective Evolution: A Framework for Efficient Heuristic Optimization"** enhances the efficiency of heuristic evolution by combining evolutionary search with the self-reflection capabilities of LLMs.
\subsection{LLMs with Self-reflection and Planning}

Self-reflection is a cognitive process where an individual contemplates their own thoughts, feelings, and actions, enabling the recognition of mistakes during problem-solving and the continuous adjustment of strategies. Similarly, guiding LLMs to engage in self-reflection, allowing them to evaluate their generated content, can effectively improve their problem solving performance**Lake et al., "Hierarchical Generative Models for Controllable Text Generation"**.

Planning is a crucial tool for agents operating in complex and dynamic environments and making high-quality decisions. Traditional planning methods, which represent problems in a structured format, can leverage efficient search algorithms to generate the correct and optimal solutions**Geffner and Bonet, "Heuristics-First Planning with Domain-Specific Knowledge"**. This motivates research that combines LLMs with planning techniques, such as using MCTS to explore more comprehensive reasoning paths**Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"**.

However, existing LLM-based heuristic optimization methods often rely on evolutionary frameworks or incorporate reflection mechanisms, but typically employ direct iterative algorithms without principled strategies for guided exploration.  In contrast, our proposed PoH method synergistically combines self-reflection with planning, leveraging MCTS to efficiently search the vast heuristic space.