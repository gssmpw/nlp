\section{Preliminaries}
\textbf{LoRA.} To reduce computational and memory requirements when fine-tuning LLMs, Low-Rank Adaptation (LoRA) \citep{hu2021lora} was introduced to drastically reduce the number of trainable parameters while fine-tuning. This is achieved by representing the weight updates $\Delta W$ as the product $\Delta W=BA$ of two low-rank matrices $A$ and $B$. LoRA enables efficient adaptation of LLMs to specific tasks while preserving the generalization capabilities of the underlying model, as gradients often exhibit a low intrinsic dimension \citep{li2018intrinsicdim, aghajanyan2020intrinsicdim}. Additionally, LoRA offers a notable advantage in an FL scenario by drastically reducing the amount of data exchanged between participants during each round. In our experiments, we achieved a reduction by a factor of 130. 

\textbf{Federated Learning.} Federated learning (FL) has been widely-studied for deep learning models in cross-silo settings \cite{huang2022fl}, where a limited number of resource-rich clients, such as organizations or institutions, collaboratively train ML models without sharing their data.
In conventional FL, the global objective function of $N$ clients is defined as
\begin{equation}
\label{fl-opt}
    \min_W F(W) = \sum_{k=1}^N p_k f_k(W),
\end{equation}
where $W$ represents parameters of a model, $\sum_{k=1}^N p_k=1$ and $f_k(W)$ is the local objective function of client $k$. Local training data $\mathcal{D}_k$ between clients often heterogeneous. A common strategy for solving Equation \ref{fl-opt} is Federated Averaging (FedAvg) \citep{mcmahan2016fedavg}. In FedAvg, clients conduct a round $t$ of training and $\theta_{t+1}$ (parameters after round $t)$ is updated as the $p_k$-weighted average of the respective $k$ gradients. These gradient weights $p_k$ can be set as $p_k=\frac{|\mathcal{D}_k|}{\sum_{k=1}^N |\mathcal{D}_k|}$ to mitigate data size bias, which we use in this work. FL has been recently applied to LLMs \cite{ye2024openfedllm, thakkar2020understanding, liu2024differentially, ramaswamy2020training} leveraging FedAvg to aggregate locally-trained model updates. In this work, we conduct experiments using LoRA-based fine-tuning and full model fine-tuning for local iterations in FL. Besides reducing communication costs, clients benefit computationally from using LoRA during local training.

\textbf{Memorization Definition.} Following previous work \citep{ippolito2023verbatim, huang2024demyst, hans2024goldfish}, we adopt the "extractable memorization" definition of \citet{memo}. Consider a string representable as a concatenation $[p||s]$ where $p$ is a prefix of length $k$ and $s$ is the remainder of the string. We define the string $s$ to be \textit{memorized with $k$ tokens of context} by a language model $f$ if $[p||s]$ is contained in the training data of $f$, and $f$ produces $s$ when prompted with $p$ using greedy decoding. In other words, we consider a string from training data memorized if an LLM can generate it when prompted by a prefix. 