\section{Introduction}
\label{introduction}
Large language models (LLMs) have been shown to achieve state-of-the-art performance over most relevant natural language processing (NLP) tasks \citep{zhao2023survey}. There is an emerging and significant interest in fine-tuning LLMs to conduct tasks over specialized domains such as medicine \citep{thirunavukarasu2023large, yang2022large} and finance \citep{wu2023bloomberggpt, li2023large}. These fields handle inherently sensitive user data, necessitating additional mechanisms to prevent data exposure. A well-studied paradigm for collaboratively training a machine learning (ML) model over a cluster of clients without sharing local data is federated learning (FL) \citep{mcmahan2016fedavg, kairouz2021advances}. 

Although FL respects data sovereignty by allowing training samples to remain decentralized, most FL works do not address the memorization problem: an FL-trained LLM may still memorize client training data. Indeed, memorization is observable in most, if not all, LLMs \citep{carlini2019secret, carlini2022quantifying, carlini2021extracting}, with some work arguing that memorization is required to learn natural speech patterns \citep{dourish2004we, feldman2020does}. While there is a wealth of research focused on preventing data reconstruction \citep{huang2021evaluating} and improving differential privacy \citep{el2022differential} within the FL literature, very few have explored the propensity and prevention of FL-trained LLMs to leak training data \citep{thakkar2020understanding}. 

In this work, we demonstrate an intuitive and efficient strategy for reducing memorization during LLM fine-tuning: low-rank adaptation (LoRA) \citep{hu2021lora}. In fact, we observe that LoRA fine-tuning mitigates regurgitation of synthetically-injected sensitive data in both the federated and centralized settings. This includes exact token matching \citep{carlini2022quantifying} and approximate reproduction \citep{ippolito2023verbatim}. As LoRA combines the benefits of reduced computational \citep{hu2021lora}, memory \citep{dettmers2024qlora}, and communication overhead \citep{liu2024differentially}, its added benefit of preventing memorization makes it an ideal strategy for FL fine-tuning of LLMs. 

Our contributions are as follows:
\begin{itemize}
    \item We discover and demonstrate that LoRA mitigates memorization in federated and centralized learning. This includes exact match rate (repeating training data exactly) and paraphrasing (partial overlap). Compared to full fine-tuning, LoRA can significantly reduce memorization even when sensitive data is replicated and the LLM is prompted with long prefixes of a sequence. 
    \item We comprehensively test models of varying size from the Llama-2 family, Llama-3 family, and Mistral-v0.3 on medical question-answering tasks to simulate a data-sensitive scenario. LoRA effectively reduces memorization while preserving high performance accuracy. 
    \item We experimentally explore how LoRA interacts with other privacy strategies. This includes differential privacy mechanisms such as gradient noising and clipping, Goldfish loss \citep{hans2024goldfish}, and post-training noise injection. We find that LoRA works synergistically with these other approaches.
    \item To facilitate reproducibility and further research, we publicly release our code and instructions at \url{https://github.com/tuneinsight/federated-llms}.
\end{itemize}