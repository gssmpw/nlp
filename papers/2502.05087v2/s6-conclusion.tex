\section{Conclusion and Limitations}
In this work, we demonstrate that LoRA is capable of reducing memorization of fine-tuning training data. In particular, this effect is observable in both centralized learning and federated learning (FL), and we find this effect is especially pronounced in the latter. Moreover, it is possible to further reduce memorization by combining LoRA with other strategies such as Goldfish loss or conventional privacy-preserving mechanisms such as Gaussian noising and gradient clipping. FL was previously shown to reduce memorization for simple LSTM-based next-word predictors \citep{hard2018federated, thakkar2020understanding} and we demonstrate that generative LLMs inherit this benefit as well. However, further theoretical analysis of this phenomenon, which may relate to the LoRA reductive effect, is needed. 

We note that LoRA is only suitable for fine-tuning, while other techniques are required for the pre-training phase. The impact of LoRA on memorization during pre-training remains an open question for future work. Additionally, further research is needed to determine whether LoRA mitigates data regurgitation under alternative definitions of memorization \citep{schwarzschild2024rethinking}.