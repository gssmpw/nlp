\section{Related Work}
\label{sec:related_work}
\subsection{Privacy in LLMs}
Exposure of sensitive data via generative models has been extensively considered in existing literature, though the choice of the privacy evaluation metric continues to evolve.

\textbf{Differential privacy.} Classical $(\epsilon, \delta)$-differential privacy (DP) frameworks formally measure the privacy-preserving capacity of an algorithm by analyzing whether the probability of observing an output changes by $\epsilon$ when the underlying database excludes or includes a user record \citep{dwork2006calibrating}.
The application of this framework to generative language tasks, in general, has proven complicated due to the rigid definition of a user record \citep{jayaraman2019evaluating}. When directly applying DP to prevent sensitive data reconstruction, it has been shown that a non-negligible compromise on privacy is required to maintain performance \citep{lukas2023analyzing}. The conventional technique of adding Gaussian noise onto clipped gradients \citep{abadi2016deep} to boost privacy has also been shown to affect model outputs: the randomness of the noise alone can significantly alter the outputs of two equally-private models \citep{kulynych2023arbitrary}. One must consider the context and length of a prompt that goads an LLM into leaking sensitive information \citep{nissenbaum2004privacy, dourish2004we} -- a condition absent from the DP perspective \citep{brown2022does}. 

\textbf{Memorization.} The ability of language models (large or otherwise) to regurgitate pieces of their training data is well-documented. However, the question of \textit{how best} to quantify the memorization capacity of an LLM is an active area of research. A seminal work by Carlini et al. introduced ``canaries", which are synthetic, out-of-distribution pieces of text injected into training data (such as \texttt{"My SSN is XXX-XX-XXXX"}) \citep{carlini2019secret}. The approach is computationally expensive, as it requires perplexity comparisons against many thousands of random sequences, and canaries should be inserted anywhere from 1 to 10,000 times to gather a full picture of exposure, thus requiring significant fine-tuning. However, it has found use in production-level studies \citep{ramaswamy2020training} and adjacent fields such as machine unlearning \citep{jagielski2022measuring}. An alternative proposal of memorization \citep{carlini2022quantifying}, the completion metric, adopted by our work, measures how often an LLM completes a piece of text taken from the training text when prompted on an initial portion (prefix) of it.

\subsection{Federated Learning}

\textbf{Privacy in FL.} Federated learning, although initially designed to protect user data \citep{mcmahan2017communication}, did not foresee leakage in the form of regurgitation as its advent preceded the development of high-performing generative language models \citep{kairouz2021advances}. Consequently, studies on the memorization capacity of FL-trained LLMs remain limited. An early survey demonstrated that federated averaging \citep{thakkar2020understanding} ameliorates unintended memorization, though only for a tiny 1.3M parameter next-word predictor \citep{hard2018federated}. However, the authors' observations on the success of non-independent and identically distributed (non-IID) clustering for improved privacy informed our federated training strategy. The addition of the DP Gaussian mechanism was shown to improve canary-based memorization for a production FL setting \citep{ramaswamy2020training}. Similar to us, \citet{liu2024differentially} leverage LoRA to conduct efficient fine-tuning. However, this work is exclusively interested in studying performance under varying budgets within the $(\epsilon, \delta)$-DP framework and does not consider memorization under the canary or completion-based framework.

\textbf{Medical applications.} Our emphasis on medical datasets is relevant: LLMs have been shown to regurgitate sensitive medical data in \citet{lehman2021does}, though their work relies on an older BERT model. 
\citet{mireshghallah2022quantifying} study the success of membership inference attacks on i2b2, though they also do not use any memorization metrics. Although federated learning has been studied and championed as an ideal paradigm for clinical settings \citep{xu2021federated, nguyen2022federated, antunes2022federated}, there is a relative lack of literature in the context of clinical memorization.