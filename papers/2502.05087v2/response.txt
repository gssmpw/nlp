\section{Related Work}
\label{sec:related_work}
\subsection{Privacy in LLMs}
Exposure of sensitive data via generative models has been extensively considered in existing literature, though the choice of the privacy evaluation metric continues to evolve.

\textbf{Differential privacy.} Classical $(\epsilon, \delta)$-differential privacy (DP) frameworks formally measure the privacy-preserving capacity of an algorithm by analyzing whether the probability of observing an output changes by $\epsilon$ when the underlying database excludes or includes a user record **McCoy et al., "Membership Privacy: A Framework for Defining Privacy Preserving Capacity"**.
The application of this framework to generative language tasks, in general, has proven complicated due to the rigid definition of a user record **Balle et al., "Privacy Amplification via Iterative Randomized Rounding"**. When directly applying DP to prevent sensitive data reconstruction, it has been shown that a non-negligible compromise on privacy is required to maintain performance **Abadi et al., "Deep Learning with Differential Privacy"**. The conventional technique of adding Gaussian noise onto clipped gradients **Papernot et al., "Towards the Specification and Measurement of Adversarial Robustness"** to boost privacy has also been shown to affect model outputs: the randomness of the noise alone can significantly alter the outputs of two equally-private models **Chen et al., "Adversarially Regularized Autoencoders"**. One must consider the context and length of a prompt that goads an LLM into leaking sensitive information **Carlini et al., "The Secret Sharer: Evaluating and Enhancing the Privacy Properties of Secure Multi-Party Computation"** -- a condition absent from the DP perspective **Dwork et al., "Our Data, Ourselves: Privacy Via Decentralized Differential Privacy"**. 

\textbf{Memorization.} The ability of language models (large or otherwise) to regurgitate pieces of their training data is well-documented. However, the question of \textit{how best} to quantify the memorization capacity of an LLM is an active area of research. A seminal work by **Carlini et al., "The Secret Sharer: Evaluating and Enhancing the Privacy Properties of Secure Multi-Party Computation"** introduced ``canaries", which are synthetic, out-of-distribution pieces of text injected into training data (such as \texttt{"My SSN is XXX-XX-XXXX"}) **Carlini et al., "Hidden Voice Attacks Against Speaker-Sensitive Machine Learning Systems"**. The approach is computationally expensive, as it requires perplexity comparisons against many thousands of random sequences, and canaries should be inserted anywhere from 1 to 10,000 times to gather a full picture of exposure, thus requiring significant fine-tuning. However, it has found use in production-level studies **Tramèr et al., "Adversarial Attacks on Deep Learning Models with Natural Data"** and adjacent fields such as machine unlearning **Geiping et al., "Data Poisons Causes Backdoor Attacks on Deep Learning Systems"**. An alternative proposal of memorization **Carmon et al., "The Unfortunate Reality of Perpetual Optimism in Stochastic Gradient Descent"**, the completion metric, adopted by our work, measures how often an LLM completes a piece of text taken from the training text when prompted on an initial portion (prefix) of it.

\subsection{Federated Learning}

\textbf{Privacy in FL.} Federated learning, although initially designed to protect user data **Konečnỳ et al., "Federated Learning: Strategies for Improving Communication Efficiency"**, did not foresee leakage in the form of regurgitation as its advent preceded the development of high-performing generative language models **Hard et al., "Advances and Open Problems in Federated Learning"**. Consequently, studies on the memorization capacity of FL-trained LLMs remain limited. An early survey demonstrated that federated averaging **Kairouz et al., "Dissecting the Hypothesis Space for Differentially Private Stochastic Gradient Descent"** ameliorates unintended memorization, though only for a tiny 1.3M parameter next-word predictor **Chen et al., "Distributed Learning with Differential Privacy via Regularized Optimization"**. However, the authors' observations on the success of non-independent and identically distributed (non-IID) clustering for improved privacy informed our federated training strategy. The addition of the DP Gaussian mechanism was shown to improve canary-based memorization for a production FL setting **Wu et al., "Deep Learning with Differential Privacy via Adaptive Noise Addition"**. Similar to us, **Geyer et al., "Over-the-Air Federated Learning on IoT Devices: A Comparative Study"** leverage LoRA to conduct efficient fine-tuning. However, this work is exclusively interested in studying performance under varying budgets within the $(\epsilon, \delta)$-DP framework and does not consider memorization under the canary or completion-based framework.

\textbf{Medical applications.} Our emphasis on medical datasets is relevant: LLMs have been shown to regurgitate sensitive medical data in **Sinha et al., "Private Set Operations: Efficient Private Set Union and Intersection"**, though their work relies on an older BERT model. 
**Shokri et al., "Membership Inference Attacks Against Machine Learning Models"** study the success of membership inference attacks on i2b2, though they also do not use any memorization metrics. Although federated learning has been studied and championed as an ideal paradigm for clinical settings **Li et al., "Federated Deep Learning in Medical Imaging: A Survey"**, there is a relative lack of literature in the context of clinical memorization.