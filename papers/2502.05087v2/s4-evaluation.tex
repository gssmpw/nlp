\section{Empirical Evaluation}
\label{experiments}
In this section, we study how LoRA affects memorization of out-of-distribution sequences injected into fine-tuning training data. We introduce the experimental setting in Section \ref{sec:setup} and explain how we quantify memorization in Section \ref{sec:mem}.

We consider conventional centralized learning in Section~\ref{cl}, where all training samples are trained on by a single client. We then consider an FL setting in Section~\ref{fl}, where training data is split among several clients. Our FL experiments are designed to mimic a medical setting where training data contains sensitive information at an unknown rate, which is a common scenario as few if not any data anonymization tools can guarantee a complete removal of sensitive data \citep{langarizadeh2018dw}. In fact,  \citet{heider2020comp} measured the accuracy of three off-the-shelf de-identification tools on the i2b2 medical record dataset \citep{phi}, which our experiments also use, and found that no system could perform a full removal.


\subsection{Experimental setup}
\label{sec:setup}
All fine-tuning was performed on a single NVIDIA A100 80GB GPU within an HPC cluster. We leveraged HuggingFace's Transformers library \citep{wolf2020hugging} to access and fine-tune pre-trained models. The experiments were conducted in a Python 3.11.9 environment, with PyTorch 2.4.0 and CUDA 12.1. Further training details are included in Appendix~\ref{sec:hp}.


We fine-tune models for domain adaptation to medical question-answering (QA).
Despite medical scenarios being extensively promoted by FL applications \citep{xu2021federated, nguyen2022federated, antunes2022federated}, and the availability of resources such as de-anonymized sensitive medical datasets \citep{johnson2016mimic, phi}, clinical memorization remains an area of uncertainty in FL.

\textbf{Fine-tuning Datasets.} In order to reproduce a plausible FL environment with non-IID data, we select 3 popular medical datasets with different types of QA. 
\begin{enumerate}
    \item \textit{MedMCQA} \citep{medmcqa} is composed of multiple-choice questions, containing almost 190k entrance exam questions (AIIMS \& NEET PG). We fine-tune on the training split and leave aside validation data as a downstream evaluation benchmark.
    \item \textit{PubMedQA} \citep{pubmedqa} consists of Yes/No/Maybe questions created from PubMed abstracts. The dataset contains 1k expert-annotated (PQA-L) and 211k artificially generated QA instances (PQA-A). We include 500 questions from the train and validation sets of PQA-L and 50k questions of PQA-A.
    \item \textit{Medical Meadow flashcards} \citep{medalpaca} contains 39k questions created from Anki Medical Curriculum flashcards compiled by medical students. We include 10k instances for fine-tuning data.
\end{enumerate}

\textbf{Medical Benchmarks.} To measure the downstream performance of the fine-tuned models, we evaluate models on 4 medical benchmarks following existing methodology \citep{wu2023pmcllama, singhal2023expert, singhal2023ka, meditron}: MedQA, PubMedQA, MedMCQA, and MMLU-Medical.
\begin{enumerate}
    \item \textit{MedQA's 4-option questions}. MedQA \citep{medqa} consists of US Medical License Exam (USMLE) multiple-choice questions. The test set contains 1278 questions with both 4 and 5-option questions. Following \citet{meditron}, we report each case separately, respectively MedQA-4 and MedQA.
    \item \textit{MedQA's 5-option questions}.
    \item \textit{PubMedQA}'s test set contains 500 expert-annotated questions. No artificially-generated questions are used during evaluation.
    \item \textit{MedMCQA}'s test set does not provide answer labels, therefore we rely on the validation set, containing 4183 instances, to benchmark downstream performance following \citet{wu2023pmcllama} and \citet{meditron}.
    \item \textit{MMLU-Medical}. MMLU \citep{mmlu} is a collection of 4-option multiple-choice exam questions covering 57 subjects. We follow \citet{meditron} and select a subset of 9 subjects that are most relevant to medical and clinical knowledge: high school biology, college biology, college medicine, professional medicine, medical genetics, virology, clinical knowledge, nutrition, and anatomy, and group them into one medical-related benchmark: MMLU-Medical.
\end{enumerate}

We use 3-shot in-context learning without any chain-of-thought reasoning and average the accuracy over 3 seeds.

\textbf{Models.} To account for the effect of model size on memorization \citep{memo, tirumala2022memorization}, we study pre-trained models ranging from 1B to 8B parameters: Llama 3.2 1B, Llama 3.2 3B, Llama 3 8B \citep{dubey2024llama}, Llama 2 7B \citep{touvron2023llama2}, and Mistral 7B v0.3 \citep{jiang2023mistral7b}.

\subsection{Quantifying memorization}
\label{sec:mem}
How we measure memorization is largely inspired by \citet{memo}. In short, we inject sensitive sequences, so-called ``canaries" \citep{carlini2019secret, jagielski2023forget, thakkar2020understanding}, into fine-tuning data and then measure the models' ability to regurgitate this information when prompted with the beginning of these sequences. In Appendix~\ref{sec:mem_score}, we give an example of memorization scores for Llama 2 7B.

\textbf{Canaries.} Unlike prior works that evaluate memorization of all training data \citep{memo, ippolito2023verbatim, hans2024goldfish}, we are interested in measuring how much sensitive information is memorized.
Similar to \citet{lehman2021does} and \citet{mireshghallah2022quantifying}, we inject medical records into our training set originating from the 2014 i2b2/UTHealth corpus dataset \citep{phi}. The i2b2 dataset contains 1304 longitudinal medical records that describe 296 patients.

Since data duplication has been shown to greatly influence memorization \citep{memo, lee2022dedup, kandpal2022dedup}, we randomly select 30\% of the medical records and duplicate them 10 times within our fine-tuning data in order to study data duplication in our experiments.

\textbf{Prompting.} To measure unintended memorization after fine-tuning, we randomly select test sequences from the medical records (one sequence per record) and split each sequence into a prefix $p$ and a suffix $s$. Conditioned on the prefix, the model generates text via greedy decoding and the generated suffix is compared with the ground truth. We set the length of the generated suffix $s$ to 50 tokens, in line with \citet{memo}, \citet{ippolito2023verbatim} and \citet{hans2024goldfish}.

Following \citet{memo}, we measure the effect of the context size by prompting the model on each test sequence several times with prompts of lengths in $\{10, 50, 100, 200, 500\}$.
The different prompts for one test sequence are constructed such that the suffix $s$ is kept identical while varying the prompt length. This ensures a fair comparison between prompt lengths, since different suffixes may be more or less difficult to regurgitate.

\textbf{Memorization scores.} To compare generated text with the ground truth, we rely on two metrics: (1) the \textbf{exact token match rate} and (2) the \textbf{BLEU score} to measure approximate reproduction, as prior works suggest that the exact match rate does not capture subtler forms of memorization \citep{ippolito2023verbatim}. In line with this work, we consider a sequence memorized if the generated suffix and the ground truth yields a BLEU score $> 0.75$. For both metrics, lower is better and a score of 1 denotes the complete memorization of all test sequences. In Appendix~\ref{sec:mem_score}, we provide an example for Llama 2 7B fine-tuning.

\subsection{Centralized Learning}
\label{cl}

To the best of our knowledge, the impact of LoRA on memorization has not been previously quantified; therefore, we begin by studying LoRA in the context of centralized learning (CL) before considering federated learning (FL).

\textbf{Training details.} In the centralized learning setting, we merge \textit{PubMedQA}, \textit{MedMCQA}  and \textit{Medical Meadow Flashcards} into one fine-tuning dataset in which we inject the \textit{i2b2} medical records to benchmark memorization after fine-tuning. We use a validation split of 10\% and for each model we search for the learning rate yielding the lowest validation loss. More details on hyperparameters can be found in Appendix~\ref{sec:hp}.

\textbf{Accuracy.} To study how LoRA mitigates unintended memorization, we must first assess if it comes at a cost in model performance. Figure~\ref{fig-cl-accuracy} illustrates the average accuracy over fine-tuning strategies. Comparing full fine-tuning against LoRA, we find that LoRA comes with a relatively negligible cost in accuracy. Every fine-tuning yields a significant accuracy improvement of the pre-trained model except for Llama 3.1 8B, in which performance minimally improved.

We hypothesize that part or all of our fine-tuning dataset has already been trained on during Llama 3.1 8B's pre-training phase. Accordingly, we exclude Llama 3.1 8B from subsequent experiments.

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\imgwidth]{accuracy_centralized.pdf}}
\caption{\textbf{Downstream accuracy of centralized learning averaged across the 5 benchmarks.} LoRA matches full fine-tuning accuracy on every model tested. We report the out-of-the-box accuracy of the pre-trained models as a control. A breakdown per benchmark is included in Appendix~\ref{sec:aux-acc}.}
\label{fig-cl-accuracy}
\vskip -0.2in
\end{center}
\end{figure}

\textbf{Memorization.} Given that LoRA matches full fine-tuning performance in our experiments, we now measure the unintended memorization occurring during fine-tuning, illustrated in Figure~\ref{fig-cl-privacy}. To account for prompt length, we include a figure (plots \textbf{(c)} and \textbf{(f)}) for each metric with the highest memorization score obtained across settings, which is systematically reached on duplicated documents with the longest prompt.

\begin{figure}[!htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\imgwidth]{memorization_scores_centralized.pdf}}
\caption{\textbf{LoRA vs full fine-tuning memorization scores in centralized learning.} LoRA consistently yields lower memorization scores (lower is better). Unless stated otherwise, scores are averaged across prompt lengths. Values are shown when bars are too small. Right-most figures denote the worst-case setting where memorization scores are the highest. Plots \textbf{(a)}-\textbf{(c)} show memorization using exact match rate with no duplication, 10x document duplication, and 10x document duplication with a 500 tokens prompt length, while \textbf{(d)}-\textbf{(f)} use BLEU score.}
\label{fig-cl-privacy}
\end{center}
\vskip -0.2in
\end{figure}

\textbf{Analysis.} Across all model sizes, data duplication greatly increases memorization and longer prompt lengths increase the extraction success. Figure~\ref{fig-cl-privacy} also illustrates that larger models memorize more \citep{memo, tirumala2022memorization}. Most importantly, we see that \textit{models fine-tuned in centralized learning with LoRA consistently exhibit lower memorization scores}, suggesting the adequacy of using of LoRA as a memorization-mitigating technique with little to no performance cost.

Additionally, we compute the memorization scores of pre-trained models without fine-tuning, to obtain control values. This is equivalent to computing the models' ability to ``guess" the suffix without having seen previously the medical records. We obtained scores an order of magnitude lower than any fine-tuned model score, which additionally confirms that none of the models had already been trained on the i2b2 dataset. Thus, while some scores in Figure~\ref{fig-cl-privacy} may appear low at first glance, the lowest memorization depicted in this figure is $>\hspace{-0.3em}10$ times higher than the control.

\subsubsection{Utility-privacy tradeoff}
To further confirm that the privacy gains observed on models trained with LoRA do not come at the cost of utility, and that the privacy loss observed with full fine-tuning is not due to overfitting or preventable by early stopping, we analyzed the utility-privacy tradeoff throughout the fine-tuning process. Figure~\ref{fig-tradeoff} illustrates the evolution of privacy and utility for Llama 3.2 3B during both LoRA and full fine-tuning. The figure shows that LoRA fine-tuning consistently follows a more privacy-preserving trend, with lower memorization scores compared to full fine-tuning at similar utility levels. Furthermore, after a certain number of fine-tuning steps, the model's tendency to memorize data increases without significant improvements in utility, due to overfitting. This highlights that \textit{early stopping during LLM training not only improves efficiency, but also helps privacy by reducing the risk of memorization.}
 
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\imgwidth]{utility_privacy_llama.pdf}}
\caption{\textbf{Accuracy vs. privacy across fine-tuning steps.} We track accuracy and memorization (BLEU score) during Llama 3.2 3B fine-tuning (10Ã— document duplication) using full fine-tuning (Full FT) and LoRA, compared to the base model. Numbers above data points indicate completed fine-tuning steps.}
\label{fig-tradeoff}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Federated Learning}
\label{fl}
Having empirically measured how LoRA reduces unintended memorization in centralized learning, we now turn to federated learning. 
The federated learning framework contains multiple key differences with centralized learning that may impact memorization, such as Federated Averaging or non-IID data across participants \citep{thakkar2020understanding}.

\textbf{Training details.} We define a heterogeneous setting with one client per dataset. In other words, we fine-tune models with 3 participants, where each participant trains locally on one of the 3 datasets MedMCQA, PubMedQA, and Medical Meadow flashcards. We split and inject i2b2 medical records into each dataset proportionally to their size. Participants fine-tune over their local dataset for one epoch between each global weight update, for a total of 5 rounds. For every model, we fine-tune the learning rate on each local dataset. More training details are included in Appendix~\ref{sec:training_details}.

To provide fair comparisons between multiple federated learning fine-tuning, Figures~\ref{fig-fl-accuracy} and \ref{fig-fl-privacy} report metrics for the last federated communication round. This ensures that each model has been fine-tuned on the medical records the same number of times. Additionally, we include the accuracy and memorization metrics for each round in Appendix~\ref{sec:aux-acc}.

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\imgwidth]{accuracy_federated.pdf}}
\caption{\textbf{Downstream accuracy in federated learning.} LoRA yields relatively similar accuracy to full fine-tuning for several LLMs in a heterogeneous FL setting.}
\label{fig-fl-accuracy}
\end{center}
\end{figure}

\textbf{Accuracy.} Figure~\ref{fig-fl-accuracy} depicts downstream accuracy of federated fine-tuning. All fine-tunings show relatively similar accuracy values between full fine-tuning and LoRA. This suggests that LoRA is a competitive technique in federated learning and can replace full fine-tuning at relatively little cost, in addition to lowering the hardware requirements and the communication overheads.


\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\imgwidth]{Llama-3.2-3B_privacy_leakage.pdf}}
\caption{\textbf{Exact match rates of FL and CL.} We compare memorization between CL and FL when fine-tuning Llama 3.2 3B.}
\label{fig-fl-remaining-mem}
\end{center}
\vskip -0.2in
\end{figure}

\textbf{Memorization.} We first start by comparing memorization in federated learning to centralized learning in Figure~\ref{fig-fl-remaining-mem}. We observe that FL can enhance privacy by reducing memorization. This is consistent with previous work \citep{thakkar2020understanding} suggesting that FedAvg and a non-IID data distribution contribute to reducing unintended memorization. However, we note that memorization increases monotonically with the number of rounds (i.e. the number of times medical records are seen). Therefore, a model fine-tuned via FL can reach similar or even greater memorization levels as the number of rounds increases. In fact, Figure~\ref{fig-fl-round-privacy} shows that, after a certain number of rounds, fine-tuning Llama 2 7B exhibits more memorization across several metrics in FL than in CL. Thus, our results expand on previous work by focusing on how memorization increases throughout the rounds. Comparisons for all models and metrics are included in Appendix~\ref{sec:mem_fl}.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\imgwidth]{memorization_scores_federated.pdf}}
\caption{\textbf{Memorization of LoRA vs full fine-tuning in federated learning.} LoRA yields
significantly lower memorization scores in every setting for an equivalent performance. Plots \textbf{(a)}-\textbf{(c)} show memorization using exact match rate with no duplication, 10x document duplication, and 10x document duplication with a 500 tokens prompt length, while \textbf{(d)}-\textbf{(f)} use BLEU score.}
\label{fig-fl-privacy}
\end{center}
\vskip -0.2in
\end{figure}

\textbf{Analysis.} Despite FL showing lower memorization than CL, all federated fine-tunings exhibit significant memorization, thus showing the need for additional privacy-preserving techniques. Figure~\ref{fig-fl-privacy} shows how using LoRA instead of full fine-tuning impacts memorization. \textit{Fine-tuning federated LLMs with LoRA displays lower memorization than full fine-tuning across all metrics and models.} LoRA fine-tuning can reduce memorization up to 10$\times$ for a negligible accuracy loss. We do note that the memorization impact of LoRA differs between similarly sized models. For example, fine-tuning Llama 2 7B with LoRA shows a drastic memorization improvement over full fine-tuning, whereas Mistral v0.3 7B shows a lower impact.

We also find that not all trends observed in centralized learning hold in federated learning: data duplication, longer context and considering paraphrasing all yield higher memorization scores, however Figure~\ref{fig-fl-privacy} shows that bigger models do not necessarily result in more memorization with full fine-tuning, as Llama 3.2 1B reaches higher memorization scores than Llama 3.2 3B. Yet the trend still holds when looking at LoRA fine-tuning. We leave further exploration of how model size influences memorization in federated learning for future work.

Finally, LoRA drastically reduces FL communication overhead. For instance, each round of our setting requires a total data exchange of 74GB for a 7B model, and \textit{using LoRA reduces the load by a factor of 152, decreasing the overhead to 498MB}.

\subsubsection{Secure Aggregations}
FL's privacy benefits can be compromised if participants gain access to each other's fine-tuned local models. While Figure~\ref{fig-fl-round-privacy} highlights reduced memorization after model aggregation, unsecured local models may still expose additional information regarding participants' datasets. In Appendix~\ref{sec:sec_agg}, we show how secure aggregation addresses this vulnerability by using a third party to aggregate encrypted local contributions using Fully Homomorphic Encryption (FHE) and decrypting the aggregated model collectively through Secure Multiparty Computation (SMPC), as described in \citet{sebert2022fhedp}. Experiments were conducted using the open-source Lattigo library~\citep{lattigo, mouchet2020lattigo}.

\subsection{Combining LoRA with other methods}

Although LoRA mitigates unintended memorization on its own, we investigate whether it can be combined with other privacy-persevering techniques without compromising performance or increasing memorization. If users are focused on reducing extractable memorization in pre-training, then they may be interested in Goldfish loss (LoRA is preferred for fine-tuning), but we investigate and verify its potential for fine-tuning. Gradient noising and clipping can be used to satisfy $(\epsilon, \delta)$-differential-privacy guarantees, which LoRA alone has not been formally proven to provide.

Nonetheless, we emphasize that Goldfish loss and DP noising/clipping are not \textit{efficient} strategies, as both require calculation of the full gradient. Hence, users will choose LoRA if they are concerned about backpropagation costs or communication overhead, which is a common scenario in FL. 

\subsubsection{Goldfish loss}

The Goldfish loss \citep{hans2024goldfish} has been introduced recently as a memorization mitigating technique for pre-training language models via a new next-token training objective. The training procedure randomly excludes tokens from the loss computation in order to prevent verbatim reproduction of training sequences. In Appendix~\ref{sec:goldfish}, we evaluate the memorization and accuracy of Llama 3.2 3B fine-tuned with LoRA in combination with Goldfish loss. We also compare it to the same model fully fine-tuned with Goldfish loss only. \textit{The combination of LoRA with Goldfish loss synergistically achieves lower memorization beyond what either strategy achieves alone.}

\subsubsection{Differential privacy}
$(\epsilon, \delta)$-Differential privacy (DP) provides formal guarantees that an individual's data cannot be inferred from a model's output, by quantifying the model's sensitivity to changes in input data. Following \citet{li2021large} and \citet{liu2024differentially}, we define sensitivity as the maximum change in model output resulting from the inclusion or removal of a single data point in the training dataset (record-level DP).

Implementing DP requires modifications to the fine-tuning pipeline to limit the influence of individual data points on model parameters. Gradient clipping, which constrains the magnitude of gradient updates, is a key technique in this process. In our experiments (see Appendix~\ref{sec:gradclip}), applying a gradient clipping value of $0.0001$ significantly reduces memorization and improves accuracy compared to the default value of $1.0$. This demonstrates gradient clipping as a privacy-enhancing method in itself, even without the addition of noise. But the use of stochastic gradient descent (SGD), required for DP-SGD, presents challenges in fine-tuning the Llama 3.2 3B model. Despite an extensive search for optimal learning rates, SGD consistently underperforms compared to Adam-derived optimizers (see Appendix~\ref{sec:adamw_vs_sgd}).