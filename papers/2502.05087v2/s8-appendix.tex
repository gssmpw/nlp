\appendix
\onecolumn
\section{Further Related Work}
\textbf{Membership inference attacks} (MIA) rely on rigorous statistical principles to assess privacy risks in machine learning models. \citep{shokri2017mia} introduced an approach for determining whether a specific data point was part of a model's training dataset. These attacks exploit differences in model behavior on training versus non-training data, posing significant privacy concerns for sensitive information. Building on this, \citep{chang2024miallm} extended these concepts to LLMs by incorporating contextual information. This study demonstrated that LLMs are particularly vulnerable to membership inference attacks, as they often retain verbatim information from their training datasets. The work highlighted the increased privacy risks associated with LLMs due to their scale and training dynamics.

\textbf{Secure Aggregations.} While the conventional FL ensures that raw data is not shared between participants during collective training, it does not address the risk of data leakage through model updates shared prior to aggregation. For example, in the honest-but-curious scenario, a server examines whether client data can be reconstructed \citep{huang2021evaluating}. This vulnerability becomes particularly critical with LLMs, given their propensity for memorization. To address the privacy risks associated with local model exchanges in FL, \citep{truex2019hybrid} proposes a hybrid approach that combines differential privacy with secure multiparty computation (SMC). In this framework, local models are encrypted and remain hidden from other participants prior to aggregation, thereby mitigating privacy leakage risks associated with individual local models by focusing them on the aggregated model during each aggregation round. While this method has been explored for general machine learning applications, to the best of our knowledge, it has not yet been investigated in the context of large language models (LLMs).

\section{Training details}
\label{sec:training_details}

\subsection{Hyperparameters}
\label{sec:hp}

In centralized learning, we sweep the learning rate $\in \{1e-5, 5e-5, 1e-4, 5e-4\}$ for full fine-tuning experiments. For LoRA experiments, we search for learning rate values $\in \{5e-5, 1e-4, 5e-4, 1e-3\}$. In federated learning experiments, we sweep the learning rate on each dataset individually for one epoch, with the same set of values as in centralized learning.

For all experiments we fine-tune models with the AdamW optimizer \citep{loshchilov2019adamw} with default parameters ($\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=1e^{-8}$, weight decay of 0.01). We used a context length of 1024 and ensured that no text inputs were longer than the context length. We use a linear warmup of 100 steps with a cosine annealing schedule. Unless mentioned otherwise, we use a global batch size of 32 with gradient accumulation and gradient checkpointing. For all LoRA experiments with use a rank of 16, an alpha of 8, drop out 0.05 and use adapters for all projection layers. Additionally, we study the impact of the LoRA rank on memorization in Section~\ref{sec:lora_rank}.

\subsection{The LoRA rank and memorization}
\label{sec:lora_rank}

We measure the influence of the LoRA hyperparameters by varying the rank and measuring the resulting memorization. We study rank values $ r \in \{4, 16, 64, 128, 256, 1024\}$ and set alpha to twice the rank, following common practice. We decrease the learning rate exponentially as the rank increase.

\begin{table}[ht]
\caption{\textbf{Impact of the LoRA rank on memorization.} We fine-tune Llama 3.2 3B with LoRA in centralized learning on increasing LoRA ranks. We find that higher ranks lead to more memorization.}
\label{tab:lora-rank}
\vskip 0.15in
\begin{center}
\begin{tabular}{@{}cccccc@{}}
\toprule
\multirow{2}{*}{LoRA rank} & \multicolumn{2}{c}{Exact match rate} & \multicolumn{2}{c}{BLEU Score}   & \multirow{2}{*}{Accuracy} \\
                      & No duplication   & 10x duplication   & No duplication & 10x duplication &                           \\ \midrule
4                     & 0.0003           & 0                 & 0.0133         & 0.0198          & 0.509                     \\
16                    & 0.0005           & 0.0031            & 0.0167         & 0.0623          & 0.512                     \\
64                    & 0.0031           & 0.2105            & 0.0258         & 0.379           & 0.511                     \\
128                   & 0.0042           & 0.3735            & 0.0305         & 0.5111          & 0.510                     \\
256                   & 0.0057           & 0.4895            & 0.0352         & 0.5809          & 0.542                     \\
1024                  & 0.0063           & 0.4981            & 0.0409         & 0.6228          & 0.530                     \\ \bottomrule
\end{tabular}
\end{center}
\vskip -0.1in
\end{table}

As shown in Table~\ref{tab:lora-rank}, increasing the rank, i.e. increasing the number of weights updated during fine-tuning, results in more memorization, ranging from virtually no verbatim memorization with a rank of 4 to almost 50\% of the medical records being memorized for rank 1024 when considering duplicated medical records.
We note that in our case, larger ranks do not necessarily imply better accuracy. We hypothesize that larger ranks might make overfitting more likely to occur. Additionally, each rank value can benefit from more extensive hyperparameter tuning.

\section{Auxiliary results}
\label{sec:aux}

\subsection{Accuracy}
\label{sec:aux-acc}
Table~\ref{tab:cl-acc-round} includes a breakdown per benchmark of the downstream accuracy of LoRA and full model fine-tuning in centralized learning as well as performance of pre-trained models without fine-tuning. Table~\ref{tab:fl-acc-round} shows the accuracy of federated fine-tuning per round.

\begin{table}[ht]
\caption{\textbf{Downstream accuracy in central learning.} Best accuracy values are marked in \textbf{bold}.}
\label{tab:cl-acc-round}
\vskip 0.15in
\begin{center}
\begin{tabular}{@{}cccccccc@{}}
\toprule
Model                                              & Fine-tuning               & MMLU-medical   & PubMedQA                   & MedMCQA & MedQA & MedQA-4 & Average \\ \midrule
\multicolumn{1}{c}{\multirow{3}{*}{Llama 3.2 1B}} & \multicolumn{1}{c|}{No fine-tuning} & 0.353 & 0.363 & 0.49                      & \textbf{0.329}                     & 0.275                     & 0.308   \\
                                                 & \multicolumn{1}{c|}{Full} & \textbf{0.456} & \textbf{0.616} & \textbf{0.431}   & 0.322 & \textbf{0.379}   & \textbf{0.441}   \\
                                                  & \multicolumn{1}{c|}{LoRA} & 0.447          & 0.594 & 0.397   & 0.312 & 0.362   & 0.422   \\\midrule
\multirow{3}{*}{Llama 3.2 3B}                      & \multicolumn{1}{c|}{No fine-tuning} & 0.432 & 0.597 & 0.122 & 0.491 & 0.446 & 0.504   \\
                      & \multicolumn{1}{c|}{Full} & 0.59           & 0.536 & \textbf{0.542}   & \textbf{0.452} & \textbf{0.507}   & 0.525   \\
                                                   & \multicolumn{1}{c|}{LoRA} & \textbf{0.608}          & \textbf{0.676} & 0.512   & 0.448 & 0.5     & \textbf{0.549}   \\\midrule
\multirow{3}{*}{Llama 2 7B}                        & \multicolumn{1}{c|}{No fine-tuning} & 0.381 & 0.426 & 0.452                     & 0.380                     & 0.292                     & 0.353   \\
                        & \multicolumn{1}{c|}{Full} & \textbf{0.562}          & 0.596 & \textbf{0.516}   & \textbf{0.395} & \textbf{0.478}   & \textbf{0.509}   \\
                                                   & \multicolumn{1}{c|}{LoRA} & 0.560          & \textbf{0.726} & 0.448   & 0.353 & 0.405   & 0.498   \\\midrule
\multirow{3}{*}{Mistral v0.3 7B}                   & \multicolumn{1}{c|}{No fine-tuning} & 0.552 & 0.635 & 0.7                       & 0.483                     & 0.438                     & 0.503   \\
                   & \multicolumn{1}{c|}{Full} & 0.659          & \textbf{0.758} & \textbf{0.588}   & \textbf{0.499 }& \textbf{0.551}   & \textbf{0.611}   \\
                                                   & \multicolumn{1}{c|}{LoRA} & \textbf{0.667}          & \textbf{0.758} & 0.572   & 0.467 & 0.54    & 0.601   \\ \bottomrule
\end{tabular}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[ht]
\caption{\textbf{Downstream accuracy per federated round}. We emphasize in \textbf{bold} the earliest round where models reach their best accuracy.}
\label{tab:fl-acc-round}
\vskip 0.15in
\begin{center}
\begin{tabular}{@{}cc|ccccc@{}}
\toprule
\multirow{2}{*}{Model}           & \multirow{2}{*}{Fine-tuning} & \multicolumn{5}{c}{Accuracy per round}            \\
                                 &                              & 1     & 2     & 3     & 4     & 5     \\ \midrule
\multirow{2}{*}{Llama 3.2 1B}    & Full                         & 0.425 & 0.438 & 0.444 & \textbf{0.445} & 0.445 \\
                                 & LoRA                         & 0.415 & 0.422 & 0.430 & 0.432 & \textbf{0.434} \\\midrule
\multirow{2}{*}{Llama 3.2 3B}    & Full                         & 0.541 & 0.561 & 0.554 & 0.573 & \textbf{0.578} \\
                                 & LoRA                         & 0.557 & \textbf{0.564} & 0.559 & 0.563 & 0.564 \\\midrule
\multirow{2}{*}{Llama 2 7B}      & Full                         & 0.468 & 0.488 & 0.482 & 0.495 & \textbf{0.511} \\
                                 & LoRA                         & 0.475 & 0.490 & 0.482 & \textbf{0.494} & 0.493 \\\midrule
\multirow{2}{*}{Mistral v0.3 7B} & Full                         & 0.181 & 0.590 & 0.599 & \textbf{0.603} & 0.602 \\
                                 & LoRA                         & 0.594 & 0.599 & 0.598 & 0.604 & \textbf{0.608} \\ \bottomrule
\end{tabular}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Memorization Score}
\label{sec:mem_score}
Figure~\ref{fig-privacy-metrics} illustrates with Llama 2 7B multiple trends that are consistent with results previously mentioned: 
\begin{enumerate}
    \item \textit{There is significantly, and alarmingly, more memorization when the medical records occur multiple times in the fine-tuning data.} 
    \item \textit{Longer prompts show higher memorization (discoverability phenomenon)}. 
    \item \textit{There is significantly more memorization with approximate generation (BLEU score)}.
\end{enumerate}

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=0.8\textwidth]{privacy_metrics_centralized.pdf}}

\caption{\textbf{An example of memorization scores for a full fine-tuning of Llama 2 7B.} We report the exact match rate and BLEU score with respect to the prompt length, with and without duplication. We also show the memorization upper bound ("Full memorization") reached when every test sequence has been memorized.}
\label{fig-privacy-metrics}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Memorization scores in FL}
\label{sec:mem_fl}
Figure~\ref{fig-fl-round-privacy} shows the memorization scores per round of federated learning. We can see that using LoRA results in lower unintended memorization than full fine-tuning at every round.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{memorization_scores_per_round_federated.pdf}}
\caption{\textbf{Memorization scores for central learning and federated learning with respect to rounds.} In all settings, LoRA results in better privacy than a full fine-tuning.}
\label{fig-fl-round-privacy}
\end{center}
\vskip -0.2in
\end{figure}

\section{Secure Aggregations}
\label{sec:sec_agg}

Secure aggregations ensure that sensitive data remains protected and prevents the aggregator from decrypting any model.
We evaluate the runtime performance of using secure aggregation in conjunction with LoRA in an FL setting. 

\textbf{Performance.}
To evaluate the performance impact of secure aggregation, we use Lattigo, an open-source library that enables secure protocols based on multiparty homomorphic encryption~\cite{lattigo, mouchet2020lattigo}. Specifically, it implements the CKKS scheme, which allows efficient encrypted computations on real-valued data, making it ideal for the secure aggregation of the LoRA models trained by the clients/participants. In our experiments, we consider 3 clients and configure CKKS parameters to enable 32-bit precision. Since our LoRA models are trained with 16-bit precision, this ensures that \textbf{secure aggregation does not introduce any accuracy loss} compared to standard aggregation in plaintext.

Secure aggregation introduces a time overhead due to encryption, homomorphic operations, and collective decryption. The duration of encrypted aggregation is influenced by the number of weights being aggregated, specifically the number of LoRA weights. In our experiments with Llama 3.2 3B, \textbf{a LoRA update contains 24,772,608 parameters, representing approximately ~0.77\% of the full modelâ€™s parameters}. In Table~\ref{tab:secagg}, we report the aggregation times for vectors of varying sizes, corresponding to the number of LoRA weights. Aggregating three vectors of the size of our LoRA takes 11.33 seconds, which is negligible compared to the time required for local fine-tuning at each round.

\begin{table}[ht]
\caption{\textbf{Execution Time of the Secure Aggregation Protocol.} The protocol aggregates three equal-sized encrypted vectors for varying sizes.}
\label{tab:secagg}
\vskip 0.15in
\begin{center}
\begin{scriptsize}
\begin{tabular}{@{}l|l@{}}
\toprule
\textbf{Aggregation Length} & \textbf{Time Taken} \\ \midrule
\(10^1\)                    & 12.16ms             \\
\(10^2\)                    & 11.61ms             \\
\(10^3\)                    & 11.32ms             \\
\(10^4\)                    & 17.29ms             \\
\(10^5\)                    & 58.91ms             \\
\(10^6\)                    & 474.46ms            \\
\(10^7\)                    & 4.37s               \\
\(2.48 \times 10^7\) (LoRA size) & \textbf{11.33s}  \\
\(10^8\)                    & 68.24s              \\ 
\bottomrule
\end{tabular}
\end{scriptsize}
\end{center}
\vskip -0.1in
\end{table}

\section{Goldfish loss}
\label{sec:goldfish}
In this section, we evaluate how LoRA combined with Goldfish loss impact the accuracy and the memorization of Llama 3.2 3B. While Goldfish loss has been designed for pre-training, we apply it to our fine-tuning and report values for various dropping frequencies $k$. We use a hashing context width $h=13$ following the authors' methodology \citep{hans2024goldfish}.

Table~\ref{tab:goldfish_lora} shows how combining Goldfish loss with LoRA mitigates memorization compared to a full fine-tuning. By contrasting memorization scores with control values, we can also note that the Goldfish loss is an effective memorization-mitigation technique.

\begin{table}[ht]
\caption{\textbf{Impact of Goldfish loss on BLEU Scores and accuracy in LoRA Fine-Tuning.} Llama 3.2 3B is fine-tuned with different dropping frequencies ($k$). Best accuracy is marked in \textbf{bold}.}
\label{tab:goldfish_lora}
\vskip 0.15in
\begin{center}
\begin{scriptsize}
\begin{tabular}{@{}c|ccc@{}}
\toprule
Goldfish $k$ & BLEU, no duplication & BLEU, 10x duplication & Accuracy       \\ \midrule
2            & 0.0133          & 0.0216          & 0.514          \\
3            & 0.0154          & 0.0426         & \textbf{0.549} \\
4            & 0.0180          & 0.0543          & 0.534          \\
5            & 0.0183          & 0.0815          & 0.540          \\
10           & 0.0256          & 0.1494          & 0.538          \\
100          & 0.0266          & 0.2852          & 0.537          \\
1000         & 0.0256          & 0.3111          & 0.533          \\
10000        & 0.0253          & 0.2944          & 0.545          \\
Control      & 0.0245          & 0.2920          & 0.550         \\ 
\bottomrule
\end{tabular}
\end{scriptsize}
\end{center}
\vskip -0.1in
\end{table}

To assess the impact of LoRA in combination with Goldfish loss, we evaluated the memorization and accuracy of fine-tuning the same model using full fine-tuning. Table~\ref{tab:goldfish_fft} presents the memorization scores and accuracy of the model fine-tuned with Goldfish loss alone, without LoRA. Our results indicate that while Goldfish loss reduces memorization, it does not achieve the same level of reduction as the combination with LoRA, especially when duplication occurs in the fine-tuning data. In summary, combining LoRA with Goldfish loss allows a privacy-utility tradeoff that cannot be achieved using Goldfish loss alone.

\begin{table}[ht]
\caption{\textbf{Impact of Goldfish loss on BLEU Scores and accuracy.} The BLEU scores and the accuracy of Llama 3.2 3B is reported for full fine-tuning across different dropping frequencies ($k$). Best accuracy is marked in \textbf{bold}.}
\label{tab:goldfish_fft}
\vskip 0.15in
\begin{center}
\begin{scriptsize}
\begin{tabular}{@{}c|ccc@{}}
\toprule
Goldfish $k$ & BLEU, no duplication & BLEU, 10x duplication & Accuracy       \\ \midrule
2            & 0.0146          & 0.0340          & 0.517          \\
3            & 0.0243          & 0.0679          & 0.513          \\
4            & 0.0282          & 0.1148          & 0.524          \\
5            & 0.0310          & 0.1568          & 0.521          \\
10           & 0.0342          & 0.3006          & \textbf{0.545} \\
100          & 0.0399          & 0.5821          & 0.534          \\
1000         & 0.0425          & 0.6235          & 0.527          \\
10000        & 0.0407          & 0.6235          & 0.516          \\
Control      & 0.0417          & 0.6235          & 0.538          \\ \bottomrule
\end{tabular}
\end{scriptsize}
\end{center}
\vskip -0.1in
\end{table}


\section{NEFTune}
\label{sec:neftune}
NEFTune is a regularization technique consisting in adding random noise to the embedding vectors to improve instruction fine-tuning. While not introduced as a privacy-preserving technique per se, we hypothesize that a fine-tuning regularization such as NEFTune may also reduce unintended memorization.

We display results after applying NEFTune with noise value $\alpha \in \{5, 10, 15, 30, 45\}$. We find that adding noise does not improve accuracy when applied to our domain adaptation fine-tuning. Secondly, increasing the noise does not yield better privacy, at least not until we set alpha to 45, which is greater than alpha values reported by the original work (5, 10, and 15).

\begin{table}[ht]
\caption{\textbf{NEFTune impact on the BLEU score and accuracy when combined with LoRA.} We analyze LoRA fine-tuning with Llama 3.2 3B and different noise scaling factors $\alpha$.}
\label{tab:neftune}
\vskip 0.15in
\begin{center}
\begin{scriptsize}
\begin{tabular}{@{}c|ccc@{}}
\toprule
$\alpha$ & No duplication & 10x duplication & Accuracy       \\ \midrule
Control     & 0.0276           & 0.4170           & 0.562 \\
5           & 0.0284           & 0.4525           & 0.560 \\
10          & 0.0300           & 0.4506           & 0.518 \\
15          & 0.0284           & 0.4525           & 0.544 \\
30          & 0.0282           & 0.4377           & 0.548 \\
45          & 0.0248           & 0.3599           & 0.518 \\
60          & 0.0227           & 0.2759           & 0.501 \\
100         & 0.0183           & 0.1006           & 0.391 \\ 
\bottomrule
\end{tabular}
\end{scriptsize}
\end{center}
\vskip -0.1in
\end{table}

\section{Differential Privacy}
\label{sec:dp}

\subsection{Gradient clipping}
\label{sec:gradclip}
Table~\ref{tab:gradient_clipping} illustrates the effect of different gradient clipping values on the BLEU score and accuracy achieved during the fine-tuning of LLama 3.2 3B.
\begin{table}[!htbp]
\caption{\textbf{Gradient clipping impact on the BLEU score and accuracy.} The BLEU score and the accuracy of Llama 3.2 3B is reported for LoRA fine-tuning. Best accuracy is marked in \textbf{bold}.}
\label{tab:gradient_clipping}
\vskip 0.15in
\begin{center}
\begin{scriptsize}
\begin{tabular}{@{}l|ccc@{}}
\toprule
Clipping Value & No duplication  & 10x duplication  & Accuracy       \\ \midrule
$1.0 \times 10^{0}$ (default) & 0.0266          & 0.4235           & 0.520          \\
$5.0 \times 10^{-1}$          & 0.0235          & 0.4235           & \textbf{0.541} \\
$1.0 \times 10^{-1}$          & 0.0229          & 0.4031           & 0.530          \\
$5.0 \times 10^{-2}$          & 0.0243          & 0.3827           & 0.534          \\
$1.0 \times 10^{-2}$          & 0.0227          & 0.3914           & 0.506          \\
$5.0 \times 10^{-3}$          & 0.0245          & 0.3914           & 0.531          \\
$1.0 \times 10^{-3}$          & 0.0250          & 0.3352           & 0.519          \\
$5.0 \times 10^{-4}$          & 0.0203          & 0.2914           & 0.528          \\
$1.0 \times 10^{-4}$          & 0.0185          & 0.0926           & 0.536          \\
$5.0 \times 10^{-5}$          & 0.0151          & 0.0438           & 0.506          \\
$1.0 \times 10^{-5}$          & 0.0086          & 0.0099           & 0.491          \\
$5.0 \times 10^{-6}$          & 0.0065          & 0.0080           & 0.449          \\
$1.0 \times 10^{-6}$          & 0.0026          & 0.0012           & 0.460          \\
$5.0 \times 10^{-7}$          & 0.0026          & 0.0012           & 0.392          \\
$1.0 \times 10^{-7}$          & 0.0026          & 0.0012           & 0.377          \\ 
\bottomrule
\end{tabular}
\end{scriptsize}
\end{center}
\vskip -0.1in
\end{table}


\subsection{Optimizer effect on loss}
Figure ~\ref{fig:adamw_vs_sgd} illustrates the loss reduction difference between Stochastic Gradient Descent (SGD) and Paged AdamW optimizers during the fine-tuning of Llama 3.2 3B. The SGD optimizer failed to achieve the same level of loss reduction as Paged AdamW.
\label{sec:adamw_vs_sgd}
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.5\textwidth]{adamw_vs_sgd.pdf}}
\caption{\textbf{Loss reduction comparison between optimizers.} The plot compares loss reduction during the fine-tuning of Llama 3.2 3B using different optimizers: SGD (blue) and Paged AdamW (orange).}
\label{fig:adamw_vs_sgd}
\end{center}
\vskip -0.2in
\end{figure}

\section{Post-fine-tuning Gaussian noise injection}
\label{sec:noise_injection}
This section provides details and results of the injection of noise into the weights of a model after fine-tuning. Specifically, the noise is sampled from a Gaussian distribution $\mathcal{N}(\mu, \sigma^2)$, where the mean $\mu$ is set to 0, and $\sigma^2$ is the variance that determines the noise's magnitude. Unlike the DP Gaussian mechanism, this approach does not provide formal privacy guarantees. However, it offers a practical and computationally light method to mitigate the memorization of sensitive information, as it does not require additional fine-tuning and can be directly applied to previously fine-tuned LLMs. Additionally, measuring the performance of this method can illustrate how other noise mechanisms similar to those used in DP might affect accuracy and privacy metrics.

In Table~\ref{tab:noise_addition}, we evaluate its effect under various noise magnitudes, along with the corresponding impact on model accuracy. We applied Gaussian noise to the LoRA weights of a fine-tuned Llama 3.2 3B model, as evaluated in earlier sections. We then compared the model's BLEU score and accuracy across different noise magnitudes.

\begin{table}[!htbp]
\caption{\textbf{Impact of noise addition on BLEU score and accuracy.} Llama 3.2 3B is fine-tuned with LoRA across various noise magnitudes ($\sigma$)}
\label{tab:noise_addition}
\vskip 0.15in
\begin{center}
\begin{scriptsize}
\begin{tabular}{@{}l|ccc@{}}
\toprule
Noise Scale ($\sigma$)   & BLEU, no Duplication & BLEU, 10x Duplication & Accuracy       \\ \midrule
0 (no noise)             & 0.0206         & 0.3012          & 0.553          \\
0.001                    & 0.0211         & 0.3049          & 0.552          \\
0.01                     & 0.0206         & 0.2877          & 0.551          \\
0.02                     & 0.0143         & 0.0994          & 0.541          \\
0.03                     & 0.0083         & 0.0111          & 0.511          \\
0.04                     & 0.0013         & 0.0006          & 0.384          \\ 
0.05                     & 0.0000         & 0.0000          & 0.110          \\
\bottomrule
\end{tabular}
\end{scriptsize}
\end{center}
\vskip -0.1in
\end{table}

We observe that the accuracy remains unaffected up to a certain noise level ($\sigma = 0.01$) and even shows slight improvement. However, beyond this threshold, accuracy decreases and reduction in memorization similarly follows, appearing to correlate with this decrease. These observations suggest that this mechanism effectively reduces excessive memorization in models that have overfitted onto their training data. Therefore, this approach offers an alternative to early stopping for controlling memorization which can be applied post fine-tuning. Figure~\ref{fig:tradeoff_noise} compares the privacy and utility of Llama 3.2 3B subject to post-fine-tuning gaussian noise injection with the evolution of the model fine-tuned with LoRA accross iterations. The noisy model, represented by red dots, has been fine-tuned for 2100 iterations before injecting the gaussian noise. Gaussian noise injection of standard deviations of $\sigma=0.2$ and $\sigma=0.3$ have been reported in the plot.

\subsection{Privacy-Utility tradeoff with Gaussian noise injection}
\label{sec:tradeoff_noise}

Figure~\ref{fig:tradeoff_noise} presents a dot plot comparing the privacy-utility tradeoffs of Llama 3.2 3B when fine-tuned with LoRA versus when Gaussian noise is injected after fine-tuning with LoRA. The results indicate that Gaussian noise injection does not enhance the privacy-utility tradeoff compared to fine-tuning with LoRA.

\begin{figure}[!htbp]

\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{utility_privacy_noise.pdf}}
\caption{\textbf{Privacy-Utility tradeoff with post-fine-tuning gaussian noise injection.} Accuracy and memorization (BLEU score with 10x document duplication) tradeoff of Llama 3.2 3B subject to post-fine-tuning gaussian noise injection with standard deviation. Values above the dots correspond to the number of iterations for LoRA fine-tuning evolution, and the standard deviation of injected noise for noisy models.}
\label{fig:tradeoff_noise}
\end{center}
\vskip -0.2in
\end{figure}