\section{Discussion}
Our experimental evaluation demonstrates that LoRA reduces memorization in both centralized and FL settings, which naturally raises the question: \textit{why does this happen?} We argue that the mechanisms by which FedAvg and LoRA mitigate memorization should be considered independently. \citet{carlini2022quantifying} empirically establish a log-linear relationship between canary duplication and memorization, thus we frame our discussion of memorization in the context of \textit{overfitting}. How and why in-distribution, non-duplicated sequences can still be regurgitated \citep{carlini2019secret} is a question that we leave to future work.

\textbf{Federated learning.} While it is known that FedAvg can reduce memorization for simpler LSTM-based next-word predictors (NWPs) \citep{ramaswamy2020training, thakkar2020understanding}, we hope that our verification of this phenomenon for LLMs on longer canaries can encourage formal investigation. Nevertheless, we note the following: in the IID FedAvg setting with identical hyperparameter settings (same number of local updates, learning rate, and initialization) the expected value of the $d$-sample stochastic gradient over $N$ clients, $\frac{1}{N}\frac{1}{d}\sum_{i=1}^kf_k(\theta, x_i \hspace{-0.1em}\sim\hspace{-0.1em}D_k)$ in Equation \ref{fl-opt} can resemble a single stochastic gradient in a centralized setting taken over a single large batch of size $Nk$ since $f_k$ and $D_k$ are homogeneous. Thus, \citet{thakkar2020understanding} observe more memorization in IID settings with larger batch sizes. The non-IID setting is significantly more complex: the optimization problem and associated loss landscape of Equation \ref{fl-opt} differs from the centralized problem. We observe in Figures \ref{fig-fl-remaining-mem} and \ref{fig-fl-privacy} that non-IID FL significantly reduces memorization, which \citet{thakkar2020understanding} also observe for their NWPs. While they do not fine-tune their learning rates to eliminate this as a confounding variable, we do\footnote{While it is possible that performing centralized learning in a curriculum-style manner with heterogeneous learning rates over training data can reduce memorization, given the small performance gap against non-IID FL, it is highly unlikely that this alone can improve its significantly worse memorization scores.}, thus suggesting that FedAvg itself is a memorization-reducing mechanism.

\textbf{LoRA.} It is possible that LoRA reduces benign overfitting \citep{bartlett2020benign}, which occurs when training data is overfitted without affecting performance. Notably, \citet{tang2023dpadambc} prove that benign overfitting can preserve out-of-distribution generalization for overparameterized linear models if there is a strong correlation between the dominant eigenvectors/components of the source and target distributions. It is possible then that our LLMs are displaying this phenomenon: in both the centralized and FL settings, our fine-tuning datasets, while heterogeneous, contain aligned components due to their shared domain. LoRA may reduce benign overfitting by ignoring minor components, which only explain a minimal (and possibly noisy) portion of the data covariance. 

Specific to FL, an alternative hypothesis is that the low-rank approximation of $\Delta W$ resembles a $\delta$-compression operator \citep{karimireddy2019error}, i.e., $||\texttt{LORA}(\Delta W)-\Delta W||^2 \leq (1-\delta)||\Delta W||^2$, and that low-$\delta$ compressors reduce memorization. Low-bias compressors, such as certain randomized projections \citep{dorfman2023docofl, rabbani2021comfetch, ivkin2019communication} and other low-rank approximations \citep{makkuva2023laser} have been shown to preserve model performance in non-IID distributed settings. While the effects of these other operators on memorization has not been extensively studied, the efficacy of gradient clipping in lowering memorization while maintaining accuracy (Table \ref{tab:gradient_clipping}) lends further credence to this hypothesis. Clipping is a low-bias compressor for heavy-tailed gradients, which is observed for general SGD \citep{mireshghallah2022quantifying} and LLM fine-tuning \citep{kenton2019bert}. Further exploration of $\delta$-compressors is warranted.