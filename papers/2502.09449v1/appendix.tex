
\appendices

% % \vskip 0.1in
% {\centering\section*{Appendix for\\``Spiking Neural Networks for Temporal Processing: Status Quo and Future Prospects''}}
% % \vskip 0.1in

\setcounter{page}{1}

\Large
\begin{center}
    {\bf \LARGE Appendix}\\
    \vspace{0.2cm}
    {\large ``Spiking Neural Networks for Temporal Processing: Status Quo and Future Prospects''}\\
	\vspace{0.2cm}
    {\normalsize Chenxiang~Ma{*}, Xinyi Chen{*}, Yanchen Li{*}, Qu Yang, Yujie~Wu, Guoqi~Li,~\IEEEmembership{Member,~IEEE}, Gang~Pan,~\IEEEmembership{Senior Member,~IEEE},  Huajin~Tang,~\IEEEmembership{Senior Member,~IEEE}, Kay~Chen~Tan,~\IEEEmembership{Fellow,~IEEE}, Jibin~Wu,~\IEEEmembership{Member,~IEEE}}\\
\end{center}
% \rule[-0.5pt]{18.1cm}{0.06em}

% \parskip=2pt

\small

% \parskip=2pt

This appendix provides the implementation details, the organization of our source code, and supplementary visualizations for the analysis of the DvsGesture dataset.

\section{Implementation Details of Section~\ref{sec:eval_bench} ``Evaluation of Neuromorphic Benchmarks Using STP''}
\label{app:imp_det_exsiting_bench}
\subsection{Datasets}
We analyze the effectiveness of ten widely used neuromorphic benchmarks in the evaluation of temporal processing capabilities, including three static image recognition datasets (i.e., MNIST~\cite{lecun1998gradient}, CIFAR10~\cite{krizhevsky2009learning}, and CIFAR100~\cite{krizhevsky2009learning}), three event-based vision datasets (i.e., N-MNIST~\cite{orchard2015converting}, DVS-CIFAR10~\cite{li2017cifar10}, and DvsGesture~\cite{amir2017low}), and four audio classification datasets (i.e., GSC~\cite{warden2018speech}, SHD~\cite{cramer2020heidelberg}, SSC~\cite{cramer2020heidelberg}, and TIMIT~\cite{TIMIT}). Details of these datasets and the data augmentation techniques adopted in our experiments are provided below.

\begin{itemize}
    \item \textbf{MNIST} dataset consists of $60,\!000$ training and $10,\!000$ testing handwritten-digit images in $10$ classes. The size of each image is $28\times28$.
    \item \textbf{CIFAR10} and \textbf{CIFAR100} datasets contain a total of $50,\!000$ training images and $10,\!000$ test images, categorized into $10$ and $100$ classes, respectively. For the training set, we apply standard data augmentation, which includes padding each sample with $4$ pixels on all sides, followed by a $32\times32$ crop and a random horizontal flip. In line with previous studies~\cite{ASGL}, we also utilize the AutoAugment and Cutout techniques for additional data augmentation.
    \item \textbf{N-MNIST} dataset is created by capturing static MNIST images with a DVS camera. Each spike pattern has a spatial dimension of $34\times34\times2$ and lasts for $300$ time steps. No data augmentation methods are used.
    \item \textbf{CIFAR10-DVS} dataset is obtained from the CIFAR-10 dataset by scanning each image with repeated closed-loop movements in front of a DVS camera. It includes $9,\!000$ training samples and $1,\!000$ testing samples, with a spatial resolution of $128\times128$. Like CIFAR-10, CIFAR10-DVS comprises $10$ classes. We utilize the standard preprocessing pipeline from SpikingJelly~\cite{2023spikingjelly} to convert events into frames for further analysis, without applying any data augmentation techniques.
    \item \textbf{DvsGesture} dataset comprises $11$ different hand gestures performed by $29$ different subjects, captured using a DVS camera under three varying illumination conditions. We apply the standard preprocessing pipeline from SpikingJelly to transform events into frames with $20$ time steps, without implementing any data augmentation techniques.
    \item \textbf{GSCv2} dataset consists of $105,\!829$ one-second audio clips featuring $35$ different spoken commands, which are recorded by various speakers in diverse environments. Following existing studies~\cite{TCLIF,ALIF}, the original classes are restructured into $12$ categories, including ten words: ``yes'', ``no'', ``up'', ``down'', ``left'', ``right'', ``on'', ``off'', ``stop'', and ``go'', along with a special ``unknown'' class that encompasses the remaining commands, plus an extra ``silence`` class extracted from background noise. The preprocessing methods align with those used in prior studies~\cite{TCLIF,ALIF}.
    \item \textbf{SHD} dataset is a keyword spotting task for spike-based audio classification, featuring spoken digits from $0$ to $9$ in both English and German, which are categorized into $20$ classes. The dataset comprises recordings from twelve distinct speakers, with two exclusively in the test set. Based on the method from previous work~\cite{TCLIF}, each original waveform has been transformed into spike trains across $700$ input channels. The training set includes $8,\!332$ samples, while the test set contains $2,\!088$ samples, with no separate validation set. 
    \item \textbf{SSC} dataset is created from the GSCv2 dataset, utilizing a biologically inspired encoding method to represent data in a spike-based format. It encompasses 35 classes contributed by a diverse array of speakers. In accordance with the method outlined by Zhang~\emph{et al.}~\cite{TCLIF}, the original waveforms have been converted into spike trains across $700$ input channels. The dataset includes $75,\!466$ samples in the training set and $20,\!382$ samples in the test set.
    \item \textbf{TIMIT} dataset is a corpus of read speech that includes time-aligned orthographic, phonetic, and word transcriptions, as well as a single-channel, 16-bit, \SI{16}{\kilo\hertz} speech waveform file for each utterance. It features broadband recordings from 630 speakers --- approximately 70\% male and 30\% female --- representing 8 major dialects of American English, with each speaker reading 10 phonetically rich sentences. Following previous work~\cite{Yin2021accurate}, the raw audio data is preprocessed into Mel Frequency Cepstral Coefficients and converted into frames with 39 input channels. Each frame is then classified into one of 61 phoneme classes for prediction.
\end{itemize}


\subsection{Training Configurations}
Our training configurations are detailed in Table \ref{tab:train_details_benchmarks}. For clarity, the following notations are employed in denoting network architectures: \texttt{C} represents a convolutional layer, \texttt{AP} stands for average pooling, and \texttt{FC} denotes a fully connected layer. Furthermore, \texttt{C3} indicates a convolutional of kernel size $3\times3$. The numerical value preceding \texttt{C} and \texttt{FC} indicates the number of output channels.


\section{Implementation Details of Section~\ref{sec:benchmark} ``Temporal Processing Benchmark Suite''}
\label{app:imp_det_seq_bench}

The training configurations are outlined in Table~\ref{tab:train_details_benchmarks}. Specifically, for the PS-MNIST and Binary Adding tasks, we utilize the AdamW optimizer \cite{loshchilov2017fixing} in conjunction with a step scheduler that reduces the learning rate by a factor of 0.8 every 10 epochs. In contrast, the PTB task employs the stochastic gradient descent (SGD) optimizer without a scheduler. This configuration has been validated as effective for training the SNN methods used in this study. Fully connected network architectures are implemented for all SNNs in the experiments. Consistent with standard practices, the hidden dimensions are set to \texttt{64FC-256FC-256FC} for the PS-MNIST task \cite{ALIF, TCLIF} and \texttt{400FC-1100FC} for the PTB task \cite{merityRegOpt}. For our proposed Binary Adding task, the hidden dimensions are configured as \texttt{128FC-128FC}. 


% \section{Implementation Details of Section~\ref{sec:benchmarking} ``Status Quo of SNNs for Sequence Modeling''}


\begin{table}[!t]
\caption{Training configurations and hyper-parameters for the evaluation of neuromorphic benchmarks}
\label{tab:train_details_benchmarks}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\hline
  \textbf{Dataset} &
  \textbf{Epochs} &
  \textbf{Optimizer} &
  \begin{tabular}[c]{@{}c@{}}\textbf{Learning} \\ \textbf{Rate}\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textbf{Learning Rate} \\ \textbf{Schedule}\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textbf{Batch} \\ \textbf{Size}\end{tabular} &
   \begin{tabular}[c]{@{}c@{}}\textbf{Time} \\ \textbf{Step} (\(T\))\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textbf{Neuronal}\\ \textbf{Decay}\end{tabular} &
  \textbf{Threshold} &
  \begin{tabular}[c]{@{}c@{}}\textbf{Network} \\ \textbf{Architecture}\end{tabular}\\ \hline
MNIST            & 100 & AdamW   & 0.0001  & Cosine Annealing & 256 & 10 & 0.5 & 0.3 &  \begin{tabular}[c]{@{}c@{}}\texttt{32C3-AP2-32C3-} \\ \texttt{AP2-128FC-10FC} \end{tabular}  \\
CIFAR10          & 200 & SGD   & 0.1 & Cosine Annealing & 128 & 4 & 0.3 & 1.0 & ResNet18  \\
CIFAR100         & 200 & SGD   & 0.1 & Cosine Annealing & 128 & 4 & 0.3 & 1.0 & ResNet18  \\
N-MNIST          & 100 & AdamW & 0.0001 & Cosine Annealing & 16 & 300  & 0.65 & 0.3 & \begin{tabular}[c]{@{}c@{}}\texttt{64C7-AP2-128C7-} \\ \texttt{128C7-AP2-10FC} \end{tabular}   \\
DVS-CIFAR10      &  200  & AdamW & 0.001 & Cosine Annealing & 64 &  10   & 0.3 & 1.0 & VGG11  \\
DvsGesture        & 200 & AdamW & 0.001 & Cosine Annealing & 32 & 20 & 0.3 & 1.0 & VGG11 \\
GSC       & 200 & AdamW & 0.01 & Cosine Annealing & 128 & 101  & 0.8 & 0.5 &  \texttt{(512FC)*6-10FC}  \\ 
SHD       & 200 & AdamW & 0.0005 & -        & 256 & 250  & 0.9 & 0.7 & \texttt{(128FC)*5-20FC}  \\
SSC       & 200 & AdamW & 0.0005 & -        & 256 & 250  & 0.9 & 0.9 & \texttt{(128FC)*5-35FC}  \\ 
TIMIT     & 200 & AdamW & 0.0005 & -        & 64  & 100  & 0.7 & 0.7 & \texttt{(1024FC)*5-61FC} \\ 
\hline
PTB       & 100 & SGD  & 3     & -          & 20  & 70 & 0.5 & 0.5 & \texttt{400FC-1100FC-400FC} \\
PS-MNIST & 100 & AdamW & 0.0005 & StepLR    & 256 & 784 & 1.0 & 0.5 & \texttt{64FC-256FC-256FC-10FC} \\
Binary Adding & 50 & AdamW & 0.0005 & StepLR & 250 & 100 & 0.98 & 0.5 & \texttt{128FC-128FC-10FC} \\ 
\hline
\end{tabular}%
}
\end{table}

\section{Details of Energy Cost Computation for Non-spiking and Spiking Neural Architectures}
\label{sec:energy_·cost}
This section outlines the computation of empirical energy cost metrics presented in Table \ref{tab:energy}. 
We first establish theoretical energy cost formulas for each hidden layer of compared models, as presented in Table \ref{tab:energy_for}. Following standard computational cost evaluation approaches for ANNs and SNNs, we count their number of 32-bit floating-point Multiply-Accumulate (MAC) and Accumulate (AC) operations per inference based on their spatiotemporal dynamics. These numbers of operations are then used to estimate the energy consumption of models on neuromorphic hardware, where $E_\text{MAC}$ and $E_\text{AC}$ denote the energy required per MAC and AC operation, respectively. In these formulas, $m$ and $n$ denote the input sizes, and hidden sizes for each layer, respectively. The variables $f_\text{in}$ and $f_\text{out}$ represent the spike frequencies of input and output spike sequences. For the SpikingTCN, $k$ denotes the convolution kernel size, $f_\text{conv2}$ specifies the spike frequency of input spikes in the second convolution layer. In the Spike-Driven Transformer, $h$ is the hidden dimension of feedforward modules, $f_{Q}$, $f_{K}$, $f_{V}$, $f_\text{attn}$, $f_\text{fc1}$, and $f_\text{fc2}$ denote the spike frequency of neurons in the self-attention and feedforward modules. These spike frequency data is recorded during inference across three tasks and used to compute the number of AC operations.

Subsequently, based on data derived from a \SI{45}{\nano\meter} CMOS process~\cite{CMOS}, the energy per operation is measured to $E_\text{AC} = \SI{0.9}{\pico\joule}$ for AC operations and $E_\text{MAC} = \SI{4.6}{\pico\joule}$ for MAC operations. All the above data are ultimately substituted into the theoretical energy cost formulas, computing the empirical energy cost for each layer. The total energy cost is then obtained by summing the energy consumption across all hidden layers, as demonstrated in the main text.


\begin{table}[h!]
\centering
\caption{Theoretical energy costs of different neural architectures}
\setlength{\tabcolsep}{26pt}
\renewcommand{\arraystretch}{1.25}
\begin{threeparttable}
    \begin{tabular}{l | c}
    \hline
    \textbf{Architecture} & \textbf{Theoretical Energy Cost} \\ \hline
    TCN  &\thead{$\left( kmn + kn^2 \right) \cdot E_\text{MAC}$}  \\ 
    SpikingTCN  &\thead{$\left( kmn \cdot f_\text{in} + kn^2 \cdot f_\text{conv2} \right) \cdot E_\text{AC}$}  \\ 
    LSTM  & \thead{$\left( 4mn + 4n^2 + 19n \right) \cdot E_\text{MAC}$} \\ 
    GSU  & \thead{$\left(2mn \cdot f_\text{in} + 2n^2 \cdot f_\text{out} \right) \cdot E_\text{AC} + 5n \cdot E_\text{MAC} $} \\ 
    Transformer & \thead{$ \left( 4n^2 + 2nT +2nh \right) \cdot E_\text{MAC}$}\\
    Spike-Driven Transformer ($T_{\text{in}}=4$) & \thead{$\left(\left( 12f_\text{in} + 4f_\text{attn} \right) \cdot n^2 +\left( 4f_{Q} \cdot f_{K} + 4f_{V} \right) \cdot nT + \left(4f_\text{fc1} + 4f_\text{fc2} \right) \cdot nh \right) \cdot E_\text{AC} + \left( 24n+4h\right) \cdot E_\text{MAC}$} \\ 
    Spike-Driven Transformer ($T_{\text{in}}=1$)  & \thead{$\left(\left( 3f_\text{in} + f_\text{attn} \right) \cdot n^2 +\left( f_{Q} \cdot f_{K} + f_{V} \right) \cdot nT + \left(f_\text{fc1} + f_\text{fc2} \right) \cdot nh \right) \cdot E_\text{AC}$} \\ 
    \hline
    \end{tabular}
\label{tab:energy_for}
\end{threeparttable}
\end{table}


\section{ Organization of the Benchmarking Library}

In this section, we provide a detailed description of the structure of our open-sourced benchmarking library. Initially, we present an overview of the library, highlighting its main components such as the framework and experimental configurations. We then examine the structure of the model design within the framework, elucidating the role and functionality of each component. This systematic exposition is designed to facilitate the rigorous evaluation and further development of SNN models and datasets in future studies.

\subsection{Overview of the Benchmarking Library}
This subsection presents the architecture of our benchmarking library, which comprises two key components: \texttt{framework} and \texttt{experiments}. Together, these components establish a comprehensive structure that supports both model development and task design while ensuring a well-defined experimental setup. They collectively provide essential interfaces that enable users to interact with the benchmark and contribute new features.

The \texttt{framework} serves as the foundation for model design, offering functionalities for defining SNN models and selecting appropriate training strategies. This component plays a crucial role in deploying models for specific tasks and incorporates Compute Unified Device Architecture (CUDA)-based acceleration kernels along with preprocessing capabilities tailored for task-specific datasets.

The \texttt{experiments} component provides a detailed set of benchmark experiments, serving as practical references for users. Each experiment is organized as an independent module, consisting of a code file and a configuration file. The code file specifies the experimental workflow, while the configuration file simplifies the process by enabling users to load and manage experiment-specific configurations, including hyperparameter settings efficiently.

% This subsection outlines the architecture of this benchmark, which consists of the \texttt{framework} and \texttt{experiments} components, together forming a complete structure for both model architecture and task design, as well as experimental setup. These components collectively provide the interfaces needed for users to interact with the benchmark and contribute new features.
% The \texttt{framework} serves as the cornerstone of the model design, facilitating the definition of SNN models and the selection of various training strategies. This component is integral for deploying these models in specific tasks and includes both Compute Unified Device Architecture (CUDA) based acceleration kernels and preprocessing capabilities for task-specific datasets. 
% Additionally, the \texttt{experiments} provides a collection of benchmark experiments, serving as experiment references for users. Each experiment is listed independently and includes a code file and a configuration file. The code file outlines the experimental workflow, while the configuration file is designed to streamline the process by allowing users to efficiently load and manage experiment-specific configurations, including hyperparameter settings.
% The entire structure is built upon the PyTorch~\cite{DBLP:conf/nips/PaszkeGMLBCKLGA19} framework, ensuring robustness and flexibility in neural network experimentation and development.

\subsection{Structure of \texttt{framework}}

The \texttt{framework} component of the benchmarking library is designed to assist users in defining, training, and deploying SNN models through structured interfaces. It consists of three key modules: \texttt{kernel}, \texttt{network}, and \texttt{utils}. A detailed breakdown of these components and their specific functionalities is presented in Table~\ref{tab:framework}. The \texttt{kernel} module facilitates CUDA-based acceleration to enhance computational efficiency, incorporating an optional kernel based on the temporal fusion method~\cite{snn_temporal_fusion_2024}, which is specifically designed to accelerate processing within spiking neuron layers during long temporal sequences in both training and inference.

Within the \texttt{network} module, three primary components are provided: \texttt{neuron}, \texttt{structure}, and \texttt{trainer}.
The \texttt{neuron} component enables users to select particular spiking neurons.
The \texttt{structure} component offers a collection of predefined network architectures.
Once the model structure is established, the \texttt{trainer} component provides various training strategies to optimize model performance.

The \texttt{utils} module offers essential functionalities for both training and deployment.
It includes the \texttt{tools} component, which provides optional utilities to facilitate model training, and the \texttt{dataset} component, which supports various neuromorphic datasets.

%, with optimized modules designed for accelerated data processing.

% The \texttt{framework} element of the benchmark is designed to assist users in defining, training, and deploying SNN models via structured interfaces. It comprises three modules -- kernel, network, and utils -- which offer system-level, algorithm-level, and application-level support, respectively. Detailed descriptions of these components and their instances are presented in Table~\ref{tab:framework}, this detailed breakdown of the \texttt{framework} highlights the extensive support provided at different stages of model development and deployment, from system optimization to practical application.

\begin{table}
\caption{Overview of the benchmarking library}
\label{tab:framework}
    \setlength{\tabcolsep}{14.2pt}
    \renewcommand{\arraystretch}{1.25}
    \centering
    \begin{tabular}{l|c|c|c}
    \hline
    \textbf{Module} & \textbf{Component} & \textbf{Instance} & \textbf{Description} \\
    \hline
    \texttt{kernel} & - & \texttt{temporal\_fusion\_kernel}, etc. & \makecell[c]{Ready-to-use accelerated CUDA kernel wrappers \\ optimized for spiking neurons.} \\
    \hline
    \multirow{3}{*}{\texttt{network}} & \texttt{neuron} &  LIF, ALIF, TDBN, TEBN, etc. & \multirow{3}{*}{\makecell[c]{Interfaces for modules defining network layers, architectures, \\ and the corresponding training methods required for SNNs.}} \\
     & \texttt{structure} & GSU, SpikingTCN, etc. & \\
     & \texttt{trainer} & \texttt{SurrogateGradient}, etc. & \\
    \hline
    \multirow{2}{*}{
    \texttt{utils}} & \texttt{dataset} & PTB, PS-MNIST, etc. 
    & \multirow{2}{*}{\makecell[c]{Encapsulation of utility functions and optimized dataset \\ modules for efficient model training and validation.}} \\
    & \texttt{tools} & \texttt{logging}, \texttt{save\_checkpoint}, etc. & \\
    \hline
    \end{tabular}
\end{table}

% The \texttt{kernel} module facilitates CUDA-based acceleration to enhance experimental efficiency, incorporating optional kernel based on the temporal fusion method~\cite{snn_temporal_fusion_2024} designed to speed up the processing in spiking neuron layers during long temporal sequences of training and inference.

% Within the \texttt{network} module, three primary components are available: \texttt{neuron}, \texttt{structure}, and \texttt{trainer}. 
% The \texttt{neuron} component allows users to select models at the neuron layer level of SNN.
% Following this, the \texttt{structure} component offers a selection of predefined network architectures, enabling users to solidify their model. Once the model is established, users can employ the training strategies provided in the \texttt{trainer} component to optimize their models.

% The \texttt{utils} module provides essential application-level functionalities for both training and deployment processes. 
% It includes the \texttt{tools} component, which provides useful and optional training tools for model training, and the \texttt{dataset} component, which supplies task-specific datasets.
% Notably, the \texttt{dataset} component supports all the datasets used in experiments of this work, with modules optimized for accelerated performance.

% \begin{figure*}[!htb]
% \centering\includegraphics[width=0.97\linewidth]{figs/dvs_gesture_vis_appendix.jpg}
% \caption{Qualitative results of samples from the DvsGesture dataset. For each sample. the selected most confident frame (highlighted with red boxes) is consistent across all three algorithms in STP. Labels are given in the leftmost column.}
% \label{Fig:app_dvs_vis}
% \end{figure*}

\begin{figure*}[!htb]
\centering\includegraphics[width=0.97\linewidth]{figs/dvs_gesture_vis_appendix_diff_t.jpg}
\caption{Qualitative results of samples from the DvsGesture dataset. In these samples, the most confident frame differs across the three algorithms in STP. Frames selected by STBP are highlighted with purple boxes, SDBP with green boxes, and NoTD with blue boxes.}
\label{Fig:app_dvs_vis_diff_t}
\end{figure*}

\begin{figure*}[!htb]
\centering\includegraphics[width=0.97\linewidth]{figs/dvs_gesture_vis_appendix_stbp_right_notd_wrong.pdf}
\caption{Visualization of samples from the DvsGesture dataset that STBP classifies correctly, while NoTD does not. The predictions by NoTD are right arm counter clockwise, left arm clockwise, air drums, hand clapping, and other gestures (from top to bottom).}
\label{Fig:app_dvs_vis_stbp_correct_notd_wrong}
\end{figure*}

\begin{figure*}[!htb]
\centering\includegraphics[width=0.97\linewidth]{figs/dvs_gesture_vis_appendix_stbp_wrong.jpg}
\caption{Visualization of samples from the DvsGesture dataset that are misclassified by STBP.  The predictions by STBP are other gestures, air drums, other gestures, hand clapping, arm roll, arm roll, hand clapping, other gestures, other gestures, air guitar, air drums, right arm counter clockwise, arm roll, air drums (from top to bottom).
}
\label{Fig:app_dvs_vis_stbpwrong}
\end{figure*}






