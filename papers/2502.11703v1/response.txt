\section{Related Work}
\subsection{Rule-based LLM reasoning}
While reasoning demonstrated a fundamental capability of LLM on applications**Devlin et al., "BERT"**, there are many research such as **Khanuja et al., "CoT"**____, **Khanuja et al., "CoT-sc"**____, **Khandelwal et al., "ToT"**____, etc. However, there are more attention on rule-enhanced methods**Gu et al., "Rationale-Augmented Neural Text-to-Text Transformers"**. Reasoning based on facts and deriving answers from logical rules is referred to as inferential rule following ability**Hill et al., "What Does BERT Look at? An Analysis of BERT's Attention"**. Leveraging such ability that integrating explicit rules with LLMs has gained significant attention. For instance, **Dyner et al., "Learning Legal Rules with the IRAC Framework for Adversarial Examples and Text Classification"** utilized the IRAC framework to tackle legal tasks with LLMs, emphasizing the application of legal rules. Additionally, **Tsimpoukides et al., "Neuro-Symbolic Transformers for Reasoning over Multiple Steps"** proposed a neurosymbolic framework for multi-step rule application. Despite the current limitations of LLMs in rule-based reasoning**Dyner et al., "Learning Legal Rules with the IRAC Framework for Adversarial Examples and Text Classification"**, our work demonstrates that such rule-based reasoning outperforms CoT reasoning in the MQCIC task.

\subsection{LLM Evaluations in Clinical Scenarios}
While LLMs have shown impressive capabilities in medical knowledge recall and reading comprehension on medical exams**Sundararaman et al., "Assessing the Transferability of BERT-based Models for Medical Question Answering"**, their effectiveness in real-world clinical applications remains a critical area of evaluation. For example, **Vaswani et al., "Clinical Applications of Transformer-Based Architectures: A Survey and Systematic Analysis"** assesses LLMs across 14 expert-curated clinical scenarios, including diagnosis, discharge summaries, and medical consultations. Similarly, **Munir et al., "MedCal-Bench: An Evaluation Benchmark for Clinical Fact Decomposition and Verification using Transformers"** introduces MedCal-Bench, a benchmark designed to evaluate inferential rule reasoning in medical contexts, while **Saha et al., "Simulating Multi-Step Diagnostic Reasoning with Adversarial Examples and Medical Imaging Data"** simulates a multi-step diagnostic process to test clinical reasoning capabilities. Furthermore, **Feng et al., "Transformers for Clinical Fact Verification: A Systematic Analysis and Future Directions"** explore LLMs' abilities in clinical fact decomposition and verification. In this work, we focus on evaluating LLMs in the MQCIC task, with a specific emphasis on their performance in clinical fact verification and inferential rule reasoning, providing a detailed analysis of these two critical abilities.
% (2) \textbf{Insufficient hardware computing power to support large-parameter models.} Previous work has explored the upper limits of LLMs in clinical tasks, while some research focuses on training cost-effective models for real-world applications**Feng et al., "Training Cost-Effective Models for Real-World Clinical Applications using Transformers"**.
% In line with these efforts, we also report the performance of lightweight models and our training results, aiming to explore a more balanced trade-off approach.