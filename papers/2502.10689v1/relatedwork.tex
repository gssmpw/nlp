\section{Related Work}
\subsection{Deep Learning on Longitudinal EHRs}
Applying deep learning models to longitudinal EHRs for predictive tasks centers around learning adequate patient representation \citep{ehr:3, baseline:16, tan2024enhancing}. To model temporal disease progression patterns, Doctor AI \citep{baseline:1} leveraged recurrent neural networks (RNNs), StageNet \citep{baseline:4} employed a stage-aware long short-term memory (LSTM) module, and Hi-BEHRT \citep{ehr:18} adopted a hierarchical Transformer effective for long visit histories. To account for irregular time gaps between visits, variants of LSTM and self-attention mechanisms that consider timestamps have been introduced \citep{baseline:9, baseline:11, ehr:20}. To utilize external medical knowledge, methods such as GRAM \citep{baseline:6, baseline:13, baseline:15, ehr:1, baseline:17} infused information from medical ontologies into representation learning via attention mechanisms; PRIME \citep{ehr:21} incorporated rule-based prior medical knowledge through posterior regularization; and SeqCare \citep{ehr:2} employed online medical knowledge graphs with adaptive graph structure learning. To address low-quality data, methods such as GRASP \citep{ehr:22}, which utilized knowledge from similar patients through graph neural networks, and MedSkim \citep{ehr:5}, which filtered out noisy diagnoses using the Gumbel-Softmax trick, were developed. Existing approaches tried to enhance model interpretability by giving weights to every past diagnosis or visit: methods such as RETAIN achieved this with attention mechanisms \citep{baseline:2, baseline:3, baseline:5, baseline:12, ehr:13, ehr:19, baseline:14, ehr:7}; AdaCare \citep{baseline:8} employed a scale-adaptive feature recalibration module. To mitigate data insufficiency in certain scenarios, \citet{ehr:6} pre-trained BERT on a large EHR corpus and fine-tuned it on smaller datasets.

\subsection{Intrinsically Interpretable Models}
Intrinsic interpretability, which means that predictive models provide their own explanations, is favored over post-hoc interpretability that necessitates a separate model for explaining a black-box model, because the explanations provided by intrinsically interpretable models are exploited in the decision-making process and faithful to model predictions \citep{ii:1, ii:2}. SENN \citep{using:4} is a class of self-explaining neural networks whose interpretability is enforced via regularization. A self-explaining deep learning model proposed by \citet{using:5} utilizes an autoencoder and a prototype classifier network to provide case-based rationales. The attention mechanism has been employed to achieve intrinsic interpretability by using attention weights as explanations \citep{ii:6}, but its reliability is arguable \citep{ii:7, ii:10, ii:11}. SITE \citep{ii:4} emphasizes robust interpretations equivariant to geometric transformations. ProtoVAE \citep{ii:5} leverages a variational autoencoder to learn class-specific prototypes. The Bort optimizer \citep{ii:9} enhances model explainability by imposing boundedness and orthogonality constraints on model parameters.

Intrinsic interpretability has been studied in different domains such as recommender systems \citep{ii:8} and healthcare \citep{ehr:17}. However, unlike \method, existing self-explaining deep learning models for EHR-based diagnosis prediction fail to provide temporal explanations that reflect the distinct comorbidities of individual patients.

The related work on deep learning on hypergraphs is discussed in \appendixref{a0}.