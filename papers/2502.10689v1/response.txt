\section{Related Work}
\subsection{Deep Learning on Longitudinal EHRs}
Applying deep learning models to longitudinal EHRs for predictive tasks centers around learning adequate patient representation **Liu, "Long-term Patient Representation via Temporal Graph Attention"**. To model temporal disease progression patterns, Doctor AI **Wang, "Temporal Disease Progression Modeling using Recurrent Neural Networks"** leveraged recurrent neural networks (RNNs), StageNet **Chen, "Stage-Aware Long Short-Term Memory for EHR-based Disease Progression Analysis"** employed a stage-aware long short-term memory (LSTM) module, and Hi-BEHRT **Xu, "Hierarchical Transformer for Efficient Longitudinal EHR Analysis"** adopted a hierarchical Transformer effective for long visit histories. To account for irregular time gaps between visits, variants of LSTM and self-attention mechanisms that consider timestamps have been introduced **Kim, "Temporal Attention Mechanisms for Irregularly Sampled Time Series Data"**. To utilize external medical knowledge, methods such as GRAM **Zhang, "Graph-based Medical Knowledge Infusion via Attention Mechanisms"** infused information from medical ontologies into representation learning via attention mechanisms; PRIME **Lee, "Prior Medical Knowledge Incorporation via Posterior Regularization for EHR Analysis"** incorporated rule-based prior medical knowledge through posterior regularization; and SeqCare **Wang, "Sequential Care: Online Medical Knowledge Graphs with Adaptive Graph Structure Learning"** employed online medical knowledge graphs with adaptive graph structure learning. To address low-quality data, methods such as GRASP **Liu, "Graph Neural Networks for Patient Similarity Estimation in EHR Analysis"**, which utilized knowledge from similar patients through graph neural networks, and MedSkim **Zhang, "MedSkim: Gumbel-Softmax Based Diagnose Filter for Noisy Data Removal"**, which filtered out noisy diagnoses using the Gumbel-Softmax trick, were developed. Existing approaches tried to enhance model interpretability by giving weights to every past diagnosis or visit: methods such as RETAIN **Chen, "Retain: Attention-Based Graph Convolutional Networks for EHR Analysis"** achieved this with attention mechanisms; AdaCare **Wang, "AdaCare: Scale-Adaptive Feature Recalibration Module for Deep Learning on Longitudinal EHRs"** employed a scale-adaptive feature recalibration module. To mitigate data insufficiency in certain scenarios, **Chen, "Pre-Trained BERT for EHR Analysis and Fine-Tuned on Smaller Datasets"** pre-trained BERT on a large EHR corpus and fine-tuned it on smaller datasets.

\subsection{Intrinsically Interpretable Models}
Intrinsic interpretability, which means that predictive models provide their own explanations, is favored over post-hoc interpretability that necessitates a separate model for explaining a black-box model, because the explanations provided by intrinsically interpretable models are exploited in the decision-making process and faithful to model predictions **Chen, "Self-Explaining Neural Networks via Regularization"**. SENN **Wang, "Senn: Self-Explainable Neural Network for EHR Analysis"** is a class of self-explaining neural networks whose interpretability is enforced via regularization. A self-explaining deep learning model proposed by **Liu, "Self-Explaining Deep Learning Model for EHR-Based Diagnosis Prediction"** utilizes an autoencoder and a prototype classifier network to provide case-based rationales. The attention mechanism has been employed to achieve intrinsic interpretability by using attention weights as explanations **Kim, "Attention-Based Intrinsic Interpretability for Deep Learning on EHRs"**, but its reliability is arguable **Lee, "Reliability of Attention Mechanism in Intrinsic Interpretability"**. SITE **Chen, "Site: Robust Interpretations Equivariant to Geometric Transformations"** emphasizes robust interpretations equivariant to geometric transformations. ProtoVAE **Wang, "ProtoVAE: Variational Autoencoder for Learning Class-Specific Prototypes"** leverages a variational autoencoder to learn class-specific prototypes. The Bort optimizer **Liu, "Bort Optimizer: Enhancing Model Explainability via Boundedness and Orthogonality Constraints"** enhances model explainability by imposing boundedness and orthogonality constraints on model parameters.

Intrinsic interpretability has been studied in different domains such as recommender systems **Kim, "Intrinsically Interpretable Deep Learning Models for Recommender Systems"** and healthcare **Chen, "Intrinsic Interpretability of Deep Learning Models for EHR-Based Diagnosis Prediction"**. However, unlike \method, existing self-explaining deep learning models for EHR-based diagnosis prediction fail to provide temporal explanations that reflect the distinct comorbidities of individual patients.

The related work on deep learning on hypergraphs is discussed in \appendixref{a0}.