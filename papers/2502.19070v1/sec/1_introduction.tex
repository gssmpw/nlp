\section{Introduction}
With today's explosive growth in Machine Learning (ML), all businesses are collecting personal data more frequently than ever~\cite{astaple3}.
As such, there is an increasing demand of privacy preservation during model training and deployment~\cite{astaple1, astaple2, astaple4, astaple5}.
However, it is well known that by exploiting the confidence score of neural networks, Model Inversion (MI) attacks can partially reconstruct the training dataset from an ML model~\cite{mi15}.
Compared with membership or property inference attacks that only infer certain membership or property information as a binary classification task~\cite{mif22sp}, an MI attack targets at the whole private information in the training dataset and reconstructs multiple private features simultaneously~\cite{misurvey22}, which poses even more severe threats than its inference counterpart.
In line with previous studies~\cite{gmi20cvpr, kedmi21iccv, ppa22icml}, we consider white-box MI attacks, where the attacker has access to the specific parameters and architecture of the victim model.

Given the inherent difficulty for MI attacks to recover every sample within a large-scale dataset, recent works~\cite{ppa22icml,mirror22ndss} have lowered down their goal to recover one or few representatives in each class label. This trend was first motivated in the field of face recognition~\cite{faceevolve21}, where each label corresponds to the identity of a person. Since these label representatives can reveal the general appearance of a person, they arise as a significant concern in recent studies~\cite{vmi21nips,rethink23cvpr}. However, the potential for privacy compromise varies across applications. In some domains beyond facial recognition, label representatives may convey little or even no private information, which in turn weakens such threats. For instance, in disease diagnosis, each label corresponds to a well-known disease, so label representatives merely encapsulate public knowledge about the conditions of this disease. However, MI attacks may still threaten {\bf sample-level} privacy by reconstructing individual samples.

To address the imperative to unearth and safeguard sample-level privacy, this study critiques existing evaluation metrics of MI by first demonstrating their inadequacy in fully capturing the multifaceted characteristics of MI attacks.
Furthermore, metrics based on distributional similarity are sensitive to the distribution of reconstructed samples, so that their evaluation results can be easily fooled by controlling the sample distribution (\emph{e.g.}, producing redundant samples or approximating the distribution with fewer samples).
To overcome these limitations, we introduce a novel metric, namely \textit{Diversity and Distance Compostie Score} (DDCS), that assesses the integrity of sample-level privacy in the context of MI attacks.
In essence, DDCS uniquely evaluates the extent of reconstruction by the attack for each sample in the training dataset, so that it can comprehensively incorporate diverse attributes (\emph{e.g.}, distance, coverage and distributional similarity) to indicate a good MI attack and is robust against sample distribution manipulations.
From the defense's perspective, DDCS quantifies the susceptibility of individual samples to MI attacks by assigning each with a reconstruction distance, enabling the identification and protection of vulnerable samples in an unsupervised manner.

Leveraging Diversity and Distance Composite Score (DDCS) to evaluate the success of reconstruction, we observe that many samples in the training dataset remain un-reconstructable by even the most advanced MI attacks~\cite{ppa22icml}.
This observation coincides with the limited diversity and coverage of current MI methodologies, mainly because prior metrics focus primarily on label-level privacy.
In response, we introduce a Generative Adversarial Network (GAN) augmentation framework designed to enhance both coverage and diversity of MI attacks, through mitigating the produced artifacts while reserving the advantages from the inversion-specific GAN~\cite{stylegan2_20cvpr, kedmi21iccv}.
To achieve this, our framework transfers a pre-trained GAN with entropy loss regularized with natural gradient descent~\cite{ngd}, so as to expand the attacker's generative ability and mitigate artifacts typically associated with deep GAN structures, such as those found in StyleGAN~\cite{stylegan2_20cvpr}.

To validate our methodologies, we conduct comprehensive experiments on face recognition and dog breed classification datasets under various model architectures and MI algorithms. The results affirm the comprehensiveness of DDCS, underscoring its utility in evaluating sample-level privacy and robustness against manipulations of sample distributions. Further, the experiments substantiate the efficacy of our GAN augmentation framework in enhancing the performance of leading MI attacks, as evidenced by improvements in Fr\'echet Inception Distance, coverage, and DDCS metrics. Additionally, we also conduct a series of ablation studies to scrutinize the influence of different experimental parameters, ensuring a thorough understanding of our framework's dynamics and its implications for privacy-preserving ML.
To summarize, our contributions in this paper are as follows:
\begin{itemize}
	\item
	We introduce DDCS, a comprehensive evaluation metric for MI attacks with great potential for sample-level privacy analysis.
	\item
	We propose a GAN augmentation framework to improve MI attacks on DDCS while retaining the image quality.
	\item
	We conduct extensive experiments on computer vision tasks to demonstrate DDCS and verify the effectiveness our framework.
\end{itemize}

The rest of the paper is organized as follows.
We first review recent work on MI attacks.
Later, we point out the limitations of evaluation metrics and introduce DDCS.
We then propose our GAN augmentation framework and conduct extensive experiments before the conclusion. 

%The rest of the paper is organized as follows.
%In Section~\ref{related} we review recent work on MI attacks.
%We point out the limitations of evaluation metrics in Section~\ref{sec:ddcs:limitation} and introduce DDCS in Section~\ref{sec:ddcs:intro}.
%We propose our GAN augmentation framework in Section~\ref{sec:gan_aug} and conduct extensive experiments in Section~\ref{sec:exp} with the conclusion in Section~\ref{sec:conclusion}. 