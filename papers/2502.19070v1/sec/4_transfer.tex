\section{Enhancing MI Attacks over DDCS}
\label{sec:gan_aug}
Based on the $\mathcal{D}_{tar}$-oriented evaluation by DDCS, we evaluate the actual extent of privacy intrusion by attackers into $\mathcal{D}_{tar}$, as depicted in Figure~\ref{fig:ddcs_coverage}.
This figure illustrates the percentage of per-label samples in $\mathcal{D}_{tar}$ that are reconstructable by at least one counterpart in $\mathcal{D}_{rec}$, reflecting the attack's coverage.
Despite the focus of prior research on producing high-quality label representatives, we note that even the most advanced MI attacks~\cite{ppa22icml} fall short in achieving comprehensive coverage.
This revelation significantly undermines the perceived threat level of MI attacks, as the ML training party can easily identify these samples using DDCS and protect them.
This insight underscores the need for us to enhance the generative capabilities of attackers in existing MI attack methodologies.

\begin{figure}[htb]%[htbp]
	\centering
 %\vspace{-0.1in}
	%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=0.9\linewidth]{figs/ddcs_coverage.pdf}
 \vspace{-0.1in}
	\caption{Visualization of DDCS indicating the proportion of samples for the first 100 labels of VGG16BN-UMDFaces that are matched to $\mathcal{D}_{rec}$ with three attacks (PPA, HLoss and Ours). As explained in the section of DDCS, a target sample $x^i_{tar}$ will be matched if its set $S^i_{tar}$ is non-empty after the execution of Algorithm~\ref{alg:ddcs}.}
	\label{fig:ddcs_coverage}
 %\vspace{-0.1in}
\end{figure}

\subsection{Difficulties on Training with Entropy Loss}
Enhancing the generative capacity of a GAN for MI attacks involves refining the training methodology of the generative network, allowing the generator $G$ to leverage knowledge from the victim model $V$. To facilitate more effective utilization of $V$'s knowledge by the attacker, KEDMI~\cite{kedmi21iccv} addresses this by incorporating an additional entropy loss $\mathcal{L}_{\text{adv}}$, into the standard DCGAN training regimen of $G$.
$\mathcal{L}_{\text{adv}}$ is calculated based on the information entropy of $V$'s confidence score for a set of generated images $G(z)$.
Minimizing $\mathcal{L}_{\text{adv}}$ aims to enhance the similarity between $G(z)$ and the private training datasets of $V$.
However, this approach introduces two notable challenges:

\noindent
\textbf{P1.}\quad
The addition of $\mathcal{L}_{\text{adv}}$ to the training deviates from the GAN's original objective of maximizing image quality, consequently compromising it.

\noindent
\textbf{P2.}\quad Training a GAN from the scratch with $\mathcal{L}_{\text{adv}}$ is redundant under the assumption of publicly available pre-trained GANs.

Compared to P2, P1 poses a more significant challenge, as evidenced by recent findings~\cite{ppa22icml} where the KEDMI approach underperforms traditional methods when integrated with StyleGAN.
We attribute P1 to the optimization of entropy loss under a loosely enforced image quality constraint, which inadvertently leads to the generation of artifacts rather than the intended private information of $V$.
This issue is exemplified in Figures~\ref{fig:transfer_quality} (a) and (b), where, despite the intention for entropy loss to enhance $V$'s confidence in the generated images, it instead prompts $G$ to produce artifacts rather than accurate representations of target dataset $\mathcal{D}_{tar}$.
In this figure, FID assesses image quality, different from its use in MI attacks where it evaluates attack performance.

\begin{figure}[htb]%[htbp]
	\centering
	%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=0.95\linewidth]{figs/transfer_quality.pdf}
	\caption{Snapshots and image quality of generated images for three different approaches. Image quality, evaluated by FIDs, are calculated with the same random seed and training configurations. Snapshots are generated using the same and fixed latent codes.}
	\label{fig:transfer_quality}
 \vspace{-0.1in}
\end{figure}

\subsection{Natural Gradient for Entropy Loss}
\label{sec:gan_aug:ngd}
Our methodology for refining entropy loss draws upon the manifold hypothesis, which states that the distribution of natural images, considered as a high-dimensional manifold, is representable through latent codes on a lower-dimensional manifold~\cite{ml16goodfellow,rogan18cvpr}.
In other words, artifacts caused by entropy loss are interpreted as the result of generated images deviating from their native manifold.
Therefore, we propose to mitigate the emergence of artifacts by maintaining the generated images within their original manifold throughout the training of $\mathcal{L}_{\text{adv}}$-assisted GAN.

To tackle the above P1, we incorporate Natural Gradient Descent (NGD)~\cite{ngd} into our methodology, aiming to preserve the manifold structure of generated images while training with entropy loss.
NGD, an approximate second-order optimization technique, is conceptualized as the optimization on the Riemannian manifold~\cite{ro}.
Its implementation relies on the inverse of Fisher Information Matrix (FIM), denoted as $\mathcal{F}^{-1}$, an intrinsic distance metric that facilitates the preservation of variable on the manifold through successive optimization steps.
To approximate $\mathcal{F}$, \cite{rogan21iclr} suggests utilizing the Hessian of a squared LPIPS distance function $H_{G(z)}(d^2(G(z),G(z_0)))$ as the metric tensor for the manifold of deep generative image models.
This insight underpins our proposed NGD-based strategy for optimizing entropy loss:
\begin{equation}
	P(\frac{\partial \mathcal{L}_{\text{adv}}}{\partial G}) = H^{-1}_{G(z)}(d^2(G(z),G(z_0))) \frac{\partial \mathcal{L}_{\text{adv}}}{\partial G(z)} \cdot \frac{\partial G(z)}{\partial G}
	\label{eq:ngd}
\end{equation}
In each optimization step, we project the original gradient $\frac{\partial \mathcal{L}_{\text{adv}}}{\partial G}$ onto its natural gradient $P(\frac{\partial \mathcal{L}_{\text{adv}}}{\partial G})$ using the inverse Hessian matrix of the squared distance function $H^{-1}_{G(z)}(d^2(G(z),G(z_0)))$ for a given batch of generated images $G(z)$ and their entropy loss $\mathcal{L}_{\text{adv}}$.
%The detailed derivation of equation~\ref{eq:ngd} is in Appendix A.
It is worth noting that alternative methodologies for specifying the metric tensor of the image manifold, such as employing the Jacobian of the generator $G$~\cite{rogan18cvpr}, are also viable within this framework.

Figure~\ref{fig:transfer_quality} (c) illustrates the efficacy of Equation~\ref{eq:ngd} in artifact mitigation and improvement of image quality, where FID is reduced from 33.73 to 10.98 and the artifacts are enhanced with realistic features.

To reduce the computational cost in P2, we adopt a transfer learning approach for our training regimen.
Utilizing a pre-trained GAN, we apply our training framework on an auxiliary dataset for a limited number of epochs.
In line with~\cite{rogan21iclr}, we employ the Hessian Vector Product (HVP) to efficiently approximate the Hessian, pre-compute a batch of HVPs, and then use them throughout the training process.
Notably, the image domains produced by a GAN trained with entropy loss diverge from those generated by a conventional (vanilla) GAN, leading to a restricted and less diverse $\mathcal{D}_{rec}$ when using a single GAN model~\cite{ppa22icml,vmi21nips}.
To counteract this limitation, we propose to augment a vanilla GAN with an $\mathcal{L}_{\text{adv}}$-enhanced GAN for a more diverse $\mathcal{D}_{rec}$.

%In the Appendix, we provide a comprehensive overview of the proposed algorithm.
To summarize, given a pre-trained GAN and victim model, we calculate the standard StyleGAN training loss as well as the entropy loss at each training step.
The gradients from the loss of standard training and entropy loss will be combined to update the generator $G$, while the gradient from the entropy loss will first be projected using Equation~\ref{eq:ngd} before applying gradient descent.
Finally, the optimized GAN, trained with this methodology, will produce the reconstructed samples following the standard MI recovery strategy.