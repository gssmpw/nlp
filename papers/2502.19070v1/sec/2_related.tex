\section{Related Work}
\label{related}
\textbf{MI attacks with DCGAN.}\quad MI attacks are broadly classified into black-box and white-box categories based on the attacker's access to the target neural network's parameters.
Initial white-box MI attacks~\cite{mi15} lacked stringent optimization constraints, leading to the generation of blurred and unrealistic samples.
To enhance sample realism, \cite{gmi20cvpr} integrated DCGAN into the MI attack framework by optimizing the latent codes of generated samples.
The use of explainable artificial intelligence tools in MI attacks was explored by \cite{xai21iccv}, while KEDMI~\cite{kedmi21iccv} advanced the field by developing an inversion-specific DCGAN featuring a multi-task discriminator and entropy loss.
Further study~\cite{rethink23cvpr} suggests improved optimization objective and model augmentation to mitigate the sub-optimum and overfitting problem.

\noindent
\textbf{MI attacks with StyleGAN.}\quad Our work primarily investigates MI inversions utilizing StyleGAN~\cite{stylegan2_20cvpr}, recognized for setting the benchmark in generating image priors for MI attacks.
\cite{vmi21nips} approached MI attacks through a probabilistic lens, employing StyleGAN to elevate image quality.
The study by \cite{mirror22ndss} introduced heuristic improvements for StyleGAN-based MI attacks in both white-box and black-box scenarios.
To overcome challenges like vanishing gradients and distributional shifts, the Plug \& Play Attack~\cite{ppa22icml} was proposed, incorporating Poincar\'e loss and a transformation-based selection mechanism.
Enhancements to this method were later made by \cite{mi23mm}, leveraging historically learned knowledge.

\noindent
\textbf{Black-box MI attacks.}\quad In the context of black-box MI attacks, where the attacker lacks access to the target model's parameters, \cite{blackbox19ccs} developed a surrogate neural network to invert the undisclosed victim model. The introduction of boundary repulsion by \cite{blackbox22cvpr} enabled MI attacks to function using only the label outputs of the victim model. Further strengthening of black-box MI attacks was achieved by \cite{blackbox23tdsc} through the integration of semantic loss regularization and adversarial example injection. The study by \cite{mi23iccvw} addressed the challenge of attacking black-box face recognition models by harnessing the stochastic properties of the denoising diffusion process from diffusion models.

\noindent
\textbf{Gradient inversion attacks.}\quad Gradient Inversion (GI)~\cite{wr1} is fundamentally different from MI attacks, where GI is concentrated on those scenarios in federated learning requiring gradient information. Notably, recent works on GI~\cite{wr2, wr3, wr4, wr6} still inherit the same evaluation metrics from previous MI works, which makes our work unique. Specifically, \cite{wr1} relaxes the strong assumption about the GI attacks and proposes a more practical scenario. Other works~\cite{wr2, wr3, wr4, wr6} propose GI attacks in FL using DCGAN and BigGAN and are assessed with basic accuracy and distance metrics as the evaluation metrics.