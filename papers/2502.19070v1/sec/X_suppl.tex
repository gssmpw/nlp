%\newpage
\appendix
%\clearpage
%\setcounter{page}{1}
%\maketitlesupplementary

% Classification accuracies of victim models

\section*{Appendix}

\section{Related Work}
\label{related}
\textbf{MI attacks with DCGAN.}\quad MI attacks are broadly classified into black-box and white-box categories based on the attacker's access to the target neural network's parameters.
Initial white-box MI attacks~\cite{mi15} lacked stringent optimization constraints, leading to the generation of blurred and unrealistic samples.
To enhance sample realism, \cite{gmi20cvpr} integrated DCGAN into the MI attack framework by optimizing the latent codes of generated samples.
The use of explainable artificial intelligence tools in MI attacks was explored by \cite{xai21iccv}, while KEDMI~\cite{kedmi21iccv} advanced the field by developing an inversion-specific DCGAN featuring a multi-task discriminator and entropy loss.
Further study~\cite{rethink23cvpr} suggests improved optimization objective and model augmentation to mitigate the sub-optimum and overfitting problem.

\noindent
\textbf{MI attacks with StyleGAN.}\quad Our work primarily investigates MI inversions utilizing StyleGAN~\cite{stylegan2_20cvpr}, recognized for setting the benchmark in generating image priors for MI attacks.
\cite{vmi21nips} approached MI attacks through a probabilistic lens, employing StyleGAN to elevate image quality.
The study by \cite{mirror22ndss} introduced heuristic improvements for StyleGAN-based MI attacks in both white-box and black-box scenarios.
To overcome challenges like vanishing gradients and distributional shifts, the Plug \& Play Attack~\cite{ppa22icml} was proposed, incorporating Poincar\'e loss and a transformation-based selection mechanism.
Enhancements to this method were later made by \cite{mi23mm}, leveraging historically learned knowledge.

\noindent
\textbf{Black-box MI attacks.}\quad In the context of black-box MI attacks, where the attacker lacks access to the target model's parameters, \cite{blackbox19ccs} developed a surrogate neural network to invert the undisclosed victim model. The introduction of boundary repulsion by \cite{blackbox22cvpr} enabled MI attacks to function using only the label outputs of the victim model. Further strengthening of black-box MI attacks was achieved by \cite{blackbox23tdsc} through the integration of semantic loss regularization and adversarial example injection. The study by \cite{mi23iccvw} addressed the challenge of attacking black-box face recognition models by harnessing the stochastic properties of the denoising diffusion process from diffusion models.

\noindent
\textbf{Gradient inversion attacks.}\quad Gradient Inversion (GI)~\cite{wr1} is fundamentally different from MI attacks, where GI is concentrated on those scenarios in federated learning requiring gradient information. Notably, recent works on GI~\cite{wr2, wr3, wr4, wr6} still inherit the same evaluation metrics from previous MI works, which makes our work unique. Specifically, \cite{wr1} relaxes the strong assumption about the GI attacks and proposes a more practical scenario. Other works~\cite{wr2, wr3, wr4, wr6} propose GI attacks in FL using DCGAN and BigGAN and are assessed with basic accuracy and distance metrics as the evaluation metrics.

\section{Differences between DDCS and KNN-dist}
One might question the differences between DDCS and K Nearest Neighbor distance (KNN-dist) since they both find closet pairwise match in a sample-level manner.
However, KNN-dist differs significantly from DDCS in evaluative approach. KNN-dist finds the closest match in target dataset for each reconstructed sample, while DDCS has two unique advantages. First, DDCS allows certain samples in target dataset (i.e., hard-to-attack samples shown in Figure~\ref{fig:eye_test}) to remain unmatched (Line~\ref{alg:rec_dist} in Algorithm~\ref{alg:ddcs}), providing a more accurate privacy leakage assessment by recognizing sample-specific challenges~\cite{mif22sp}. Second, KNN-dist averages all reconstructed samples, ignoring sample diversity, which may incentivize attacks to generate a single high-quality sample. In contrast, DDCS averages over all target samples, thus encouraging MI attacks to improve both diversity and accuracy for higher coverage. 

\section{NGD-based Approach}
Denote the Euclidean gradient for the entropy loss $\mathcal{L}_{\text{adv}}$ regarding the parameters of generator $G$ as $\frac{\partial \mathcal{L}_{\text{adv}}}{\partial G}$ and the projected gradient as $P(\cdot)$.
Since the enropty loss is obtained by feeding the generated images $G(z)$ to the victim model with the latent codes $z$, we can expand $\frac{\partial \mathcal{L}_{\text{adv}}}{\partial G}$ with the chain rule:
\begin{equation}
	\frac{\partial \mathcal{L}_{\text{adv}}}{\partial G} = \frac{\partial \mathcal{L}_{\text{adv}}}{\partial G(z)} \cdot
	\frac{\partial G(z)}{\partial G}
	\label{eq:chain_rule}
\end{equation}

$\frac{\partial \mathcal{L}_{\text{adv}}}{\partial G(z)}$ denotes the gradient of $\mathcal{L}_{\text{adv}}$ w.r.t. the generated images $G(z)$, which will be ill-conditioned under the soft constraint during the optimization.
Such unconstrained update direction will bring artifacts to $G(z)$ and thus causes them to deviate from the original manifold.

Recall the natual gradient~\cite{ngd} of $\frac{\partial G(z)}{\partial G}$, represented as $\Delta G(z)$, is computed as:
\begin{equation}
	\Delta G(z) = F^{-1} \frac{\partial \mathcal{L}_{\text{adv}}}{\partial G(z)},
	\label{eq:ngd_def}
\end{equation}
where $F^{-1}$ denotes the inverse of FIM, which is approximated as the inverse of metric tensor~\cite{rogan18cvpr}:
\begin{equation}
	F^{-1} \approx H^{-1}_{G(z)}(d^2(G(z),G(z_0)))
	\label{eq:ngd_approx} = v\lambda^{-1}v^T,
\end{equation}
where $G(z_0)$ is another batch of generated images with latent codes $z_0$ that are close to $z$. $H_{G(z)}(d^2(G(z),G(z_0)))$ is computed by taking the second order partial derivative of squared LPIPS distance $d^2(G(z),G(z_0))$ w.r.t. $G(z)$. The Hessian can then be decomposed as $v\lambda v^T$, using the Hessian's eigenvalue $\lambda$ and eigenvector $v$.

Summarizing Equations~\ref{eq:chain_rule},~\ref{eq:ngd_def} and~\ref{eq:ngd_approx}, the projected gradient of $\mathcal{L}_{\text{adv}}$ can be expressed as:
\begin{equation}
\begin{split}
	P(\frac{\partial \mathcal{L}_{\text{adv}}}{\partial G})=F^{-1} \frac{\partial \mathcal{L}_{\text{adv}}}{\partial G(z)} \cdot \frac{\partial G(z)}{\partial G} \\
	\approx H^{-1}_{G(z)}(d^2(G(z),G(z_0))) \frac{\partial \mathcal{L}_{\text{adv}}}{\partial G(z)} \cdot \frac{\partial G(z)}{\partial G}
\end{split}
\end{equation}

\section{GAN Augmentation Algorithm}
Our framework leverages DDCS's insights, showing existing MI attacksâ€™ low coverage of the target dataset. This inspired expanding sample diversity by utilizing the victim model in fine-tuning the generator, forming the basis of our transfer framework.
Figure~\ref{fig:overview_gan_augmentation} illustrates the overview of our framework. In our attack, we assume the attacker has access to some public AI models (\emph{e.g.}, from open source or model marketplace) so that he/she could download the pre-trained generative network. Given a victim model with its parameters, our framework will first perform Hessian Vector Product (HVP) to obtain an approximated Hessian. In each epoch, the approximated Hessian will help to project ill-conditioned gradient into its natural gradient. As a result, the fine-tuned StyleGAN can generate more diverse samples related to the private information of the victim model while preserving the image quality. Such approach increases per-epoch runtime from 285 to 829 seconds, a manageable overhead with fewer than 20 epochs.

\begin{figure}[htb]
	\centering
	\includegraphics[width=1.0\linewidth]{figs/gan_augmentation.jpg}
	\caption{Overview of our transfer learning attack framework.}
	\label{fig:overview_gan_augmentation}
\end{figure}

Algorithm~\ref{alg:gan_aug} illustrates the complete process of our GAN augmentation approach, where the subscript $_{\text{van}}$ denotes the vanilla transfer and $_{\text{adv}}$ denotes the transfer with entropy loss.
For every step, the generator $G$ will produce a batch of fake images $G(z)$ (Line~\ref{alg2:batch_fake}) to perform a normal training (Line~\ref{alg2:normal_train}) and obtain the entropy loss (Line~\ref{alg2:entropy}).
We update $G$ with the projected gradient of $\mathcal{L}_{\text{adv}}$ using Equation~\ref{eq:ngd} (Line~\ref{alg2:ngd}).

\begin{algorithm}[htb]
	\caption{GAN Augmentation}
	\label{alg:gan_aug}
	\textbf{Input}: Pre-trained generator $G$ and discriminator $D$, auxiliary dataset $\mathcal{D}_{aux}$, victim model $V$ \\
	\textbf{Output}: Recovered dataset $\mathcal{D}_{rec}$
	\begin{algorithmic}[1]
	\STATE Generate $\mathcal{D}_{\text{vanilla}}$ with $G$ and $V$
	
	\FOR {each iteration step}
    \STATE Generate a batch of images $G(z)$ with random latent code $z$ \label{alg2:batch_fake}
	\STATE Update $G$ and $D$ with $\mathcal{D}_{aux}$ and $G(z)$ \label{alg2:normal_train}
	\STATE Calculate adversarial loss $\mathcal{L}_{\text{adv}}$ with $V$ and $G(z)$ \label{alg2:entropy}
	\STATE Update $G$ with gradient $H^{-1}_{G(z)}(d^2(G(z),G(z_0))) \frac{\partial \mathcal{L}_{\text{adv}}}{\partial G(z)} \cdot \frac{\partial G(z)}{\partial G}$ \label{alg2:ngd}
	\ENDFOR
	
	\STATE Generate $\mathcal{D}_{\text{adv}}$ with $G$ and $V$
	
	\STATE $\mathcal{D}_{rec} \gets \{\mathcal{D}_{\text{vanilla}}, \mathcal{D}_{\text{adv}}\}$
 \end{algorithmic}
\end{algorithm}

%[TODO: I think a summary of your total augmenting mechanism using pseudo-code will be nice here to conclude this section.]

\section{Improvements of DDCS by Producing More $\mathcal{D}_{rec}$}
Intuitively, generating more diverse samples by a generative network can result in more samples from $\mathcal{D}_{tar}$ being reconstructed by $\mathcal{D}_{rec}$.
As shown in Figure~\ref{exp:ms_iq} (a), as the number of samples generated by the attacker on each label increases, both DDCS$_{\text{avg}}$ and DDCS$_{\text{best}}$ demonstrate a significant improvement at the beginning and the improvement tends to decelerate when the number of samples are great enough.
This indicates that DDCS will increase with larger sample size per label of $\mathcal{D}_{rec}$, especially when the sample size is originally small.
Nevertheless, such relationship is not linear, which in turn suggests that more redundant samples are also produced. As such, to further improve the strength of MI attacks, we should turn to other approaches such as GAN augmentation.

\begin{figure}[htb]%[htbp]
	\centering
	\begin{minipage}[t]{0.495\linewidth}
		\includegraphics[width=\linewidth]{figs/more_samples.pdf}
		%\captionsetup{font={scriptsize}}
		%\caption*{(a)}
	\end{minipage}
	\begin{minipage}[t]{0.495\linewidth}
		\includegraphics[width=\linewidth]{figs/image_quality.pdf}
		%\captionsetup{font={scriptsize}}
		%\caption*{(b)}
	\end{minipage}
	\caption{(a). left sub-figure indicates the corresponding DDCS$_\text{avg}$ and DDCS$_\text{best}$ when the attacker uses a different number of generated samples for each label. (b). right sub-figure shows the change of FID during the training of StyleGAN with different training settings. We use the VGG16BN-UMDFaces setting for both figures.}
	\label{exp:ms_iq}
\end{figure}

\section{Image Quality of Training with Adversarial Loss}
In Figure~\ref{exp:ms_iq} (b), we measure FID, which in this section represents the naturalness of generated images by comparing the generated images with real images. The red dash line and the black solid line denote entropy loss with and without our training approach in Section~\ref{sec:gan_aug:ngd}, respectively. Although there is a noticeable improvement of entropy loss on DCGAN~\cite{kedmi21iccv,rethink23cvpr}, the entropy loss continuously damages the naturalness of images on deeper generative networks, \emph{i.e.}, StyleGAN.
On the other hand, our approach preserves the images into the manifold and ensures the convergence of StyleGAN when training with adversarial loss.
Figure~\ref{fig:transfer_quality} illustrates the visual effect, in which our approach removes most visible artifacts caused by the entropy loss.
%[TODO: any observation here?]

\section{Results for Stanford Dogs}
\begin{table*}[htb]
	\centering
	\begin{tabular}{ | c | c | c c c c c | c c | }
		\hline
		Model & Attack & $\uparrow$ Acc@1 & $\uparrow$ Acc@5 & $\downarrow$ Dist & $\downarrow$ FID & $\uparrow$ Coverage & $\uparrow$ DDCS$_{\text{avg}}$ & $\uparrow$ DDCS$_{\text{best}}$ \\
		\hline
		\multirow{3}{*}{VGG16BN} & HLoss & 17.93\% & 42.32\% & 11984.44 & 152.08 & 0.0033 & 0.1434 & 0.1450 \\
        & PPA & \bf 45.70\% & \bf 77.01\% & \bf 8616.43 & 57.63 & 0.0569 & 0.1777 & 0.1797 \\
		& Ours & 44.23\% & 76.00\% & 8766.75 & \bf 57.21 & \bf 0.0782 & \bf 0.2787 & \bf 0.2830 \\
		\hline
		\multirow{3}{*}{ResNet50} & HLoss & 13.67\% & 41.80\% & 11456.74 & 177.82 & 0.0014 & 0.1512 & 0.1530 \\
        & PPA & \bf 52.64\% & \bf 82.00\% & \bf 8139.98 & 53.67 & 0.0650 & 0.1663 & 0.1682 \\
		& Ours & 52.17\% & 81.90\% & 8194.13 & \bf 51.94 & \bf 0.0930 & \bf 0.2604 & \bf 0.2644 \\
		\hline
		\multirow{3}{*}{IR50-SE} & HLoss & 9.37\% & 23.22\% & 12325.54 & 199.59 & 0.0006 & 0.1339 & 0.1354 \\
        & PPA & \bf 38.27\% & \bf 71.73\% & \bf 8862.13 & 58.10 & 0.0499 & 0.1585 & 0.1603 \\
		& Ours & 37.88\% & 71.11\% & 8907.61 & \bf 55.46 & \bf 0.0741 & \bf 0.2537 & \bf 0.2573 \\
		\hline
		\multirow{3}{*}{AlexNet} & HLoss & 13.80\% & 35.80\% & 11655.26 & 135.11 & 0.0047 & 0.1464 & 0.1480 \\
        & PPA & 28.12\% & 54.25\% & 9950.77 & 60.80 & 0.0389 & 0.1613 & 0.1632 \\
		& Ours & \bf 28.17\% & \bf 54.93\% & \bf 9938.49 & \bf 57.17 & \bf 0.0601 & \bf 0.2513 & \bf 0.2552 \\
		\hline
	\end{tabular}
	\caption{Comparison on Stanford Dogs dataset between our approach (Ours), HLoss and PPA accross different metrics. $\uparrow$ and $\downarrow$ denote the higher the better and the lower the better, respectively. The best values for each metric and each model are in bold.}
	\label{exp:result:stanforddogs}
 %\vspace{-0.2in}
\end{table*}

Figure~\ref{fig:eye_test_dogs} represents several target samples and their reconstruction pairs.
Since dog breed classification has a lower MI attack success rate than face recognition in terms of accuracy, FID and coverage, the reconstruction distances between $\mathcal{D}_{tar}$ and $\mathcal{D}_{rec}$ are also larger than that in Figure~\ref{fig:eye_test}.
However, many important private details are still captured by the attack, such as the dog's posture and the general scene it is in.
Future work could focus on mining the privacy of tasks beyond face recognition including dog breed classification, and reconstructing samples with more complex features as shown in the right most column in Figures~\ref{fig:eye_test} and~\ref{fig:eye_test_dogs}.

\begin{figure}[htbp]%[htbp]
	\centering
	%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=0.95\linewidth]{figs/exp_eyetest_dogs.png}
	\caption{Visualization of reconstruction pairs between target and reconstructed samples for ResNet50-StanfordDogs with reconstruction distance attached on their bottom right corner. Samples on the right most column have no reconstruction pairs.}
	\label{fig:eye_test_dogs}
\end{figure}

\section{Implementation Details}
In selecting the distance function $d(\cdot,\cdot)$ for DDCS in Algorithm~\ref{alg:ddcs}, we adopt Learned Perceptual Image Patch Similarity (LPIPS)~\cite{lpips18}. Other distance functions (\emph{e.g.}, KNN distance~\cite{kedmi21iccv}) assess samples from a distributional perspective. In contrast, LPIPS judges the semantic differences between individual samples, aligning with our focus on sample-level privacy.
Additionally, to mitigate the dependency on pre-trained networks, DDCS can incorporate a simplified version of the LPIPS metric~\cite{lpips21}.
Since we target at a general ML scenario where sample-level privacy of $\mathcal{D}_{tar}$ are much more important than label-level privacy, we allow label intersection between the training datasets and the auxiliary datasets (\emph{e.g.}, the label of \textit{Airedale} might be appeared in both target and auxiliary datasets). As for training framework, we adopt Face.evoLVe~\cite{faceevolve21} to train our target models, and fork the PyTorch training framework~\footnote{https://github.com/rosinality/stylegan2-pytorch} of StyleGAN with Adaptive Discriminator Augmentation~\cite{stylegan2ada_20nips} with default settings.

Regarding the dataset selection, we primarily focus on face datasets in this paper, as they contain numerous sensitive attributes and effectively illustrate the risks associated with MI attacks. Datasets like the disease or dog breed dataset are less suited to sample-level privacy demonstration, as individual images contain limited private information. Additionally, most existing pre-trained StyleGAN models are trained on face datasets. Since we make minimal assumptions about the pre-trained generator, we use the default settings from the official StyleGAN repository~\footnote{https://github.com/NVlabs/stylegan2} and fine-tune the model with human face and dog breed datasets.
Furthermore, StyleGAN is pre-trained on Flickr-Faces-HQ, whereas we utilize UMDFaces for training the victim model to minimize sample overlap between the target and pre-training datasets. UMDFaces captures varied real-world conditions~\cite{umdfaces}, while Flickr-Faces-HQ spans diverse attributes like age, ethnicity, and facial features~\cite{stylegan2ada_20nips}. Given these differences, achieving meaningful sample overlap is challenging. Even if overlap occurs, the victim model must first memorize these samples, as MI attacks focus on maximizing confidence in the victim model. Consequently, any sample memorized solely by StyleGAN, without memorization by the victim model, is unlikely.

\subsection{Comparison Results of MI Attacks}
These results in Tables~\ref{exp:result:umdfaces} and~\ref{exp:result:stanforddogs} suggest that our proposed method is more effective in reconstructing images and inverting the target models compared to other baseline MI attacks.
In the previous metrics, our attack achieve significant improvements on coverage and FID. These improvements are accounted by DDCS as well, so that our attack also improve DDCS very effectively.
Furthermore, DDCS of HLoss decreases significantly as all previous metrics are deteriorated in such attack, indicating the improvement of our approach on removing the artifacts introduced by HLoss in StyleGAN situation.
Notably, the results reflect that DDCS manages to evaluate multiple characteristics comprehensively to identify good MI attacks.
For example, in the case of VGG16BN-UMDFaces, our approach improves the coverage and FID to 0.3907 and 47.64 respectively, while maintaining the top-1 accuracy, top-5 accuracy and feature distance to 96.67\%, 99.86\% and 7065.28. As a result, DDCS$_{\text{avg}}$ and DDCS$_{\text{best}}$ of our approach are improved to 0.6772 and 0.7114.
In the same condition, PPA achieves inferior FID and coverage of 48.35 and 0.3213, consequently with lower DDCS$_{\text{avg}}$ and DDCS$_{\text{best}}$ of 0.4162 and 0.4308.
Despite the notable performance of the baseline attacks in previous metrics, their DDCS values have considerable room for improvement.
Recall that, compared to previous metrics, DDCS provides a more comprehensive evaluation for assessing MI attacks from the perspective of an ideal attack model, the lower performance of prior MI attacks on DDCS reflects their inherent limitations in targeting sample-level privacy. Thus, from the experimental results, we could observe that previous attacks underperform due to fundamental weaknesses not captured by existing metrics. Additionally, given DDCS's broader evaluation capacity, we encourage future MI attacks and defenses to strive for improved performance under DDCS to more accurately assess privacy risks in machine learning.

In top-1 accuracy, top-5 accuracy and feature distance, though these metrics have been shown defective on evaluating MI attacks in our paper, our method still achieves comparative or equivalent accuracy to the baseline models.
Unlike previous MI works on improving image naturalness, our framework enhances reconstruction coverage and preserves image quality, thus Acc and Dist fall short of quantifying the private samples our approach recovers. However, DDCS, FID and coverage successfully capture our framework's improvements, reflecting a meaningful advancement in MI attacks beyond traditional metrics.
These observations suggest that our approach mitigates the drawbacks from HLoss, so that the attack does not compromise the accuracy and feature distance of reconstructed samples.
As for distance metric on distributional similarity (\emph{i.e.}, FID and coverage), our method consistently achieves a lower FID than the PPA and HLoss, demonstrating higher distributional similarity between the recovered dataset and the target dataset.
Since both our method and baselines generate many redundant samples, we suggest one can implement supervised removal of redundant samples to reduce FID in the future.
Besides, the proposed method consistently achieves higher coverage values compared to the baselines, signifying better reconstruction on the target dataset and more realistic generative samples.

%Apart from the main comparisons, some side observations can be inferred from the tables.
%First, our method preserves a high consistency across different models and datasets.
%For all models (VGG16BN, ResNet50, IR50-SE and AlexNet) and all datasets, our method achieves similar improvement over the baseline in terms of most metrics, suggesting its robustness across various architectures and application scenarios.

Furthermore, we notice that in the results of Stanford Dogs, the overall performance of all attacks is lower than that in UMDFaces.
This is because compared to UMDFaces where faces are all well aligned and cropped, the feature distribution of Stanford Dogs dataset is much more intricate (\emph{e.g.}, dogs in the pictures are in different poses, different positions, and on different backgrounds).
We also observe that it is more difficult to attack AlexNet victim model than that of other architectures.
This is because compared to other more advanced architectures, AlexNet does not generalize well to the testing dataset and might remember some unnatural and trivial pixel-wise features from the training dataset.
Since the memorization of these victim models is poor, it in turn impacts on the success of MI attacks, which coincides with the findings of recent works~\cite{mldoctor22usenix,gmi20cvpr} that difficult training tasks can undermine the performance of MI attacks.

%We also find that the deterioration of adversarial loss without natural gradient is worse on complex tasks than simple tasks (\emph{i.e.}, dog breed classification on Stanford Dogs).
%Compared with UMDFaces, HLoss in Stanford Dogs is even more damaging to MI attacks, which can be reflected in FID, density and coverage of Table~\ref{exp:result:stanforddogs}.
%However, our method is still able to achieve a decent improvement in such cases, thanks to the protection of image quality by NGD.

\section{Redundant Samples from MI Attacks}
\begin{figure}[t]%[htbp]
	\centering
	%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=0.8\linewidth]{figs/redundant_samples.pdf}
	\caption{Several redundant samples from a target person generated with VGG16BN-UMDFaces setting.}
	%\vfill
	\label{fig:redundant_samples}
\end{figure}
Existing MI attacks optimize each reconstructed sample independently, so these samples can easily fall into similar local optima, resulting in the emergence of redundant samples.
As illustrated in Figure~\ref{fig:redundant_samples}, for each reconstructed sample, there will be a varying number of redundant samples.
These samples are very similar to each other, with only minor differences in details.
These repeated samples offer very limited effectiveness in further mining the privacy of $\mathcal{D}_{tar}$, but will instead change the distribution of $\mathcal{D}_{rec}$.


