\section{DDCS: A New Evaluation Metric for MI}
Prior studies on MI attacks have employed a range of evaluation metrics, each with its own limitations.
Notably, many of these metrics overlook critical attributes of the reconstructed data, such as diversity, and are susceptible to alteration in data distribution caused by non-essential factors, such as the presence of redundant samples.
This issue is prevalent across MI attack evaluations.
In the rest of this section, we delve into the impact of these limitations on the thorough assessment of MI attacks and introduce a novel metric designed to overcome these challenges.

\subsection{Limitations of Existing Metrics}
\label{sec:ddcs:limitation}
An ideal MI attack~\cite{vmi21nips}, as the name suggests, aims to accurately restore the target training dataset $\mathcal{D}_{tar}$. As such, attacks which can successfully reconstruct every sample in $\mathcal{D}_{tar}$ are more severe than those only recover sensitive labels.
While recent studies have highlighted the risks of label-level privacy breaches in tasks like face recognition, where label representatives can disclose an individual's identity and appearance~\cite{mirror22ndss}, we argue that the reconstruction of sample-level data poses a far greater threat to privacy.
For example, in face recognition, the variety in reconstructed samples can expose more nuanced personal details, such as an individual's preferred hairstyles, accessories, and expressions~\cite{vmi21nips}.
This is particularly true in fields beyond face recognition, such as disease diagnosis and dog breed classification, where labels convey commonly known information.

The oversight of sample-level privacy may stem from the inadequacy of existing evaluation metrics to fully and precisely evaluate crucial characteristics indicative of successful MI attacks. To address this gap, we draw three fundamental traits that are essential for an ideal MI attack on sample-level privacy --- accuracy/distance, coverage, and distributional similarity. This discussion also explores the shortcomings of current evaluation methodologies in accurately assessing these critical attributes.

\noindent
\textbf{Accuracy/Distance.}\quad
Accuracy and distance both encourage MI attacks focusing on label-level privacy to generates label representatives.
Recall that the objective function of MI is to maximize the sample's output confidence given the victim model~\cite{mi15}.
Thus, the optimization of MI attacks can easily overfit to the specific structure of the victim neural network, and a successful MI attack should then be independent of the classifier architecture.
Therefore, previous works train a model with a different structure from the victim model and measure the accuracy of reconstructed dataset $\mathcal{D}_{rec}$.
In this context, high classification accuracy on the reconstructed dataset $\mathcal{D}_{rec}$ signifies a reduced distance between $\mathcal{D}_{rec}$ and the target dataset $\mathcal{D}_{tar}$.
Comparable to accuracy, feature distance is another metric frequently employed in MI assessments, quantifying the $l_2$ distance between reconstructed and target samples within the evaluator's feature space.

\noindent
\textbf{Coverage.}\quad Coverage refers to the proportion of $\mathcal{D}_{tar}$ that are reconstructed by $\mathcal{D}_{rec}$ and can be increased by improving the diversity of $\mathcal{D}_{rec}$. While accuracy and feature distance are the primary metrics for evaluating MI attacks, they overlook the diversity of samples and fail to comprehensively assess the coverage of MI attacks.
In the toy example of Figure~\ref{fig:toy_accuracy} where $\mathcal{D}_{tar}$ only contains five images, there is a rich attack that can successfully recover all images in $\mathcal{D}_{tar}$ and a poor attack that recovers only one of them.
If all samples from the rich and poor attack are identical to some images in $\mathcal{D}_{rec}$, both attacks achieve the same average accuracy, albeit the rich attack is obviously more threatful than the poor attack.

\begin{figure}[htb]%[htbp]
	\centering
	%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=0.9\linewidth]{figs/toy_accuracy.pdf}
	\caption{When each reconstructed sample is close enough to the target dataset, average accuracy or distance fail to differentiate between the rich and poor attack.}
	\label{fig:toy_accuracy}
\end{figure}

\noindent
\textbf{Distributional Similarity.}\quad
Being aware of the limitation in accuracy and distance, recent works~\cite{kedmi21iccv,vmi21nips,ppa22icml} have incorporated distributional similarity, \emph{e.g.}, Fr\'echet Inception Distance (FID)~\cite{fid}, as an additional metric by treating $\mathcal{D}_{tar}$ and $\mathcal{D}_{rec}$ as two distinct image distributions.
Since the diversity of $\mathcal{D}_{rec}$ will affect its distribution, the problem of Figure~\ref{fig:toy_accuracy} can be alleviated with distributional similarity.
However, the distribution of images can be easily affected by factors that are trivial in privacy leakage, leading to misjudgment of MI attacks.
One example is the bias caused by redundant samples that MI attacks can easily generate, which leads to a distribution shift between $\mathcal{D}_{tar}$ and $\mathcal{D}_{rec}$, thereby causing a worse FID that is not true.
As in Figure~\ref{fig:toy_fid}, there is an attack that can restore most of the samples without any duplication (\emph{i.e.}, strong and concise attack), and an attack that can restore the same diverse samples with some duplications (\emph{i.e.}, strong but verbose attack).
The former attack can achieve a better FID, but it is premature to conclude that the former attack is more powerful than the latter, because they both successfully restore four images in $\mathcal{D}_{tar}$.
This phenomenon arises because distributions can be approximated using varying sample batches, allowing for the manipulation of metrics by altering the sample distribution. We will empirically show how FID is susceptible to such manipulations in the experiment, and therefore these factors do not fundamentally characterize a good MI attack.

\begin{figure}[htb]%[htbp]
	\centering
	%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=0.9\linewidth]{figs/toy_fid.pdf}
  \vspace{-0.1in}
	\caption{Redundant samples change the data distribution, thus affecting the FID calculation, although both attacks successfully recover four images from the target dataset.}
	\label{fig:toy_fid}
 \vspace{-0.2in}
\end{figure}

\subsection{Diversity and Distance Composite Score}
\label{sec:ddcs:intro}
We believe the core reason for the above limitations is that these metrics overly concentrate on the distribution of reconstructed samples ($\mathcal{D}_{rec}$-oriented), which leads to an incomplete assessment of privacy leakage in the target samples $\mathcal{D}_{tar}$.
This approach diverges from the objectives of ideal MI attacks, as outlined previously.
Consequently, an MI algorithm could generate high-fidelity label representatives and manipulate the distribution of $\mathcal{D}_{rec}$ to perform favorably according to these metrics.

Different from $\mathcal{D}_{rec}$-oriented metrics introduced in previous works, we truly measure the degree to which $\mathcal{D}_{tar}$ is recovered by $\mathcal{D}_{rec}$, and propose a more robust and comprehensive evaluation metric, namely \textit{Diversity and Distance Composite Score} (DDCS).
We start from the ideal MI attack, where each sample from $\mathcal{D}_{tar}$ has its one-to-one matching in $\mathcal{D}_{rec}$ (\emph{i.e.}, this attack achieves perfect diversity and distance regarding $\mathcal{D}_{tar}$), and any other attack that reduces $\mathcal{D}_{rec}$'s diversity and distance to $\mathcal{D}_{tar}$ is a suboptimal attack, which inspires us how to calculate DDCS, as shown in Algorithm~\ref{alg:ddcs}.
One worth-noting term used in DDCS is the \textit{Reconstruction Distance} $d^i_{tar}$ for every target sample $x^i_{tar} \in \mathcal{D}_{tar}$.
Specifically, given a distance metric $d(\cdot,\cdot)$, if $x^j_{rec}$'s closest target sample is $x^i_{tar}$ with the distance as $d(x^j_{rec}, x^i_{tar})$, $x^i_{tar}$ is said to be reconstructed by $x^j_{rec}$ with a reconstruction distance of $d(x^j_{rec}, x^i_{tar})$ (Line~\ref{alg:rec_dist}).
$d(x^j_{rec}, x^i_{tar})$ is then recorded into a set, denoted by $S_{tar}^{i}$ of $x^i_{tar}$ for further handling.

\begin{algorithm}[htb]
	\caption{DDCS Computation}
	\label{alg:ddcs}
    \textbf{Input}: Target dataset $\mathcal{D}_{tar}$, reconstructed dataset    $\mathcal{D}_{rec}$, constant $c$ \\
    \textbf{Output}: DDCS$_{\text{avg}}$, DDCS$_{\text{best}}$ %for $\mathcal{D}_{rec}$
	\begin{algorithmic}[1]
	\STATE Initialize $S_{tar}^i=\emptyset$ for $x^i_{tar} \in \mathcal{D}_{tar}$
	
	\FOR {$x^j_{rec} \in \mathcal{D}_{rec}$}
    \STATE Record $d_{tar}^i=d(x^i_{tar},x^j_{rec})$ into $S_{tar}^i$, where $x^{j}_{rec}$ and $x^{i}_{tar}$ achieve the smallest $d_{tar}^i$ for $x^{i}_{tar} \in \mathcal{D}_{tar}$ \label{alg:rec_dist}
	\ENDFOR
	
	\STATE Initialize DDCS$_{\text{avg}}=0$, DDCS$_{\text{best}}=0$
	
	\FOR {$x^i_{tar} \in \mathcal{D}_{tar}$}
    \STATE DDCS$_{\text{avg}} += mdf[\text{avg}(S_{tar}^i)+c]$ \label{alg:ddcs:avg}
	\STATE DDCS$_{\text{best}} += mdf[\text{min}(S_{tar}^i)+c]$ \label{alg:ddcs:best}
    \ENDFOR
	
	\STATE Normalize DDCS$_{\text{avg}}$ and DDCS$_{\text{best}}$ with $|\mathcal{D}_{tar}|$ 
 \end{algorithmic}
 \label{alg:ddcs:normalize}
\end{algorithm}

In Lines~\ref{alg:ddcs:avg} and~\ref{alg:ddcs:best}, we propose two variants of DDCS tailored to distinct scenarios.
To ensure that higher DDCS values indicate better outcomes, we employ monotonically decreasing functions, $mdf[\cdot]$, in its calculation, specifically utilizing the simple reciprocal function on $\text{avg}(S_{tar}^i)+c$ and $\text{min}(S_{tar}^i)+c$ in this paper.
DDCS$_{\text{avg}}$ quantifies the overall privacy risk of $\mathcal{D}_{tar}$ against $\mathcal{D}_{rec}$, calculated as the sum of the reciprocals of the averages for each $S^i_{tar}$, \emph{i.e.}, $\text{avg}(S_{tar}^i)$.
DDCS$_{\text{best}}$ assesses the minimum reconstruction distance across all $S^i_{tar}$, \emph{i.e.}, $\text{min}(S_{tar}^i)$, offering insight into the potential maximum effectiveness of an attack on $\mathcal{D}_{tar}$.
If $x^i_{tar}$ is not recovered by any reconstructed sample, we set their $mdf[\text{avg}(S^i_{tar}) + c]$ and $mdf[\text{min}(S^i_{tar}) + c]$ to 0.
Besides, we reserve a constance $c$ to regularize the range of DDCS when $d^i_{tar}$ is so small.
As a final note, both DDCS$_{\text{avg}}$ and DDCS$_{\text{best}}$ are normalized by the total number of samples $|\mathcal{D}_{tar}|$ in the target dataset (Line~\ref{alg:ddcs:normalize}).

DDCS mitigates those limitations discussed before from two aspects.
Firstly, this metric addresses the privacy for {\bf every} sample in the target dataset ($\mathcal{D}_{tar}$-oriented).
Thus, DDCS encourages MI attacks to focus on sample-level privacy instead of label representatives, with high robustness against distribution change of $\mathcal{D}_{rec}$ as in Figure~\ref{fig:toy_fid}.
Secondly, DDCS takes multiple important characteristics into account, so that an MI attack can win this metric by truly attacking the privacy of target samples.
For example, if an attack reaches higher coverage, more samples from $\mathcal{D}_{tar}$ will be reconstructed, increasing DDCS value and mitigating the problem in Figure~\ref{fig:toy_accuracy}.
On the other hand, when diversity remains unchanged, producing better reconstructed sample to reduce the reconstruction distance can also improve DDCS. Besides, reconstruction distance of DDCS can also be harnessed beyond attacks, such as measuring per-example vulnerability for $\mathcal{D}_{tar}$ to selectively protect those vulnerable samples, which will be discussed in detail in the experiment.
This application scenario  has been recently studied in the context of membership inference attacks~\cite{mif22sp}, but we are the first to study it in MI scenarios.




