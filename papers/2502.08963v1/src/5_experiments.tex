% \TSK{
% \input{components/fig_search}
% }
\TSK{
\input{components/table_acc_causal} 
}
\TSK{
\input{components/table_acc_forecast} 
}
In this section, we evaluate the performance of \method using both synthetic and real-world datasets.
% We answer the following questions by performing experiments with regard to \method.
We performed extensive experiments to answer the following questions.
{\setlength{\leftmargini}{17.2pt}
\begin{enumerate}
    \renewcommand{\labelenumi}{\textit{Q\arabic{enumi}.}}
    \item 
    \textit{Effectiveness}: How well does it find the \relation?
    \item 
    \textit{Accuracy}: How accurately does it discover \relation and forecast future values?
    \item 
    \textit{Scalability}: How does it scale in terms of computational time?
\end{enumerate}
}
% \subsection{Datasets \& Experimental Setup}
% \vspace{2em}
\myparaitemize{Datasets \& experimental setup}
% We used four real datasets related to epidemiology, web activity, and human movement.
% The four real datasets we used for evaluating forecasting performance follow:
% We used four real datasets below:
We used the following datasets:
% \vspace{-0.22em}
{\setlength{\leftmargini}{15pt}
\begin{itemize}
    \item
    \psynthetic:
    % its data generation process was modeled the same as structural equation model~\cite{pearl2009causality}.
    was generated based on a structural equation model~\cite{pearl2009causality}.
    % and the number of observed was set to 5. The causal adjacency matrices were generated from random graph models, Erdös-Rényi (ER)~\cite{erdos1960evolution} with edge density $0.5$.
    Details
    % for this datasets
    are provided in Appendix \ref{section:app:experiments:setting}.
    \item
    \pcovid: was obtained from Google COVID-19 Open Data~\cite{googlecovid19}
    % \footnote{\url{https://health.google.com/covid-19/open-data/}}
    %~\cite{googlehealth}
    and consists of the number of infections in Japan, the United States, China, Italy, and the Republic of South Africa, covering over 900 daily entries.
    \item
    % \pgoogletrend: was obtained from Google Trends
    % %~\cite{googletrends}
    % and contains web-search counts collected over ten years related to beer queries.
    \pgoogletrend: consists of web-search counts collected over ten years related to beer queries on Google~\cite{gooogletrend}.
    % \footnote{\url{https://trends.google.co.jp/trends/}}.
    \item
    \pchickendance, \pexercise: were obtained from the CMU motion capture database~\cite{cmumocap}
    % \footnote{\url{http://mocap.cs.cmu.edu/}}
    and consist of four dimensional vectors (left/right legs and arms).
    % \vspace{-1.0em}
\end{itemize}
}
% \vspace{-1.3em}
\noindent
We compared our algorithm with the following seven baselines for causal discovery, 
namely
CASPER~\cite{liu2023discovering},
DARING~\cite{he2021daring},
NoCurl~\cite{yu2021dag},
NOTEARS-MLP
% (hereinafter referred to briefly as NO-MLP)
(NO-MLP)
\cite{zheng2020learning},
NOTEARS~\cite{zheng2018dags},
LiNGAM~\cite{shimizu2006linear}, and
GES~\cite{chickering2002optimal}.
Besides, we also compared with the five following competitors for forecasting, namely
TimesNet~\cite{wu2023timesnet},
PatchTST~\cite{Yuqietal-2023-PatchTST},
DeepAR~\cite{salinas2020deepar},
OrbitMap~\cite{matsubara2019dynamic}, and
ARIMA~\cite{box1976arima}.
Details regarding the experimental
settings are also provided in Appendix \ref{section:app:experiments:setting}.
\par

% \subsection{Q1. Effectiveness}
\myparaitemize{Q1. Effectiveness}
We first demonstrated how effectively \method discovers the \relation and forecasts future values in a streaming fashion
using the epidemiological data stream (i.e., \covid).
Recall that Figure \ref{fig:crown} shows \method modeling and forecasting results.
Figure \ref{fig:crown} (a/b) shows graphical representations of the causal adjacency matrix $\mB$ and the eigenvalues $\eigs$.
Most importantly, the causal relationships evolve over time in accordance with
the transitions of distinct dynamical patterns in the inherent signals $\mE$.
\method can continuously detect new actual causative events around the world
(e.g., the discovery of a new lineage of the coronavirus in South Africa,
the abrupt increase in coronavirus infections in the United States,
and the strict, long-term lockdown in Shanghai).
Figure \ref{fig:crown} (c) shows streaming time series forecasting results. There have been multiple distinct patterns (e.g., a rapid decrease in infection numbers in the Republic of South Africa), \method adaptively captures the exponential patterns and forecasts future values close to the originals.
% Also, these \relation are based on facts, so \method
\par\noindent
\myparaitemize{Q2-1. Causal discovering accuracy}
We next showed how accurately \method can discover the \relation.
We reported the structural Hamming distance (SHD) and the structural intervention distance (SID)~\cite{peters2015structural}.
SHD quantifies the difference in the causal adjacency matrix by counting missing, extra, and reversed edges and
SID is particularly suited for evaluating causal discovering accuracy since it counts the number of couples $(i, j)$ such that the interventional distribution $p(x_j\mid\text{do}(X_i=\bar{x}))$ would be miscalculated if we used the estimated causal adjacency matrix.
Both metrics should be lower to represent better estimated adjacency matrices. 
Table \ref{table:discovering_accuracy} shows the causal discovering results of \method and its baselines for various synthetic datasets, where the best and second-best levels of performance are shown in \textbf{bold} and \underline{underlined}, respectively.
Our method outperformed all baselines for every temporal sequence, which is consistent with the analysis provided in Lemma \ref{lemma:causal}.
% Because all competitors cannot handle the \relation in data streams.
This is because none of the competitors can handle the \relation in data streams.
\par
\TSK{
\input{components/table_acc_ablation}
}
\noindent
\myparaitemize{Q2-2. Forecasting accuracy}
We evaluated the quality of \method in terms of $l_s$-steps-ahead forecasting accuracy.
For this evaluation, we adopted the root mean square error (RMSE) and the mean absolute error (MAE),
both of which provide good results when they are close to zero.
For all methods, we used one-third of the sequences to tune their parameters.
Table \ref{table:forecasting_accuracy} presents the overall forecasting results,
where the best results are in \textbf{bold} and the second-best are \underline{underlined}.
For brevity, we only reported results of a representative synthetic dataset, which has the most complicated temporal sequence, ``1, 2, 3, 2, 1''.
We compared the two metrics
when we varied the forecasting step
(i.e., $l_s \in \{5, 10, 15\}$).
Our method achieved remarkable improvements over its competitors.
While deep learning models (TimesNet, PatchTST, and DeepAR)
exhibit high generality for time series modeling,
% they reduced the forecasting accuracy
their forecasting accuracy was poorer
because they could not adjust the model parameters incrementally.
OrbitMap is capable of handling multiple discrete nonlinear dynamics but
misses the \relation,
and thus was outperformed by our proposed method.
ARIMA assumes linear relationships between time series data
and so fails to accommodate complex and nonlinear data
resulting in decreased forecasting accuracy.
\par
\myparaitemize{Q2-3. Ablation study}
To quantitatively evaluate the impact of causal relationships on forecasting effectiveness,
we additionally performed an ablation study by comparing a limited version of our method, namely \textit{w/o causality},
whose demixing matrix $\demixing$ was fixed to the identity matrix.
Table \ref{table:ablation} presents the overall results of our ablation study on \method using both synthetic and real-world datasets.
We can see that the \textit{w/o causality} causes a significant drop in forecasting accuracy across all experimental settings.
Therefore, we observed that the discovery of \relation in data streams boosts forecasting accuracy.
% \subsection{Q3. Scalability}
\TSK{
\input{components/fig_time}
}
\par\noindent
\myparaitemize{Q3. Scalability}
Finally, we evaluated the computational time needed by our streaming algorithm.
Figure \ref{fig:time} compares the computational efficiencies
of \method and its competitors.
It presents the computational time at each time point $t_c$ on the left,
and the average computational time on the right.
Note that both figures are shown on linear-log scales.
Our method consistently outperformed its competitors
in terms of computational time
thanks to our incremental update,
which aligns with the discussion presented in Lemma \ref{lemma:stream_time}.
OrbitMap was competitive, but
it estimates model parameters via iterative optimization, the expectation-maximization algorithm,
which makes it slower than our proposed algorithm.
Other methods require a significant amount of learning time because
they cannot update their models incrementally.
