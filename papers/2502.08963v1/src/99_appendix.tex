% \section{Proposed model}
% \label{section:app:model}
% Table \ref{table:define} lists the symbols and their definitions used in this paper. \par
% % \vspace{-1.5em}
% % \TSK{
% % \input{components/table_symbols}
% % }
% \vspace{0.6em}

\section{Optimization Algorithm}
% Algorithm \ref{alg:model} is the overall procedure of \method. Algorithm \ref{alg:estimator}, namely, \modelestimator continuously updates the full parameter set $\modelparam$ and the model candidate $\candparam$, which describes the current window $\mX^c$.
\TSK{
\input{components/alg_overall}
}\par
\TSK{
\input{components/alg_estimator}
}
% Algorithm \ref{alg:model} shows the overall procedure for \method,
% including \modelestimator (Algorithm \ref{alg:estimator}).
% \modelestimator continuously updates the full parameter set $\modelparam$ and
% the model candidate $\candparam$, which describes the current window $\mX^c$.
% \par
\label{section:app:algorithm}
% \myparaitemize{Details in Eq. \eqref{eq:update_trans}}
\subsection{Details of Eq. (\ref{eq:update_trans})}
% \myparaitemize{Details of Eq. (\ref{eq:update_trans})}
Here, we introduce the recurrence relation of transition matrix $\ith{\trans}$.
As mentioned earlier, we use the following cost function (below, index $i$ denoting $i$-th dimension is omitted for the sake of simplicity, e.g., we write $\ith{\trans}$ as $\trans$):
\begin{align*}
    \mathcal{E} &= \sum_{t'=t_m+h}^{t_c}\forgetting^{t_c-t'}||\embed{e(t')} - \trans\embed{e(t'-1)}||_2^2 \\
    &= \sum_{l=1}^h (\mat{L}(l, :) - \trans(l, :)\mat{R})\Forgetting(\mat{L}(l, :) - \trans(l, :)\mat{R})^\top
\end{align*}
where,
% $\Forgetting = diag(\forgetting^{N-2}, ..., \forgetting^0) \in \R^{(N-1) \times (N-1)}$ and
$\Forgetting, \mat{L}$ and $\mat{R}$ are synonymous with the definition in Section \ref{section:alg:creation}.
Because we want to obtain $\trans$ that minimizes this cost function $\mathcal{E}$, we differentiate it with respect to $\trans$.
\begin{align*}
    \dfrac{\partial}{\partial\trans(l, :)}\mathcal{E}
    &= -2(\mat{L}(l, :) - \trans(l, :)\mat{R}) \Forgetting \mat{R}^\top
\end{align*}
Solving the equation $\partial\mathcal{E}/\partial\trans(l, :) = 0$ for each $l$, $1 \leq l \leq h$,
the optimal solution for $\trans$ is given by
% the optimal solution is $ \trans = (\mat{L}\Forgetting\mat{R}^\top)(\mat{R}\Forgetting\mat{R}^\top)^{-1} $
$$ \trans = (\mat{L}\Forgetting\mat{R}^\top)(\mat{R}\Forgetting\mat{R}^\top)^{-1} $$
where we define
\begin{align*}
    \mat{Q} = \mat{L}\Forgetting\mat{R}^\top,\quad
    \mat{P} = (\mat{R}\Forgetting\mat{R}^\top)^{-1}
\end{align*}
The recurrence relations of $\mat{Q}$ can be written as
\begin{align*}
    \mat{Q} &= \sum_{t'=t_m+h}^{t_c}\forgetting^{t_c-t'}\embed{e(t')}\embed{e(t'-1)}^\top \\
    &= \forgetting\sum_{t'=t_m+h}^{t_c-1}\forgetting^{t_c-t'-1}\embed{e(t')}\embed{e(t'-1)}^\top + \embed{e(t_c)}\embed{e(t_c-1)}^\top
\end{align*}
\begin{align}
    \label{eq:Q}
    \therefore \mat{Q}^{new} = \forgetting\mat{Q}^{prev} + \embed{e(t_c-1)}\embed{e(t_c)}^\top
\end{align}
and similarly
\begin{align}
    \label{eq:bP}
    \mat{P}^{new} &= (\forgetting{(\mat{P}^{prev})}^{-1} + \embed{e(t_c)}\embed{e(t_c)}^\top)^{-1}
\end{align}Here, we apply the Sherman-Morrison formula~\cite{sherman1950adjustment} to the RHS of Eq. \eqref{eq:bP}.
Note that $\embed{e(t_c)}^\top\mat{P}^{prev}\embed{e(t_c)} > 0$
because $\mat{P}^{-1} = \mat{R}\Forgetting\mat{R}^\top$ is positive definite by definition.
\begin{align}
    \label{eq:P}
    \therefore \mat{P}^{new} = \frac{1}{\forgetting}(\mat{P}^{prev} - \frac{\mat{P}^{prev}\embed{e(t_c-1)}\embed{e(t_c-1)}^\top\mat{P}^{prev}}{\forgetting + \embed{e(t_c-1)}^\top\mat{P}^{prev}\embed{e(t_c-1)}})
\end{align}
Finally, combining Eq. \eqref{eq:Q} and Eq. \eqref{eq:P} gives the recurrence relations of $\trans$ for Eq. \eqref{eq:update_trans}.
\begin{align*}
    \begin{split}
        \trans^{new} &= \trans^{prev} + (\embed{e(t_c)} - \trans^{prev}\embed{e(t_c-1)}\boldsymbol\gamma \\
        \boldsymbol\gamma &= \frac{\embed{e(t_c-1)}^\top\mat{P}^{prev}}{\forgetting + \embed{e(t_c-1)}^\top\mat{P}^{prev}\embed{e(t_c-1)}}
    \end{split}
\end{align*}
    
% \end{enumerate}
\par
% \TSK{
% \input{components/fig_search}
% }
\setcounter{lemma}{1}
\subsection{Proof of Lemma \ref{lemma:create_time}}
\begin{proof}
The dominant steps in \textsc{RegimeCreation} are I, IV, and VI.
The decomposition $\mX$ into $\demixing^{-1}$ and $\mE$ using ICA requires $O(d^2N)$.
For each observation,
the SVD of $\ith{\mat{R}}\mat{M}$ requires $O(h^2N)$, and the eigendecomposition of $\ith{\tilde{\trans}}$ takes $O(k_i^3)$.
The straightforward way to
process IV and VI
is to perform the calculation $d$ times sequentially, i.e., they require $O(dh^2N+\sum_ik_i^3)$ in total.
However, since these operations do not interfere with each other,
they are simultaneously computed by parallel processing.
Therefore, the time complexity of \textsc{RegimeCreation} is $O(N(d^2+h^2)+k^3)$, where $k=\max_i(k_i)$.
\end{proof}
\subsection{Proof of Lemma \ref{lemma:causal}}
\begin{proof}
First, we need to formulate the causal structure.
Here, we utilize the structural equation model~\cite{pearl2009causality}, denoted by $\mX_{\text{sem}} = \mB_{\text{sem}}\mX_{\text{sem}} + \mE_{\text{sem}}$.
Because this model is known as the general formulation of causality, if $\mB_{\text{sem}}$ in this model is identified, then it can be said that we discover causality.
In other words, we need to prove that our proposed algorithm can find the causal adjacency matrix $\mB$ aligning with this model.
Solving the structural equation model for $\mX_{\text{sem}}$, we obtain 
$\mX_{\text{sem}} = \demixing^{-1}_{\text{sem}}\mE_{\text{sem}}$
where $\demixing_{\text{sem}} = \mat{I} - \mB_{\text{sem}}$.
It is shown that we can identify $\demixing_{\text{sem}}$ in the above equation by ICA,
except for the order and scaling of the independent components, if the observed data is a linear, invertible mixture of non-Gaussian independent components~\cite{comon1994independent}.
Thus, demonstrating that \modelgenerator precisely resolves the two indeterminacies of a mixing matrix $\mW^{-1}$ (i.e., the inverse of $\demixing \in \regime^c$) suffices to complete the proof because $\demixing$ is computed by ICA in \textsc{RegimeCreation}. \par
First, we reveal that our algorithm can resolve the order indeterminacy.
We can permutate the causal adjacency matrix $\mB$ to strict lower triangularity thanks to the acyclicity assumption~\cite{bollen1989structural}.
%, which is without loss of generality.
Thus, correctly permuted and scaled $\mW$
is a lower triangular matrix with all ones on the diagonal.
It is also said that there would only be one way to permutate $\mW$, which meets the above condition~\cite{shimizu2006linear}.
Thus, \modelgenerator can identify the order of a mixing matrix by the process in step I (i.e., finding the permutation of rows of a mixing matrix that yields a matrix without any zeros on the main diagonal).
Next, with regard to the scale of indeterminacy,
it is apparent that we only need to focus on the diagonal element,
remembering that the permuted and scaled $\mW$ has all ones on the diagonal.
Therefore, we prove that \modelgenerator can resolve the order and scaling of the indeterminacies of a mixing matrix $\demixing^{-1}$.
\end{proof}
% \myparaitemize{Proof of Lemma \ref{lemma:time}} \par
\subsection{Proof of Lemma \ref{lemma:stream_time}}
\begin{proof}
For each time point, \method first runs \modelestimator,
which estimates the optimal full parameter set $\modelparam$ and the model candidate $\candparam$ for the current window $\mX^c$.
If the current regime $\regime^c$ fits well,
it takes $O(N\sum_i k_i)$ time.
Otherwise, it takes $O(RN\sum_i k_i)$ time to find a better regime in $\regimeset\in\modelparam$.
Furthermore, if \method encounters an unknown pattern,
it runs \textsc{RegimeCreation}, which takes $O(N(d^2+h^2)+k^3)$ time.
Subsequently, it runs \modelgenerator to identify the causal adjacency matrix and forecast an $l_s$-steps-ahead future value,
which takes $O(d^2)$ and $O(l_s)$ time, respectively.
Note that $l_s$ is negligible because of the small constant value.
Finally, when \method does not create a new regime,
it executes \regimeupdate, which needs $O(dh^2)$ time.
Thus, the total time complexity is at least $O(N\sum_ik_i+dh^2)$ time and at most $O(RN\sum_i k_i+N(d^2+h^2)+k^3)$ time per process.
\end{proof}

% \input{components/table_acc_app_forecast}
% \input{components/table_acc_ablation}
\section{Experimental Setup}
% \label{section:app:experiments}
\label{section:app:experiments:setting}
In this section, we describe the experimental setting in detail.
% \subsection{Experimental Setting}
% \myparaitemize{Experimental Setting}
We conducted all our experiments on
% \unclear{<server spec>}.
an Intel Xeon Platinum 8268 2.9GHz quad core CPU
with 512GB of memory and running Linux.
We normalized the values of each dataset based on their mean and variance (z-normalization).
The length of the current window $N$ was $50$ steps in all experiments.
\par
\myparaitemize{Generating the Datasets}
We randomly generated synthetic multivariate data streams containing multiple clusters, each of which exhibited a certain causal relationship.
For each cluster, the causal adjacency matrix $\mB$ was generated from a well-known random graph model, namely Erdös-Rényi (ER)~\cite{erdos1960evolution} with edge density $0.5$ and the number of observed variables $d$ was set at 5.
The data generation process was modeled as a structural equation model~\cite{pearl2009causality},
where each value of the causal adjacency matrix $\mB$ was sampled from a uniform distribution $\mathcal{U}(-2, -0.5)\cup(0.5, 2)$.
In addition, to demonstrate the time-changing nature of exogenous variables, 
we allowed the inherent signals variance $\sigma^2_{i, t}$ (i.e., $\ith{e}(t)\sim\text{Laplace}(0, \sigma_{i, t}^2)$)
to change over time.
Specifically, we introduced $h_{i, t}=\text{log}(\sigma^2_{i, t})$, which evolves according to an autoregressive model, where the coefficient and noise variance of the autoregressive model were sampled from $\mathcal{U}(0.8, 0.998)$ and $\mathcal{U}(0.01, 0.1)$, respectively.
% however, 

The overall data stream was then generated by constructing a temporal sequence of cluster segments and each segment had $500$ observations (e.g., ``$1,2,1$'' consists of three segments containing two types of causal relationships and its total sample size is $1,500$). We ran our experiments on five different temporal sequences: ``$1,2,1$'', ``$1,2,3$'', ``$1,2,2,1$'', ``$1,2,3,4$'', and ``$1,2,3,2,1$'' to encompass various types of real-world scenarios.
\par
\myparaitemize{Baselines}
The details of the baselines we used throughout our extensive experiments are summarized as follows:
\par\noindent
(1) Causal discovering methods
{\setlength{\leftmargini}{11pt}
\vspace{-0.3ex}
\begin{itemize}
    \item CASPER~\cite{liu2023discovering}: is a state-of-the-art method for causal discovery, integrating the graph structure into the score function and reflecting the causal distance between estimated and ground truth causal structure. We tuned the parameters by following the original paper setting.
    \item DARING~\cite{he2021daring}: introduces an adversarial learning strategy to impose an explicit residual independence constraint for causal discovery. We searched for three types of regularization penalties $\{\alpha, \beta, \gamma\}\in\{0.001, 0.01, 0.1, 1.0, 10\}$.
    % aiming to improve the learning of acyclic graphs.
    \item NoCurl~\cite{yu2021dag}: uses a two-step procedure: initialize a cyclic solution first and then employ the Hodge decomposition of graphs. We set the optimal parameter presented in the original paper.
    % and learn a DAG structure by projecting the cyclic graph to the gradient of a potential function.
    \item NOTEARS-MLP~\cite{zheng2020learning}: is an extension of NOTEARS~\cite{zheng2018dags} (mentioned below) for nonlinear settings, which aims to approximate the generative structural equation model by MLP.
    We used the default parameters provided in authors' codes\footnote[2]{\url{https://github.com/xunzheng/notears} \label{fot:notears}}.
    \item NOTEARS~\cite{zheng2018dags}:
    % is specifically designed for linear settings and
    is a differentiable optimization method with an acyclic regularization term to estimate a causal adjacency matrix.
    We used the default parameters provided in authors' codes\footref{fot:notears}.
    % estimates the true causal graph by minimizing the fixed reconstruction loss with the continuous acyclicity constraint.
    \item LiNGAM~\cite{shimizu2006linear}:
    exploits the non-Gaussianity of data to determine the direction of causal relationships. It has no parameters to set.
    % and we used the authors source codes\footnote{https://github.com/cdt15/lingam}.
    \item GES~\cite{chickering2002optimal}: is a traditional score-based bayesian algorithm that discovers causal relationships in a greedy manner.
    It has no parameters to set.
    We employed BIC as the score function and utilized the open-source in~\cite{kalainathan2020causal}.
\end{itemize}
\vspace{-0.5ex}}
\par\noindent
(2) Time series forecasting methods
{\setlength{\leftmargini}{11pt}
\vspace{-0.3ex}
\begin{itemize}
    \item TimesNet/PatchTST~\cite{wu2023timesnet, Yuqietal-2023-PatchTST}: are state-of-the-art TCN-based and Transformer-based methods, respectively.
    The past sequence length was set as 16 (to match the current window length).
    % Other parameters follow the parameter settings suggested in the original paper.
    Other parameters followed the original paper setting.
    % \item PatchTST~\cite{Yuqietal-2023-PatchTST}: is a state-of-the-art Transformer-based method for time series forecasting. The past sequence length is set as 16 for the same reason as above.
    \item DeepAR~\cite{salinas2020deepar}: models probabilistic distribution in the future, based on RNN. We built the model with 2-layer 64-unit RNNs. We used Adam optimization~\cite{adam} with a learning rate of 0.01 and let it learn for 20 epochs with early stopping.
    % to choose the best model.
    \item OrbitMap~\cite{matsubara2019dynamic}:
    % is a stream forecasting algorithm that finds important time-evolving patterns with multiple discrete non-linear dynamical systems.
    finds important time-evolving patterns for stream forecasting.
    We determined the optimal transition strength $\rho$ to minimize the forecasting error in training.
    \item ARIMA~\cite{box1976arima}: is one of the traditional time series forecasting approaches based on linear
    equations. We determined the optimal parameter set using AIC.
\end{itemize}}
