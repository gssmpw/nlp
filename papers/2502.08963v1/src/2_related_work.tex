\TSK{
\input{components/table_capabilities} 
}
In this section, we briefly describe investigations related to our work.
Table \ref{table:capability} summarizes the relative advantages of \method
with regard to
% \unclear{xxx}
five
aspects.
% where none of the existing methods satisfies all the requirements.
% Note that ``P" in this table represents partial advantages. 
\par
\myparaitemize{Time series modeling and forecasting}
Time series modeling and forecasting are important areas that
have attracted huge interest in many fields.
Autoregressive integrated moving average (ARIMA)~\cite{box1976arima} and Kalman filters (KF)~\cite{durbin2012time} are representative examples of traditional modeling and forecasting methods, and there have been many studies of their derivatives~\cite{
papadimitriou2003adaptive,
li2010parsimonious,
de2011forecasting,
shi2020block}.
TICC~\cite{hallac2017toeplitz} characterizes the interdependence
between different observations based on a Markov random field
but cannot capture the causal relationships.
% but cannot capture directed graphs representing causality.
Additionally, streaming algorithms have become more critical
in terms of processing a substantial amount of data under time/memory limitations,
and they have proved highly significant to the data mining and database community~\cite{
aggarwal2007data,
hahsler2016clustering,
matsubara2016regime,
kawabata2020non}. 
OrbitMap~\cite{matsubara2019dynamic} is the latest general method focusing on stream forecasting,
and it can find the transitions between major dynamic time series patterns.
However, it cannot discover the \relation between observations.
Research on deep learning models for forecasting has been very active in recent years~\cite{
% zhou2021informer,
Zeng2022AreTE,
Yuqietal-2023-PatchTST,
salinas2020deepar}.
TimesNet~\cite{wu2023timesnet} is a TCN-based method that transforms a 1D time series into 2D space based on multiple periods and captures complex temporal variations for forecasting.
% PatchTST~\cite{Yuqietal-2023-PatchTST} revealed the effectiveness of Transformer in forecasting by leveraging two key components, patching and channel-independence.
% DeepAR~\cite{salinas2020deepar} has an encoder-decoder structure that employs an auto-regression RNN modeling probabilistic distribution in the future.
Although deep learning-based methods are compelling,
their applicability to forecasting in a streaming fashion is limited
due to the prohibitively high computational costs associated with time series analysis,
which hinders continuous model updating with the most recent observations.
\par
% \myparaitemize{Operator theoretic data analysis}
% \myparaitemize{Koopman theory}
% % There have been numerous studies on data analysis based on operator theory.
% % These studies often center around the application of the Koopman operator
% % There have been numerous studies on data analysis based on the Koopman operator.
% Koopman theory~\cite{
% koopman1931hamiltonian,
% mezic2005spectral,
% rowley2009spectral} has emerged over decades to understand dynamical systems~\cite{kawahara2016dynamic,
% kostic2022learning,
% nimrod2023koopman}.
% This concept has revitalized the field of non-linear dynamical systems analysis
% by providing a framework
% for converting non-linear systems into linear ones in an infinite-dimensional space.
% The ability of the Koopman operator
% to linearize complex dynamics without sacrificing the richness of the original system and 
% to reveal the fundamental dynamics by calculating the spectrum of an operator
% has made it a powerful tool in operator theory-based data analysis.
% Dynamic Mode Decomposition (DMD)~\cite{tu2013dynamic, kutz2016dynamic} is a numerical decomposition method
% that coincides with modal decomposition
% via the Koopman operator under certain conditions.
% In many cases,
% % the above conditions are met,
% it can be assumed that the above conditions are met,
% and DMD is used in many fields,
% such as fluid mechanics~\cite{schmid2010dynamic},
% video processing~\cite{erichson2016randomized, grosek2014dynamic},
% neuroscience~\cite{brunton2016extracting},
% and epidemiology~\cite{proctor2015discovering}.
% HAVOK~\cite{brunton2017chaos} decomposes chaotic dynamics into a linear model 
% with forcing, utilizing the Koopman theory.
% Note that this method is fundamentally unsuitable for our setting;
% it is not for forecasting and
% can only analyze univariate time series.

\par
\myparaitemize{Causal inference/discovery}
Over decades,
a wide range of studies have been conducted on
causal inference/discovery~\cite{
shimizu2006linear,
he2021daring,
fujiwara2023causal,
jiang2023cfgode,
liu2023discovering} and 
addressing challenges based on the concept of causality~\cite{
richens2020improving,
% he2021daring,
dai2022graph,
% liu2022scinet,
wu2023causal}.
NOTEARS~\cite{zheng2018dags} is a new differentiable optimization framework for learning directed acyclic graphs, utilizing an acyclic regularization as a replacement for a combinatorial constraint.
Granger causality~\cite{granger1969investigating} has been widely used to analyze multivariate time series data.
However, Granger causality only indicates the presence of a predictive relationship~\cite{peters2017elements}.
Specifically, typical causality represents whether one observation causes another, while Granger causality represents whether one observation forecasts another~\cite{granger2014forecasting}.
In this paper, we focus on the cause-and-effect relationships that evolve over time in a data stream.
We try to discover them based on the structural equation model~\cite{pearl2009causality}, which is one of the most general formulations of causality.
\par
Consequently, none of these methods specifically 
focused on the discovery of the \relation
and forecasting future values in a streaming fashion, simultaneously.
% \vspace{-1.0em}