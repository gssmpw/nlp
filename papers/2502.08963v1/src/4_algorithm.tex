\TSK{
\input{components/fig_algorithm}
}
Thus far, we have shown how we describe the major dynamical pattern using the demixing matrix $\demixing$ which can be transformed into a causal adjacency matrix $\mB$.
In this section, we present an effective algorithm
to identify the \relation $\mathcal{B}$ and
estimate the regime set $\regimeset$.
Figure \ref{fig:algorithm} shows an overview of our proposed algorithm.
We first present an effective way to estimate a new model parameter set from a multivariate time series, where we assume it has only a single regime.
We then describe a streaming algorithm to identify $\mathcal{B}$ and maintain $\regimeset$ incrementally for multiple distinct 
dynamical patterns, simultaneously.
\subsection{\textsc{RegimeCreation}}
\label{section:alg:creation}
We first propose an algorithm, namely \textsc{RegimeCreation}, for estimating a single regime parameter set $\theta = \{ \demixing, \first{\selfdynamics}, ..., \dth{\selfdynamics} \}$ from a data stream $\mX$.
The algorithm consists of two main steps: (i) it decomposes $\mX$ into a demixing matrix $\demixing$ and inherent signals $\mE$ and (ii) it computes $d$ self-dynamics factor sets $\{ \first{\selfdynamics}, ..., \dth{\selfdynamics} \}$ according to Eq. \eqref{eq:univariate}.
We use ICA for the decomposition of $\mX$, where it is essential for identifying the optimal causality.
Next, as regards computing the $i$-th self-dynamics factor set $\ith{\selfdynamics}$, assume that we have the following data matrices based on the Hankel matrix $\ith{\hankel}$:
\begin{align*}
    \ith{\mat{L}} = 
    \begin{bmatrix}
    \embed{\ith{e}(h+1)} & \cdots & \embed{\ith{e}(t)}
    \end{bmatrix} \in \R^{h \times (t-h)} \\
    \ith{\mat{R}} = 
    \begin{bmatrix}
    \embed{\ith{e}(h)} & \cdots & \embed{\ith{e}(t-1)}
    \end{bmatrix} \in \R^{h \times (t-h)}
\end{align*}
And, we use the following weight cost function:
\begin{align}
    \begin{split}
    \label{eq:loss}
    & \min_{\trans}\sum_{t'=h}^{t-1}\forgetting^{2(t-1-t')}||\embed{\ith{e}(t'+1)} - \ith{\trans}\embed{\ith{e}(t')}||_2^2 \\
    = & \min_{\trans} || (\ith{\mat{L}}-\ith{\trans}\ith{\mat{R}})\Forgetting ||_F^2
    \end{split}
\end{align}
where, $\ith{\trans}$ is the transition matrix, from which the eigendecomposition yields the modes $\imode$ and the corresponding eigenvalues $\ieig$, and $\Forgetting = \mathrm{diag}(\forgetting^{t-h-1}, ..., \forgetting^{0}) \in \mathbb{R}^{(t-h)\times(t-h)}$ is defined as a forgetting matrix, based on the recursive least squares principle.
In addition, according to Koopman theory~\cite{koopman1931hamiltonian}, while the transition matrix $\ith{\trans}$ is a linear operator,
it is applicable even to nonlinear
dynamical systems, unlike the classical modal decomposition of linear time-invariant systems.
Specifically, the analytical algorithm proceeds as follows:

{\setlength{\leftmargini}{15pt}
\begin{enumerate}
    \renewcommand{\labelenumi}{\Roman{enumi}.}
    \item Compute the ICA of $\mX = \demixing^{-1}\mE$.
    \item Form the Hankel matrix $\ith{\hankel}$
    according to Eq. \eqref{eq:timedelay}.
    % from the $i$-th inherent signal $\ith{\ind}$.
    \item Build a pair of data matrices $(\ith{\mat{L}}, \ith{\mat{R}})$.
    \item Compute the SVD of $\ith{\mat{R}}\Forgetting = \ith{\mat{U}}\ith{\boldsymbol{\Sigma}}{\ith[\top]{\mat{V}}}$.
    We automatically determine the optimal number of singular values $\nmodes_i$ by~\cite{gavish2014optimal}.
    \item Project the transition matrix $\ith{\trans}$
    onto the $\nmodes_i$-dimensional subspace spanned by the left singular vector $\mat{U}^{(i)}$.
    \begin{align*}
        \ith{\tilde{\trans}} = \ith[\top]{\mat{U}}\ith{\trans}\ith{\mat{U}} = {\ith[\top]{\mat{U}}}\ith{\mat{L}}\Forgetting\ith{\mat{V}}\ith{\mat{\Sigma}}^{-1} \in \R^{\nmodes_i \times \nmodes_i}
        % \\ \vspace{-0.7em}
    \end{align*}
    \item Compute the eigendecomposition of $\ith{\tilde{\trans}}\ith{\mat{Z}} = \ith{\mat{Z}}\ieig$.
    Note that the eigenvalues $\ieig$
    are identical to the $\nmodes_i$ leading eigenvalues of $\ith{\trans}$
    because the left singular vector $\ith{\mat{U}}$ is an orthogonal matrix.
    \item Compute the modes $\imode = \ith{\mat{U}}\ith{\mat{Z}}$.
\end{enumerate}
}
\begin{lemma}[Time complexity of \textsc{RegimeCreation}]
\label{lemma:create_time}
The time complexity of \textsc{RegimeCreation} is $O(N(d^2+h^2)+k^3)$, where $k=\max_i(k_i)$.
Please see Appendix\,\ref{section:app:algorithm} for details.
\end{lemma}
\subsection{Streaming Algorithm}
Our next step is to answer the most important question:
how can we employ our proposed model
for identifying the causal adjacency matrix $\mB$ from the demixing matrix $\demixing\in\regime$ and 
forecasting future values in a streaming fashion?
Before turning to the main topic,
we provide the definitions of some key concepts.
\begin{definition}[Update parameter: $\update$] Let $\update$ be a parameter set for updating a regime $\theta$,
% i.e., $\update = \{ \{ \ith{\bm{U}} \}_{i=1}^d, \{ \ith{\bm{P}} \}_{i=1}^d, \{ \ith{\bm{\epsilon}} \}_{i=1}^d \}$,
i.e., $\update = \{ \{ \ith{\bm{P}} \}_{i=1}^d, \{ \ith{\bm{\epsilon}} \}_{i=1}^d \}$,
where
% $\ith{\bm{U}}$ is the left-singular vectors of the Hankel matrix $\ith{\hankel}$ which is formed from the $i$-th inherent signal $\ith{\ind}$ and 
$\ith{\bm{P}} = 
% (\ith{\bm{U}}^\top\bm{H}_0(\ith{\bm{U}}^\top\bm{H}_0)^\top)^{-1}$.
(\ith{\mat{R}}\Forgetting{\ith[\top]{\mat{R}}})^{-1}$ and $\ith{\bm{\epsilon}}$ is the energy.
% Please see Section \ref{section:alg:update} for the detailed application of this parameter set.
\end{definition}
% \vspace{-0.75em}
\begin{definition}[Full parameter set: $\modelparam$] Let $\modelparam$ be a full parameter set of \method,
i.e., $\modelparam = \{ \regimeset, \updateset \}$,
where $\regimeset$ and $\updateset$ consist of $R$ regimes and update parameters, respectively,
% where $\regimeset$ consists of $R$ regimes,
namely, $\regimeset = \{ \regime^1, ..., \regime^R \}$,
% and $\updateset$ is the update parameters set,
and $\updateset = \{ \update^1, ..., \update^R \}$.
\end{definition}
% \vspace{-0.15em}
\noindent With the above definitions, the formal problem is as follows:
% \vspace{-0.25em}
\begin{problem}
\textbf{Given} a multivariate data stream $\mX$,
where $\vx(t_c)$ is the most recent value at time point $t_c$,
\begin{itemize}
    \item \textbf{Find} the optimal full parameter set, i.e., $\modelparam = \{ \regimeset, \updateset \}$,
    \item \textbf{Discover} the \relation, i.e., $\mathcal{B}$,
    \item \textbf{Forecast} an $l_s$-steps-ahead future value, i.e., $\vvec(t_c+l_s)$.
\end{itemize}
\end{problem}
% \vspace{-0.25em}
% \vspace{-0.5em}
\noindent Here, we refer to the regime for the current window $\mX^c = \mX[t_m:t_c]$ as $\regime^c$, and the update parameter corresponding to $\regime^c$ as $\update^c$. In addition, we need the latent vectors $\mat{S}(t_c)$ at the current time $t_c$
for forecasting an $l_s$-steps-ahead future value $\vvec(t_c+l_s)$, and so keep it as $\mat{S}^c_{en}$.
In summary, our proposed algorithm keeps them as the model candidate $\candparam = \{ 
\regime^c, \update^c, \mat{S}^c_{en} \}$ for stream processing.
% \vspace{-0.2em}
\subsubsection{Overview}
We now introduce our streaming algorithm, \method, which consists of the following algorithms.
For detailed descriptions and pseudocode, we would refer you to Algorithm \ref{alg:model} in Appendix \ref{section:app:algorithm}.
{\setlength{\leftmargini}{15pt}
\begin{itemize}
    \item \modelestimator:
    Estimates the optimal full parameter set $\modelparam$ and the model candidate $\candparam$ which appropriately describes the current window $\mX^c$.
    \item \modelgenerator:
    Forecasts an $l_s$-steps-ahead future value, i.e., $\vvec(t_c+l_s)$, and identifies the causal adjacency matrix $\mat{B}$, using the model candidate $\candparam$.
    \item \regimeupdate:
    Updates the current regime $\regime^c$ using update parameter $\update^c\in\candparam$ and the most recent value $\vx(t_c)$. This is only performed if a new regime is not created.
\end{itemize}
}
\noindent
The following sections provide detailed explanations.
% \vspace{0.3em}
% Given a new value $\vx(t_c)$ at the current time $t_c$,
% it updates the full parameter set $\modelparam$ and 
% the model candidate $\candparam$ by using \modelestimator.
% Next, it generates an $l_s$-steps-ahead future value $\vvec(t_c+l_s)$ and
% the causal adjacency matrix $\mB$ from the demixing matrix $\demixing\in\regime^c$ using \modelgenerator.
% Finally, if a new regime is not created,
% it also updates the model candidate $\candparam$
% with a new value $\vx(t_c)$.
% \vspace{-0.2em}
\subsubsection{\modelestimator}
Given a new value $\vx(t_c)$ at the current time $t_c$, we first need to update incrementally the full parameter set $\modelparam$ and the model candidate $\candparam$, which best describes the current window $\mX^c$.
Algorithm \ref{alg:estimator} (See Appendix \ref{section:app:algorithm}) is the \modelestimator algorithm in detail.
% Here, we define the new function $f(\cdot)$
Here, let $f(\mX^c; \mat{S}_0^c, \regime^c)$ be a new function
for estimating the optimal parameter
so that it minimizes the mean square errors
between the current window $\mX^c$ and the estimated window $\mat{V}^c$ in Model \ref{model:multi},
i.e., $f(\mX^c; \mat{S}_0^c, \regime^c) = \sum_{t=t_m+h-1}^{t_c}||\bm{x}(t) - \bm{v}(t)||$,
where $\mat{S}_0^c$ represents the latent vectors at time point $t_m+h-1$.
Note that when embedding the time series using $\embed{\cdot}$,
the number of data points (namely, the number of columns in the Hankel matrix $\hankel$) 
is partially reduced compared with before embedding.
The most straightforward way to determine $\mat{S}_0^c$
is to adopt $\{ {\modes_{(i)}^\dagger}\embed{\ith{e}(t_m + h - 1)}\}_{i=1}^d$
according to Eq. \eqref{eq:multivariate}.
However, the noisy initial conditions give rise to unexpected forecasting.
Therefore, we optimize $\mat{S}_0^c$ by using the Levenberg-Marquardt (LM) algorithm~\cite{more2006levenberg}
and thus enable the effects of noise in observations to be removed.
Here, we return to the \modelestimator algorithm, which proceeds as follows:
{\setlength{\leftmargini}{15pt}
\begin{enumerate}
    \renewcommand{\labelenumi}{\Roman{enumi}.}
    \item It optimizes initial condition $\mat{S}_0^c$,
    so that it minimizes the errors between the current window $\mX^c$ and the current regime $\regime^c$.
    \item If $f(\mX^c; \mat{S}_0^c, \regime^c) > \tau$, it searches for a better regime $\regime \in \regimeset$.
    \item If $f(\mX^c; \mat{S}_0^c, \regime^c) > \tau$ still holds, it creates a new regime for $\mX^c$ using \textsc{RegimeCreation}, and inserts it into $\regimeset$.
\end{enumerate}
}
\par
\par
\subsubsection{\modelgenerator}
\label{section:alg:generator}
The next algorithm is \modelgenerator,
which incrementally identifies the causal adjacency matrix $\mB$ and
forecasts an $l_s$-steps-ahead future value $\est(t_c+l_s)$
by using the model candidate $\candparam$.
As for forecasting,
it generates the value of $\est(t_c+l_s)$ according to Eq. \eqref{eq:multivariate}
with the most suitable regime $\regime^c$ for $\mX^c$,
which is selected by $\modelestimator$.
On the other hand,
we identify the causal adjacency matrix $\mB$ from the demixing matrix $\demixing \in \regime^c$.
A mixing matrix (i.e., the inverse of a demixing matrix) typically has the two major indeterminacies:
the order and scaling of the independent components;
however, we must address the above difficulties if we are to identify the optimal causal adjacency matrix~\cite{shimizu2006linear}.
Consequently, the algorithm for identifying the causal adjacency matrix $\mB$ proceeds as follows:
{\setlength{\leftmargini}{15pt}
\begin{enumerate}
\renewcommand{\labelenumi}{\Roman{enumi}.}
\item Find the permutation of rows of $\demixing$ that yields a matrix $\tilde{\demixing}$ without any zeros on the main diagonal.
\item Divide each row of $\tilde{\demixing}$ by its corresponding diagonal element to yield a new matrix $\tilde{\demixing'}$ with all ones on the diagonal.
\item Compute an estimate $\hat{\mB}$ of $\mB$ using $\hat{\mB} = \mat{I} - \tilde{\demixing'}$.
\item Finally, to find a causal order, compute the permutation matrix $\mat{K}$ of $\hat{\mB}$ that yields a matrix $\tilde{\mat{B}} = \mat{K}\hat{\mB}\mat{K}^\top$, which minimizes the sum of the elements in the upper triangular part of $\tilde{\mat{B}}$.
\end{enumerate}
}
\noindent This algorithm resolves two major indeterminacies in a mixing matrix in steps I and II.
Moreover, it finds the causal order, in other words, it removes the insufficient connection in step IV.
The causal relationships are already identified up to step III, but this step is important for visualizing the resulting directed acyclic graph.
\begin{lemma}[Causal identifiability]
\label{lemma:causal}
Causal discovery in \method is equivalent to
finding the causal adjacency matrix $\mB$ in \modelgenerator.
Please see Appendix\,\ref{section:app:algorithm} for details.
\end{lemma}
\noindent This lemma demonstrates theoretically that our proposed algorithm is capable of discovering causal relationships.
\par
\subsubsection{\regimeupdate}
\label{section:alg:update}
Finally, when an existing regime is selected as the current regime $\regime^c$ from the regime set $\regimeset$,
we update its parameters (i.e., $\demixing, \selfdynamics^{(1)}, ..., \selfdynamics^{(d)}$) using a new value $\vx(t_c)$
to ensure that this regime represents a more sophisticated dynamical pattern. 
In short, \regimeupdate has two parts:
(i) update the demixing matrix $\demixing$ and 
(ii) update each self-dynamics factor set $\ith{\selfdynamics}$.
In part (i),
we use an algorithm based on adaptive filtering techniques~\cite{yang1995projection, haykin2002adaptive}.
It is so efficient in terms of computational and memory requirements,
while converging quickly,
with no special parameters to tune.
Here, we briefly describe the update process for $\demixing$:
{\setlength{\leftmargini}{15pt}
\begin{enumerate}
    \renewcommand{\labelenumi}{\Roman{enumi}.}
    \item Compute the $i$-th univariate inherent signal $\embed{\ith{e}(t_c)}$ at the current time $t_c$, by projecting $\vx(t_c)$ onto $\rowvect{\vect{w}}_i$, which is the $i$-th row vector of $\demixing$, before the update.
    \item Estimate the reconstruction error and the energy $\ith{\bm{\epsilon}}$ based on the value of $\embed{\ith{e}(t_c)}$, % store in the model candidate $\candparam$.
    % $d_i\leftarrow d_i + y_i^2$
    \item Update the estimates of $\rowvect{\vect{w}}_i$ using error and energy $\ith{\bm{\epsilon}}$.
\end{enumerate}}
\noindent In part (ii),
we update the self-dynamics factor set $\ith{\selfdynamics}$ by utilizing the following recurrence:
\begin{align}
    \label{eq:update_trans}
    \begin{split}
        \ith[new]{\trans} &= \ith[prev]{\trans} + (\embed{\ith{e}(t_c)}-\ith[prev]{\trans}\embed{\ith{e}(t_c-1)})\ith{\boldsymbol\gamma} \\
        \ith{\boldsymbol\gamma} &= \frac{\embed{\ith{e}(t_c-1)}^\top\ith[prev]{\bm{P}}}{\forgetting + \embed{\ith{e}(t_c-1)}^\top\ith[prev]{\bm{P}}\,\embed{\ith{e}(t_c-1)}} \\
        \ith[new]{\bm{P}} &= \frac{1}{\forgetting}(\ith[prev]{\bm{P}} - \ith[prev]{\bm{P}}\,\embed{\ith{e}(t_c-1)}\ith{\boldsymbol\gamma})
    \end{split}
\end{align}
In this equation, $\imode$ and $\ieig$ are eigenvectors and eigenvalues of $\ith{\trans}$, respectively.
This recurrence minimizes the weighted cost function as outlined in Eq. \eqref{eq:loss}, focusing on a new value $\vx(t_c)$, thereby adapting to most recent patterns.
See Appendix \ref{section:app:algorithm} for details regarding the above formula.
In summary, \regimeupdate proceeds in the following manner:
{\setlength{\leftmargini}{15pt}
\begin{enumerate}
    \renewcommand{\labelenumi}{\Roman{enumi}.}
    \item In accordance with the part (i) algorithm, update the demixing matrix $\demixing$ using a new value $\vx(t_c)$.
    \item Compute the current inherent signals $\mE^c$ from the current window $\mX^c$ using the updated demixing matrix $\mW$.
    \item Update each self-dynamics factor set $\ith{\selfdynamics}$ according to Eq. \eqref{eq:update_trans}.
\end{enumerate}}
% , and more specifically, \par
% \begin{enumerate}
%     \renewcommand{\labelenumi}{\Roman{enumi}.}
%     \renewcommand{\labelenumii}{\Roman{enumi}-\alph{enumii}.}
%     \item Update the demixing matrix $\demixing$.
%     \begin{enumerate}
%         \item initialize $\acute{\vx}_1 \coloneqq \vx(t_c)$.
%         \item Buf Buf.
%         \item I don't want to work any more.
%     \end{enumerate}
%     \item Update the self-dynamics factor set $\selfdynamics$.
% \end{enumerate}
% \myparaitemize{1. Update the demixing matrix}
% test \par
% \myparaitemize{2. Update the self-dynamics factor set}
% test
\par
\begin{lemma}[Time complexity of \method]
\label{lemma:stream_time}
Based on Lemma \ref{lemma:create_time}, the time complexity of \method is
at least $O(N\sum_ik_i+dh^2)$,
and at most $O(RN\sum_i k_i+N(d^2+h^2)+k^3)$ per process.
Please see Appendix\,\ref{section:app:algorithm} for details.
\end{lemma}
\noindent This theoretical analysis indicates that
our proposed algorithm requires only constant computational time
with regard to the entire data stream length $t_c$.
Therefore, \method is practical for semi-infinite data streams in terms of execution speed.
% \vspace{-0.2em}
