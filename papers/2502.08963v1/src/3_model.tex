\TSK{
\input{components/fig_model}
}
\TSK{
\input{components/table_symbols}
}
In this section, we present our proposed model.
The main symbols we use in this paper are described in
Table \ref{table:define}.
Here,
before introducing the main topic,
we briefly describe the principles and concepts of \method.
We design our proposed model based on the structural equation model (SEM)~\cite{pearl2009causality},
which is written as $\mX_{\text{sem}} = \mB_{\text{sem}}\mX_{\text{sem}} + \mE_{\text{sem}}$,
where $\mX_{\text{sem}}$ is the observed variables,
$\mB_{\text{sem}}$ is the causal adjacency matrix, and
$\mE_{\text{sem}}$ is a set of mutually independent exogenous variables with a non-Gaussian distribution.
Note that we assume that
the data generating process is linear,
the causal network is a directed acyclic graph, and
there are no unobserved confounders in this paper.
The structural equation model can express a typical causality,
however, in real-world applications,
causal relationships change over time
in accordance with the transitions of distinct dynamical patterns.
In our model, we assume that the exogenous variables behave as a dynamical system;
however, it is inappropriate to consider
their time-evolution
as a single dynamical system
due to their independence from each other.
In summary, given a multivariate data stream
which contains various distinct dynamical patterns (i.e., regimes),
we aim to discover the \relation and summarize an entire data stream on the above assumption.
Specifically, we need to capture the following properties
% for the achievement of
to achieve
the above objective:
\begin{itemize}
    \item [\textbf{(P1)}] latent temporal dynamics of exogenous variables
    \item [\textbf{(P2)}] dynamical pattern in a single regime
\end{itemize}
So, how can we build our model that expresses both \textbf{(P1)} and \textbf{(P2)}?
What is the acceptable mathematical model that summarizes a data stream and discovers the \relation?
To handle \textbf{(P1)}, we express each of the exogenous variables as the superposition of computed basis vectors (i.e., modes).
We model \textbf{(P2)} by combining the above components.
We provide our answers below.
% \vspace{-0.3em}
\subsection{Proposed Solution: \method}
We now present our model in detail.
First, we provide the definition for our proposed method.
% \vspace{-0.1em}
\begin{definition}[Inherent signals: $\mE$]
    Let $\mE$ be a bundle of $d$ mutually independent signals with a non-Gaussian distribution, i.e., $\mE = \{ \ith{\ind} \}_{i=1}^d$, where $\ith{\ind} = \{ \ith{e}(1), ..., \ith{e}(t) \}$ is the $i$-th univariate inherent signal. The main property is that they evolve over time.
\end{definition}
% \vspace{-0.1em}
\noindent Figure \ref{fig:model} is an illustration of our proposed model.
In the first half of this section,
we describe \textbf{(P1)} the latent temporal dynamics of the $i$-th univariate inherent signal $\ith{\ind}$
by introducing the self-dynamics factor set $\ith{\selfdynamics}$,
and next, we propose the parameter set $\regime$ to represent \textbf{(P2)} regimes and an entire data stream.
\subsubsection{Latent temporal dynamics of an inherent signal (P1)}
\label{section:uni}
First, we answer the fundamental question, namely,
how can we extract the latent temporal dynamics from the $i$-th inherent signal (i.e., $\ith{\ind} = \{ \ith{e}(1), ..., \ith{e}(t) \}$) and express it as a superposition of the modes?
The difficulty arises from the fact that
the latent dynamics in the system are generally multi-dimensional,
making a single dimension inadequate for modeling the system.
Here, we utilize state space augmentation methods
to compensate for this inadequacy.
In particular, we adopt time-delay embedding,
which is effective in capturing nonlinear dynamics.
Specifically, this is an established method
for the geometric reconstruction of attractors
for nonlinear systems
based on the measurement of generic observables,
$\embed{\ith{e}(t)} \coloneqq (\ith{e}(t), \ith{e}(t-1), ..., \ith{e}(t-h+1)) \in \R^{h}$,
where $h$ is the embedding dimension.
We form the Hankel matrix $\ith{\hankel} \in \R^{h \times (t-h+1)}$
by using the above observable $\embed{\cdot}$.
\begin{align}
    \label{eq:timedelay}
    \ith{\hankel} &= 
    \begin{bmatrix}
        % \footnotesize
        | & | &  & | \\
        \embed{\ith{e}(h)} & \embed{\ith{e}(h+1)} & \cdots & \embed{\ith{e}(t)} \\
        | & | &  & |
    \end{bmatrix}
\end{align}
As seen in Eq. \eqref{eq:timedelay},
each state represented by a single measurement function is augmented with its past history.
Furthermore, according to Takens' embedding theorem~\cite{takens2006detecting},
it is guaranteed that time-delay embedding produces a vector
whose dynamics are diffeomorphic to the dynamics of the original state under certain conditions.
Intuitively,
the reconstruction theoretically preserves the properties of the original dynamical system,
allowing an analysis of Hankel matrix to reveal important features that may not be directly apparent in the original data alone.
In many cases, an embedding dimension may be chosen without sacrificing the diffeomorphism. \par
We now extract the latent temporal dynamics
expressed as the superposition of the modes
from the $i$-th univariate inherent signal $\ith{\ind}$
using the above method.
We thus introduce a time-evolving activity for describing the dynamical system of the $i$-th inherent signal $\ith{\ind}$.
This activity is a latent vector $\ith{\latent}(t)\in\C^{\nmodes_i}$, which is $\nmodes_i$-dimensional complex-valued latent vector at time point $t$, where $\nmodes_i$ is the number of modes.
Consequently,
the dynamical system for the $i$-th univariate inherent signal $\ith{\ind}$
can be described with the following equations:
\begin{model}
\label{model:uni}
Let $\ith{\latent}(t)$ be the $\nmodes_i$-dimensional latent vector at time point $t$ for $i\in\{1, \ldots, d\}$.
The following equations govern the $i$-th univariate inherent signal $\ith{\ind}$,
\begin{align}
    % \small
    \label{eq:univariate}
    \begin{split}
        % \frac{d\latent(t)}{dt} &= \eigs\latent(t) \\
        \ith{\latent}(t + 1) &= \ieig\ith{\latent}(t) \\
        \ith{e}(t) &= \invembed{\imode\ith{\latent}(t)}
    \end{split}
    \normalsize
\end{align}
where $\invembed{\cdot}:\R^h\to\R$ is the inverse of the observables $\embed{\cdot}$,
each column of $\imode$ is one mode, and $\ieig$ is a diagonal matrix containing the $\nmodes_i$ eigenvalues corresponding to those modes.
\end{model}
\noindent Note that the latent vector $\ith{\latent}(t)$ is expressed as a superposition of $\nmodes_i$ modes.
The eigenvalues $\ieig\in\C^{\nmodes_i\times\nmodes_i}$ describe latent dynamical activities, and
the modes $\imode\in\C^{h\times\nmodes_i}$ and $\invembed{\cdot}$ are the observation projections
that generate the $i$-th univariate inherent signal $\ith{e}(t)$ at each time point $t$.
% \unclear{In addition, }
Consequently, we have the following:

\begin{definition}[Self-dynamics factor set: $\ith{\selfdynamics}$]
    Let $\ith{\selfdynamics}$ be a set of modes $\imode$ and eigenvalues $\ieig$,
    i.e., $\ith{\selfdynamics} = \{ \imode, \ieig \}$,
    which represents the latent temporal dynamics of the $i$-th univariate inherent signal $\ith{e}$.
\end{definition}

\noindent We can interpret the features of the above model by focusing on the self-dynamics factor set $\ith{\selfdynamics}$.
Specifically,
the eigenvalues $\ieig$ imply the temporal dynamics of the modes $\imode$,
such as exponential growth/decay and oscillations.
These are derived from a characteristic of a discrete dynamical system.
Considering the eigenvalues $\ieig$ represent
the behavior of a discrete dynamical system with sampling interval $\Delta t$, decay rate $a$ and temporal frequency $b$ of the $j$-th mode $\varphi_j$ are shown as follows, using the $j$-th eigenvalue $\lambda_j$:
\begin{align*}
a = \frac{\Re (\log{\lambda_j})}{\Delta t}, \quad b = \frac{\Im (\log{\lambda_j})}{\Delta t}
\end{align*}
% \normalsize
where $\Re$ and $\Im$ are the real and imaginary parts, respectively.
In addition, note that $\log{\lambda_j} = \ln{|\lambda_j|} + i\arg{\lambda_j}$, and
it can be said that the decay rates and temporal frequencies of the modes are given by the absolute values and arguments of the eigenvalues, respectively.
\subsubsection{Dynamical pattern in a single regime (P2)}
Thus, we have seen how to model the latent temporal dynamics of the $i$-th univariate inherent signal $\ith{\ind}$
using self-dynamics factor set $\ith{\selfdynamics}$.
Here, let us tackle the next question, namely
how can we describe
the major dynamical pattern (i.e., regime)
considering the \relation between observations in a data stream?
\noindent
We establish a model to combine the $d$ self-dynamics factor sets (i.e., $\first{\selfdynamics}, ..., \dth{\selfdynamics}$) for generating the estimated vector $\est(t)\in\R^d$ at time point $t$.
Also, we need a set of $d$ latent vectors (i.e., $\mat{S}(t) = \{ \ith{\latent}(t) \}_{i=1}^d$).
Consequently, we extend Model \ref{model:uni}, and the dynamical system for $d$-dimensional multivariate time series can be described with the following equations:

\begin{model}
    \label{model:multi}
    Let $\ith{\latent}(t)$ be the $\nmodes_i$-dimensional latent vector for the $i$-th univariate inherent signal $\ith{e}(t)$ at time point $t$,
    $\ind(t)$ be the $d$-dimensional inherent signals at time point $t$ (i.e., $\ind(t) = \{ \ith{e}(t) \}_{i=1}^d$), and
    $\est(t)$ be the $d$-dimensional estimated vector at time point $t$.
    The following equations govern the single regime,
    \begin{align}
        \label{eq:multivariate}
        \begin{split}
            % \frac{d\latent_i(t)}{dt} &= \ieig\latent_i(t) \\
            \ith{\latent}(t + 1) &= \ieig\ith{\latent}(t) \quad (1 \leq i \leq d) \\
            \ith{e}(t) &= \invembed{\imode\ith{\latent}(t)} \quad (1 \leq i \leq d) \\
            % \vect{v_i}(t) &= \sum_{i \neq j}b_{ij}\vect{v_j}(t) + \vect{e_i}(t)
            \vect{v}(t) &= \demixing^{-1}\ind(t)
        \end{split}
    \end{align}
\end{model}

\noindent In Model \ref{model:multi},
we require an additional parameter, demixing matrix $\demixing$,
which represents the relationships among $d$ inherent signals (i.e., $\first{\ind}, ..., \dth{\ind}$) and
is instrumental in identifying the \relation in data streams.
However, there are indeterminacies in a mixing matrix  (i.e., the inverse of a demixing matrix),
so it is not the same matrix as the causal adjacency matrix $\mB$. We present the algorithm for obtaining $\mB$ from $\demixing$ in Section \ref{section:alg:generator}.
Consequently, we have the following:
\begin{definition}[Single regime parameter set: $\regime$]
Let $\regime$ be a parameter set of a single regime, i.e., $\regime = \{ \demixing, \first{\selfdynamics}, ..., \dth{\selfdynamics} \}$, where $\demixing$ serves as the basis for generating the causal adjacency matrix $\mB$.
\end{definition}
\noindent Furthermore, we want to detect the transitions of regimes, which induce changes in causal relationships.
Let $R$ denote the optimal number of regimes up to the time point $t$.
Then, a data stream $\mX$ is summarized using a set of $R$ regimes (i.e., $\regime^1, ..., \regime^R$).
Consequently, a regime set for a data stream $\mX$ and \relation are defined as follows:
\begin{definition}[Regime set: $\regimeset$]
Let $\regimeset$ be a parameter set of multiple regimes, i.e.,$\regimeset = \{ \regime^1, ..., \regime^R \}$, which describes multiple distinct dynamical patterns in an entire data stream.
\end{definition}
\begin{definition}[\Relation: $\mathcal{B}$]
    Let $\mathcal{B}$ be a parameter set of causal adjacency matrices, i.e., $\mathcal{B} = \{ \mB^1, ..., \mB^R \}$, which describes time-changing of causal relationships. Note that $\mB^i$ is a causal adjacency matrix corresponding to the $i$-th regime $\regime^i$.
\end{definition}
