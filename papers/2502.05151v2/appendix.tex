% \documentclass[manuscript,screen,review]{acmart}

% \usepackage[capitalise]{cleveref}
% \usepackage{acronym}
% \usepackage{tcolorbox}
% \usepackage[size=tiny,disable]{todonotes}
% \usepackage[normalem]{ulem}
% \usepackage{xcolor}
% \usepackage{soul}
% %%% new commands
% %\newcommand{\mybox}[1]{\begin{tcolorbox}[colback=blue!10, colframe=blue, arc=3mm, %boxrule=0.5mm, width=\textwidth]
% %#1
% %\end{tcolorbox}}
% \definecolor{boxback}{HTML}{dde5ef}
% \definecolor{boxframe}{HTML}{4b72a6}
% \newcommand{\mybox}[1]{\begin{tcolorbox}[colback=boxback, colframe=boxframe, arc=2mm, boxrule=0.3mm, width=\textwidth]
% #1
% \end{tcolorbox}}
% %
% \newcommand{\limitations}[1]{\begin{tcolorbox}[colback=red!30, colframe=blue, arc=3mm, boxrule=0.5mm, width=\textwidth]
% #1
% \end{tcolorbox}}

% \usepackage{adjustbox}
% \usepackage{smartdiagram}
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage{booktabs}
% \usepackage{xcolor}
% \usepackage{colortbl}

% \usepackage[edges]{forest}
% \definecolor{hidden-draw}{RGB}{106,142,189} 
% \definecolor{hidden-blue}{RGB}{194,232,247} 
% \definecolor{hidden-orange}{RGB}{217, 232, 252} 
% \definecolor{layer-1}{HTML}{6a60a9}
% \definecolor{layer-2}{HTML}{a5d296}
% \definecolor{layer-3}{HTML}{FDD692}
% \definecolor{layer-4}{HTML}{6AAFE6}
% \definecolor{layer-5}{HTML}{FADAD8}
% \definecolor{search-attribute-AI}{HTML}{D8E6E7}
% \definecolor{search-attribute-Sum}{HTML}{f8ecc9}
% \definecolor{search-attribute-Citation}{HTML}{f4f7f7}
% \definecolor{search-attribute-Pers}{HTML}{d8e9ef}
% \newcommand{\badge}[2]{
% \begin{tikzpicture}
% \node[
%     draw, 
%     fill=#2, 
%     rounded corners=2pt, 
%     inner xsep=8pt, 
%     inner ysep=3pt, 
%     minimum height=15pt,
%     font=\sffamily
% ] {#1};
% \end{tikzpicture}
% }



% \usepackage{makecell} 
% \newcommand{\boldparagraph}[1]{\vspace{0.2cm}\noindent{\bf #1:} }
% \newcommand{\collaborators}[1]{\textcolor{red}{#1}}
% % was blue
% \newcommand{\se}[1]{\textcolor{black}{#1}}
% \usepackage{xspace}
% \newcommand{\eg}{e.g.,\xspace}

% \usepackage{jabbrv} % For ISO 4 abbreviations

% %% Rights management information.  This information is sent to you
% %% when you complete the rights form.  These commands have SAMPLE
% %% values in them; it is your responsibility as an author to replace
% %% the commands and values with those provided to you when you
% %% complete the rights form.
% \setcopyright{rightsretained}
% \copyrightyear{2025}
% \acmYear{2025}
% \acmDOI{XXXXXXX.XXXXXXX}

% %%
% %% Submission ID.
% %% Use this when submitting an article to a sponsored event. You'll
% %% receive a unique submission ID from the organizers
% %% of the event, and this ID should be used as the parameter to this command.
% %%\acmSubmissionID{123-A56-BU3}

% %%
% %% The majority of ACM publications use numbered citations and
% %% references.  The command \citestyle{authoryear} switches to the
% %% "author year" style.
% %%
% %%\citestyle{acmauthoryear}

% \begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\appendix

% \vspace*{0.5cm}
{\huge\bfseries Appendix}
\vspace{0.5cm}


% \maketitle
\renewcommand{\thesection}{A.\arabic{section}}  

\title[Appendix]
{Appendix}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

% Order of authors to be determined later.  Possible orderings:

% 1. Greatest to least contribution
% 2. Alphabetical order
% 3. Least to most seniority

% \author{Steffen Eger}
% \email{steffen.eger@utn.de}
% \orcid{0000-0003-4663-8336}
% \affiliation{%
%   \institution{University of Technology Nuremberg (UTN)}
%   \city{Nuremberg}
%   \country{Germany}
% }

% \author{Yong Cao}
% \email{yong.cao@uni-tuebingen.de}
% \orcid{0000-0002-3889-0382}
% \affiliation{%
%   \institution{University of Tübingen, Tübingen AI Center}
%   \city{Tübingen}
%   \country{Germany}
% }



% % \author{Christine Bauer}
% % \orcid{0000-0001-5724-1137}
% % \email{christine.bauer@plus.ac.at}
% % \affiliation{%
% %   \institution{Paris Lodron University Salzburg}
% %   \department{Department of Artificial Intelligence and Human Interfaces}
% %   \city{Salzburg}
% %   \country{Austria}
% % }

% \author{Jennifer D'Souza}
% \email{jennifer.dsouza@tib.eu}
% \orcid{0000-0002-6616-9509}
% \affiliation{%
%   \institution{TIB Leibniz Information Centre for Science and Technology}
%   \city{Hannover}
%   \country{Germany}
% }


% \author{Andreas Geiger}
% \email{a.geiger@uni-tuebingen.de}
% \orcid{0000-0002-8151-3726}
% \affiliation{%
%   \institution{University of Tübingen, Tübingen AI Center}
%   \city{Tübingen}
%   \country{Germany}
% }


% \author{Christian Greisinger}
% \email{christian.greisinger@utn.de}
% \orcid{}
% \affiliation{%
%   \institution{University of Technology Nuremberg (UTN)}
%   \city{Nuremberg}
%   \country{Germany}
% }
% \author{Stephanie Gross}
% \email{stephanie.gross@ofai.at}
% \orcid{0000-0002-9947-9888}
% \affiliation{%
%   \institution{Austrian Research Institute for Artificial Intelligence}
%   \city{Vienna}
%   \country{Austria}
% }


% \author{Yufang Hou}
% \email{yufang.hou@it-u.at}
% \orcid{0000-0003-2897-6075}
% \affiliation{%
%   \institution{IT:U Interdisciplinary Transformation University Austria}
%   \city{Linz}
%   \country{Austria}
% }
% \author{Brigitte Krenn}
% \email{brigitte.krenn@ofai.at}
% \orcid{0000-0003-1938-4027}
% \affiliation{%
%   \institution{Austrian Research Institute for Artificial Intelligence}
%   \city{Vienna}
%   \country{Austria}
% }

% \author{Anne Lauscher}
% \email{anne.lauscher@uni-hamburg.de}
% \orcid{0000-0001-8590-9827}
% \affiliation{%
%   \institution{University of Hamburg}
%   \city{Hamburg}
%   \country{Germany}
% }


% \author{Yizhi Li}
% \email{yizhi.li-2@manchester.ac.uk}
% \orcid{0000-0002-3932-9706}
% \affiliation{%
%   \institution{University of Manchester}
%   \city{Manchester}
%   \country{United Kingdom}
% }
% \author{Chenghua Lin}
% \email{chenghua.lin@manchester.ac.uk}
% \orcid{0000-0003-3454-2468}
% \affiliation{%
%   \institution{University of Manchester}
%   \city{Manchester}
%   \country{United Kingdom}
% }
% \author{Nafise Sadat Moosavi}
% \email{n.s.moosavi@sheffield.ac.uk}
% \orcid{0000-0002-8332-307X}
% \affiliation{%
%   \institution{University of Sheffield}
%   \city{Sheffield}
%   \country{United Kingdom}
% }



% \author{Wei Zhao}
% \email{wei.zhao@abdn.ac.uk}
% \orcid{0000-0001-7249-0094}
% \affiliation{%
%   \institution{University of Aberdeen}
%   \city{Aberdeen}
%   \country{United Kingdom}
% }


% \author{Tristan Miller}
% \email{Tristan.Miller@umanitoba.ca}
% \orcid{0000-0002-0749-1100}
% \affiliation{%
%   \institution{University of Manitoba}
%   \city{Winnipeg}
%   \state{Manitoba}
%   \country{Canada}
% }


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Eger et al.}


%%
%% The abstract is a short summary of the work to be presented in the
%% article.
% \begin{abstract}
%   %Abstract goes here
% With the advent of large multimodal language models, science is now at a threshold of an AI-based technological transformation. Recently, a plethora of new AI models and tools have been proposed, promising to empower researchers and academics worldwide to conduct their research more effectively and efficiently. This includes all aspects of the research cycle, especially (1) searching for relevant literature; (2) generating research ideas and conducting experimentation; generating (3) text-based and (4) multimodal content (e.g., scientific figures and diagrams); and (5) AI-based automatic peer review. In this survey, we provide an in-depth overview over these recent advances, which promise to fundamentally alter the scientific research process for good. Our survey covers the five aspects outlined above, indicating relevant datasets, methods and results (including evaluation) as well as limitations and scope for future research. Ethical concerns regarding shortcomings of these tools and potential for misuse (fake science, plagiarism, harms to research integrity) take a particularly prominent place in our discussion. We hope that our survey will not only become a reference guide for newcomers to the field but also a catalyst for new AI-based initiatives in the area of ``AI4Science''.
% \end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

% %%
% %% Keywords. The author(s) should pick words that accurately describe
% %% the work being presented. Separate the keywords with commas.
% %\keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
% %  Your, Paper}
% \keywords{Language Language Models, Science, AI4Science, Search, Experimentation, Idea Generation, Multimodal Content Generation, Evaluation, Peer Review}

% \iffalse COMMENT THIS BACK IN FOR JOURNAL SUBMISSION
% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}
% \fi
% \todo{SE: Comment some things back in for journal submission}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.


This appendix provides supplementary materials intended to support and extend the main text. It includes a background overview, further elaboration on AI support for specific topics and tasks, and a section on AI use cases that illustrates how AI tools were integrated into the workflow and phrasing of this paper.

% \section{Appendix}

\input{background}


\section{Supplement on AI Support for Specific Topics and Tasks}

\subsection{Additional Literature Search, Summarization, and Comparison}
\label{ax:search_engine}

\paragraph{Search engines}

\begin{table}[h!]
\vspace{1.5cm}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lllll|llll|lllll|lllll|ll}
\multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{c}{\textbf{Platform}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Search}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Recommendations}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Collections}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Citation Analysis}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Trending Analysis}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Author Profiles}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Visualization Tools}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Paper Chat}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Idea Generation}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Paper Writing}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Summarization}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Paper Review}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Datasets}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Code Repositories}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{LLM Integration}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Web API}}}} & \multicolumn{1}{c}{\rotatebox[origin=l]{45}{\makebox[0pt][l]{\textbf{Personalization}}}} & \multicolumn{1}{c}{\textbf{Cost}} & \multicolumn{1}{c}{\textbf{Data Source}} \\ \midrule
\multirow{12}{*}{\centering \rotatebox[origin=c]{90}{\textbf{Search Engines}}} & \href{https://scholar.google.com}{Google Scholar} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ &  & $\checkmark$ &  &  &  &  &  &  &  &  &  &  & $\checkmark$ & Free &  \\ 
 & \href{https://www.semanticscholar.org}{Semantic Scholar} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ &  & $\checkmark$ &  &  & $\checkmark$ &  &  &  & $\checkmark$ & $\checkmark$ & $\checkmark$ & Free & 214 million \\ 
 & \href{https://xueshu.baidu.com}{Baidu Scholar} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ &  &  &  &  &  &  &  &  & $\checkmark$ &  & $\checkmark$ & Freemium & 680 million \\ 
 & \href{https://www.base-search.net}{BASE} & $\checkmark$ &  & $\checkmark$ &  &  &  &  &  &  &  &  &  &  &  &  & $\checkmark$ &  & Free & 415 million \\ 
 & \href{https://scholar.archive.org/}{Internet Archive Scholar} & $\checkmark$ &  &  &  &  &  &  &  &  &  &  &  &  &  &  & $\checkmark$ &  & Free & 35 million \\ 
 & \href{https://www.scilit.net}{Scilit} & $\checkmark$ &  & $\checkmark$ & $\checkmark$ &  & $\checkmark$ &  &  &  &  &  &  &  &  &  &  &  & Free & 172 million \\ 
 & \href{https://www.lens.org/}{The Lens} & $\checkmark$ &  & $\checkmark$ &  &  & $\checkmark$ &  &  &  &  &  &  &  &  &  & $\checkmark$ &  & Freemium & 284 million \\ 
 & \href{https://science.gov}{Science.gov} & $\checkmark$ &  &  &  &  &  & $\checkmark$ &  &  &  &  &  &  &  &  &  &  & Free & several million \\ 
 & \href{https://www.academia.edu/}{Academia.eu} & $\checkmark$ &  & $\checkmark$ &  &  & $\checkmark$ &  &  &  &  &  &  &  &  &  &  &  & Freemium & 55 million \\ 
 & \href{https://openalex.org/}{OpenAlex} & $\checkmark$ &  &  &  &  & $\checkmark$ &  &  &  &  &  &  &  &  &  & $\checkmark$ &  & Freemium &  \\ 
 & \href{https://www.acemap.info/}{AceMap} & $\checkmark$ &  &  & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ &  &  &  &  &  & $\checkmark$ &  &  &  &  & Free & 260 million \\ 
 & \href{https://www.ncbi.nlm.nih.gov/research/pubtator3/tutorial}{PubTator3} & $\checkmark$ &  & $\checkmark$ & $\checkmark$ &  &  &  &  &  &  &  &  &  &  &  & $\checkmark$ &  & Free & 6 million \\ \midrule
\multirow{4}{*}{\centering \rotatebox[origin=c]{90}{\textbf{Benchm.}}} & \href{https://portal.paperswithcode.com/}{Papers with Code} & $\checkmark$ &  &  &  &  &  &  &  &  &  &  &  & $\checkmark$ & $\checkmark$ &  &  &  & Free & 154 thousand \\ 
 & \href{https://github.com/OSU-NLP-Group/ScienceAgentBench}{ScienceAgentBench} &  &  &  &  &  &  &  &  &  &  & $\checkmark$ &  & $\checkmark$ & $\checkmark$ & $\checkmark$ &  &  & Free &  \\ 
 & \href{https://orkg.org/benchmarks}{ORKG Benchmarks} &  &  &  &  & $\checkmark$ &  & $\checkmark$ &  &  &  &  &  & $\checkmark$ &  &  &  &  & Free &  \\ 
 & \href{https://huggingface.co/}{Huggingface} & $\checkmark$ &  & $\checkmark$ &  & $\checkmark$ &  &  &  &  &  &  &  & $\checkmark$ & $\checkmark$ &  &  &  & Freemium &  \\ 
\end{tabular}%
}
\vspace{0.01cm}
\caption{Overview of popular literature search, summarization and comparison tools and their key features.}
\label{tab:ax_google_scholar}
\end{table}


Traditional academic search engines such as \href{https://scholar.google.com}{Google Scholar}, \href{https://www.semanticscholar.org}{Semantic Scholar}, \href{https://xueshu.baidu.com}{Baidu Scholar}, \href{https://science.gov}{Science.gov}, and \href{https://www.base-search.net}{BASE}, as shown in Table~\ref{tab:ax_google_scholar}, are characterized by their broad literature coverage, citation tracking capabilities, and keyword-based search functionality. Their primary advantages include extensive indexing of scholarly content, which involves aggregating and organizing vast amounts of academic documents from various sources such as publisher websites, institutional repositories, and open-access archives. This comprehensive indexing spans multiple disciplines and document types, ensuring that users can access a diverse set of resources. Additionally, these platforms offer robust citation analysis features that allow researchers to track citation counts, measure the impact of publications, and explore citation networks to identify influential works and emerging trends within a given field. Another significant advantage is their free access to a wide range of academic resources, such as peer-reviewed journal articles, conference papers, preprints, theses and dissertations, technical reports, books and book chapters, as well as grey literature like white papers, government reports, and institutional research outputs. However, these search engines have certain limitations, such as limited AI-driven filtering options and relatively basic relevance ranking mechanisms compared to more advanced AI-enhanced search tools. 

\paragraph{Benchmarks and leaderboards}

Code and Dataset-Focused Search Engines include platforms such as \href{https://portal.paperswithcode.com/}{Papers with Code}, \href{https://github.com/OSU-NLP-Group/ScienceAgentBench}{ScienceAgentBench}, and \href{https://huggingface.co/}{Huggingface}, which are specifically designed to bridge the gap between academic publications and practical implementation by linking research papers with associated code and datasets. These platforms facilitate reproducibility and practical application of research findings by aggregating code repositories, enabling researchers and practitioners to easily explore implementations, compare results, and benchmark their models. A key feature of such platforms is their ability to provide dataset discovery tools, which allow users to identify relevant datasets for specific research problems, fostering collaboration and accelerating experimentation cycles. These search engines are particularly valuable for machine learning practitioners, as they facilitate quick access to ready-to-use codebases, helping them implement cutting-edge research more efficiently. Based on these community-curated leaderboards, some studies have proposed models for constructing leaderboards directly from scientific papers \cite{hou-etal-2019-identification,kardas-etal-2020-axcell,sahinuc-etal-2024-efficient}. 

\clearpage

\subsection{Extended AI-Driven Scientific Discovery: Ideation, Hypothesis Development, and Experimentation}
\label{ax:scientific_discovery}
Fig. \ref{fig:hypotheses_idea_experimentation_overview} provides a broad methodological overview, grouping methods applied in hypothesis generation, idea generation, and automated experimentation. In contrast, Fig. \ref{fig:hypotheses_idea_experimentation_overview_new} presents concrete examples and references to exemplary papers.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{image/automatic_experimentation_old.pdf} 
  \caption{Visualization of the hypothesis generation, idea generation, and automated experimentation process. Most works in hypothesis generation focus on reducing hallucinations, handling long contexts, and iteratively refining outputs. To reduce hallucinations, an initial hypothesis is validated against a knowledge base for refinement. For long-context inputs, different contexts are summarized and integrated, while refinement strategies iteratively improve the hypothesis until it meets a satisfactory level. A similar iterative refinement strategy is also applied to idea generation. Additionally, alignment strategies are employed to make generated ideas more thoughtful and feasible. In multi-agent systems, multiple agents collaborate to enhance the idea generation process. In contrast, automated experimentation often relies on tree search for selecting optimal examples, multi-agent workflows where LLMs collaborate on distinct tasks, and iterative refinement to improve task performance. While hypothesis and idea generation leverage diverse sources such as scientific literature, web data, and datasets, automated experimentation operates on predefined ideas and requires access to computational models, simulations, and raw data.}
  \label{fig:hypotheses_idea_experimentation_overview}
\end{figure*}

\clearpage

\subsection{Text-based Content Generation} 
\label{ax:content_generation}

In this section, we provide an additional figure (Fig.~\ref{fig:ax_content_generation_overview}) to illustrate the content generation process for academic papers, covering title, abstract, related work, and bibliography generation with their respective methods.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{image/content_generation.pdf} 
  %\caption{Visualization of the content generation process for academic papers. Title generation methods focus on abstract-to-title, paper content-to-title, and future work-to-title mappings. For abstract generation, existing approaches typically involve title-to-abstract and keywords-to-abstract techniques. Related work generation is divided into extractive methods, which extract and reorder sentences, and abstractive methods, which rewrite and reconstruct sentences or paragraphs from multiple papers. Bibliography generation is categorized into non-parametric methods, which retrieve references from external sources and parametric methods where LLMs generate references solely based on its preexisting knowledge without relying on any retrieval mechanisms. The former is splitted into pre-hoc and post-hoc. For pre-hoc, before generating a text, it will first be determined if a citation is required or not. If it is required, the reference is retrieved and afterwards the final text generated. For post-hoc, the text ist first written by an LLM and afterwards it will be checked if a citation is needed or not. It the citation is required, the reference is retrieved from a source and appended to the already generated text.} %parametric methods, where an LLM generates references based on preexisting knowledge, and non-parametric methods, which retrieve sources. Citation retrieval can occur post-hoc, after content generation, or in a preemptive manner, determining the need for citations before content creation.}
  \caption{Visualization of the content generation process for academic papers. Title generation methods include abstract-to-title, content-to-title, and future work-to-title mappings. Abstract generation typically involves title-to-abstract and keywords-to-abstract techniques. Related work generation follows either extractive methods (reordering extracted sentences) or abstractive methods (rewriting content from multiple papers). Bibliography generation is categorized into non-parametric methods (retrieving references from external sources) and parametric methods (LLMs generating references from preexisting knowledge without retrieval). Non-parametric methods are further divided into pre-hoc (determining citation needs before text generation and retrieving references beforehand) and post-hoc (checking for citations after text generation and appending retrieved references as needed).}
  \label{fig:ax_content_generation_overview}
\end{figure*}


\subsection{Multimodal content generation and understanding: additional information}
\label{ax:multimodal_generation}

\input{topics/multimodal_table2}


\subsubsection{Methods and results.} In the following, we provide a summary of representative approaches for multimodal content generation and understanding in Table \ref{tab:section4.4_Method}, illustrate the process of scientific figure generation in Fig. \ref{fig:figure_generation_overview}, and present an extended description of scientific slide and poster generation.

% \paragraph{Scientific figure understanding.} Scientific figure understanding is typically framed in terms of (visual) QA, e.g., whether models are able to adequately answer questions on a given input figure. %Early work in this direction considers science school diagrams \citep{} 
% As an early work in this direction, \citet{10.1007/978-3-319-46493-0_15} consider answering questions on science school diagrams, where they `syntactically parse' the input diagram with LSTMs and leverage an attention-based model for diagram question answering. %They also provide a dataset with over 5k richly annotated diagrams and over 15k questions and answers. %\citet{10.1007/s00799-022-00329-y}
% \citet{ebrahimi2018figureqa} %introduce FigureQA, a synthetic dataset of over 100k scientific-style figures, training 
% train 
% baseline models such as \cite{NIPS2017_e6acf4b0} on the FigureQA benchmark, finding them to substantially struggle with the task. %from five classes: line plots, dot-line plots, vertical and
% %horizontal bar graphs, and pie charts. Associated with the images are more than 1m questions that are generated using 15 different templates. 
% %More recently, 
% %Later research focuses on harder and more realistic QA pairs as the templated questions may lead to an overly optimistic assessment of the capabilities of recent models. 
% \citet{masry-etal-2022-chartqa} %present ChartQA, which provides complex reasoning questions over charts sourced from various sources related to science such as statista.com and pewresearch.org, which address social and economic issues. They 
% leverage transformer based architectures, demonstrating their limitations in answering complex reasoning questions.
% \citet{wang2024charxiv} explore multiple proprietary and open-source models for their benchmark %introduce 
% CharXiv. %, a chart comprehension benchmark (where charts are particularly relevant in science or the financial domain). Their main contribution is a manually curated dataset of 2.3k ``natural, challenging, and diverse'' charts from %scientific 
% %Arxiv 
% %papers and %(involving 
% %both descriptive and reasoning questions for them. 
% They 
% show a big gap between proprietary models like GPT4o and the strongest open-source models and a big gap of all models to human performance. 
% \citet{li-etal-2024-multimodal-arxiv} %introduce ArxivQA, a dataset of 35k scientific figures sourced from Arxiv for which GPT4o generates 100k QA pairs after manual filtering. 
% likewise explore multiple recent LLMs on their benchmark ArxivQA, finding  
% %The authors show 
% that recent open-source %LLMs 
% models such as Qwen do not perform well on the benchmark but can be improved by fine-tuning on it. 
% %In contrast to focusing on question-answering for scientific figures, \citet{xu2024chartadapterlargevisionlanguagemodel} consider the chart summarization problem and a datset comprising more than 190k instances building on top of existing datasets such as ChartSumm \citep{Rahman2023ChartSummAC}. 
% %\citet{}
% For chart summarization, \citet{Rahman2023ChartSummAC} find that older PLMs such as BART and T5 suffer from hallucination and missing out of important data points. \citet{xu2024chartadapterlargevisionlanguagemodel} propose ChartAdapter, a lightweight transformer module that can be combined with LLMs for improved modeling of chart summarization. %\citet{wang2024charxiv} use GPT4o for automatic evaluation of answers, due to the variety of possible correct answers for questions. 
%  % Rahman2023ChartSummAC, xu2024chartadapterlargevisionlanguagemodel
% \textbf{Evaluation} of scientific figure understanding benchmarks %likewise leverages both humans and 
% mostly leverages 
% automatic metrics. For example, \citet{xu2024chartadapterlargevisionlanguagemodel} report out-dated and unreliable metrics such as BLEU and ROUGE for evaluating chart summaries; %casting some doubts on their findings; 
% \citet{pramanick2024spiqa} report both human and automatic evaluation, using traditional QA metrics such as Meteor, Rouge, and BERTScore and novel LLM based metrics. 

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{image/figure_generation.pdf} 
  \caption{Overview of the scientific figure generation process. Various input types including sketches, screenshots, and text, can be used to generate TikZ code with tools such as AutomaTikZ \cite{belouadi2024automatikz} and DeTikZify \cite{belouadi2024detikzify}. The generated code is then rendered into high-quality vector graphics images.}
  \label{fig:figure_generation_overview}
\end{figure*}

% \paragraph{Scientific figure generation.}

% %Scientific figure generation is %recently 
% %a nascent field of research, despite im. 
% %While early research 
% Early work in the context of visualization for science (and beyond) dates back to the 1980s and 1990s at least \citep{mackinlay1986automating,roth1991automating,roth1994interactive}. %From the start, these papers articulate diversity and inclusion of the user base as well as cost reduction as motivation for such tools. 
% For instance, \citet{mackinlay1986automating} design APT (``a presentation tool''), which lets users specify queries over databases (``Present the Price and Mileage Relation'') which are then supposed to be rendered in a graphics language (which can be converted into an image). To achieve this, \citet{mackinlay1986automating} use the ingredients of the time, such as logic programming, including rules and heuristics, for synthesis. %of a formal graphics language.
% \citet{roth1991automating,roth1994interactive} develop SAGE, a multi-component tool consisting of a three-level process of selection, refinement and integration with a similar purpose, extending APT. Later research such as \citet{sun2014articulate,narechania2020nl4dv,10.1145/3534678.3539330} further develop rule-based  or hybrid approaches, involving parsers and grammars, while custom neural architectures were developed in \citet{8744242,liu2021advisor,10.1145/3514221.3520150}, including RNNs, transformers such as BERT or encoder-decoder architectures. The majority of those papers have originated in the visualization community and take (i) a database or dataframe and a natural language query as input and (ii) output a `code' in a graphics language, which can be rendered as an image. 

% %Later research \citep{sun2014articulate}
% %\citet{8744242} leverage RNN-based encoder-decoder networks to learn to transform JSON datasets into graphical representations using the Vega-Lite graphics language. 

% % https://arxiv.org/abs/2412.20715
% % ChartAdapter: Large Vision-Language Model for Chart Summarization

% Most recently and with the advent of %large multimodal language models, 
% multimodal LLMs, 
% scientific figure generation has also been targeted in the NLP community. %For example, 
% While \citet{10121440} explore diverse pre-trained LLMs, such as ChatGPT and GPT3, %for scientific figure generation. 
% \citet{voigt2024plots} investigate smaller LLMs for real-time graphics generation on a CPU. 
% %with the advent of large language models, 
% \citet{belouadi2024automatikz} %,belouadi2024detikzify,voigt2024plots} 
% treat the problem as a TikZ code generation problem where the input is a scientific caption, disregarding dataframes or databases as input. %output is code, such as TikZ or Vega\footnote{\url{https://vega.github.io/vega-lite/}}, 
% %and the input is (i) an (extended) scientific caption or instruction (\textit{text-to-figure generation}) or (ii) an image or sketch (\textit{figure-to-figure generation}). %
% \citet{belouadi2024detikzify} treat the problem as image-to-code generation where the input is a raster image or sketch. 
% Methodologically, \citet{belouadi2024automatikz} use  LLM fine-tuning on 100k+ pairs of input captions and output code, obtained from Arxiv submissions, and \citet{belouadi2024detikzify} additionally use Monte-Carlo Tree Search (MCTS) to guide the generation process. The workflow of image/caption-to-code generation is visualized in Figure \ref{fig:figure_generation_overview}. \citet{shi2024chartmimicevaluatinglmmscrossmodal} %also 
% aim to generate %output 
% Python code %(which can be converted into an image) 
% from instructions and/or images, specifically focusing on charts as important subproblem of scientific images. %\todo{SE: add methods} 
% %Their main contribution is that of a manually curated benchmark of 1000 triplets of (figure, instruction, code) instances on which 
% They evaluate 3 proprietary and 11 open-weight LLMs on their ChartMimic benchmark, finding that even the best models (GPT-4 and Claude-3-opus) have substantial room for improvement. 
% \citet{zhang2024scimagegoodmultimodallarge} provide a template based approach to evaluate various multimodal LLMs in generating scientific images, focusing on different comprehension dimensions (spatial, numeric and attribute understanding) for a targeted assessment of models. For evaluation, they use human experts across three evaluation dimensions (correctness, relevance, scientificness). They explore LLMs that can generate code (TikZ and Python) and ones that directly generate images, without intermediate code synthesis and in addition consider different input languages (English, German, Chinese, Farsi). They find that, except for GPT4o, most models struggle substantially in generating adequate scientific images and that code-generation models have more problems with the spatial dimension while direct image generation models have more problem with numeric comprehension. %\todo{SE: mention focus on targeted evaluation}
% % 
% \citet{Zala2023DiagrammerGPT} %\todo[inline]{SE: [diagrams; We introduce the AI2D-Caption dataset for the text-to-diagram generation task. AI2D-
% %Caption is built on top of AI2 Diagrams (AI2D) dataset (Kembhavi et al., 2016), which
% %provides annotations of around 4.9K diagrams covering diverse scientific domains, from
% %Grade 1-6 science textbook]}
% explore the diagram generation task where LLMs first generate diagram plans and then the diagrams themselves. The generated diagrams cover diverse scientific domains from Grade 1-6 science textbooks. \citet{mondal-etal-2024-scidoc2diagrammer} explore the same task with an additional refinement (feedback from multiple critic models) to enhance factual correctness. 
% % 
% Inspired by the success of code generation, several papers have adopted the approaches for domains not primarily related to science, such as generating everyday multimodal objects from code \citep{rismanchian2024turtlebench,wu2024chat2svgvectorgraphicsgeneration,zou2024vgbench,rodriguez2024starvectorgeneratingscalablevector}.  
% % 
% %\citet{rismanchian2024turtlebench} [only related to science, though]
% %
% %\citet{wu2024chat2svgvectorgraphicsgeneration} [not scientific domain primarily, though]
% %
% %\citet{zou2024vgbench} [not primarily science domain though]
% %
% %\todo{SE: @SE: add evaluation and remove overlap with the dataset section!!}
% \textbf{Evaluation} of the models comprises automatic metrics including  DreamSim \citep{10.5555/3666122.3668330}, for image similarity, crystal Bleu \citep{10.1145/3551349.3556903} for code similarity and ClipScore \citep{hessel-etal-2021-clipscore} for text-image similarity, and human evaluation by `domain experts'. The former are typically reported to have low or medium correlation with the latter, establishing the need for domain specific evaluation approaches in future approaches.

% \paragraph{Scientific Table Understanding.} \emph{Structure-aware methods} explicitly model the inherent relationships and hierarchies within tables to enhance reasoning and generation fidelity. These include (a) \emph{Intermediate Representations}: Techniques such as LoFT (Logic Form Transformer) \citep{zhao-etal-2023-loft} and SORTIE \citep{zhao-etal-2023-sortie} transform tables into logical forms or programming language-based formats to guide reasoning and ensure factual accuracy. Graph-based approaches %, such as those by 
% \citep{10.1162/tacl_a_00641} %, 
% convert tables into graph structures, capturing dependencies and enabling relational reasoning for more contextually enriched text generation. (b) \emph{Structure-Aware Pretraining}: Pretraining strategies incorporate table-specific tasks to refine LLMs by integrating datasets tailored for numerical and logical reasoning \citep{petrak-etal-2023-arithmetic,korkmaz-del-rio-chanona-2024-integrating,10707812}. (c) \emph{Structure-Aware Self-Attention Mechanisms}: These mechanisms adapt the attention process to explicitly capture relational and hierarchical table structures. For example, graph-based attention mechanisms prune self-attention into order-invariant graph attention, capturing connections within rows and columns while maintaining robustness to structural transformations \citep{wang-etal-2022-robust}. Similarly, prefix-tuning \citep{10.1145/3622896.3622919} attaches continuous prompts to encode structural nuances, guiding pretrained models to better align with table-specific features.

\paragraph{Scientific slide and poster generation.}


For scientific slide generation, early works 
%in the design of scientific slide generation systems 
typically relied on heuristic rule-based approaches. For instance, \citet{Sravanthi2009SlidesGenAG} develop a rule-based system to generate slides for each section and subsection of a paper. The slide text content is generated using a query-based extractive summarization system. %The authors also extract graphical elements from the paper and place them at appropriate locations within the slides.  
Later, researchers began to leverage machine learning approaches to extract key phrases and their corresponding important sentences. \citet{Hu2013PPSGenLT} use a Support Vector Regression (SVR) model to learn the importance of each sentence in a paper. The slides are then generated using an integer linear programming (ILP) model to select and align key phrases and sentences. \citet{Wang2017PhraseBasedPS} propose a system to generate slides for each section of a given paper, focusing on creating two-layer bullet points. The authors first extract key phrases from the paper using %the Stanford Sentence Parser 
a parser 
and then use a random forest classifier to predict the hierarchical relationships between pairs of phrases. %\todo{SE: how can I generate a slide from keyphrases? That's probably not an image, but a sort of bullet point? -- YH: some work only generate text content; others generate text content + extract imanges/tables from the paper; no work explore to generate an image beyond the existing imanges from the paper, this could be a nice future research direction. SE: perhaps worth mentioning somewhere? (here or in limitations). Also text only approaches are still multimodal? - YH: 1) it's reflected in the future direction section; 2)text bullet points are presented in the format of multiple slides, so it should be counted as multimodal if we count slides as another modality?}
\citet{Li2021TowardsTS} develop two sentence extractors—a neural-based model and a log-linear model—within a mutual learning framework to extract relevant sentences from papers. These sentences are used to generate draft slides for four topics: \emph{Contribution}, \emph{Dataset}, \emph{Baseline}, and \emph{Future Work}.  

It is important to note that all the aforementioned works focus on extracting sentences or phrases from the given paper to serve as the slide text content. In contrast, \citet{Fu2021DOC2PPTAP} and \citet{sun-etal-2021-d2s} take a different approach by training sequence-to-sequence models to generate sentences %\todo{SE: extractive vs. abstractive}
for the slide text content. This distinction is analogous to the difference between ``extractive'' and ``abstractive'' summaries in text summarization. More specifically, \citet{Fu2021DOC2PPTAP} design a hierarchical recurrent sequence-to-sequence architecture to encode the input document, including sentences and images, and generate a slide deck. In contrast, \citet{sun-etal-2021-d2s} assume that slide titles would be provided by end users. The authors 
use these titles to retrieve relevant and engaging text, figures, and tables from the given paper using a dense retrieval model. They then summarize  the retrieved content into bullet points with a fine-tuned long-form question answering system based on BART. %\todo{SE: which of these systems generate text vs. figures in slides? -- YH: I tried to explain this in the text for each paper, but this is a nice dimension for the summarized table or figure}


With recent advancements in %large language models (LLMs) 
LLMs and vision-language models (VLMs), researchers have started utilizing these technologies for generating scientific presentation slides. \citet{mondal-etal-2024-presentations} propose a system to generate persona-aware presentation slides by fine-tuning LLMs such as \emph{text-davinci-003} and \emph{gpt-3.5-turbo} with a small training dataset containing personalized slide decks for each paper. \citet{maheshwari-etal-2024-presentations} focus solely on generating text content and develop an approach that combines graph neural networks (GNNs) with LLMs to capture non-linearity in presentation generation, while attributing source paragraphs to each generated slide within the presentation. \citet{bandyopadhyay-etal-2024-enhancing-presentation1} design a bird’s-eye view document representation to generate an outline, map slides to sections, and then create text content for each slide individually using LLMs. The approach then extracts images from the original papers by identifying text-image similarity in a shared subspace through a VLM. %\todo{SE: so regarding images, all of these approaches are extractive, right? -- YH: yes}

Generating posters from scientific papers has received less attention compared to scientific slide generation. \citet{Qiang2016LearningTG} introduce a graphical model to infer key content, panel layouts, and the attributes of each panel from data. \citet{Xu2022PosterBotAS} develop a demo system for automated poster generation. The system first identifies important sections using a trained classifier. It then employs a summarization model to extract key sentences and related graphs from each section to construct corresponding panels. Finally, the system generates a LaTeX document for the poster based on the template selected by the user.



\section{AI use case and abbreviations}
\label{ax:ai_usage}

% \todo{YC: I'll rephrase this paragraph.}
%Optional: describe which portions of your section (figures, tables, text, etc.) have been assisted by AI and how.


Throughout this paper, we have integrated AI tools to support specific aspects of the research workflow. For example, in the subsection of \textit{Literature Search, Summarization, and Comparison}, we used Google Search, ChatGPT, NotebookLM, and Scholar Inbox to retrieve relevant tools and related work. Additionally, LLMs assisted with grammar and spell checking, as well as generating code for formatting tables.


The abbreviations used in our paper are summarized in Table \ref{tab:acronyms}.


\begin{table}[h]
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Acronym} & \textbf{Full Name} \\
        \midrule
        AI4Research & Towards a Knowledge-grounded Scientific Research Lifecycle \\
        AISD & AI \& Scientific Discovery \\
        CoI & Chain of Ideas \\
        CV & Computer Vision \\
        FM4Science & Foundation Models for Science \\
        GNNs & Graph Neural Networks \\
        ILP & Integer Linear Programming \\
        KGs & Knowledge Graphs \\
        LLMs & Large Language Models \\
        LSTM & Long Short-Term Memory Networks \\
        ML & Machine Learning \\
        MSE & Mean-Square Error \\
        MTEB & Massive Text Embedding Benchmark \\
        NLP & Natural Language Processing \\
        NSLP & Natural Scientific Language Processing and Research Knowledge Graphs \\
        QA & Question Answering \\
        RAG & Retrieval-Augmented Generation \\
        S2ORC & Scholar Open Research Corpus \\
        SVR & Support Vector Regression \\
        TFR & Text-Figure Relevance \\
        VLMs & Vision-Language Models \\
        \bottomrule
    \end{tabular}
    \caption{List of Acronyms and Their Full Names} 
    \label{tab:acronyms}
\end{table}

% \section{Ethical concerns for Specific Topics / Tasks}

% \paragraph{Literature search, summarization, and comparison}

% %Identify and discuss any ethical issues related to the (mis)use of the data or the application of the methods, as well as strategies for mitigations.

% The use of AI in scientific search, summarization and comparison raises ethical considerations, particularly in ensuring transparency, accountability, and equity. AI can significantly accelerate the pace of discovery, automate search tasks, and uncover patterns that may elude human researchers, but it also introduces risks and biases. %such as perpetuating biases present in training data, \todo{SE: can you avoid general ethical issues in this subsection and only focus on ones related to your search direction?}
% %undermining the integrity of scientific processes (\eg %\todo{SE: be sure to use `e.g.' correctly} 
% %authorship and credit assignment), and enabling the misuse of findings. 
% Existing dynamics such as the Matthew effect, where well-known researchers receive disproportionate attention, might be reinforced by the AI algorithms, intensifying inequalities. We believe that research should follow a human-centric approach, in which the human researcher is provided with advanced tools but remains fully responsible for executing the research and summarizing the results in research papers. It is also important to develop algorithms to reduce biases by recommending relevant work to researchers based on the \textit{content} of the research, independent of the popularity of the authors. Tools that are able to uncover gaps in the existing literature might even lead to a more uniform allocation of researchers to topics, reducing the bias towards overpopulated areas.

% \paragraph{Designing and conducting experiments; AI-based discovery}

% %\todo{Identify and discuss any ethical issues related to the (mis)use of the data or the application of the methods, as well as strategies for mitigations.}
% %The integration of AI into hypotheses generation, idea formation, and automated experimentation introduces significant ethical challenges. 
% AI-generated hypotheses may lack transparency, making it difficult to assess their validity or underlying assumptions, which could lead to flawed experiments. For example, an AI might identify a statistical correlation in its training data and propose  hypotheses without clearly revealing the underlying assumptions or data sources, making it difficult for researchers to verify its scientific soundness or hold anyone accountable if the hypotheses proves misleading. In the area of idea generation, there is a risk of reinforcing established research paradigms. AI systems trained on the basis of existing literature may favor popular paths and neglect underrepresented research directions. As a result, unconventional ideas may be unintentionally marginalized. %For example, an AI might repeatedly suggest incremental improvements in a dominant field rather than proposing entirely new lines of research, thereby limiting the diversity of scientific thinking. 
% Automated experimentation presents its own ethical challenges. The speed and volume in which AI can design and execute experiments can lead to insufficient ethical oversight and inadequate safety controls. Consider an AI system that suggests experimental protocols in biomedical research (e.g., chemical components with unknown toxicity) without the rigorous human review needed to identify potential risks. This could lead to experiments that pose unforeseen dangers or violate established ethical standards.


% \paragraph{Text-based content generation}

% %Identify and discuss any ethical issues related to the (mis)use of the data or the application of the methods, as well as strategies for mitigations.

% In scientific work, authorship and plagiarism in AI generated texts are major concerns. In general, it is a challenge to distinguish between AI generated and human generated texts. %Although there is a number of tools to detect AI-generated text (e.g., GPTZero or Hive), \citet{anderson2023ai} have shown that after applying automatic paraphrasing, the detection of human generated text using GPT-2 Output Detector increased, e.g., the probability of the text being generated by a human from 0.02\% to 99.52\%.
% Although there is a number of tools to detect AI-generated text (e.g., GPTZero or Hive), \citet{anderson2023ai} show that after applying automatic paraphrasing to AI generated text, the probability of a text to be human generated, increases. %todo{SE: repeated text}
% %identified by GPT-2 Output Detector to be written by a human, increased (e.g., from 0.02\% to 99.52\%).
% Therefore it is not possible to reconstruct if a text is an original work from a scientist or has been generated by an AI. 
% In addition, it is also found that ChatGPT generated texts easily pass plagiarism detectors \cite{else2023chatgpt,altmae2023artificial}. 
% Moveover, \citet{macdonald2023can} raise the concern that the frequent use of LLMs for drafting research articles might lead to similar paragraphs and structure of many papers in the same field. This again raises the question whether there should be a threshold for the acceptable amount of AI-generated content in scientific work \cite{macdonald2023can}.


% \paragraph{Multimodal content generation and understanding}

% %\todo{Identify and discuss any ethical issues related to the (mis)use of the data or the application of the methods, as well as strategies for mitigations.}
% Ethical concerns relating to models for figure, table, slide and poster generation especially include that these tools are technically limited (e.g., they may hallucinate content, be factually incorrect, and not correspond to the authors' intentions), which could be overlooked, ignored or maliciously abused by human authors. Such tools could also be misused to attack the scientific process, by purposefully producing incorrect results (e.g., as a testcase for adequate reviewing). Such risks may also be relevant in an educational context, e.g., when students use such tools for preparing term papers or theses.

% \paragraph{Peer review}

% %Identify and discuss any ethical issues related to the (mis)use of the data or the application of the methods, as well as strategies for mitigations.
% Given the critical role of scientific peer review for science, and, accordingly, for society as a whole, ethical considerations around AI-supported peer review are of utmost importance. As the general concerns around unfair biases in AI and the resulting harms apply~\cite{kuznetsov2024can}, research on safe peer-reviewing support needs to be prioritized. For instance, \citet{10.1001/jama.2023.24641} recently showed that %large language models 
% LLMs exhibit 
% %showed 
% affiliation biases when reviewing abstracts. In this context, any AI-support for peer reviewing needs to be critically evaluated~\cite{schintler2023critical}, and solutions that target only a particular aspect in a collaborative environment that leaves the scientific autonomy to the human expert, may need to be preferred over end-to-end reviewing systems.


% @article{Zhang2023ArtificialIF,
%   title={Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems},
%   author={Xuan Zhang and Limei Wang and Jacob Helwig and Youzhi Luo and Cong Fu and Yaochen Xie and Meng Liu and Yu-Ching Lin and Zhao Xu and Keqiang Yan and Keir Adams and Maurice Weiler and Xiner Li and Tianfan Fu and Yucheng Wang and Haiyang Yu and Yuqing Xie and Xiang Fu and Alex M Strasser and Shenglong Xu and Yi Liu and Yuanqi Du and Alexandra Saxton and Hongyi Ling and Hannah Lawrence and Hannes St{\"a}rk and Shurui Gui and Carl N. Edwards and Nicholas Gao and Adriana Ladera and Tailin Wu and Elyssa F. Hofgard and Aria Mansouri Tehrani and Rui Wang and Ameya Daigavane and Montgomery Bohde and Jerry Kurtin and Qiang Huang and Tuong Phung and Minkai Xu and Chaitanya K. Joshi and Simon V. Mathis and Kamyar Azizzadenesheli and Ada Fang and Al{\'a}n Aspuru‐Guzik and Erik J. Bekkers and Michael M. Bronstein and Marinka Zitnik and Anima Anandkumar and Stefano Ermon and Pietro Lio' and Rose Yu and Stephan Gunnemann and Jure Leskovec and Heng Ji and Jimeng Sun and Regina Barzilay and T. Jaakkola and Connor W. Coley and Xiaoning Qian and Xiaofeng Qian and Tess E. Smidt and Shuiwang Ji},
%   journal={ArXiv},
%   year={2023},
%   volume={abs/2307.08423},
%   XXXurl={https://api.semanticscholar.org/CorpusID:259937507}
% }

% \bibliographystyle{ACM-Reference-Format}
% \bibliography{2025_THAISCI_survey}

% \end{document}
