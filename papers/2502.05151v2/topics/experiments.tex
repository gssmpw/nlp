
%\subsection{Designing and Conducting Experiments; AI-based Discovery}
%\subsection{AI-Driven Idea Generation, Hypothesis Development, and Experimentation}
\subsection{AI-Driven Scientific Discovery: Ideation, Hypothesis Generation, and Experimentation}

\label{sec:experiments}

%\collaborators{Wei, Steffen, Christian}

%Provide a concise description of the task here, indicate why it is important, and provide any necessary background information/references to contextualize the following subsections.
\mybox{
% Designing and conducting scientific experiments deals with automatically generating research hypotheses and ideas and doing experiments (e.g., in computer science) automatically.
Idea formation, hypothesis generation, and experimentation are fundamental to scientific discoveries. Idea formation, particularly in AI, focuses on proposing new tools or benchmarking existing ones. Hypothesis formation involves formulating specific, testable questions that guide empirical or theoretical justifications.  Experimentation then tests hypotheses and evaluates ideas through systematic observation, data collection, and analysis. In AI research, this often includes benchmarking models, running simulations, or conducting ablation studies. Traditionally, these processes have been carried out by human researchers. However, in an age of rapidly growing scientific literature, efforts of moving from literature review to hypothesis and idea formation have become increasingly time-consuming. %for researchers of various disciplines given their limited capability in reviewing increasing body of related work, which potentially hinders scientific progress. 
Experimentation adds further complexity, requiring careful methodological design, large-scale simulations, and in-depth result analysis. %These complexities can slow down scientific progress, particularly when multiple iterations are required for refinement.
% Recently, there has been growing interest in the potential of LLMs to assist in hypothesis generation and idea formation, as they can efficiently process and synthesize vast amounts of literature. Beyond this, LLMs can be integrated with computational tools to enable automated experimentation where they assist in designing methodologies, conducting simulations, and analyzing results. The findings can then be used to iteratively refine hypotheses, creating feedback loops that could accelerate discovery. 
% In this section, we will provide an overview of AI-driven hypothesis generation, idea formation and experimentation. The review of each  will focus on its datasets, methods, results, limitations, ethical concerns, among others.
%In this section, we will provide an overview of how LLMs are leveraged to assist in hypothesis generation, idea formation and experimentation. The review of each will focus on its datasets, methods, results, limitations, ethical concerns, among others.
%Last but not least, given that AI-driven hypothesis, idea generation and evaluation becomes much more efficient, it is crucial for the research community to conduct a comprehensive evaluation of a large number of candidates, and further select high-quality candidates that are meaningful, relevant and potentially novel and worth experimentation and validation.
As AI-driven approaches accelerate hypothesis generation, idea formation, and evaluation, it is essential for the research community to assess a broad range of candidates, selecting those that are most meaningful, relevant, and potentially novel for further validation.

% While these advancements promise increased efficiency, a critical question remains: Can LLMs generate meaningful and novel hypotheses and ideas? Can they contribute effectively to experimentation in a way that leads to real scientific advancements? In the following, we explore these questions by reviewing related work, examining LLM-assisted research methods, evaluating benchmarking datasets, and assessing the quality of generated hypotheses, ideas, and experimental contributions.

% Additionally, we highlight concrete examples of LLM-proposed research directions and discuss their potential impact across disciplines.
%Hypothesis and idea formation processes are crucial steps in scientific research. Although both rely on literature review to identify knowledge gaps, their aims outlined in the literature are substantially different, leading to separate scientific quests. A well-defined scientific hypothesis is often said to be a very specific and testable question that inspires empirical or theoretical justifications []. For instance, Euclid`s parallel postulates are a very popular hypothesis in mathematics, which was proven to be violated in curved spaces, leading to the discovery of hyperbolic geometry []. Such hypotheses also exist in the core AI research community. A popular and recent example is the model scaling laws in Large Language Models that hypothesizes model performance scales with model size and the amount of training data \cite{kaplan2020scaling}. This hypothesis was justified and led to efforts towards developing larger LLMs. In contract, idea formation typically includes a proposal that outlines the ideas for developing new tools [] or benchmarking existing tools [], which is the mainstream of research in AI communities. 

%Typically, both processes are executed by human researchers. However, in an age of rapidly growing scientific output, efforts of moving from literature review to hypothesis and idea formation have become very time-consuming for researchers of various disciplines, given their limited capability in reviewing increasing body of related work, which potentially hinders scientific progress. 

%Recently, there has been a growing interest in the potential of %Large Language Models (LLMs) 
%LLMs 
%to assist in the formation of research hypotheses and ideas, given that LLMs can conduct literature review much more efficiently than human researchers. However, despite efficiency, are LLMs able to generate meaningful and potentially novel research hypotheses and ideas? In the following, we aim to address this question by reviewing related works, with a perspective of discussing methods (different uses of LLMs to do so), datasets (used for evaluating LLMs) and the evaluation results (the quality of generated hypotheses and ideas). Additionally, we will showcase hypotheses and ideas proposed by LLMs in related works, and discuss their potential to drive scientific discoveries in various disciplines. 
} %\todo{SE: I think this introductory part should be shorter and include all aspects relevant here, including experimentation}

\subsubsection{Data}

%\todo{Give an overview of the most important curated/annotated datasets, or sources of raw data, that are used (or potentially useful for) this task.}

% datasets (data statistics, setup) 
\begin{table*}[h]
\centering 
\footnotesize
\setlength\tabcolsep{4pt} 
% \begin{tabular}{lp{1.5cm}cp{2cm}ccc}
\begin{tabular}{lccccc}
\toprule
\textbf{Dataset} & \textbf{Source} & \textbf{Data Size} & \textbf{Domain} & \textbf{Time Span} & \textbf{Task}\\ 
\toprule
SciMON \cite{chai2024exploring} & ACL Anthology & 135,814 papers &  NLP   & 1952-2022 & Idea Generation\\ 
IDEA Challenge \cite{ege2023idea} & University of Bristol & 240 prototypes &  Engineering  & 2022 & Idea Generation\\ 
%Impact of GenAI on Ideation and Innovation Team Performance [] & Mendeley Data & 1,000+ ideas &  Business  & 2022 & Idea Generation\\ 
SPACE-IDEAS+ \cite{garcia-silva-etal-2024-space} & COLING & 1020 ideas &  Physics  & 2024 & Idea Generation\\ 
TOMATO-Chem \cite{yang2024moose} & Nature and Science & 51 papers & Chemistry & 2024 & Hypothesis Generation\\
TOMATO \cite{yang2023large} & Unspecified & 50 papers & Social Science & 2023-2024 & Hypothesis Generation\\
LLM4BioHypoGen \cite{qi2024large} & PubMed & 2,900 papers & Medicine & 2000-2024 & Hypothesis Generation\\
DevAI \cite{zhuge2024agent} & Meta & 55 tasks & AI & 2024 & Automated Experimentation\\
ScienceAgentBench \cite{chen2024scienceagentbenchrigorousassessmentlanguage} & OSU NLP & 44 papers & Diverse\footnote{Bioinformatics, Computational Chemistry, Geo-graphical Information Science, and Psychology \& Cognitive Neuroscience} & 2024 & Automated Experimentation\\
SWE-bench \cite{jimenez2024swebench} & ICLR & 2,294 issues & SWE & 2024 & Automated Experimentation\\
MLGym-Bench \cite{nathani2025mlgymnewframeworkbenchmark} & Meta & 13 tasks & Diverse\footnote{Data Science, Game Theory, Computer Vision, Natural Language Processing, and Reinforcement Learning} & 2025 & Automated Experimentation\\
\bottomrule
\end{tabular}
\caption{Overview of datasets for idea and hypothesis generation and experimentation}
\label{tab:section4.2_dataset}
\vspace{-5mm}
\end{table*}

%\todo{SE: this is data for what? Hypothesis generation?}
% In this section, 
We %provide a 
\se{survey} diverse %set of 
datasets for evaluating LLMs in hypothesis generation, idea formation and experimentation. These datasets are collected from various sources such as ACL Anthology, Nature and Science, PubMed and ICLR, and span multiple domains over NLP, Engineering, Physics, Chemistry and many more. For \textbf{idea generation}, datasets consist of paper abstracts \cite{chai2024exploring}, prototype ideas stemming from a design hackathon \cite{ege2023idea}, innovative ideas for space-related technologies and missions \cite{garcia-silva-etal-2024-space}. For \textbf{hypothesis generation}, datasets contain scientific papers, and each paper is annotated by domain experts with background, research questions, related works and hypotheses in various domains \cite{yang2024moose, yang2023large, qi2024large}. For \textbf{automated experimentation}, datasets consist of structured problem-solving tasks that require generating or modifying Python code. These include AI development tasks with hierarchical user requirements \cite{zhuge2024agent}, scientific discovery automation based on data and expert knowledge \cite{chen2024scienceagentbenchrigorousassessmentlanguage}, software engineering tasks requiring code edits to resolve issues \cite{jimenez2024swebench}, and the MLGym framework \cite{nathani2025mlgymnewframeworkbenchmark}, which evaluates agents on open-ended AI research tasks across multiple domains. Details of these datasets are presented in Table \ref{tab:section4.2_dataset}.
% and time periods. %as listed below:

% \begin{itemize}
%     \item 
\iffalse
    For idea generation, several datasets have been curated for evaluating LLM-based approaches. For instance, \textbf{SciMON} \cite{chai2024exploring} is a subset of the Semantic Scholar Open Research Corpus (S2ORC) \cite{lo2019s2orc} to focus on paper abstracts from ACL publications from 1952 to 2022. The dataset contains 135,814 paper abstracts, divided into training (before 2021), validation (2021), and test (2022) sets. \todo{SE: and how is it related to idea generation?}
    Text preprocessing tools, such as PL-Marker \cite{ye2021packed} and the structure classifier \cite{cohan2019pretrained}, are leveraged to annotate the abstracts. Each abstract is segmented into sentences categorized as background, research ideas, and keywords. \textbf{IDEA Challenge} \cite{ege2023idea} from 2022 contains 240 prototype entries and 1,049 connections from a design hackathon. It focuses on ideation and idea generation in prototyping, capturing how different prototypes are connected and can inspire new ideas. It aids in understanding how design thinking and innovation can evolve over iterative idea generation processes. \textbf{SPACE-IDEAS+} \cite{garcia-silva-etal-2024-space} focuses on detecting salient information in innovation ideas within the space industry. The dataset includes diverse text samples from informal, technical, academic, and business writings, aiming to help generate innovative ideas for space-related technologies and missions by analyzing the richness and potential of early-stage concepts.

    % \item 
    For hypothesis generation, examples of datasets include: 
    \textbf{TOMATO-Chem} \cite{yang2024moose} contains 51 Chemistry and Material Science papers published in Nature or Science in 2024. These papers are annotated by several  Chemistry PhD students. For each paper, the annotations include background, research questions, 2-3 related and important works that potentially inspire  the paper, hypotheses and experiments for hypothesis justification. Additionally, 3,000 papers are provided as a reference pool for LLMs to select %select 
    related works when generating hypotheses.
    \textbf{TOMATO} \cite{yang2023large} consists of 50 social science papers published in top journals after January 2023. The dataset covers diverse social science topics such as Communication, Human Resource Management, and Information Systems, among others. Data annotation in the dataset is completed by a social science PhD student. For each paper, relevant paper content is extracted by the student and categorized as main hypothesis, research background, and inspirations. Additionally, the dataset includes background and inspiration not only extracted from scientific papers but also web data such as Wikipedia. In this setup, LLMs are given the opportunity to generate hypotheses from web data.
    \textbf{LLM4BioHypoGen} \cite{qi2024large} consists of 2,900 medical publications sourced from PubMed. The data is divided into 2,500 papers for the training set and 200 papers for the validation set (both published before January 2023), with 200 papers in the test set (published after August 2023). 
    %Such a data split %is to ensure 
    %ensures 
    %that the test set is not exposed to LLMs during training. 
    Each paper is annotated by employing GPT to generate background and hypothesis pairs. Human annotators are then leveraged to filter out low-quality pairs.

    For automated experimentation, examples include:
    \textbf{DevAI} \cite{zhuge2024agent} is a benchmark comprising 55 realistic AI development tasks, each accompanied by hierarchical user requirements (365 in total). It is designed to facilitate the evaluation of agentic AI systems across various domains, including supervised learning, reinforcement learning, computer vision, natural language processing, and generative models.
    \textbf{ScienceAgentBench} \cite{chen2024scienceagentbenchrigorousassessmentlanguage} is designed to evaluate the abilities of LLM agents in automating data-driven scientific discovery tasks. It comprises 102 tasks derived from 44 peer-reviewed publications across four disciplines: Bioinformatics, Computational Chemistry, Geographical Information Science, and Psychology \& Cognitive Neuroscience. Each task requires an agent to generate a self-contained Python program based on a natural language instruction, a dataset, and optional expert-provided knowledge. The benchmark employs multiple evaluation metrics, including Valid Execution Rate, Success Rate, CodeBERTScore, and API Cost, to assess the generated programs' correctness, execution, and efficiency.
    \textbf{SWE-bench} \cite{jimenez2024swebench} evaluates the ability of LLMs to generate pull requests that resolve GitHub issues in real-world software engineering tasks. It is constructed from 2,294 tasks derived from 12 popular open-source Python repositories.  %filtering high-quality instances where merged pull requests resolve issues, contribute tests, and pass execution-based validation. 
    Each task requires the model to edit a full codebase based on an issue description, producing a patch that must apply cleanly and pass fail-to-pass tests. The benchmark features long, complex inputs, robust evaluation via real-world testing, and the ability to be continually updated with new issues.
\fi

% \todo{SE: Table 2 must be referenced}
   
% \end{itemize}

%\paragraph{Evaluation setup.} 

\subsubsection{Methods and Results}
%\todo{Describe the state-of-the-art methods and their results, noting any significant qualitative/quantitative differences between them where appropriate.}
%In this section, we will describe some state-of-the-art methods and their results for hypothesis generation, idea formation and automatic experimentation. For this purpose, Figure \ref{fig:hypotheses_idea_experimentation_overview_old} provides a rough idea about how their respective works can be divided. For example, most works in hypotheses generation focus on reducing hallucination, tackling very long contexts or iteratively refining the outputs, steering to better ideas. Automated experimentation on the other hand either heavily utilizes tree search to select the best working examples for a corresponding task, uses multi-agent workflows to let different LLMs interact with each other which focus on different tasks and also iterative refinement to enhance performance. Both hypotheses and idea generation rely on a variety of sources such as from scientific literature, web data and datasets, whereas for automated experimentation, the idea/hypotheses is already predetermined which means that it needs access to a variety of computational models \& simulations as well as the raw data to reach its goal.
Here, we discuss state-of-the-art methods and results in hypotheses generation, idea formation, and automated experimentation. Fig.~\ref{fig:hypotheses_idea_experimentation_overview_new} provides some examples for each approach.

% A figure should be placed here, with the elements listed below:
% - hallucination: related works [273], knowledge base [267]
% - long context input: extraction of relevant information from the literature [27]
% - refinement 
% - human alignment: chain of ideas [132]

% \begin{table*}[h]
% \centering 
% % \footnotesize
% \setlength\tabcolsep{3pt} 
% \begin{tabular}{lccccccc}
% \toprule
% \textbf{Paper} & \textbf{Discipline} & \textbf{External Data} & 
% \textbf{Refinement} & \textbf{Model Input} & \textbf{Ref-free} & \textbf{Ref-based}\\ 
% Yang et al.\cite{yang2024moose}  &  Chemistry  &  N   & &  \\  
% Liu et. al \cite{liu2024literature} & Social Sciences & & \\
% Xiong et al.\cite{xiong2024improving}& Chemistry & \\
% Chai et al. \cite{chai2024exploring}& & \\
% Zhou et al. \cite{zhou2024hypothesis}& & \\
% Qi et. al \cite{qi2024large}& & \\
% Qi et. al \cite{qi2023large}& & \\
% Yang et al.\cite{yang2023large}& & \\
% \bottomrule
% \end{tabular}
% \caption{XYZ}
% \label{tab:papers}
% \end{table*}

% \begin{itemize}
% \item Hypothesis generation \cite{liu2024literature, yang2024moose, chai2024exploring, qi2024large, wang2023scientific, xiong2024improving, yang2023large, zhou2024hypothesis, tong2024automating, bai2024advancing, wang2024llm, park2024can, qi2023large, chen2024use, liu2024beyond, banker2024machine, tang2023less, jing2024data}
% \item Idea generation \cite{lu2024aiscientist, su2024two, radensky2024scideator, hu2024nova, li2024chain, sandholm2024semantic, pu2024ideasynth, liuproposal, si2024llmsgeneratenovelresearch} 
% \item Automated experimentation
%     \begin{itemize}
%         \item multi-agent workflow \cite{trirat2024automl, liu2024drugagent, huang2024mlagentbench}
%         \item tree search \cite{chi2024sela, schmidt2024introducing}
%         \item iterative refinement \cite{wang2024openhands}
%     \end{itemize}
% \end{itemize}

%\todo{JD: is it possible to add human icons to the figure components where fitting?}
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{image/automatic_experimentation_new.pdf} 
  \caption{Examples of idea generation, hypothesis generation, and automated experimentation follow a four-component structure: \textit{task}, \textit{sample input}, \textit{methods}, and \textit{sample output}. The \textit{task} defines the goal of each process. \textit{Sample input} consist of benchmark datasets for each task. \textit{Methods} encompass relevant scientific approaches. \textit{Sample output} differs by process: idea generation and hypothesis yield textual outputs (descriptions or explanations), whereas automated experimentation produces executable code.}
  \label{fig:hypotheses_idea_experimentation_overview_new}
\end{figure*}
%\todo{YC: Update the citation in Figure 2, when the final version is ready.}

% Examples for hypothesis generation, idea generation, and automated experimentation. Each process is structured into four components: task, data sources, methods, and results. The task defines the goal of each process, where a task description can serve as an input specifying the user’s objective. Data sources for hypothesis and idea generation include scientific literature, web data, and datasets, while automated experimentation relies on predefined ideas and requires access to computational models, simulations, and raw data. Methods refer to different approaches from scientific literature that can be applied to accomplish each task. The results vary depending on the process: hypothesis and idea generation produce textual outputs, such as short descriptions or explanations, while automated experimentation generates executable source code that solves the given task.




\paragraph{Idea Generation} 
%\todo{JD: new paper https://arxiv.org/abs/2410.09403}
Research ideation has become a critical area where %large language models (LLMs) 
LLMs 
are increasingly %being 
applied to enhance novelty and accelerate discovery. Several methods have been proposed to improve the creative %capabilities 
abilities 
of LLMs, %often 
focusing on iterative refinement, %\textbf{structural frameworks}, 
multi-agent systems, human alignment
and evaluation %protocols
\cite{lu2024aiscientist, su2024two, radensky2024scideator, hu2024nova, li2024chain}. 

% sandholm2024semantic, pu2024ideasynth, liuproposal, si2024llmsgeneratenovelresearch}. %Most current approaches thereby leverage LLMs. 

% \cite{lu2024aiscientist, su2024two, radensky2024scideator, hu2024nova, li2024chain, sandholm2024semantic, pu2024ideasynth, liuproposal, si2024llmsgeneratenovelresearch} 

% \begin{itemize}
%     \item 
    For \textbf{iterative refinement}, \citet{hu2024nova} introduce an iterative planning and search framework aimed at enhancing the novelty and diversity of ideas generated by LLMs. By systematically retrieving external knowledge, the approach addresses the limitations of existing models in producing simplistic or repetitive suggestions. 
    % Experimental results show that their approach, Nova, generates %3.4 times 
    % more unique ideas compared to standard methods. %significantly improving novelty. 
    Similarly, \citet{pu2024ideasynth} focus on iterative refinement by providing literature-grounded feedback. Representing research ideas as nodes on a canvas, their approach, IdeaSynth, facilitates the iterative exploration and composition of idea facets,  %Compared to strong baselines, it 
    enabling %researchers 
    to develop more detailed and diverse ideas, particularly at various stages of ideation. 
    \iffalse
    Complementing these efforts, \citet{sandholm2024semantic} provide a semantic navigation approach that leverages constrained problem-solution spaces and introduces automated input filtering to enhance coherence and relevancy, resulting in improved user engagement and generation quality.
    \fi
    % \item 
    For \textbf{human alignment}, other works %have sought 
    seek to organize information in ways that mirror human research processes. Chain of Ideas (CoI) \citep{li2024chain} proposes structuring literature into a chain to emulate the progressive development of research domains. This facilitates the identification of meaningful directions and has been shown to outperform existing methods in generating ideas comparable in quality to those produced by human researchers. Scideator \citep{radensky2024scideator}, in contrast, focuses on recombining facets (e.g., purposes, mechanisms, and evaluations) from existing research papers to synthesize novel ideas. By incorporating automated novelty assessments, Scideator enables users to identify overlaps and refine their ideas. %effectively. 
    %Both CoI and Scideator emphasize structured frameworks to improve LLM-assisted ideation, though they differ in their focus, with CoI emphasizing progression within a domain and Scideator prioritizing creativity through facet recombination.
    % \item 
    % Regarding multi-agent systems, 
    % beyond ideation tools, 
    Moreover, research %ers have 
    has 
    explored fully \textbf{autonomous and multi-agent systems} for scientific discovery. For instance, the AI Scientist \citep{lu2024aiscientist} presents a framework for automating the entire research pipeline, including idea generation, experiment execution, and paper writing. 
    %Applied across various machine learning subfields, it demonstrates the feasibility of creating high-quality research outputs with minimal cost. %marking a significant step toward open-ended automated discovery. 
    %In a complementary vein, 
    VirSci \citep{su2024two} employs a multi-agent system %to replicate the collaborative nature of scientific research. B
    %by organizing 
    of virtual agents to collectively generate, evaluate, and refine ideas, %VirSci 
    which outperforms individual LLMs, underscoring the potential of teamwork in enhancing scientific innovation.

    %\item \textbf{Evaluation protocols.} Finally, the effectiveness of LLMs in research ideation has been systematically evaluated. A large-scale study \citep{si2024llmsgeneratenovelresearch} comparing human researchers and LLMs finds that LLMs generate ideas judged to be more novel but slightly less feasible. The study highlights key challenges, such as limited diversity and self-evaluation failures, which constrain the broader adoption of LLMs in ideation workflows. These findings provide a foundation for addressing gaps identified in existing methods, such as those employed by Nova, Scideator, and IdeaSynth.
% \end{itemize}

%Together, these approaches demonstrate the potential of LLMs to transform research ideation, while also highlighting persistent challenges related to feasibility, diversity, and the integration of human-aligned evaluation protocols. These insights pave the way for future advancements in leveraging AI to drive scientific discovery.

% \todo{SE: perhaps we should indicate here how evaluation has been done? Humans, automatically?}




% \textbf{Evaluation} %\textbf{Evaluation protocols.} Finally, 
%The effectiveness of LLMs in research ideation has been systematically evaluated. 
% A large-scale study \citep{si2024llmsgeneratenovelresearch} comparing human researchers and LLMs finds that LLMs generate ideas judged to be more novel but slightly less feasible. The study highlights key challenges, such as limited diversity and self-evaluation failures, which constrain the broader adoption of LLMs in ideation workflows. %These findings provide a foundation for addressing gaps identified in existing methods, such as those employed by Nova, Scideator, and IdeaSynth.
% %\paragraph{Evaluation}

% Although LLMs make hypothesis generation faster and more efficient compared to human researchers, it is unclear how often generated hypotheses in various disciplines are truly useful and lead to new scientific discoveries, given that hypotheses are typically theoretical and cannot be validated without costly justification. Such impact of generated hypotheses on scientific discoveries is assessed in closed-ended and open-ended setups. \todo{SE: this could go in the limitations section} 



\paragraph{Hypothesis Generation}

% A table will be placed to mark the following papers in several aspects: (a) methods (using external knowledge or not, domains, web data or scientific articles, iterative refinement or not, etc) (b) evaluation metrics (c) human evaluation (how?) (d) evaluation aspects (novelty, validity, etc) (e) model selection. \todo{SE: is this still coming?}

Recently, there have been many works that leverage LLMs to generate hypotheses and ideas \cite{liu2024literature, yang2024moose, chai2024exploring, qi2024large, wang2023scientific, xiong2024improving}.
% , yang2023large, zhou2024hypothesis, tong2024automating, bai2024advancing, wang2024llm, park2024can}.
% , qi2023large, chen2024use, liu2024beyond, banker2024machine, tang2023less, jing2024data}.
These works differ in their use of LLMs for addressing various technique challenges, including (a) handling long context input due to the need for LLMs to analyze related works, (b) strategies for refining LLMs to generate meaningful hypotheses, (c) lowering the possibility of generating hallucinated hypotheses and ideas. 
% \begin{itemize}
%     \item 

    For \textbf{hallucinated hypotheses}, %Yang et al.
    \citet{yang2024moose} address the hallucination issue
    through a pipeline that starts with a search for related works. The identified related works and a given research question are provided as input for LLMs to generate hypotheses.
    % in the chemistry domain 
    The generated hypotheses are evaluated against ground-truth hypotheses published in Nature and Science. Their results show that many generated hypotheses exhibit a very high degree of similarity to the ground-truth ones. %Xiong et al.
    \iffalse
    \citet{xiong2024improving} ground LLM-integrated formation of research hypotheses in scientific knowledge bases. Their approach incorporates external, structured data from knowledge graphs into the hypothesis formation process, and leverages the idea of structured reasoning to 
    % generate a chain of ideas, which, combined with knowledge groups, is used to produce 
    knowledge-grounded hypotheses, i.e., leveraging the search for evidence to mitigate hallucination during hypothesis formation.
    \fi
% Tong et al.\cite{tong2024automating} frame hypothesis generation as a link prediction task. Specifically, they employ LLMs to generate a causal graph from a collection of papers, where novel links between nodes in the causal graph represent new hypotheses. They found that their generated hypotheses, which are evaluated by human experts, are more novel than those generated by using LLMs only and are comparably novel to human-generated ones.
    % \item 
    Regarding \textbf{long context input},
    \citet{chai2024exploring} focus on the efficient use of limited context size of LLMs. They introduce a selection mechanism that extracts important and relevant information from the literature and takes them as input for LLMs to generate hypotheses. Their results show that filtering out unnecessary information helps improve the quality of generated hypotheses.
    % \item 
    For \textbf{refinement strategies}, many works have explored strategies for refining LLMs to generate hypotheses. Major strategies include (a) few-shot learning, (b) fine-tuning on training data and (c) iterative refinement. For instance, 
    \citet{qi2023large} 
    % compare the impact of few-shot learning and fine-tuning on LLMs for hypothesis generation against zero-shot learning. Their results 
    show that hypotheses generated by few-shot learning
    % that 
    are judged by humans more testable than those generated in the zero-shot setup; while fine-tuning improves the overall quality of hypotheses, the improvement is limited to the domain of training data; in unseen domains, fine-tuning harms hypothesis quality, particularly the novelty aspect.  
    \citet{zhou2024hypothesis} iteratively refine hypotheses through reinforcement learning, with the aim 
    of 
    increasing the similarity between a given research problem and a generated hypothesis. A recent advancement in this space is the AI co-scientist\footnote{\url{https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/}}, a multi-agent system that employs a generate-debate-evolve framework. It iteratively enhances its hypotheses through collaboration, grounding in prior evidence and tournament-based selection.
    
    % This idea is further extended by  
    % \citet{qi2024large} through a multi-agent framework to support the refinement process. They split hypothesis generation into multiple tasks: hypothesis drafting, hypothesis evaluation and refinement, operated by various agents.
% \end{itemize} 

\iffalse 
Additionally, %Liu et. al 
\citet{liu2024literature} discuss data sources that LLMs rely on to generate hypotheses. For instance, scientific literature is provided as a source of model input for LLMs to generate hypotheses. Additionally, LLMs take as model input a paper review and generate testable hypotheses regarding whether the review is written by Generative AI. %Yang et al.
\citet{yang2023large} propose to incorporate web data (e.g., scientific comments on social media) into the hypothesis generation process. This allows hypotheses to be informed by public opinions.
\fi
% \todo{SE: for space reasons, I removed this `additionally' part}
% \todo{SE: perhaps worthwhile to say that this whole field is extremely now, with most papers coming from 2024?}

% Additionally, Yang et al.\cite{yang2023large} focus on generating hypotheses in social science based on large-scale, open-domain, unstructured web data. They propose a sequence of tasks to achieve this, including discovering background literature, identifying relevant inspiration from web data, and combining the background and inspiration to generate hypotheses. LLMs are further leveraged to provide feedback on the generated hypotheses in order to refine them. Their results show that their generated hypotheses are judged by human experts more novel and valid than the ones generated by the LLM baselines




\paragraph{Automated Experimentation.} %Experimentation is at the core of  AI-driven scientific research, with the components including formation of machine learning tasks, implementing research ideas, designing and running experiments, analyzing results, and iterating the process to improve model performance. Automated experimentation is an active topic of research aiming to automate the experimental workflow of the aforementioned components. Popular examples are Neural Architecture Search \cite{elsken2019neural} and AutoML \cite{he2021automl}. 
%In the age of LLMs, automated experimentation could reach a new level where the experimentation process has the possibility to be executed through human language prompts. For instance, AutoML-GPT \cite{tsai2023automl} and MLcopilot \cite{zhang2023mlcopilot} leverage LLMs to automate hyperparameter tuning such as weight decay and learning rate. MLAgentBench \cite{huang2024mlagentbench} proposes to benchmark primitive tasks including file system operations and python script execution. Beyond isolated tasks, researchers are now investigating more sophisticated frameworks that incorporate multi-agent collaboration, tree search algorithms, and iterative refinement techniques to improve automation in scientific experimentation.
Experimentation is central to AI-driven research, encompassing task formulation, implementation, evaluation, and iteration. Automated experimentation aims to streamline this workflow, with approaches like Neural Architecture Search \cite{elsken2019neural} and AutoML \cite{he2021automl}. LLMs further enhance this by enabling automation through natural language prompts. AutoML-GPT \cite{tsai2023automl} and MLcopilot \cite{zhang2023mlcopilot} use LLMs for hyperparameter tuning, while MLAgentBench \cite{huang2024mlagentbench} benchmarks fundamental automation tasks. Recent work explores advanced frameworks incorporating multi-agent collaboration, tree search, and iterative refinement for scientific experimentation.
% More recently, Li et al., 2024 \todo{SE: wrong citations throughout}
% leverage LLMs to design experiments, generate code and execute scripts to run experiments.

%\begin{itemize}
        %\item multi-agent workflow \cite{trirat2024automl, liu2024drugagent, huang2024mlagentbench}
        %item tree search \cite{chi2024sela, schmidt2024introducing}
        %\item iterative refinement \cite{wang2024openhands}
%\end{itemize}

% \begin{itemize}
    For \textbf{multi-agent workflow}, GVIM \cite{ma2025gvim} enhances chemical research with domain-specific functions, while DrugAgent \cite{liu2024drugagent} employs LLMs for task planning in drug discovery. AutoML-Agent \cite{trirat2024automl} integrates retrieval-augmented planning for AutoML tasks, and MLAgentBench \cite{huang2024mlagentbench} benchmarks LLM-driven agents in machine learning experimentation. The Agent-as-a-Judge framework \cite{zhuge2024agent} introduces structured agent evaluation. %\todo{SE: can this paragraph be shortened?} 
    For \textbf{tree search}, AIDE \cite{schmidt2024introducing} applies Solution Space Tree Search to refine solutions in Kaggle challenges. The "Tree Search for Language Model Agents" framework \cite{koh2024tree} enables LLM agents to plan multi-step interactions using best-first tree search, pruning less promising options. SELA \cite{chi2024sela} combines LLM-generated insights with Monte Carlo Tree Search, iteratively refining machine learning experiments by selecting promising configurations and executing them.
    % \item \textbf{Iterative refinement.} 
    For \textbf{Iterative refinement}, APEx \cite{conti2024benchmarking} automates LLM-based experimentation with an orchestrator, execution engine, benchmark generator, and model library. OpenHands \cite{wang2024openhands} enables AI agents to interact with software, execute actions in a sandboxed runtime, and collaborate across tasks using predefined benchmarks.
    %\todo{SE: can this paragraph be shortened?}
    %The platform supports the creation and implementation of agents with an extensible set of actions and observation mechanisms, enabling broad real-world applications.

% \end{itemize}

\paragraph{Evaluation.} 
% \todo{SE: does that deserve an own paragraph? Is it on the same level as the others? If it's on the same level, should it be introduced earlier?}
% Although LLMs make hypothesis and idea generation faster and more efficient compared to human researchers, 
While LLMs accelerate ideation and hypothesis generation,
% and experimentation,
it is crucial for the research community to assess their usefulness and identify those worth further validation. Many evaluation approaches have been proposed, differing primarily in whether gold hypotheses and ideas are available. 
% as well as in the setup, namely the criteria against which hypotheses are evaluated. 
When \textbf{gold hypotheses and ideas are available}, 
    % \textbf{Reference-based setup} refers to the scenario where gold hypotheses are provided in test sets for comparing against generated hypotheses. The gold hypotheses are derived from scientific discoveries reported in publications, i.e., that they are not just hypotheses but truly scientific findings. This setup, referred to as scientific rediscovery, has been used in several works to evaluate domain-specific hypotheses generated by LLMs. For evaluation, 
    metrics such as BLEU \cite{papineni2002bleu} 
    % and BERTScore \cite{zhang2019bertscore} 
    % among many others \cite{zhao-etal-2019-moverscore, zhao-etal-2023-discoscore} 
    have been used to assess hypothesis and idea quality by measuring similarity to known scientific discoveries \cite{yang2024moose, chai2024exploring}. %However, these metrics primarily capture linguistic or semantic resemblance rather than scientific validity. 
    % \todo{SE: limitations subsection?}
    \citet{qi2024large} leverage
    % address scientific judgments by leveraging 
    LLMs-as-a-metric to evaluate hypotheses based on four scientific aspects: (a) novelty, (b) relevance to the given background, (c) significance within the research community, and (d) verifiability, i.e., testability. 
    % A key metric for evaluating reference-based automated experimentation is the AUP score, which summarizes an agent's performance across tasks while accounting for task difficulty \cite{nathani2025mlgymnewframeworkbenchmark}. Variants such as AUPbs@4 and AUPba@4 further distinguish between an agent’s ability to submit its best solution versus its exploratory performance. 
    When \textbf{gold hypotheses and ideas are unavailable}, a generated hypothesis or idea is typically assessed by human experts, who compare it with the given research question and provide feedback \cite{yang2023large}. 
    % For automatic experimentation evaluation, computational cost, premature terminations, incomplete runs, and structural complexity can also be considered. %To address the time burden of human assessment, particularly in complex multi-agent automated experiments, structured evaluation systems like Agent-as-a-Judge \cite{zhuge2024agent} analyze project structures, execution logs, and dependencies through modular assessment mechanisms.
    % Human evaluation, on the contrary, which leverages researchers to evaluate hypotheses, is more reliable but costly and time-consuming. Therefore, automated metrics are typically leveraged to diagnose LLMs in hypothesis generation on the full datasets, and justify their results by cross-checking with the results from human evaluators on a subset \cite{qi2024large}. 
    
    %For automatic experimentation, performance is evaluated using performance ratios, which compare a method’s results to the best observed outcome for a given task. This ensures fair comparisons across tasks of varying difficulty. Methods that fail to produce valid solutions or underperform relative to a baseline are marked as infeasible and assigned a penalty score to prevent unreliable models from distorting rankings. To assess robustness, experiments track two profiles: the best submission profile, which evaluates an agent’s ability to finalize its strongest attempt, and the best attempt profile, which measures overall exploration capability. A key metric for summarizing performance is the area under the performance profile (AUP) score \cite{nathani2025mlgymnewframeworkbenchmark}, which aggregates performance across tasks while accounting for difficulty differences. Variants such as AUPbs@4 and AUPba@4 distinguish between an agent’s ability to submit its best solution versus its exploratory performance.
    % , a large-scale is costly, therefore 
    % several researchers are leveraged to evaluate a subset of hypotheses based on the same aspects, and compare the evaluation results with those from automated metrics such as 
    % They observe a positive correlation between human and LLM assessments.
    % \item 
    \iffalse
    \textbf{Reference-free setup} refers to the scenario where gold hypotheses are unavailable, meaning that the correctness of generated hypotheses is unknown to human researchers. In this setup, a generated hypothesis is compared against the given research question, for example, by measuring the relevance of the hypothesis to the research question, while human evaluation provides detailed feedback on generated hypotheses. For instance, the hypothesis \textit{``In collectivist cultures, individuals engage in more conspicuous consumption behaviors compared to individualistic cultures.''} is generated by \citet{yang2023large}. Analyses by social scientists show that this hypothesis is potentially novel and counterintuitive, given that previous research indicated that collectivist cultures typically focus on group harmony and cooperation over individual desires, and therefore  individual with collectivist cultures  may less likely display their personal wealth or status through conspicuous consumption. However, the LLM-generated hypothesis suggests the opposite. Such detailed feedback helps identify candidate hypotheses that are meaningful and worthy of further experimentation.
    \fi
    %For automatic experimentation, cost and error metrics track computational expenses, premature terminations, and incomplete runs to ensure efficiency and reliability. Structural and action-based metrics analyze decision patterns, including action distributions, code changes, and unit test performance. Information retrieval and similarity measures, such as retrieving relevant files via BM25 \cite{robertson2009probabilistic} or using CodeBERTScore \cite{zhou2023codebertscore} to compare generated and reference code, assess whether the system retrieves and generates relevant information. AI-assisted judging, using models like GPT-4o, scores outputs based on predefined rubrics covering data processing, modeling, and execution correctness. Finally, the Agent-as-a-Judge \cite{zhuge2024agent} system offer structured evaluations by analyzing project structures, execution logs, and dependencies through modular retrieval and assessment mechanisms.
    % If confirmed, social scientists believe this hypothesis will greatly influence the understanding of culture on conspicuous consumption.
    
% \end{itemize}

% %%%%%%%%%%%% Moved to appendix  %%%%%%%%%%%%%%%%
\subsubsection{Ethical Concerns}

%\todo{Identify and discuss any ethical issues related to the (mis)use of the data or the application of the methods, as well as strategies for mitigations.}
%The integration of AI into hypotheses generation, idea formation, and automated experimentation introduces significant ethical challenges. 
In the area of idea generation, there is a risk of reinforcing established research paradigms. AI systems trained on the basis of existing literature may favor popular paths and neglect underrepresented research directions. As a result, unconventional ideas may be unintentionally marginalized. %For example, an AI might repeatedly suggest incremental improvements in a dominant field rather than proposing entirely new lines of research, thereby limiting the diversity of scientific thinking. 
AI-generated hypotheses may lack transparency, making it difficult to assess their validity or underlying assumptions, which could lead to flawed experiments. For example, an AI might identify a statistical correlation in its training data and propose  hypotheses without clearly revealing the underlying assumptions or data sources, making it difficult for researchers to verify its scientific soundness or hold anyone accountable if the hypotheses proves misleading. 
Automated experimentation presents its own ethical challenges. The speed and volume in which AI can design and execute experiments can lead to insufficient ethical oversight and inadequate safety controls. Consider an AI system that suggests experimental protocols in biomedical research (e.g., chemical components with unknown toxicity) without the rigorous human review needed to identify potential risks. This could lead to experiments that pose unforeseen dangers or violate established ethical standards.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Domains of Application}
%\todo{Indicate whether any of the data, methods, ethical concerns, etc. are specific to a given domain (biology, health, computer science, etc.).}
%The formation of research ideas and hypotheses requires domain-specific knowledge. Therefore, it is crucial to curate datasets separately for each domain, ensuring that datasets contain specialized research questions and relevant background that LLMs can take as input to generate domain-specific ideas and hypotheses. 
Regarding domains of interest, previous works have addressed idea and hypothesis in NLP, Engineering, Physics, Chemistry, Social Science and Medicine.  Similarly, automated experimentation also relies on domain-specific datasets to guide the process of designing and testing experiments. In contrast, methods are typically domain-agnostic, though they are often developed and evaluated in specific domains. These methods address fundamental issues in LLMs, such as long-context inputs, post-tuning strategies, and model hallucination, making them potentially applicable across multiple domains. 
% \todo{SE: which domains are addressed? Social Science? Medicine? Computer Science?}

% \begin{itemize}
%     \item Computer Science (NLP and ML)
%     \item Clinic research \cite{qi2024large, jing2024data} 
%     \item Materials science \cite{liu2024beyond}
%     \item Chemistry \cite{yang2024moose}
%     \item Psychology \cite{banker2024machine, tong2024automating}
%     \item Energy \cite{chen2024use}
%     \item ..
% \end{itemize}

\subsubsection{Limitations and Future Directions}
%\todo{Summarize the limitations of current approaches; point out any notable gaps in the research and future directions.}

% Despite having creativity to some degree \cite{sun2024large, kumar2024human, mamonov2024impact},
% \begin{itemize}
%     % \item low diversity \cite{si2024llmsgeneratenovelresearch} 
%     % \item low feasibility \cite{si2024llmsgeneratenovelresearch, liu2024aigs}     
%     \item false ideas \cite{liu2024aigs}
%     \item ..

%     \item: method, scope, etc
%     \item: focus on text, reinforce stereotypes, not inclusive for under-represented problem, underfunded problems, domain limit, 
% \end{itemize}

 %These findings provide a foundation for addressing gaps identified in existing methods, such as those employed by Nova, Scideator, and IdeaSynth.
%\paragraph{Evaluation}

A large-scale study \citep{si2024llmsgeneratenovelresearch} comparing human researchers and LLMs finds that LLMs generate ideas judged to be more novel but slightly less feasible, highlighting challenges like limited diversity and self-evaluation failures.
% The study highlights key challenges, such as limited diversity and self-evaluation failures, which constrain the broader adoption of LLMs in ideation workflows. 
Additionally, 
% given that hypotheses and ideas are typically theoretical and cannot be validated without costly justification, it is unclear whether generated hypotheses and ideas are truly useful and lead to new scientific discoveries.
given that ideas and hypotheses are theoretical and costly to validate, it is unclear whether they could lead to scientific discovery.
Furthermore, previous methods lack due diligence through data, 
% LLMs only take as model input a research question and several related works without due diligence through data, 
and therefore generated ideas and hypotheses are often too general 
% and lack methodological details 
\cite{yang2024moose}. Moreover, LLMs may generate recently discovered ideas and hypotheses, as they lack access to recent scientific papers \cite{liu2024beyond}. Their outputs are very sensitive to the framing of input prompts \cite{park2024can}. Future work should focus on improving feasibility and diversity of ideas and hypotheses, incorporating real-time scientific papers, refining ideation and hypothesis generation through data inspection.

Automated experimentation with LLMs faces several additional challenges. First, LLMs often make critical errors, such as hallucinating results or outputting invalid references, which disrupt the precise steps required for experimental workflows. Another significant limitation is that LLMs struggle to integrate and align different modalities, such as video, audio, or sensory data, which are increasingly essential in modern scientific experimentation. Moreover, LLMs lack the critical analysis capabilities necessary to identify flaws or refine hypotheses during experimentation. In highly specialized scientific domains such as biology and chemistry, they may also struggle with precise reasoning and tool usage, which are vital for ensuring experimental success \cite{reddy2024scientificdiscoverygenerativeai}. %Future work should focus on enhancing LLMs' capabilities in multimodal data processing, enabling the generation of figures, \todo{SE: see above/below} and improving their ability to refine hypotheses and ideas through iterative feedback during experiments.

%- Potentially promising hypotheses and ideas are often too challenging to implement
%- LLMs make critical errors and hallucinate results in experimentation (for example comparing two numbers, invalid file paths, incorrect figure reference)
%- Experimentation tasks do not include figure generation
%- Simulations are only done in environments (integration with physical lab spaces and cloud robotics)
%- Not effectively able to work with and align different modalities (video, audio, sensory data, ...)
%- LLMs struggle with deep critical analysis and identifying scientific flaws during experimentation to refine hypothesis/research ideas
%- Struggle with specialized scientific reasoning and correct tool usage

% Analyses by experts in Chemistry indicate that MOOSE-Chem \todo{SE: citation? How is it evaluated?} could generate theoretical hypotheses that are factually correct without the need for costly experimentation and data analysis, despite some limitations. For instance, MOOSE-Chem hypothesizes that the combination of ruthenium (Ru) and D$_{2O}$ as a deuterium source could lead to efficient reductive deuteration in the electrocatalytic system. Although this hypothesis is judged by experts as novel and correct, MOOSE-Chem fails to outline how this approach works. Despite this issue, such a hypothesis, which sheds light on on the right direction of a new discovery, is still useful and sets the stage for experimentation, and it is even groundbreaking to some degree, given that MOOSE-Chem takes as input only a research question and several related works without due diligence through data.  
    
% \subsubsection{AI use case}
% % \todo{SE: Used GPT4o to summarize and contrastively evaluate the papers on idea generation}
% %Optional: describe which portions of your section (figures, tables, text, etc.) have been assisted by AI and how.

