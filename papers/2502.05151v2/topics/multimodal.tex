\subsection{Multimodal Content Generation and Understanding}
\label{sec:multimodal}

%\collaborators{Steffen Eger, Yufang Hou, Nafise}

%Provide a concise description of the task here, indicate why it is important, and provide any necessary background information/references to contextualize the following subsections.
\mybox{\textbf{Multimodal content generation in the scientific domain} refers to generating multimodal scientific content such as figures  %equations 
and tables in scientific papers %, in an intra-publication process, 
or, e.g., slides and posters %or similar multimodal scientific content 
in a post-publication process. Automatizing such tasks via AI is important for %at least three reasons: 
multiple reasons: 
(i) generating high-quality figures, tables, slides and posters is \emph{difficult and time-consuming} (dimension: cost); %even for human authors; 
%\todo[inline]{SE: evidence: StackExchange questions?} 
(ii) high-quality %figures and tables 
multimodal content in a paper 
can have a large \textbf{effect} (dimension: benefit for authors) on citation or acceptance decisions \citep{Lee2016ViziometricsAV}; (iii) tables, figures, posters and slides make scientific content easier accessible for a scientific audience and often represent compact representations of research results (dimension: benefit for readers).  %and aide in understanding of scientific content for the scientific audience \citep{}; (iii) [...]. 
\textbf{Multimodal scientific content understanding} refers to understanding scientific images and tables, %(which often requires reasoning processes), %\todo[inline]{YH: it's not just about understanding images and tables, I think we should emphasize more on understanding of the content that presented in different modalities which often times require heavy inference, maybe "the ability to process and comprehend information from multiple modalities (e.g., text, images, tables, charts) present in scientific documents"?} 
e.g., answering questions about the multimodal scientific content,  providing captions or summaries for scientific figures and tables. Automatizing %such tasks 
this understanding process 
promises to allow to automatically describe such multimodal objects, which can likewise be time-consuming and costly for human authors, and helps readers  digest the content more easily. %(e.g., by allowing to ask arbitrary questions). 
}

\subsubsection{Data}

%Tables 
Table \ref{tab:section4.4_data} %and  \ref{tab:section4.4_Method} 
provides an overview over datasets %and methods 
for multimodal content generation and understanding. Below, we give details on individual datasets and benchmarks. 

\begin{table}[t]
\begin{small}
    \begin{tabular}{p{3cm}|p{6cm}|p{5cm}} %|p{1.5cm}}
        \toprule
      \textbf{Dataset} & \textbf{Size} & \textbf{Data Sources}  %Application?
      \\
    \midrule  
     \multicolumn{3}{c}{\textbf{Scientific Figure Understanding}} \\ \hline
ArxivCap \cite{li-etal-2024-multimodal-arxiv} & 6.4m images and 3.9m captions from 572k papers & Arxiv \\ 
FigureQA \cite{ebrahimi2018figureqa} & $>$ 100k scientific-style figures& Synthetic\\
ChartQA \cite{masry-etal-2022-chartqa} & 4.8k charts, 9.6k QA pairs& statista.com, pewresearch.com, etc. \\ %ourworldindata.org, oecd.org\\
CharXiv \cite{wang2024charxiv} & 2.3k charts with descriptive and reasoning questions& Arxiv\\
ArxivQA \cite{li-etal-2024-multimodal-arxiv} & 35k figures with 100k QA pairs& Arxiv\\
SPIQA \cite{pramanick2024spiqa} & 152k figures with 270k QAs & 19 top-tier academic conferences\\
ChartSumm \cite{xu2024chartadapterlargevisionlanguagemodel} & 84k charts & Knoema\\ 
SciMMIR\cite{wu-etal-2024-scimmir} & 530K  figures and tables image-text pairs & Arxiv \\
\hline
 \multicolumn{3}{c}{\textbf{Scientific Figure Generation}} \\ \hline
DaTikZ \cite{belouadi2024automatikz,belouadi2024detikzify} & 180k-360k pairs of text captions and Tikz code & Mostly Arxiv and TeX.StackExchange \\
ScImage \cite{zhang2024scimagegoodmultimodallarge} & 404 instructions and 3k generated scientific images & Manual (template) construction \\
%SketchFig & & \\
SciDoc2DiagramBench \cite{mondal-etal-2024-scidoc2diagrammer} & 1,080 Extrapolated-Diagrams with the format of ``<paper(s), intent of diagram, gold diagram>'' & ACL Anthology \\ 
ChartMimic \citep{shi2024chartmimicevaluatinglmmscrossmodal} & 1000 triplets of (figure, instruction, code) instances & Physics, Computer Science, Economics, etc.\\
\hline

\multicolumn{3}{c}{\textbf{Scientific Table Understanding}} \\ \hline
%xxx & & \\ \hline
SciGen \cite{BENCHMARKS2021_149e9677} & 1.3k pairs of scientific tables and their descriptions & Arxiv (especially cs.CL and cs.LG) \\ 
 NumericNLG \cite{suadaa-etal-2021-towards} & 1.3k pairs of scientific tables and their descriptions & ACL Anthology \\
SciXGen \cite{chen-etal-2021-scixgen-scientific} & 484k tables from 205k papers & Arxiv  \\
\hline 
    \multicolumn{3}{c}{\textbf{Scientific Table Generation}} \\ \hline
    %\\
ArXivDigestTables \cite{newman-etal-2024-arxivdigestables} & 2,228 literature review tables extracted from arXiv papers that synthesize a total of 7,542 research paper &  literature review tables from ArXiv papers from April 2007 until
November 2023 %& 
\\
\hline
  \multicolumn{3}{c}{\textbf{Scientific Slides and Poster Generation}} \\ \hline
SciDuet \cite{sun-etal-2021-d2s} & 1,088 papers and 10,034 slides by their authors & NeurIPS/ICML/ACL  Anthology  \\ 
DOC2PPT \cite{Fu2021DOC2PPTAP} & 5,873 papers and 98,856 slides by their authors  & CV (CVPR, ECCV,
BMVC), NLP (ACL, NAACL, EMNLP), ML (ICML, NeurIPS, ICLR) \\ 
Persona-Aware-D2S \cite{mondal-etal-2024-presentations} & 75 papers from SciDuet, and 300 slides %created by two Upwork workers 
& ACL Anthology \\

 
%\hline
%  \multicolumn{4}{c}{\textbf{Scientific Multimodel Content Understanding}} \\ \hline
%xxx & & & \\ 
\bottomrule
    \end{tabular}
    \caption{Multimodal content generation and understanding datasets.}
    \label{tab:section4.4_data}
\end{small}
\end{table}




%Give an overview of the most important curated/annotated datasets, or sources of raw data, that are used (or potentially useful for) this task.
%\todo{SE: maybe make this a table with dimensions of size, type, etc.}


%\begin{itemize}
%  \item Datikz: caption-tikz pairs
%  \item ScImage: controlled evaluation data (instruction->figure)
%  \item MetaFig [from Detikzify]
%  \item ArxivCap [https://arxiv.org/abs/2403.00231] 
%  \item SciGen [https://arxiv.org/abs/2104.08296]
%  \item 
%\end{itemize}

%- https://arxiv.org/abs/2406.09961
\paragraph{Scientific Figure Understanding.} 
Scientific figure understanding benchmarks typically contain QA pairs for scientific figures or summaries of figures.
\citet{10.1007/978-3-319-46493-0_15} provide a dataset with over 5k richly annotated diagrams and over 15k questions and answers. %\citet{10.1007/s00799-022-00329-y}
\citet{ebrahimi2018figureqa} introduce \textbf{FigureQA}, a synthetic dataset of over 100k scientific-style figures from five classes: (dot-)line plots, vertical and
horizontal bar graphs, and pie charts. Associated with the images are more than 1m questions %that are 
generated using 15 different templates. 
%More recently, 
Later research focuses on harder and more realistic QA pairs. %as the templated questions may lead to an overly optimistic assessment of the capabilities of recent models. 
\citet{masry-etal-2022-chartqa} present \textbf{ChartQA}, which provides complex reasoning questions over charts sourced from various sources related to science. %such as \url{statista.com} and \url{pewresearch.org}, which address social and economic issues. 
\citet{wang2024charxiv} introduce \textbf{CharXiv}, %a chart comprehension benchmark. %(where charts are particularly relevant in science or the financial domain). 
%Their main contribution 
%CharXiv is 
a manually curated dataset of 2.3k ``natural, challenging, and diverse'' charts from %scientific 
Arxiv 
papers and %(involving 
both descriptive and reasoning questions for them. \citet{li-etal-2024-multimodal-arxiv} introduce \textbf{ArxivQA}, a dataset of 35k scientific figures sourced from Arxiv for which GPT4o generates 100k QA pairs after manual filtering. 
\citet{pramanick2024spiqa} present \textbf{SPIQA}, a dataset of 270k manually and automatically created QA pairs to interpret complex scientific figures and tables. 
In contrast to focusing on question-answering for scientific figures, \citet{xu2024chartadapterlargevisionlanguagemodel} consider the chart summarization problem and a dataset comprising more than 190k instances building on top of existing datasets such as \textbf{ChartSumm} \citep{Rahman2023ChartSummAC}, which contains more than 84k charts along with their metadata and descriptions covering a wide range of topics and chart types to generate short and long summaries. 

\paragraph{Scientific Figure Generation.} Recently, several datasets for scientific figure generation have been proposed. %across different communities. %using different strategies such as obtaining code underlying scientific images from human annotators or from Arxiv submissions and using annotators to %determine 
%rate 
%high-quality scientific images %for 
%given (template) instructions.
% extracting instructions and code from Arxiv submissions
\citet{belouadi2024automatikz,belouadi2024detikzify} propose \textbf{DaTikZ} and \textbf{DaTikZ-v2} which contain (augmented) captions of scientific figures as instructions along with corresponding TikZ code, sourced from Arxiv submissions. %which allows for fine-tuning or evaluating LLMs.
\citet{belouadi2024detikzify} also provide \textbf{SketchFig}, a benchmark of 549 figure-sketch pairs to convert sketches into scientific figures; SketchFig is sourced from TEX stackexchange.  \citet{shi2024chartmimicevaluatinglmmscrossmodal} provide \textbf{ChartMimic}, a manually curated benchmark of 1000 triplets of %(figure, instruction, code) 
(instruction, code, figure)  
instances for Chart generation across various domains (Physics, Computer Science, Economics, etc.). %In contrast to \citet{belouadi2024automatikz} who extract DaTikZ from existing Arxiv papers, 
ChartMimic is obtained by having human annotators write Python code for prototype charts.  \citet{zhang2024scimagegoodmultimodallarge} provide \textbf{ScImage}, which contains targeted template instructions focusing on different understanding dimensions (spatial, numeric, attribute). %relevant for the science domain. 
For a subset of the data, the authors also provide reference figures, %which were 
evaluated as being of high-quality by human annotators. In contrast to the other %two 
benchmarks, ScImage also contains instructions in non-English languages. \citet{mondal-etal-2024-scidoc2diagrammer} provide \textbf{SciDoc2DiagramBench}, %\todo{SE: @Yufang, please provide more details}
a benchmark comprising 1,000 extrapolated diagrams paired with 89 ACL papers, along with human-written intents. All diagrams are extracted from the corresponding presentation slides of these papers. %and are not explicitly illustrated within the papers themselves. 
The intents describe how the content from each paper can be translated into the extrapolated diagrams for presentation purposes. %Finally, 
\citet{Luo2021nvBenchAL} provide \textbf{nvBench}, a benchmark of 25k tuples of natural language queries and corresponding visualizations. nvBench is based on 153 databases and contains more than 7k visualizations on seven chart types. nvBench is synthesized from natural language to SQL benchmarks. 

\iffalse 
\paragraph{Table Reasoning Benchmarks}

Reasoning over tables has become a critical capability for AI systems designed to advance scientific discovery. Tables in scientific literature are often used to concisely represent experimental data, results, and observations. Developing models that are capable of interpreting, analyzing, and synthesizing information from these structured formats can play an important role in automating scientific workflows. Table reasoning tasks can be broadly divided into table-to-text generation, which produces natural language summaries or insights from tabular data, and text-to-table generation, %\todo{SE: Are all approaches in this category reasoning tasks? SE: I just meant to say that the term reasoning is a bit a broad one that may have been used in an inflationary manner in the NLP community}  
where structured tables are extracted or generated from textual descriptions. 
\fi 


%\paragraph{Table-to-Text Generation}
\paragraph{Scientific Table Understanding}
%Table-to-text generation 
\se{Table understanding often comes %in the 
as table-to-text generation, which}
focuses on producing accurate %and coherent 
textual descriptions that reflect %the content of a given table. 
table content. 
%In the scientific domain, this task often requires arithmetic reasoning (e.g., aggregations, comparisons), logical inference, and domain-specific language modeling. 
% 
%General-purpose benchmarks have been a driving factor in advancing table-to-text generation \citep{chen-etal-2020-logical,nan-etal-2021-dart,lebret-etal-2016-neural,chen-etal-2021-wikitablet,van-der-lee-etal-2020-cacapo,nan-etal-2022-fetaqa,parikh-etal-2020-totto,pasricha-etal-2023-cured4nlg,yermakov-etal-2021-biomedical,li-etal-2021-twt-table}. 
%For example, datasets like LogicNLG \citep{chen-etal-2020-logical} and DART \citep{nan-etal-2021-dart} have introduced significant challenges and innovations, emphasizing logical inference by introducing operations like counting and comparisons for Wikipedia tables. Meanwhile, DART \citep{nan-etal-2021-dart} fosters out-of-domain generalization through its diverse collection of over 82,000 instances sourced from semantic parsing datasets and spoken dialogue systems.
% 
%Controlled text generation in table-to-text generation ensures that outputs are tailored to specific user requirements or dataset characteristics. For instance, ToTTo’s controlled generation method \citep{parikh-etal-2020-totto}, which grounds outputs in highlighted table cells, ensures models focus on the most relevant parts of a table. Similarly, the TWT dataset \citep{li-etal-2021-twt-table} extends controlled generation by pairing structured table data with user-provided prefixes, enabling directed and contextually appropriate text generation. 
% 
%General-purpose benchmarks have been instrumental in advancing table-to-text reasoning \citep{chen-etal-2020-logical,nan-etal-2021-dart,lebret-etal-2016-neural,chen-etal-2021-wikitablet,van-der-lee-etal-2020-cacapo,nan-etal-2022-fetaqa,parikh-etal-2020-totto}. For instance, LogicNLG introduces logical operations like counting, comparisons, and majority detection, emphasizing logical inference over Wikipedia tables. 
%Similarly, DART \citep{nan-etal-2021-dart} provides an open-domain structured data-to-text generation dataset with over 82k instances, constructed using semantic triples extracted from tables. By integrating diverse sources like semantic parsing datasets and spoken dialogue systems, DART encourages out-of-domain generalization and introduces challenges related to table structure representation and semantic fidelity.
% 
%WikiBio \citep{lebret-etal-2016-neural} \todo{SE: not for science, can be omitted, I think} 
%focuses on generating short biographical summaries from Wikipedia infobox tables. With over 700k instances, WikiBio serves as a foundational benchmark for end-to-end table-to-text generation, particularly in producing fluent and coherent descriptions from key-value pairs. While large in scale, it primarily addresses simpler generation tasks without requiring logical inference or domain-specific reasoning.
%
% 
%WikiTableT \citep{chen-etal-2021-wikitablet} \todo{SE: same here; do we need that, given that our section is too long anyway?}
%expands long-form data-to-text generation by pairing Wikipedia sections with their tabular data and metadata. It provides millions of instances covering diverse topics, enabling models to explore long-form generation while addressing challenges in coherence and factuality.
% 
% 
%FeTaQA \citep{nan-etal-2022-fetaqa} focuses on table-based question answering where models must produce free-form answers. FeTaQA requires systems to retrieve, integrate, and infer information from table cells to generate coherent and natural descriptions. This task reflects more complex, real-world reasoning needs, such as combining multiple table entries into a single meaningful response.
% 
%ToTTo \citep{parikh-etal-2020-totto} introduces a controlled generation approach, where the output text is grounded in highlighted table cells. This ensures that models focus on the most relevant portions of a table, improving the faithfulness and relevance of the generated summaries.
% 
%Multilingual datasets have also emerged to support table-to-text generation. CACAPO \citep{van-der-lee-etal-2020-cacapo} is a multilingual, multi-domain dataset designed for training both pipeline and end-to-end data-to-text systems. It contains nearly 10,000 sentences in Dutch and English, paired with attribute-value data from news domains like sports, weather, stocks, and incidents. \todo{SE: But is any of that in the science domain?}
% 
\textbf{SciGen} \citep{BENCHMARKS2021_149e9677} and \textbf{numericNLG} \citep{suadaa-etal-2021-towards} are benchmarks %that are 
specifically focused on scientific table reasoning, both emphasizing arithmetic reasoning over numerical tables, containing %to generate textual summaries grounded in scientific insights. 
%Each dataset comprises 
1.3k expert-annotated tables. %The annotation include the tables and parts of the scientific paper that describe the corresponding findings of the annotated tables. 
The annotations include the tables and parts of the scientific papers that describe the corresponding findings of the annotated tables. %In SciGen, the annotations are done by the corresponding authors of each paper. 
%SciGen further introduces an automated data extraction pipeline that retrieves additional tables with their corresponding descriptions from LaTeX sources in scientific articles. 
%The retrieved table descriptions are not as fine-grained as the expert annotations; however, this approach can be scaled to any domain where access to the corresponding LaTeX source of each paper is available. %\citet{BENCHMARKS2021_149e9677} show that incorporating this data improves the reasoning capability of the examined models.
% 
A specific subtask of these benchmarks is explored in %Generating Textual Explanations for Machine Learning models’ Performance 
\citet{ampomah-etal-2022-generating}, which focuses on generating textual explanations for tables reporting ML model performance metrics. This dataset pairs numerical tables of classification performance (e.g., precision, recall, and accuracy) with expert-written textual explanations that analyze and interpret the metrics. %The study emphasizes the challenges of correctly verbalizing numerical data, and proposes a Metrics Processing Unit (MPU) to improve the accuracy of generated explanations. 
%
%Besides these core datasets tailored to the scientific domain, other benchmarks offer valuable insights that can enhance scientific table-to-text generation by addressing specific aspects of the task. \todo{SE: does that mean they're not directly for the scientific domain?} 
%For example, Logic2Text \citep{chen-etal-2020-logic2text} grounds textual descriptions in logical forms derived from structured table data, enabling controllable and faithful generation. CONTLOG \citep{liu-etal-2022-plog} extends this work by focusing on table content directly, ensuring outputs remain grounded in the most relevant information without relying on logical forms.
%
Datasets like %TaKG \citep{qi-etal-2022-takg} \todo{SE: is that about science?}
%and 
\textbf{HiTab} \citep{cheng-etal-2022-hitab} %addresses more nuanced challenges. %TaKG incorporates external knowledge graphs to enhance table inputs, supporting paragraph-level generation tasks that benefit from additional context and reasoning. 
%HiTab 
tackle the complexity of hierarchical tables commonly found in statistical reports, introducing numerical reasoning tasks that require models to account for implicit relationships and hierarchical indexing within tables.
% 
\textbf{SciXGen} \citep{chen-etal-2021-scixgen-scientific} broadens the scope of table-to-text generation with context-aware scientific text generation. By drawing from over 200k %,000 
scientific papers, SciXGen requires models to generate descriptions for tables, figures, and algorithms, grounded in the surrounding body text. %This dataset highlights the importance of contextual understanding and paragraph-level reasoning, which are vital for accurate and coherent scientific outputs.


%Domain-specific benchmarks have also been developed to address reasoning challenges in specialized fields \citep{pasricha-etal-2023-cured4nlg,yermakov-etal-2021-biomedical}. 
%The CURED4NLG dataset \citep{pasricha-etal-2023-cured4nlg} focuses on public health table-to-text generation, comprising 280 table-document pairs extracted from WHO COVID-19 reports. It highlights the challenge of generating factually accurate summaries, particularly for numerical values. \todo{SE: science relevance?}
%As an example, BioLeaflets \citep{yermakov-etal-2021-biomedical} introduces a benchmark for data-to-text generation in the biomedical domain, using package leaflets of European medicines. The dataset requires models to generate multi-sentence textual descriptions from structured data, presenting challenges in data fidelity and content generation in biomedical domain.


%TaKG \citep{qi-etal-2022-takg} addresses paragraph-level table-to-text generation and introduces external knowledge graphs to enhance table inputs. Containing ~750,000 samples across three domains, TaKG focuses on mitigating data divergence through Wikidata-based knowledge integration, supporting long-form generation tasks that require external context and reasoning.

%Logic2Text \citep{chen-etal-2020-logic2text} \todo{SE: science relevance?}
%introduces high-fidelity table-to-text generation by grounding textual descriptions in logical forms rather than raw table records. The dataset includes 10,753 logical descriptions derived from structured data and logical inferences across table rows, reflecting common logic types such as comparisons and aggregations. By formulating logical forms as input, Logic2Text ensures controllable and faithful generation, addressing challenges like hallucination and low precision that often arise in unconstrained generation.
%Building upon Logic2Text, CONTLOG \citep{liu-etal-2022-plog} reorganizes the original dataset by detecting and highlighting relevant table cells based on annotated logical forms. CONTLOG focuses directly on table-to-text generation without relying on logical forms, ensuring that outputs remain grounded in the most relevant table content.

%The HiTab dataset \citep{cheng-etal-2022-hitab} addresses the unique challenge of hierarchical tables, which are prevalent in statistical reports and cross-domain applications. HiTab introduces complex numerical reasoning tasks that require models to account for hierarchical indexing and implicit relationships within tables. The dataset features fine-grained annotations for quantity and entity alignment, supporting both QA and NLG tasks. 




%The TWT dataset \citep{li-etal-2021-twt-table} extends table-to-text reasoning by introducing controlled generation. TWT pairs structured table data with user-provided prefixes, enabling directed text generation. The dataset incorporates factual and logical statements, addressing challenges such as hallucination and improving content fidelity.

%Lastly, SciXGen \citep{chen-etal-2021-scixgen-scientific} goes beyond table-to-text generation by introducing a broader task of context-aware scientific text generation. Derived from over 200,000 scientific papers, SciXGen requires models to generate descriptions for various scientific objects, including tables, figures, and algorithms, based on surrounding body text. It further supports paragraph-level generation tasks grounded in references, highlighting the importance of contextual understanding for producing accurate and coherent outputs.

%These benchmarks collectively address key challenges in table-to-text generation that can be encountered in scientific discovery, including logical reasoning, arithmetic operations, hierarchical structure handling, multilingual generation, content integration, faithfulness, factual consistency, structured content grounding, controlled generation, and context-awareness. Together, they provide a robust foundation for advancing table interpretation across scientific and domain-specific applications.


\todo{SE: I changed the order here --> important for overview figure}
\paragraph{%Text-to-
Scientific Table Generation}

\se{Table generation often comes in the form of text-to-table generation \citep{shi2024ct,deng-etal-2024-text,jiang-etal-2024-tkgt},} 
%Text-to-table generation, 
the process of converting unstructured textual information into structured tabular formats. %represents a new paradigm in information extraction. 
%\todo{SE: At Univ. of Mannheim, there's a professor - Chris Bizer - working on this for a long time, it seems to me}
%Unlike traditional information extraction tasks that rely on predefined schemas, text-to-table generation allows for the creation of large, complex tables directly from long texts in a fully data-driven manner without explicitly defined structures \citep{wu-etal-2022-text-table}. 
This process is particularly valuable for scientific domains where textual data often contains detailed experimental results, observations, or findings that need transformation into structured tables. 
%Automating this process supports tasks such as literature mining, result summarization, and structured representation of insights \se{(e.g., of scientific content)}. %ultimately accelerating scientific discovery. %\todo{SE: and scientific content generation - I could generate a table from my description or vice versa}
%Several benchmarks have been developed to address challenges such as table structure inference, content fidelity, and evaluation robustness.
% 
In the scientific domain, \textbf{ArxivDIGESTables} \citep{newman-etal-2024-arxivdigestables} addresses the specific challenge of automating the creation of literature review tables. Rows in these tables represent individual papers, while columns capture comparative aspects such as methods, datasets, and results. %The dataset includes 2,228 tables extracted from 7,542 research papers on ArXiv, emphasizing schema generation and value extraction. %ArxivDIGESTables also introduces DecontextEval, an evaluation method aligning table elements with corresponding aspects despite surface-level variations. These innovations 
\se{ArxivDIGESTables} 
support\se{s} the generation of literature review tables by leveraging additional grounding context, such as captions and in-text references. 
%
\iffalse
Similarly, \textbf{SciDaSynth}  \todo{SE: does that need to be added in Table 4?} \citep{wang2024scidasynth} is an interactive system designed for structured knowledge extraction from scientific literature. Using %large language models, 
LLMs, 
it dynamically generates and refines tables based on user queries. %enabling deeper exploration of research findings. 
%Unlike static approaches, SciDaSynth adapts to evolving research needs, making it particularly suited for literature synthesis and comparative analysis in scientific domains.

\todo{SE: Is SciDaSynth a dataset? Here, we review datasets..}
\fi

%
%Beyond ArxivDIGESTables, other benchmarks contribute to advancing text-to-table generation. 
\iffalse 
The \textbf{Text-Tuple-Table} ($T^3$) framework \citep{deng-etal-2024-text} introduces the LiveSum dataset, which focuses on summarizing unstructured textual streams into structured tables. $T^3$ extracts global tuples from narrative content, such as real-time commentaries, and integrates them into coherent tables, emphasizing informativeness and content integration. 

For non-English languages, benchmarks like CT-Eval \citep{shi2024ct} and CPL \citep{jiang-etal-2024-tkgt} expand the applicability of text-to-table generation. CT-Eval, a large-scale dataset for Chinese text-to-table generation with 88.6K samples across 28 domains, focuses on minimizing hallucinations and ensuring domain-aligned structured outputs. Similarly, the CPL (Chinese Private Lending) dataset addresses domain-specific text-to-table tasks derived from legal judgments. The accompanying Text-KG-Table pipeline introduces a two-stage process, combining a Mixed-IE  method for generating domain knowledge graphs with a Hybrid Retrieval-Augmented Generation approach to transform these graphs into structured tables.

Although current benchmarks and methodologies primarily address general-purpose or domain-specific applications, \todo{SE: The benchmarks with `Arxiv' in the name are probably scientific. I still think we shouldn't discuss the other ones at length. And I feel, as a reader, better marking the scientific ones would be helpful. I am still not always sure which are general benchmarks and which ones are scientific ones}
their techniques and insights can inform the development of more robust approaches for scientific text-to-table generation. 
\fi 

\paragraph{Scientific Slide and Poster Generation.} %The automatic generation of presentation slides and posters from scientific papers has received increasing attention in recent years. 
Most early efforts to automatically generate presentation slides from scientific papers relied on relatively small datasets for system development and evaluation. For example, \citet{Sravanthi2009SlidesGenAG} collect source code (.tex files and figures) from eight papers and generate presentations based on them. Similarly, \citet{Hu2013PPSGenLT} and \citet{Wang2017PhraseBasedPS} utilize 1,200 paper-slide pairs and 175 paper-slide pairs, resp., within the %computer science 
CS domain. For scientific poster generation, \citet{Qiang2016LearningTG} 
construct a dataset %consisting 
of 25 pairs of scientific papers and their corresponding posters. %The authors further
%annotated panel attributes, such as panel width and panel height. 
However, these datasets are often inaccessible to the public due to various constraints.

Two open-source datasets have emerged as widely used resources for follow-up research for scientific slide generation: \textbf{DOC2PPT} \citep{Fu2021DOC2PPTAP} and \textbf{SciDuet} \citep{sun-etal-2021-d2s}. DOC2PPT \citep{Fu2021DOC2PPTAP}  comprises 5,873 paired scientific documents and their associated presentation slide decks with around  100,000 slides, drawn from three research communities: computer vision (CVPR, ECCV, BMVC), natural language processing (ACL, NAACL, EMNLP), and machine learning (ICML, NeurIPS, ICLR). SciDuet \citep{sun-etal-2021-d2s} %, on the other hand, includes 
\se{comprises}
1,088 papers and 10,034 slides from conferences such as ICML, NeurIPS, and the ACL Anthology. %Additionally, \citet{mondal-etal-2024-presentations} focus on a subset of 75 papers from SciDuet and created four persona-aware slide configurations for each paper: expert-long, expert-short, non-expert-long, and non-expert-short.

\subsubsection{Methods and Results} In the following, we survey approaches to multimodal content generation and understanding. 
A summary table, along with additional related works, is provided in Appendix \ref{ax:multimodal_generation}.
 
\paragraph{Scientific figure understanding.} Scientific figure understanding is typically framed in terms of (visual) QA, e.g., whether models are able to adequately answer questions on a given input figure \cite{10.1007/978-3-319-46493-0_15}. %Early work in this direction considers science school diagrams \citep{} 
%As an early work in this direction, \citet{10.1007/978-3-319-46493-0_15} consider answering questions on science school diagrams, where they `syntactically parse' the input diagram with LSTMs and leverage an attention-based model for diagram question answering. %They also provide a dataset with over 5k richly annotated diagrams and over 15k questions and answers. %\citet{10.1007/s00799-022-00329-y}
Several recent works train baseline models such as transformers \cite{masry-etal-2022-chartqa} or alternatives \cite{ebrahimi2018figureqa} such as \cite{NIPS2017_e6acf4b0} as well as explore recent LLMs on benchmarks \cite{wang2024charxiv,li-etal-2024-multimodal-arxiv}. 
%
\iffalse 
\citet{ebrahimi2018figureqa} %introduce FigureQA, a synthetic dataset of over 100k scientific-style figures, training 
train 
baseline models such as \cite{NIPS2017_e6acf4b0} on the FigureQA benchmark, finding them to substantially struggle with the task. %from five classes: line plots, dot-line plots, vertical and
%horizontal bar graphs, and pie charts. Associated with the images are more than 1m questions that are generated using 15 different templates. 
%More recently, 
%Later research focuses on harder and more realistic QA pairs as the templated questions may lead to an overly optimistic assessment of the capabilities of recent models. 
\citet{masry-etal-2022-chartqa} %present ChartQA, which provides complex reasoning questions over charts sourced from various sources related to science such as statista.com and pewresearch.org, which address social and economic issues. They 
leverage transformer based architectures, demonstrating their limitations in answering complex reasoning questions.
\citet{wang2024charxiv} explore multiple proprietary and open-source models for their benchmark %introduce 
CharXiv. %, a chart comprehension benchmark (where charts are particularly relevant in science or the financial domain). Their main contribution is a manually curated dataset of 2.3k ``natural, challenging, and diverse'' charts from %scientific 
%Arxiv 
%papers and %(involving 
%both descriptive and reasoning questions for them. 
\fi 
They generally  
show a big gap between proprietary models like GPT4o and the strongest open-source models and a big gap of all models to human performance. 
\iffalse 
\citet{li-etal-2024-multimodal-arxiv} %introduce ArxivQA, a dataset of 35k scientific figures sourced from Arxiv for which GPT4o generates 100k QA pairs after manual filtering. 
likewise explore multiple recent LLMs on their benchmark ArxivQA, finding  
%The authors show 
that recent open-source %LLMs 
models such as Qwen do not perform well on the benchmark but can be improved by fine-tuning on it. 
\fi 
%In contrast to focusing on question-answering for scientific figures, \citet{xu2024chartadapterlargevisionlanguagemodel} consider the chart summarization problem and a datset comprising more than 190k instances building on top of existing datasets such as ChartSumm \citep{Rahman2023ChartSummAC}. 
%\citet{}
For chart summarization, \citet{Rahman2023ChartSummAC} find that older PLMs such as BART and T5 suffer from hallucination and missing out of important data points. \citet{xu2024chartadapterlargevisionlanguagemodel} propose ChartAdapter, a lightweight transformer module that can be combined with LLMs for improved modeling of chart summarization. %\citet{wang2024charxiv} use GPT4o for automatic evaluation of answers, due to the variety of possible correct answers for questions. 
 % Rahman2023ChartSummAC, xu2024chartadapterlargevisionlanguagemodel
\textbf{Evaluation} of scientific figure understanding benchmarks %likewise leverages both humans and 
mostly leverages 
automatic metrics. For example, \citet{xu2024chartadapterlargevisionlanguagemodel} report out-dated and unreliable metrics such as BLEU and ROUGE for evaluating chart summaries; %casting some doubts on their findings; 
\citet{pramanick2024spiqa} report both human and automatic evaluation, using traditional QA metrics such as Meteor, Rouge, and BERTScore and novel LLM based metrics. 


\paragraph{Scientific Figure Generation.}

%Scientific figure generation is %recently 
%a nascent field of research, despite im. 
%While early research 
Early work in the context of visualization for science (and beyond) dates back to the 1980s and 1990s at least \citep{mackinlay1986automating,roth1991automating,roth1994interactive}. %From the start, these papers articulate diversity and inclusion of the user base as well as cost reduction as motivation for such tools. 
\iffalse 
For instance, \citet{mackinlay1986automating} design APT (``a presentation tool''), which lets users specify queries over databases (``Present the Price and Mileage Relation'') which are then supposed to be rendered in a graphics language (which can be converted into an image). To achieve this, \citet{mackinlay1986automating} use the ingredients of the time, such as logic programming, including rules and heuristics, for synthesis. %of a formal graphics language.
\citet{roth1991automating,roth1994interactive} develop SAGE, a multi-component tool consisting of a three-level process of selection, refinement and integration with a similar purpose, extending APT. 
\fi 
Subsequent research %such as \citet{sun2014articulate,narechania2020nl4dv,10.1145/3534678.3539330} 
\cite{sun2014articulate,narechania2020nl4dv,10.1145/3534678.3539330}
further developed rule-based or hybrid approaches, involving parsers and grammars, while custom neural architectures were later also explored %in 
\cite{8744242,liu2021advisor,10.1145/3514221.3520150}. %including RNNs, transformers such as BERT or encoder-decoder architectures. 
%The majority of those papers have originated in the visualization community and take (i) a database or dataframe and a natural language query as input and (ii) output a `code' in a graphics language, which can be rendered as an image. 

%Later research \citep{sun2014articulate}
%\citet{8744242} leverage RNN-based encoder-decoder networks to learn to transform JSON datasets into graphical representations using the Vega-Lite graphics language. 

% https://arxiv.org/abs/2412.20715
% ChartAdapter: Large Vision-Language Model for Chart Summarization

%Most recently and with the advent of %large multimodal language models, 
%multimodal LLMs, 
%scientific figure generation has also been targeted in the NLP community. %For example, 
Most recent work leverages multimodal LLMs. 
While \citet{10121440} explore diverse pre-trained LLMs, such as ChatGPT and GPT3, %for scientific figure generation. 
\citet{voigt2024plots} investigate smaller LLMs for real-time graphics generation on a CPU. 
%with the advent of large language models, 
\citet{belouadi2024automatikz,belouadi2024detikzify} %,belouadi2024detikzify,voigt2024plots} 
treat the problem as a TikZ code generation problem where the input is (i) a scientific caption \cite{belouadi2024automatikz} or (ii) a sketch or image \cite{belouadi2024detikzify}. Both works fine-tune custom LLMs on datasets leveraged from Arxiv. %disregarding dataframes or databases as input. %output is code, such as TikZ or Vega\footnote{\url{https://vega.github.io/vega-lite/}}, 
%and the input is (i) an (extended) scientific caption or instruction (\textit{text-to-figure generation}) or (ii) an image or sketch (\textit{figure-to-figure generation}). %
%\citet{belouadi2024detikzify} treat the problem as image-to-code generation where the input is a raster image or sketch. 
%Methodologically, \citet{belouadi2024automatikz} use  LLM fine-tuning on 100k+ pairs of input captions and output code, obtained from Arxiv submissions, and \citet{belouadi2024detikzify} additionally use Monte-Carlo Tree Search (MCTS) to guide the generation process. 
%The workflow of image/caption-to-code generation is visualized in Figure \ref{fig:figure_generation_overview}. 
\citet{shi2024chartmimicevaluatinglmmscrossmodal} %also 
aim to generate %output 
Python code %(which can be converted into an image) 
from instructions and/or images, specifically focusing on charts. %as important subproblem of scientific images. %\todo{SE: add methods} 
%Their main contribution is that of a manually curated benchmark of 1000 triplets of (figure, instruction, code) instances on which 
They evaluate 3 proprietary and 11 open-weight LLMs on their ChartMimic benchmark, finding that even the best models (GPT-4 and Claude-3-opus) have substantial room for improvement. 
\citet{zhang2024scimagegoodmultimodallarge} provide a template based approach to evaluate various multimodal LLMs in generating scientific images. %focusing on different comprehension dimensions (spatial, numeric and attribute understanding) for a targeted assessment of models. 
%For evaluation, they use human experts across three evaluation dimensions (correctness, relevance, scientificness). 
They explore LLMs that can generate code (TikZ and Python) and ones that directly generate images, without intermediate code synthesis and in addition consider different input languages (English, German, Chinese, Farsi). They find that, except for GPT4o, most models struggle substantially in generating adequate scientific images. %and that code-generation models have more problems with the spatial dimension while direct image generation models have more problem with numeric comprehension. %\todo{SE: mention focus on targeted evaluation}
% 
\citet{Zala2023DiagrammerGPT} %\todo[inline]{SE: [diagrams; We introduce the AI2D-Caption dataset for the text-to-diagram generation task. AI2D-
%Caption is built on top of AI2 Diagrams (AI2D) dataset (Kembhavi et al., 2016), which
%provides annotations of around 4.9K diagrams covering diverse scientific domains, from
%Grade 1-6 science textbook]}
explore the diagram generation task where LLMs first generate diagram plans and then the diagrams themselves. %The generated diagrams cover diverse scientific domains from Grade 1-6 science textbooks. 
\citet{mondal-etal-2024-scidoc2diagrammer} explore the same task with an additional refinement (feedback from multiple critic models) to enhance factual correctness. 
% 
%Inspired by the success of code generation, several papers have adopted the approaches for domains not primarily related to science, such as generating everyday multimodal objects from code \citep{rismanchian2024turtlebench,wu2024chat2svgvectorgraphicsgeneration,zou2024vgbench,rodriguez2024starvectorgeneratingscalablevector}.  
% 
%\citet{rismanchian2024turtlebench} [only related to science, though]
%
%\citet{wu2024chat2svgvectorgraphicsgeneration} [not scientific domain primarily, though]
%
%\citet{zou2024vgbench} [not primarily science domain though]
%
%\todo{SE: @SE: add evaluation and remove overlap with the dataset section!!}
\textbf{Evaluation} of the models comprises automatic metrics including  DreamSim \citep{10.5555/3666122.3668330}, for image similarity, crystal Bleu \citep{10.1145/3551349.3556903} for code similarity and ClipScore \citep{hessel-etal-2021-clipscore} for text-image similarity, and human evaluation by `domain experts'. The former are typically reported to have low or medium correlation with the latter, establishing the need for domain specific evaluation in future work. 

%\clearpage


\paragraph{%Table-to-text generation techniques.}
\se{Scientific Table Understanding.}}
%\todo{SE: given that these techniques are not tailored to science, can we reduce that substantially in the interest of space? --> Nafise}
Table-to-text generation encompasses a range of methodologies designed to transform structured tabular data into coherent and accurate textual descriptions. These techniques process, reason over, and utilize tabular structures to address challenges such as logical reasoning, content fidelity, and domain-specific adaptation. %This section 
%In the following, we 
%categorize prominent methodologies in the field, highlighting key advancements and contributions. 
%\todo{SE: @Nafise, please highlight what is actually done in **scientific** table understanding papers; the below can still be kept, perhaps with an indication that this is what people generally do}
%
\emph{Serialization} is a foundational approach where tables are linearized into sequences compatible with transformer-based language models. In this method, tables are converted into linear text sequences using special characters to delineate structure %enabling seamless integration with models such as T5, GPT, and BART 
\citep{BENCHMARKS2021_149e9677,parikh-etal-2020-totto,andrejczuk-etal-2022-table}. %While this approach is straightforward and effective for transforming structured inputs into natural language sequences, it can lose critical relational and hierarchical information, limiting its utility for complex reasoning tasks.   
% 
\emph{Structure-aware methods} explicitly model the inherent relationships and hierarchies within tables to enhance reasoning and generation fidelity. These include (a) \emph{Intermediate Representations} %: Techniques such as LoFT (Logic Form Transformer) 
\cite{zhao-etal-2023-loft,zhao-etal-2023-sortie,10.1162/tacl_a_00641}; %and SORTIE \citep{zhao-etal-2023-sortie} transform tables into logical forms or programming language-based formats to guide reasoning and ensure factual accuracy. 
%Graph-based approaches %, such as those by 
%\citep{10.1162/tacl_a_00641} %, 
%convert tables into graph structures, capturing dependencies and enabling relational reasoning for more contextually enriched text generation. 
(b) \emph{Structure-Aware Pretraining} %: Pretraining strategies incorporate table-specific tasks to refine LLMs by integrating datasets tailored for numerical and logical reasoning 
\cite{petrak-etal-2023-arithmetic,korkmaz-del-rio-chanona-2024-integrating,10707812}; (c) \emph{Structure-Aware Self-Attention Mechanisms}  %These mechanisms adapt the attention process to explicitly capture relational and hierarchical table structures. For example, graph-based attention mechanisms prune self-attention into order-invariant graph attention, capturing connections within rows and columns while maintaining robustness to structural transformations 
\cite{wang-etal-2022-robust,10.1145/3622896.3622919}. %\todo{SE: these methods not used for scientific table understanding? If so, we should please say so} 
%Similarly, prefix-tuning \citep{10.1145/3622896.3622919} attaches continuous prompts to encode structural nuances, guiding pretrained models to better align with table-specific features. %\todo{SE: is any of that used in science papers?}
%
% \citep{10.1162/tacl_a_00641}
%
\iffalse
\emph{Controlled table-to-text generation} focuses on selecting relevant parts of a table to guide the content of generated text. This %selection 
can be manual, where users specify the table regions of interest \citep{10446479}, or automated, where models identify the most pertinent sections \citep{bian2023helm}. These approaches ensure outputs are concise, contextually aligned, and tailored to specific informational needs. %enhancing precision and relevance in scientific reporting and technical summarization.
% 
\emph{Retrieval-Augmented Generation (RAG)} techniques integrate external knowledge sources to enhance the contextual relevance and informativeness of generated text. RAG can retrieve external knowledge to complement table content, enabling more comprehensive descriptions in complex scenarios \citep{10602781}, or combine retrieval with domain-specific knowledge graphs to ensure alignment between the generated text, the table, and external domain knowledge \citep{jiang-etal-2024-tkgt}. %These approaches significantly improve output quality for tasks requiring extensive background knowledge, particularly in domain-specific or knowledge-intensive applications.
%
% 
Emerging techniques, such as \emph{diffusion-based approaches}, aim to improve the diversity and quality of generated text \citep{10630478}. Additionally, knowledge distillation techniques, which transfer reasoning capabilities from advanced closed-source models to smaller models using augmented data generation, are also gaining attention \citep{yang-etal-2024-effective}.
\fi 
%
\textbf{Evaluation}: Common metrics like BLEU and BARTScore are widely used to evaluate the fluency and relevance of generated text against reference outputs. However, ensuring faithfulness to the source table remains a significant challenge, often requiring human evaluation for accurate assessment \cite{BENCHMARKS2021_149e9677,petrak-etal-2023-arithmetic}.

%\todo{SE: btw. you call it `text-to-table' or `table-to-text', while I would call the former: `table generation' and the latter `table understanding'}
\paragraph{Scientific %Text-to-Table 
Table 
Generation.} 

While none of the existing approaches have yet been applied specifically to scientific %text-to-
table generation, several methodologies present promising directions. %strategies that could be adapted for this task. 
% 
The gTBLS (Generative Tables) approach \citep{sundar2024gtbls} proposes a two-stage table generation process. The first stage infers the table structure from input text, while the second stage generates table content by formulating table-guided questions; %This method significantly 
this enhances %the 
syntactic validity and logical coherence of generated tables.  %showcasing advancements over previous baselines on multiple datasets.
% 
\iffalse 
For domain-specific tasks, MedT2T \citep{ZHAO2024586} introduces an adaptive text-to-table generation method tailored to medical information extraction. %Built on the BART backbone, 
%MedT2T addresses challenges unique to the medical domain, such as dense medical terminology, numerical constraints, and flexible table formats. 
It employs an Encoder-Decoder-Adapter architecture, where the adapter incorporates an adaptive pointer generation mechanism to accurately reference medical terms and numerical entities.
\fi 
% 
In the context of open-structure table extraction, OpenTE \citep{10448427} tackles the task of extracting tables with intrinsic semantic, calculational, and hierarchical structure from unstructured text. OpenTE introduces %the Identification-Extraction-Grounding framework, 
a three-step pipeline that identifies semantic and relational connections among table columns, extracts structured data, and grounds the output by aligning extracted data with the source text and table structure.
% 
The \textbf{evaluation} of text-to-table generation should focus on structural accuracy, value fidelity, and semantic coherence. TabEval \citep{ramu-etal-2024-bad} provides a promising direction by introducing a decomposition-based framework that breaks tables into atomic statements and evaluates them using entailment-based measures, though comprehensive evaluation still requires further advancements.

\todo{SE: shall we bold the key methods here?}

%evaluation
%To address the challenge of evaluating generated tables, TabEval \citep{ramu-etal-2024-bad} introduces a decomposition-based evaluation framework. TabEval breaks down tables into atomic statements and compares them with the ground truth using entailment-based measures. 



% \paragraph{Scientific slide generation.} The automatic generation of presentation slides from scientific papers has received increasing attention in recent years. In the following, we present an overview of several notable studies in this domain, highlighting their datasets, methodologies, and key contributions.

% \textbf{Datasets.} Most early efforts to automatically generate presentation slides from scientific papers relied on relatively small datasets for system development and evaluation. For example, \citet{Sravanthi2009SlidesGenAG} collected source code (.tex files and figures) from eight papers and generated presentations based on them. Similarly, \citet{Hu2013PPSGenLT} and \citet{Wang2017PhraseBasedPS} utilized 1,200 paper-slide pairs and 175 paper-slide pairs, respectively, within the computer science domain. However, these datasets are often inaccessible to the public due to various constraints.

% Two open-source datasets have emerged as widely used resources for follow-up research in this area: DOC2PPT \citep{Fu2021DOC2PPTAP} and SciDuet \citep{sun-etal-2021-d2s}. DOC2PPT \citep{Fu2021DOC2PPTAP}  comprises 5,873 paired scientific documents and their associated presentation slide decks with around  100,000 slides, drawn from three research communities: computer vision (CVPR, ECCV, BMVC), natural language processing (ACL, NAACL, EMNLP), and machine learning (ICML, NeurIPS, ICLR). SciDuet \citep{sun-etal-2021-d2s}, on the other hand, includes 1,088 papers and 10,034 slides from conferences such as ICML, NeurIPS, and the ACL Anthology. Additionally, \citet{mondal-etal-2024-presentations} focused on a subset of 75 papers from SciDuet and created four persona-aware slide configurations for each paper: expert-long, expert-short, non-expert-long, and non-expert-short.

\paragraph{Scientific Slide and Poster Generation.} For scientific slide generation, early works 
%in the design of scientific slide generation systems 
typically relied on heuristic rule-based approaches \cite{Sravanthi2009SlidesGenAG}. 
%For instance, \citet{Sravanthi2009SlidesGenAG} develop a rule-based system to generate slides for each section and subsection of a paper. The slide text content is generated using a query-based extractive summarization system. %The authors also extract graphical elements from the paper and place them at appropriate locations within the slides.  
Later, researchers began to leverage machine learning approaches to extract key phrases and their corresponding important sentences \cite{Hu2013PPSGenLT,Wang2017PhraseBasedPS, Li2021TowardsTS}. 
%\citet{Hu2013PPSGenLT} use a Support Vector Regression (SVR) model to learn the importance of each sentence in a paper. The slides are then generated using an integer linear programming (ILP) model to select and align key phrases and sentences. \citet{Wang2017PhraseBasedPS} propose a system to generate slides for each section of a given paper, focusing on creating two-layer bullet points. The authors first extract key phrases from the paper using %the Stanford Sentence Parser 
%a parser 
%and then use a random forest classifier to predict the hierarchical relationships between pairs of phrases. %\todo{SE: how can I generate a slide from keyphrases? That's probably not an image, but a sort of bullet point? -- YH: some work only generate text content; others generate text content + extract imanges/tables from the paper; no work explore to generate an image beyond the existing imanges from the paper, this could be a nice future research direction. SE: perhaps worth mentioning somewhere? (here or in limitations). Also text only approaches are still multimodal? - YH: 1) it's reflected in the future direction section; 2)text bullet points are presented in the format of multiple slides, so it should be counted as multimodal if we count slides as another modality?}
%\citet{Li2021TowardsTS} develop two sentence extractors—a neural-based model and a log-linear model—within a mutual learning framework to extract relevant sentences from papers. These sentences are used to generate draft slides for four topics: \emph{Contribution}, \emph{Dataset}, \emph{Baseline}, and \emph{Future Work}.  
%It is important to note that 
All the aforementioned works focus on extracting sentences or phrases from the given paper to serve as the slide text content (``extractive approach''). In contrast, \citet{Fu2021DOC2PPTAP} and \citet{sun-etal-2021-d2s} take a different approach by training sequence-to-sequence models to generate sentences %\todo{SE: extractive vs. abstractive}
for the slide text content (``abstractive approach''). %This distinction is analogous to the difference between ``extractive'' and ``abstractive'' summaries in text summarization. 
%More specifically, \citet{Fu2021DOC2PPTAP} design a hierarchical recurrent sequence-to-sequence architecture to encode the input document, including sentences and images, and generate a slide deck. 
%In contrast, \citet{sun-etal-2021-d2s} assume that slide titles would be provided by end users. The authors 
%use these titles to retrieve relevant and engaging text, figures, and tables from the given paper using a dense retrieval model. They then summarize  the retrieved content into bullet points with a fine-tuned long-form question answering system based on BART. %\todo{SE: which of these systems generate text vs. figures in slides? -- YH: I tried to explain this in the text for each paper, but this is a nice dimension for the summarized table or figure}
With recent advancements, 
%in %large language models (LLMs) 
%(multimodal) LLMs, %and vision-language models (VLMs), 
researchers have started utilizing %these technologies 
(multimodal) LLMs 
for generating scientific presentation slides \cite{mondal-etal-2024-presentations,maheshwari-etal-2024-presentations,bandyopadhyay-etal-2024-enhancing-presentation1}. %Note that among the previously mentioned works, only a few consider incorporating visual elements into the generated slide content. 
Notably, all existing approaches take an extractive approach, where they extract images or tables directly from the original papers rather than generating new ones \cite{Sravanthi2009SlidesGenAG,sun-etal-2021-d2s,Fu2021DOC2PPTAP,mondal-etal-2024-presentations,bandyopadhyay-etal-2024-enhancing-presentation1}.
%\citet{mondal-etal-2024-presentations} propose a system to generate persona-aware presentation slides by fine-tuning LLMs such as \emph{text-davinci-003} and \emph{gpt-3.5-turbo} with a small training dataset containing personalized slide decks for each paper. \citet{maheshwari-etal-2024-presentations} focus solely on generating text content and develop an approach that combines graph neural networks (GNNs) with LLMs to capture non-linearity in presentation generation, while attributing source paragraphs to each generated slide within the presentation. \citet{bandyopadhyay-etal-2024-enhancing-presentation1} design a bird’s-eye view document representation to generate an outline, map slides to sections, and then create text content for each slide individually using LLMs. The approach then extracts images from the original papers by identifying text-image similarity in a shared subspace through a VLM. %\todo{SE: so regarding images, all of these approaches are extractive, right? -- YH: yes}
%Compared to scientific slide generation,
Generating posters from scientific papers has received less attention. Previous work has mainly explored different machine learning-based methods for generating key content and panel layouts from data \cite{Qiang2016LearningTG,Xu2022PosterBotAS}.
\textbf{Evaluation}: For scientific slide generation, most works evaluate the effectiveness of proposed approaches using automatic evaluation metrics and conduct human evaluation. %studies. Since the task bears similarity to (query-based) text summarization, 
The most common automatic evaluation metric is ROUGE. %Additionally, some works have proposed task-specific automatic evaluation metrics. For instance, 
\citet{Fu2021DOC2PPTAP} introduce the Longest Common Figure Subsequence, which measures the quality of figures in the generated slides; Text-Figure Relevance (TFR), which assesses the similarity between the text of the ground truth slide and the generated slide containing the same figure; and Mean Intersection over Union, which evaluates layout quality. Recent studies have also begun utilizing LLMs to assess the quality of the generated slides \citep{bandyopadhyay-etal-2024-enhancing-presentation1,maheshwari-etal-2024-presentations}. For scientific poster generation, in addition to conducting user studies, \citet{Qiang2016LearningTG} also measure the mean-square error (MSE) of the panel parameters (e.g., panel size, aspect ratio). 

%Describe the state-of-the-art methods and their results, noting any significant qualitative/quantitative differences between them where appropriate.

% %%%%%%%%%%%% Moved to appendix  %%%%%%%%%%%%%%%%
\subsubsection{Ethical Concerns}

%\todo{Identify and discuss any ethical issues related to the (mis)use of the data or the application of the methods, as well as strategies for mitigations.}
Ethical concerns relating to models for figure, table, slide and poster generation especially include that these tools are technically limited (e.g., they may hallucinate content, be factually incorrect, and not correspond to the authors' intentions), which could be overlooked, ignored or maliciously abused by human authors. Such tools could also be misused to attack the scientific process, by purposefully producing incorrect results (e.g., as a testcase for adequate reviewing). Such risks may also be relevant in an educational context, e.g., when students use such tools for preparing term papers or theses.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\begin{itemize}
%\item Researchers may overly trust tools, not adequately taking their limitations into account (e.g., a generated scientific figure with wrong labels/legend), leading to factuality errors in scientific publications
%\item Researchers may purposefully misuse the tools to cheat (e.g. early career researchers) or to adversarially attack the scientific process (even though this could also be achieved by other, manual means, potentially at a higher cost, however)
%\item A similar risk occurs in the educational context, e.g., in teaching in seminars and lectures
%\item ...
%\end{itemize}

\subsubsection{Domains of Application}

%\todo{Indicate whether any of the data, methods, ethical concerns, etc. are specific to a given domain (biology, health, computer science, etc.).}
Many recent datasets for multimodal content generation and understanding are from ArXiv and more generally the STEM domain (science, technology, engineering and mathematics). Models such as DeTikZify and AutomaTikZ have also been fine-tuned on such data. This indicates a limitation both in terms of application scenarios and model assessments, as these may perform worse when applied in cross-domain scenarios. 

%\begin{itemize}
%\item Many of my papers are generally from `arxiv', covering multiple (STEM) domains
%\item The computer vision community has done the early papers, which may have considered other datasets
%\item There is a natural association to STEM, as these are more chart-/figure-heavy
%\end{itemize}

\subsubsection{Limitations and Future Directions}


%\todo{Summarize the limitations of current approaches; point out any notable gaps in the research and future directions.}
%blabla
Common limitations among all multimodal generating and understanding approaches discussed include: (1) comparatively small size of datasets for fine-tuning models; (2) models often perform considerably below human performance on recently proposed benchmarks; (3) this concerns especially open-source non-proprietary models; (4) models and benchmarks are often limited to STEM domains and particularly Arxiv; (5) models lack reasoning abilities; (6) evaluation of models is difficult, especially for generation models, and current automatic metrics are often unsatisfactory. Specific problems occur in subfields: for example, for table generation, input text may be very long, which constitutes a problem for many current LLMs; for slide generation, there are no approaches that can generate slides from multiple documents (e.g., for tutorials) or that generate content beyond a paper (which may be necessary to include relevant background); for figure generation, models like AutomaTikZ are trained on captions, which are often not appropriate for generating the corresponding figure (e.g., a caption may simply be `Proof of Theorem X'), leading to mismatch between input descriptions and output figures. 

%These limitations indicate the scope for future research. 
%All these limitations indicate  of scope for future research.

%\limitations{Common limitations include: (1) lack of reasoning, (2) inadequate evaluation, (3) size of data, (4) datasets only in restricted domains such as STEM, (5) unreliable models and clearly lacking behind closed-source models}


%limitation for table reasoning

%\begin{itemize}
%    \item end to end reasoning in text generation is still a big challenge
%    \item For table to text, we don't have reasoning-aware evaluation metrics, and it is not enough to only compare the generated text with the original description, the completely different descriptions may convey the same message, we don't yet have reliable faithfulness evaluation in such tasks
%    \item size of the data for scientific domains, to finetune models for these specialized domains
 %   \item the input size is often very large, and many models would struggle with that
%\end{itemize}

\iffalse 
limitations and future directions for scientific slides generation:
\begin{itemize}
    \item lack of dataset beyond CS domain
\item lack of reliable automatic evaluation metrics 
\item generating slides beyond the paper content but relevant for the talk (e.g., background knowledge, intriguing examples)
\item generating slides from multiple documents (tutorials)
\item generating figures or diagrams based on paper content, currently prior work mainly extract figures from the original papers
\item more fine-grained personalized slides generation 
\end{itemize}
\fi 

\iffalse 
For scientific figures:
\begin{itemize}
\item Detikzify, Automatikz: mismatch between text and images (captions are not descriptions)
\item Size of data
\item No reliable evaluation metrics
\item Open-source models considerably lack behind commercial models
\item Reasoning (like for other fields)
\item Some models cannot account for input data (e.g., x- and y-values), at least in their training data
\item Lack of data for some domains (e.g., social science?) due to heavy focus on STEM
\item No exploration of different output codes, and different models use different ones (Tikz, Vega, Python)
\item Multi-agent systems are lacking but are promising because models have different capabilities
\item In practice, systems seemingly don't work well even they have multiple hundred github stars (e.g. Detikzify), indicating potentially misleading evaluation and/or the difficulty of the problem (maybe also inability of domain adaptation)
\item ...
\end{itemize}
\fi 

%\subsubsection{AI use case}

%\todo{Optional: describe which portions of your section (figures, tables, text, etc.) have been assisted by AI and how.}
