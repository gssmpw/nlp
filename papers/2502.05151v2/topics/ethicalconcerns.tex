%\subsection{On generative AI as a Tool in Academic Writing}  
%(BK, SG) 

%In the following, a brief overview of the current discussions on generative AI and ethics is given. First of all, prevalent topics discussed are identified based on meta studies of recent literature on generative AI and ethics. Then the focus is shifted to ethical considerations/concerns regarding the use of generative AI in academia, including positions from editorial boards of journals. 
%Furthermore, taking Willcocks’ eight-point ‘Artificial Imperfections’ test as a starting point, we discuss the needs, expectations and ethical concerns related to the use of LLMs as a supportive tool (i) in academic writing including the potentials and caveats of LLMs in  researching/summarizing the state-of-the art in ever growing bodies of scientific publications, as well as (ii) within peer reviewing.

By now, there is a body of work addressing major ethical concerns related to generative AI. %Baldassarre et al. 
\citet{baldassarre2023social}, for instance, present a systematic literature review regarding the social impact of generative AI, especially taking into account 71 papers on ChatGPT. They identify the following areas of concern: privacy, inequality, bias, discrimination, and stereotypes. Another literature review on ethics and generative AI conducted by %Hagendorff 
\citet{hagendorff2024mapping} identifies the following topics as becoming increasingly of interest: jailbreaking, hallucination, alignment, harmful content, copyright, models leaking private data, impacts on human creativity. The work also identifies 19 distinct clusters of ethics topics with fairness/bias being the most frequently mentioned, followed by safety, harmful content/toxicity, hallucinations, privacy, interaction risks, security/robustness on ranks two to six, and writing/research on rank 18. 
%Ali and Aysan 
\citet{ali2024ethical} review 364 recent papers on generative AI and ethics published from 2022 to 2024 in different domains including the use of generative AI in scientific research. Regarding academia, the prevalent topics identified as critical are the authenticity of the work, intellectual property and academic integrity. 
\citet{sun2024trustllm} argue that in application areas such as scientific research, ensuring the trustworthiness of LLMs is crucial. %of particular importance. %Based on an extensive literature research, they identify eight key aspects that define the trustworthiness of LLMs: (i) truthfulness (ii) safety, (iii) fairness, (iv) robustness, (v) privacy, (vi) machine ethics, (vii) transparency, and (viii) accountability. \citet{sun2024trustllm} emphasize that 
In particular truthfulness, i.e., the accurate representation of information, facts and results by an AI systems, is an essential challenge for LLMs. % and highly relevant in the scientific context. 

Some benchmarks and datasets are designed to evaluate different aspects of truthfulness, for instance: TruthfulQA \cite{lin2021truthfulqa}, HaluEval \cite{li2023halueval}, and the FELM dataset \cite{zhao2024felm} identify hallucinations. SelfAware \cite{yin2023large} assesses the awareness of knowledge limitations. FreshQA \cite{vu2023freshllms} and Pinocchio \cite{yin2023large} explore adaptability to rapidly evolving information. 
%All of these are aspects with which an AI system generating text in the scientific context needs to be able to deal with. 
TrustLLM \cite{sun2024trustllm} is an extensive benchmark  incorporating existing and new datasets on the six aspects truthfulness, safety, fairness, robustness, privacy, and machine ethics to assess the trustworthiness of LLMs. Their results show that, in general, proprietary LLMs (e.g., ChatGPT, GPT-4) outperform most open-source LLMs in trustworthiness --- with %An exception is 
Llama2 \cite{touvron2023llama} as an exception. %, which surpass proprietary LLMs in trustworthiness in many tasks. 
However, both proprietary LLMs, as well as Llama2 often struggled to provide truthful responses when relying solely on internal knowledge. However, their performance improved significantly with additional external knowledge. Moreover, the authors observed a positive correlation between trustworthiness and the functional effectiveness of the model in %natural language processing 
\se{downstream}
tasks. %Also, the authors observed a notable discrepancy among different hallucination tasks: most LLMs showed fewer hallucinations in multiple-choice question-answering tasks compared to more open-ended tasks such as knowledge grounded dialogue.

Editors of scientific publications are particularly challenged as the proportion of AI generated text in academic manuscripts is \se{steadily} increasing   \cite{gray2024chatgptcontaminationestimatingprevalence, kobak2024delving, liang2024mapping, cheng2024have} and AI models are also on the brink of being used in peer reviewing, cf. Section \ref{sec:peer_review}. The editors-in-chief of the Journal of Information Technology, for instance, elaborate in \cite{schlagwein2023chatgpt} on the limitations and risks of using generative AI in the production of scientific publications. They refer to the eight-point ‘Artificial Imperfections’ test to illustrate current limitations of generative AI: %(the 8 points are quoted from \cite{schlagwein2023chatgpt}): 
AI is (1) brittle, (ii) opaque, (iii) greedy, (iv) shallow and tone-deaf, (v) manipulative and hackable, (vi) biased, (vii) invasive, (viii) `faking it'.  
\iffalse 
\begin{itemize}
\item	AI is brittle. It provides only narrow and non-human ‘intelligence’.
\item	AI is opaque. Its inner workings are black-boxed.
\item	AI is greedy. It requires large data training sets as well as processing power, memory and energy.
\item	AI is shallow and tone-deaf. It produces decisions and knowledge claims without understanding, feeling, empathising, seeing, creating, reflecting or even learning in any human sense of these terms.
\item	AI is manipulative and hackable. The digitalisation of many decisions makes them amenable to manipulation by corporations and governments at scale, or open to hacking by malicious actors.
\item	AI is biased. It is inherently biased in that it encodes and enshrines any biases found in the training data.
\item	AI is invasive. It commoditises private data.
\item	AI is faking it. It is ‘bullshitting-as-a-service’. AI is not creative; it fakes creativity. It is not emotional; it fakes emotions, etc.
\end{itemize}
\fi 
Nevertheless, the editors conclude that the use of AI should not be forbidden, however, if it is used, the authors must take full responsibility of the outcome, adhere to the “scientific principle of transparency” and give full and transparent disclosure of the usage of AI in the respective publication, and moreover that “it is then up to the reviewers and editors to assess and make decisions on the specific use of that generative AI in a specific piece of research.”
Similarly, \citet{pu2024chatgpt} in their editorial to iMeta 3(2), 2024 state that AI‐assisted technologies cannot be recognized as authors, the use of generative AI in a scientific manuscript/publication must be transparently disclosed, including the prompts, specific versions of the tools used. The authors are fully responsible for the integrity of their manuscripts. They must address ethical concerns and ensure the accuracy and fairness of AI-generated content, comply with data protection and privacy laws, and also consider copyright and intellectual property issues surrounding AI‐generated content. The use of AI‐generated images and multi-media must be specifically allowed. They explicitly prohibit the use of AI in the reviewing process.

\iffalse
\todo{SE: what's the purpose of the text below? Didn't we have this earlier as well?}
%So far so good, 
The proportion of AI generated text in academic manuscripts is \se{steadily} increasing. Recently, a number of studies have been published that examine the use of LLMs in scientific writing. A prevalent approach is to analyze scientific publications with respect to the occurrence of words identified as predominant in LLM generated text, and to compare the occurrence of respective words before and after the launch of ChatGPT in 2022, e.g.,  \cite{gray2024chatgptcontaminationestimatingprevalence, kobak2024delving}.  \citet{liang2024mapping} conduct an analysis across 950,965 papers published in arXiv, bioRxiv, and Nature journals between January 2020 and February 2024 and find a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5\%). Moreover, the authors find that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. \citet{cheng2024have} analyze a dataset of 45129 preprint manuscripts uploaded to arXiv, bioRxiv, and medRxiv from January 2022 to March 2024 and find that (i) the use of AI in the manuscripts  correlates with the trend of ChatGPT web searches; (ii) it is widespread across scientific domains but highest in computer science and engineering; (iii) the usage varies with language backgrounds and geographic location of the author affiliations, with Italy and China leading (>5\%); and (iv) AI-generated texts are most significantly used in hypothesis formulation and conclusion summarization.
Accordingly, detection of LLM-generated text is an emerging field of research; see for instance \cite{tang2024science, wu2025survey} for surveys.

Last but not least AI models are on the brink of being used in peer reviewing: \todo{SE: what do to with this? Remove? Part of peer reviewing section?}
\citet{checco2021ai}, for instance, train an AI model on 3 300 past conference papers and the associated review scores. When showing unreviewed papers, the AI model could often predict the peer review outcome by only using the textual content of the new manuscript. The authors also discuss explainability methods such as LIME to explain the model decisions on a specific document. This has the potential to uncover biases in the review process.
Summing up, the use of AI models in scientific writing and peer reviewing has become reality. Editors are very well aware of it and try to regulate and curb the use of AI. This requires both trust in the authors' veracity as well as technical means for verification. %Accordingly, automatic detection of AI-generated text is an emerging field of research.
\fi

