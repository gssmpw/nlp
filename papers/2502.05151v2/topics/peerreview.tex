\subsection{Peer Review}
\label{sec:peer_review}

%\todo{CG: Correctly locate figures/tables after finishing writing}
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{image/peer_review.pdf}
  \caption{Process of AI-enhanced peer review. In the analysis step, the LLM reviewer examines research manuscripts and evaluates peer reviews to assess scientific rigor. The review step involves providing feedback on the paper and verifying scientific claims. Finally, the gathered information is synthesized to generate a final meta-review. } 
  \label{fig:peer_review_overview}
\end{figure*}
% : analysis, where an LLM assesses manuscripts and reviews for rigor; review, where it provides feedback and verifies claims; and synthesis, where it generates a final meta-review.
% In the analysis step, the LLM reviewer examines research manuscripts and evaluates peer reviews to assess scientific rigor. The review step involves providing feedback on the paper and verifying scientific claims. Finally, the gathered information is synthesized to generate a final meta-review.
%Provide a concise description of the task here, indicate why it is important, and provide any necessary background information/references to contextualize the following subsections.

The highest standard in scientific quality control is \emph{peer reviewing}. In this process, the authors present their scientific argument (e.g., the findings of a study, a grant proposal, etc.), in form of a manuscript to their peers, who then assess its scientific validity and excellence. Often, this process has multiple stages, as shown in Fig. \ref{fig:peer_review_overview}. For instance, in the ACL Rolling Review system,\footnote{\url{https://aclrollingreview.org}} \emph{reviewers} write detailed assessments. Afterwards, the \emph{authors} may rebut the reviewers' arguments and clarify questions to convince them to raise their scores. Finally, a meta-reviewer re-evaluates the whole scientific discussion and gives a final acceptance recommendation (which the overall program chairs may or may not adhere to). During this process, multiple (potentially multi-modal) artifacts are involved and created, mainly \emph{the manuscript} under review, \emph{the written reviews}, \emph{the author-reviewer discussion texts}, and \emph{the meta-review}. 
In general, peer review is considered a challenging, and  subjective process, where reviewers are prone to unfair biases like sexism and racism, often relying on quick, simple heuristics~\cite[e.g.,][]{strauss2023racism, regner2019committees}. At the same time, we are faced with an exploding number of submissions in some fields like AI~\cite{kunzli2022not}, pushing peer reviewing systems to the limits of their capacities.

To counteract this problematic situation, researchers have worked on several problems under the umbrella of AI-supported peer review. Related overviews on the topic (or on some of its aspects) are given by \cite{kousha,drori2024human,staudinger-etal-2024-analysis,lin2023automated,checco,kuznetsov2024can}.  %\cite{
%drori2024human %}, \cite{
%staudinger-etal-2024-analysis, %}, %\cite{
%lin2023automated, 
%}, and \cite{
%checco}, 
pointing to the high relevancy of this problem. %Kutznetsov et al. \cite{kuznetsov2024can} provide an extensive discussion, outlining the potential of applying natural language processing techniques along all stages of the peer reviewing process. 
Here, we focus on existing works targeting the most established tasks, following the same structure as before, and provide an update on the recently published literature.

\subsubsection{Data}
%Give an overview of the most important curated/annotated datasets, or sources of raw data, that are used (or potentially useful for) this task.
% TODO: add all of the raw data mentioned here to the table below
Peer reviewing data is generally scarce, given that the scientific communities do not always make all reviewing artifacts publicly available under openly accessible licenses, with some exceptions like \href{https://iclr.cc}{ICLR}. %Accordingly, these exceptions make up the majority basis for several annotated datasets (see below). 
%Other works have also collected raw datasets, possibly further enriched and extended. For instance, 
As some exceptions, 
the PeerRead \cite{kang-etal-2018-dataset} collection of data from various sources (e.g., ACL, ICRL) and CiteTracked \cite{Plank2019CiteTrackedAL} %is
are 
published along with citation information. %which the authors specifically collected. 
As a prime example of how larger-scale open publishing of raw peer reviewing data may work, \citet{dycke-etal-2023-nlpeer} recently published the NLPeer corpus based on ARR reviews, for which they explicitly obtained the consent of the respective actors involved. 
%
%
% some works are missing given that almost every work also introduced or extended a data set
%
\begin{table}[th!]
\small
    \centering
    % \begin{tabular}{l l l l}
    \begin{tabular}{p{2.5cm} p{2.5cm}p{2.3cm}p{6.2cm}}
    \toprule
       \textbf{Dataset}  & \textbf{Size} & \textbf{Sources} & \textbf{Application} \\
       \midrule
         %\multicolumn{3}{c}{\textbf{Style Analysis, e.g., uncertainty detection}} \\ \midrule
        % uncertainty detection
        HedgePeer \cite{10.1145/3529372.3533300} & 2,966 documents & ICLR 2018 reviews & Uncertainty detection\\
PolitePeer \cite{politepeer} &2,500 sentences & Various, %sources, 
e.g., ICLR & Politeness Analysis\\
 %        \multicolumn{3}{c}{\textbf{Comparison Analyses}} \\ \midrule
COMPARE \cite{singh2021compare} & 1,800 sentences & ICLR & Comparison Analysis\\

ReAct \cite{Choudhary_2021} & 1,250 comments & ICLR & Actionability Analysis\\
MReD \cite{shen-etal-2022-mred} & 7,089 meta-reviews & ICLR & Meta-review analysis and generation\\ 
CiteTracked \cite{Plank2019CiteTrackedAL} & 3,427 papers and 12k reviewss & NeurIPS & citation prediction \\
MOPRD \cite{Lin_2023} & 6,578 papers & PeerJ & Review Comment Generation \\ 
Revise and Resubmit \cite{10.1162/coli_a_00455} & 5.4k papers & F1000Research & Tagging, Linking, Version Alignment  \\
ORB \cite{szumega2023open} & 92,879 reviews & OpenReview, SciPost & Acceptance Prediction\\ 
ARIES \cite{d2023aries} & 3.9k comments & OpenReview & Feedback-Edits Alignment, Revision Generation  \\ 
DISAPERE \cite{kennard-etal-2022-disapere} & 506 review-rebuttal pairs & ICLR & review action analysis, polarity prediction, review aspect \\
PeerReviewAnalyze \cite{10.1371/journal.pone.0259238} & 1,199 reviews & ICLR & Review Paper Section Correspondence, Paper Aspect Category Detection, Review Statement Role Prediction, Review Statement Significance Detection, and Meta-Review Generation\\
%\multicolumn{3}{c}{\textbf{Argumentation Analysis}} \\ \midrule
JitsuPeer \cite{purkayastha-etal-2023-exploring} & 9,946 review and 11,103 rebuttal sentences& ICLR & Argumentation Analysis, Canonical Rebuttal Scoring, Review Description Generation, End2End Canonical Rebuttal Generation\\

\bottomrule
    \end{tabular}
    \caption{Annotated or task-specific datasets for analyzing peer reviewing.}
    \label{tab:data_peer_reviewing}
    \vspace{-5mm}
\end{table}
%
%
%
For several tasks around peer review analyses, researchers have created annotated datasets. An overview of annotated and/ or task-specific datasets focusing on diverse aspects of peer review is provided in Table~\ref{tab:data_peer_reviewing}. %\todo{SE: fix link - CL: Done} 
%For instance, \cite
Most recently, researchers focused on curating resources for supporting more complex tasks, like understanding the effect of peer review feedback on revisions of the manuscript~\cite{d2023aries} or on identifying the underlying attitudes that cause a specific criticism in peer review~\cite{purkayastha-etal-2023-exploring}.


\subsubsection{Methods and Results}

%Describe the state-of-the-art methods and their results, noting any significant qualitative/quantitative differences between them where appropriate.
%Generally, the overall trends for computational approaches targeting the processing of peer reviews follow the  overall trends in NLP.  %natural language processing. 
Initial works were mostly based on more traditional machine learning methods and targeted simpler analyses involving sentence classification tasks. Later, deep learning approaches (also based on pre-trained language models) and more complex analyses, e.g., argumentation analyses, were defining the state-of-the-art in computational peer review processing. Nowadays, researchers started exploring %the use of large language models 
LLMs 
in prompting-based frameworks for complex tasks like peer review generation and meta-review generation. \todo{SE: add citations here?}
% First simpler analyses, then more like relationship exploitation, then LLMs


\paragraph{Analysis of Peer Reviews}
%Preceding 
Prior 
works have analyzed peer reviews for a multitude of aspects, like uncertainty~\cite{10.1145/3529372.3533300}, politeness~\cite{politepeer}, and sentiment~\cite{Chakraborty_2020}. 
%\cite{10.1145/3529372.3533300} uncertainty detection with base models like scibert and xlnet, also try mtl with sentiment but that doesn't work
%\cite{politepeer} politeness detection wiht scibert, toxicbert, word2vec
%\cite{Chakraborty_2020} aspect-based sentiment analysis traditional approaches like svm and mulitnominal naive bayes vs. deep learning like cnn, scibert embeddings
However, given that science as a whole and especially peer review relies to a large extent on convincing %our 
peers, large efforts have 
%, in particular, 
been spent on understanding arguments or argument-related aspects (e.g., substantiation of arguments) in peer review artifacts~\cite[e.g., ][]{Fromm2021,hua-etal-2019-argument}.
Here, most approaches leveraged pre-trained language models. For instance, \citet{hua-etal-2019-argument} work on mining the arguments in peer reviews using conditional random fields, %and %bi-directional long short-term memory networks (LSTMs) 
LSTMs, 
and BERT. In contrast, \citet{guo-etal-2023-automatic}  and \citet{Fromm2021} fully rely on (domain adjusted) pre-trained language models for argument mining like SciBERT, ArgBERT, and PeerBERT. \citet{cheng-etal-2020-ape}  leverage multi-task learning approaches based on LSTMs and BERT. In a similar vein,  \citet{purkayastha-etal-2023-exploring} study the generation of rebuttals for author-reviewer discussions based on Jiu-Jitsu argumentation, a specific  theory in argumentation theory. \todo{SE: where's the recent SOTA?}
%Given that science as a whole and especially peer review relies to a large extent on convincing our peers, large efforts have also been spent on understanding arguments in peer review artifacts.
%\cite{Fromm2021} argument mining, argbert, peerbert
% \cite{cheng-etal-2020-ape} argument pair extraction, mtl with lstms and bert
%\cite{guo-etal-2023-automatic} analysis of substantiation (also new dataset), SciBERT, RoBERTa, SpanBERT
% \cite{hua-etal-2019-argument} argument mining, crf, bilstm crf, elmo

\paragraph{Paper Feedback and Automatic Reviewing}
Several works have explored methods to provide general feedback on scientific publications to fully or partially automate peer reviews.  %i.e., peer feedback in its institutionalized form. Note, that relevant to this task, other works also focused on educational contexts (e.g., \cite{nguyen-litman-2014-improving}, \cite{su-etal-2023-reviewriter}, and \cite{bjet}).
%Originally, researchers mostly focused on score prediction given a particular reviewing artefact. 
For instance, \citet{li-etal-2020-multi-task} propose a multi-task learning approach for peer review score prediction, where different aspect score prediction tasks (e.g., novelty) can inform each other. \citet{ghosal-etal-2019-deepsentipeer} leverage the concept of sentiment to predict scores based on review texts. In a similar vein, \citet{10.1007/978-3-030-91669-5_33} leverage paper-review interactions to predict final decisions of a review process. %The idea is to envision a collaborative effort, in which reviews are human-written, and the AI supports the final decision.
%Importantly, 
\citet{wang-etal-2020-reviewrobot} focus on explainability during review score prediction for several review categories by constructing knowledge graphs (e.g., one which represents the background of a paper). More recent works have included the generation of feedback texts into the problem setup.
%Interestingly, 
\citet{bartoli2020} frame the problem as exploring the potential of GPT-2 for conducting academic fraud by generating fake reviews. 
In contrast, \citet{10.1613/jair.1.12862} ask whether it would be possible to automate reviewing leveraging %the potential 
targeted summarization models, \se{a recently trending topic}. 
%and similarly, many of the most recent works, do not study automatically generating reviews under the perspective of academic fraud. 
%Researchers are exploring more and more to what extent LLMs can be leveraged to automate peer reviews, or to support reviews in this process. 
For instance, \citet{liu2023reviewergpt} explore prompting-based review generation with several LLMs like GPT-4, Vicuna, Llama. They find that GPT-4 performs best among the models tested and that task granularity matters. Similarly, \citet{robertson2023gpt4} find GPT-4 to be ``slightly'' helpful for peer reviewing, and \citet{liang2023can} demonstrate in a comparative study that users of a GPT-4-based peer review system found the feedback to be (very) helpful in more than half of the cases. 
 \citet{d2024marg} show a multi-agent approach with LLMs that engage in a discussion to produce better results than a single model. %\todo{YC: The citation format of this paragraph is different from other sections. It's better to keep consistent, e.g. Chan et al. [29]. SE: please fix, not comment}




%\cite{Biswas2023ChatGPTAT}













\paragraph{Scientific Rigor} 

%Despite its fundamental importance, existing guidelines or definitions for rigor are often vague and general, such as the NIH's suggestion to justify the methodology, identify potential weaknesses, and address limitations~\cite{johnson2020review,wilson2021three}. 
%\citet{sansbury2022rigor} highlight the importance of rigor in study design and conduct, statistical procedures, data preparation, and availability. In addition, there exist many domain-specific requirements for rigor proposed by researchers. For example, \citet{lithgow2017long} believe stricter variability control is necessary for animal research, 
%following strict handling protocols, and adhering to precise methodological guidelines. However, these %aforementioned 
%criteria are predominantly developed in a top-down fashion, relying heavily on domain experts' experience. 

Several attempts have been made to computationally analyze the rigor of scientific papers. For example, \citet{Wael} investigate how researchers use the word ``rigor'' in information system literature but discovered that the exact meaning was ambiguous in current research. Additionally, various automated tools have been proposed to assess the rigor of academic papers. \citet{phillips2017online} develop an online software that spots genetic errors in cancer papers, while \citet{sun2022assessing} use knowledge graphs to assess the credibility of papers based on meta-data such as publication venue, affiliation, and citations. 
However, these methods are neither domain-specific nor do they provide sufficient guidance for authors to improve their narrative and writing.
In contrast, SciScore \cite{SciScore_2024} is an online system that uses language models to produce rigor reports for paper drafts, helping authors identify weaknesses in their presentation. 
%However, they rely on existing rigor checklists suggested by NIH and MDAR \cite{chambers2019towards}, which are not easily scalable or transferable to other domains. %\todo{SE: could go to limitations - CL: Done} 
More recently, \citet{james-etal-2024-rigour} propose a bottom-up, data-driven framework that automates the identification and definition of rigor criteria while assessing their relevance in scientific texts. Their framework integrates three key components: rigor keyword extraction, detailed definition generation, and the identification of salient criteria. Additionally, its domain-agnostic design allows for flexible adaptation across different fields.
%Furthermore, it is domain-agnostic, allowing for tailoring to the evaluation of scientific rigor across different areas by accommodating distinct salient criteria specific to each field. 

\paragraph{Scientific Claim Verification} 

The increasing number of publications requires the development of automated methods for verifying the validity and reliability of research claims. Scientific fact verification, which aims to assess the accuracy of scientific statements, often relies on external knowledge to support or refute claims \citep{vladika2023scientific, dmonte2024claim}. Several datasets have been developed to address this including SciFact-Open \citep{wadden2022scifact}, which provides scientific claims and supporting evidence from abstracts. However, they are limited to the use of abstracts as the primary source of evidence. As the statements in abstract can also be inaccurate (e.g. overstated claims), it is important to evaluate the evidence in the main body of the paper to determine if the statements made in the abstract are well supported. On the other side, \citet{glockner-etal-2024-missci,glockner2024groundingfallaciesmisrepresentingscientific} %and \citet{glockner2024groundingfallaciesmisrepresentingscientific} 
propose a theoretical argumentation model to reconstruct fallacious reasoning of false claims that misrepresent scientific publications.
% 
The need to contextualize claims with supporting evidence is highlighted by \citet{chan2024overview}, who introduce a dataset of claims extracted from lab notes. Unlike other datasets, this resource contains claims ``actually in use'', providing a more realistic understanding of how researchers interact with scientific findings. The authors annotate these claims with links to figures, tables, and methodological details, and develop associated tasks to improve retrieval. While this provides valuable resources for context-based verification, it primarily focuses on factual verification and does not evaluate the potential for overstated claims.
% 
Beyond factual correctness, there is a growing recognition for the need to analyze how researchers present their findings. This includes the detection of overstatements, where authors exaggerate their achievements, and understatements, where the true impact of the research is downplayed \citep{kao2024we}. Such analysis goes beyond the simple fact of a claim and is necessary to understand the presentation of a claim. \citet{schlichtkrull2023intended} present a qualitative analysis of how intended uses of fact verification are described in highly-cited NLP papers, particularly focusing on the introductions of the papers, to understand how these elements are framed. The work suggests that claims should be supported by relevant prior work and empirical results.

% Related to the area of scientific rigor analysis as well as ultimately, to the task of peer review feedback generation, is scientific claim verification. The idea is, to either verify claims made about a field based on the related literature or  ...
% \citet{wadden-etal-2022-scifact}

% \citet{papadopoulos2023red}

% \citet{wadden-etal-2022-multivers} 

% \cite{wadden-etal-2020-fact}

\paragraph{Meta Review Generation}
%While, for long, meta reviews have not been the focus of NLP/AI research, lately, more works look at tasks related to this particular artifact in the peer reviewing process. %As such, 
\citet{9651825} tackle meta-review generation using a multi-encoder transformer network, and \citet{li-etal-2023-summarizing} use a multi-task learning approach for refining pre-trained language models for the task. \citet{stappen2020uncertainty} explore the aggregation of reviews for providing additional computational decision support to editors based on uncertainty-aware methods like soft labeling. Both \citet{zeng2023meta} and \citet{santu2024prompting} rely on %large language models 
LLMs 
which they specifically prompt for the task.
%\cite{9651825} meta review generation, multi-encoder transformer network
% \cite{li-etal-2023-summarizing} % pretrained lms, multi-task training
%\cite{santu2024prompting}
% \cite{zeng2023meta}
%\todo{YC: The citation format of this paragraph is different from other sections. It's better to keep consistent, e.g. Chan et al. [29]. SE: Please fix it, no need for a comment ...}


% What to do with those? --> these are nnot inside the text yet
%\cite{nguyen-etal-2016-instant} traditional feature engineering with logistic regression for solution prediction
%\cite{kumar-etal-2023-reviewers} reviewer disagreement prediction, utilize Multi- Instance Multi-Label Learning Network 
% \cite{SUN2024101501} citation prediction based on textual features from peer review

% %%%%%%%%%%%% Moved to appendix  %%%%%%%%%%%%%%%%
\subsubsection{Ethical Concerns}
%Identify and discuss any ethical issues related to the (mis)use of the data or the application of the methods, as well as strategies for mitigations.
Given the critical role of scientific peer review for science, and, accordingly, for society as a whole, ethical considerations around AI-supported peer review are of utmost importance. As the general concerns around unfair biases in AI and the resulting harms apply~\cite{kuznetsov2024can}, research on safe peer-reviewing support needs to be prioritized. For instance, \citet{10.1001/jama.2023.24641} recently showed that %large language models 
LLMs exhibit 
%showed 
affiliation biases when reviewing abstracts. In this context, any AI-support for peer reviewing needs to be critically evaluated~\cite{schintler2023critical}, and solutions that target only a particular aspect in a collaborative environment that leaves the scientific autonomy to the human expert, may need to be preferred over end-to-end reviewing systems.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\cite{10.1001/jama.2023.24641}
%\cite{schintler2023critical}


\subsubsection{Domains of Application}
%Indicate whether any of the data, methods, ethical concerns, etc. are specific to a given domain (biology, health, computer science, etc.).
Generally, peer review comes in many variations. For instance, the specific aspects to review for, how much textual content to produce, the specific scoring schemes, and the envisioned stages and dynamics of the reviewer and reviewer-author discussions may change. Thus, while none of the studies presented above targets a problem that is truly unique to any scientific domain, the particularities will likely be very different for each specific community and existing systems will need to be carefully evaluated before deployment.
 \todo{SE: but aren't most models and datasets originate from AI/NLP?}

\subsubsection{Limitations and Future Directions}
%Summarize the limitations of current approaches; point out any notable gaps in the research and future directions.
For existing studies on peer review, in particular, the variety of scientific domains that have been studied is still limited. As most of the works rely on data from OpenReview, most studies focus on peer review within the ICLR and ACL communities~\cite[e.g.,][]{Choudhary_2021,kennard-etal-2022-disapere}. \todo{SE: see above}
To the best of our knowledge, for some domains, no single data set (yet, a data set further enriched with annotations or other additional information) exists (e.g., legal studies). 
Furthermore, scientific rigor, a critical aspect of peer review, remains underexplored. Most existing studies rely on predefined rigor checklists, such as those suggested by the NIH and MDAR \cite{chambers2019towards}, which are not easily scalable or transferable across different domains. 
Given these gaps, future research could benefit from exploring new domains of peer review, developing domain adaptation approaches, and advancing models for assessing scientific rigor. Additionally, in light of the ethical concerns discussed earlier, it is crucial to prioritize research on trustworthy AI support for peer review - ensuring that human experts retain autonomy in the review process. \todo{SE: what about the quality of models?}

%Variety of domains and data sources

%\subsubsection{AI use case}

%Optional: describe which portions of your section (figures, tables, text, etc.) have been assisted by AI and how.