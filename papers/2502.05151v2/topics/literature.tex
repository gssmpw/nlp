\subsection{Literature Search, Summarization, and Comparison}
\label{sec:literature_search}

%\collaborators{Jennifer D'Souza, Andreas Geiger, Yong Cao}

%Provide a concise description of the task here, indicate why it is important, and provide any necessary background information/references to contextualize the following subsections.

% \todo{SE: several abbreviations already used, especially LLMs -- no need to reintroduce them}
\mybox{
%The rapid growth of scientific literature presents a significant challenge for researchers who need to search, analyze, and synthesize vast amounts of information efficiently. The process of literature search, summary, and comparison is crucial in enabling researchers to stay updated with the latest developments, identify relevant studies, and draw meaningful conclusions from existing work. AI-powered tools are transforming these tasks by leveraging natural language processing (NLP), machine learning (ML), large language models (LLMs), citation graphs and knowledge graphs (KGs) to automate the retrieval, extraction, and synthesis of scientific information. Unlike traditional search engines that rely on basic keyword matching, AI-enhanced systems provide context-aware, semantic search capabilities that retrieve more relevant and precise results. These systems not only assist in finding relevant papers but also offer structured summaries and comparative insights, helping researchers identify gaps, trends, and contradictions across multiple studies.

The rapid growth of scientific literature presents a significant challenge for researchers who need to search, analyze, and summarize vast amounts of information efficiently. AI-powered tools are transforming these tasks by leveraging NLP, machine learning (ML), LLMs, citation and knowledge graphs (KGs) to automate the retrieval, extraction, and summarization of scientific information. Unlike traditional search engines that rely on basic keyword matching, AI-equipped systems provide context-aware, semantic search with additional features that enhance the overall search experience. These systems go beyond finding relevant papers; they generate answers to research questions from the search results, provide structured summaries, and offer comparative insights, helping researchers identify gaps, trends, and contradictions across multiple studies.
%
% In this section, we will discuss each of these search systems and provide an overview of the most popular AI-based search tools including their key features. 


% We will start with a review of scientific paper repositories which form the data source for most scientific search systems.

% (1) search engines, which retrieve documents based on keyword queries; 
% and (6) benchmarks and leaderboards, which compare the performance of models on standardized datasets and metrics.
% Broadly, search systems for scientific literature can be categorized into four main types: (1) AI-enhanced search, which integrates NLP and ML for advanced, context-aware retrieval; (2) graph-based search, which leverages citation networks and knowledge graphs to explore relationships between papers; (3) paper chat, which enables interactive, conversational engagement with scientific content;  and (4) recommender systems, which suggest relevant research based on user preferences, citations, or topic modeling. 
% In this section, we will discuss each of these search systems and provide an overview of the most popular tools including their key features. We will start with a review of scientific paper repositories which form the data source for most scientific search systems.

%repositories which form the data source for literature search methods, followed by 

%By categorizing and leveraging these approaches, researchers can more effectively navigate the expanding body of scientific knowledge, ensuring they access the most relevant and insightful information for their work.

%AI applications in literature processing support evidence-based research by providing automated summarization and comparison features that simplify complex scientific findings. Tools such as \textit{Elicit} and \textit{Consensus} offer quick literature synthesis, while platforms like \textit{Papers with Code} link research papers to practical implementations, enabling better reproducibility. Additionally, KG-based systems, such as \textit{ASK ORKG}, enhance contextual understanding by structuring scientific contributions and facilitating ontology-driven exploration. These AI-powered solutions reduce the time and effort required for systematic literature reviews and meta-analyses, making them indispensable for researchers across various disciplines. The following subsections explore different categories of AI-enhanced literature processing tools, including general academic search engines, AI-driven summarization platforms, code and dataset-focused search engines, and KG-based systems, offering insights into their features, advantages, and limitations.
}

% \begin{table*}[!t]
%   \centering
%   \includegraphics[width=0.95\textwidth]{image/figure_1.pdf}
%   \caption{Table 1. Overview of 53 research platforms and tools, highlighting their key features, relevant techniques, and attributes. \colorbox[HTML]{fff2ba}{Free}, \colorbox[HTML]{a4d396}{Freemium} and \colorbox[HTML]{fde9a9}{Permium} indicate the cost structure for accessing features.  \colorbox[HTML]{00dffd}{AI} denotes platforms with AI integration, \colorbox[HTML]{ff7373}{S} indicates platforms offering summarization functionality, and \colorbox[HTML]{abd0ce}{P} denotes platforms supporting personalization.
%   }
%   \label{fig:search_engine_statistics}
% \end{table*}

\subsubsection{Data}

%Give an overview of the most important curated/annotated datasets, or sources of raw data, that are used (or potentially useful for) this task.

%Scientific search engines rely on vast and diverse publisher databases to provide comprehensive access to scholarly literature. These databases serve as the foundation for literature search tools, offering researchers access to a wide range of scientific outputs. Understanding the structure and types of scientific publisher repositories is crucial for assessing the coverage, reliability, and utility of search engines for evidence-based research.

Scientific search engines rely on vast publisher databases to provide access to scientific literature. Understanding the classification of these repositories is essential for assessing search engines' coverage, reliability, and effectiveness in evidence-based research. Repositories vary by \textbf{access model}, \textbf{subject focus}, and \textbf{content type}, each serving a distinct role in academic discovery and knowledge dissemination. By \textbf{access model}, repositories fall into \textit{open access repositories}, which provide unrestricted access to research articles (\eg \href{https://pubmed.ncbi.nlm.nih.gov/}{PubMed Central}, \href{https://arxiv.org/}{arXiv}); \textit{subscription-based repositories}, requiring institutional or individual subscriptions (\eg \href{https://www.sciencedirect.com/}{ScienceDirect}, \href{https://link.springer.com/}{SpringerLink}); and \textit{hybrid repositories}, offering both free and paywalled content (\eg \href{https://www.tandfonline.com/}{Taylor \& Francis Online}, \href{https://academic.oup.com/}{Oxford Academic}). By \textbf{subject focus}, repositories are either \textit{multidisciplinary}, covering broad disciplines (\eg \href{https://clarivate.com/academia-government/scientific-and-academic-research/research-discovery-and-referencing/web-of-science/}{Web of Science}, \href{https://www.scopus.com/home.uri}{Scopus}), or \textit{subject-specific}, specializing in fields such as medicine (\href{https://pubmed.ncbi.nlm.nih.gov/}{PubMed}), physics (\href{https://inspirehep.net/}{INSPIRE-HEP}), and social sciences (\href{https://www.ssrn.com/index.cfm/en/}{SSRN}). By \textbf{content type}, \textit{institutional repositories} archive research outputs from specific organizations (\eg \href{https://dspace.mit.edu/}{MIT DSpace}, \href{https://dash.harvard.edu/}{Harvard DASH}); \textit{preprint repositories} enable early dissemination of research before peer review (\eg \href{https://www.biorxiv.org/}{bioRxiv}, \href{https://chemrxiv.org/engage/chemrxiv/public-dashboard}{chemRxiv}); and \textit{government and public sector repositories} provide access to publicly funded research (\eg \href{https://ui.adsabs.harvard.edu/}{NASA ADS}, \href{https://www.openaire.eu/}{OpenAIRE}). \textit{Data repositories} (\eg \href{https://datadryad.org/stash}{Dryad}, \href{https://zenodo.org/}{Zenodo}) store research datasets, supporting transparency and reproducibility, while \textit{aggregator repositories} (\eg \href{https://www.base-search.net/}{BASE}, \href{https://core.ac.uk/}{CORE} \cite{knoth2023core}) index content from multiple sources for broader searches. Lastly, \textit{grey literature repositories} (\eg \href{https://opengrey.eu/}{OpenGrey}, \href{https://ethos.bl.uk/}{EThOS}) provide access to non-traditional research outputs such as theses, reports, and white papers, which may not be available through conventional publisher platforms.



%Scientific publisher repositories can be categorized based on their access models, subject focus, and content types. \textit{Access-based repositories} include \textit{open access repositories}, which provide unrestricted access to research articles (e.g., \href{https://pubmed.ncbi.nlm.nih.gov/}{PubMed Central}, \href{https://arxiv.org/}{arXiv}, \href{https://europepmc.org/}{Europe PMC}), and \textit{subscription-based repositories}, which require institutional or individual subscriptions to access full-text content (e.g., \href{https://www.sciencedirect.com/}{Elsevier's ScienceDirect}, \href{https://link.springer.com/}{SpringerLink}, \href{https://onlinelibrary.wiley.com/}{Wiley Online Library}). Additionally, \textit{hybrid repositories} offer a mix of open access and paywalled content, allowing researchers to access some articles freely while requiring payment for others (e.g., \href{https://www.tandfonline.com/}{Taylor \& Francis Online}, \href{https://academic.oup.com/}{Oxford Academic}).


%From a subject focus perspective, scientific repositories can be categorized into \textit{multidisciplinary repositories}, which cover a broad range of disciplines (e.g., \href{https://clarivate.com/academia-government/scientific-and-academic-research/research-discovery-and-referencing/web-of-science/}{Web of Science}, \href{https://www.scopus.com/home.uri}{Scopus}), and \textit{subject-specific repositories}, which focus on specific fields such as medicine (e.g., \href{https://pubmed.ncbi.nlm.nih.gov/}{PubMed}), physics (e.g., \href{https://inspirehep.net/}{INSPIRE-HEP}), and social sciences (e.g., \href{https://www.ssrn.com/index.cfm/en/}{SSRN}). \textit{Institutional repositories}, managed by universities and research institutions, provide access to scholarly work produced within specific organizations (e.g., \href{https://dspace.mit.edu/}{MIT DSpace}, \href{https://dash.harvard.edu/}{Harvard DASH}), while \textit{preprint repositories} (e.g., \href{https://www.biorxiv.org/}{bioRxiv}, \href{https://chemrxiv.org/engage/chemrxiv/public-dashboard}{chemRxiv}, \href{https://arxiv.org/}{arXiv}) facilitate early dissemination of research findings prior to formal peer review.


%Government and public sector repositories, such as NASA ADS and the European Commission’s OpenAIRE, provide access to publicly funded research and promote open science initiatives. In addition, \textit{data repositories} (e.g., \href{https://datadryad.org/stash}{Dryad}, \href{https://zenodo.org/}{Zenodo}, \href{https://figshare.com/}{Figshare}) specialize in storing research datasets, supporting transparency, reproducibility, and data-driven research. \textit{Aggregator repositories}, such as BASE and CORE \cite{knoth2023core}, collect and index content from multiple sources, enabling researchers to search across a wide range of academic publications from various disciplines. Lastly, \textit{grey literature repositories} (e.g., \href{https://opengrey.eu/}{OpenGrey}, \href{https://libguides.ials.sas.ac.uk/az/ethos-electronic-theses-online-system}{EThOS}) provide access to non-traditional research outputs, such as theses, reports, and white papers, which may not be available through conventional publisher platforms.

The structure of scientific repositories shapes AI-enhanced search. While broad AI-based search engines like \href{https://elicit.com}{Elicit} and \href{https://ask.orkg.org}{ORKG ASK} query multiple publisher repositories, similar to \href{https://scholar.google.com}{Google Scholar}, tools like \href{https://notebooklm.google}{NotebookLM} focus on user-selected documents, and recommender systems such as \href{https://www.scholar-inbox.com/}{Scholar Inbox} rank new literature by relevance. AI-driven search enables customizable knowledge bases while optimizing discovery, retrieval, and personalization in research.

%The availability and structure of these repositories significantly influence the effectiveness of search engines in retrieving relevant information, conducting systematic reviews, and supporting comparative analysis of research contributions. While classical academic search engines such as \href{https://scholar.google.com}{Google Scholar} or AI-enhanced search systems like \href{https://elicit.com}{Elicit} search within very large repositories (\eg 300 million documents), paper chatting and QA systems such as \href{https://notebooklm.google}{NotebookLM} allow users to retrieve information from a smaller set of articles (\eg 50 pdf documents) that are often selected or provided by the user. In contrast, recommender systems like \href{https://www.scholar-inbox.com/}{Scholar Inbox} rank recent repository additions according to the researcher's relevance in order to provide users with a daily or weekly newsfeed of relevant literature. 
%\todo{SE: can this part be shortened, given our length restrictions? The relation to AI is there, but maybe not super strong. Also, please use "e.g." correctly, i.e., not with two commas}

\subsubsection{Methods and Results}

% \begin{table}[h]
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{|l|c|c|c|c|c|c|}
% \hline
% \textbf{Feature} & \textbf{Search Engines} & \textbf{AI-Enh. Search} & \textbf{Graph-Based} & \textbf{Paper Chat} & \textbf{Recommender} & \textbf{Benchm.} \\ \hline
% \textit{AI Integration}    & Limited & High & High & High & Limited & No \\ \hline
% \textit{Summarization}     & No & Yes & Yes & Yes & No & Yes \\ \hline
% \textit{Citation Analysis} & Yes & Limited & Yes & No & Limited & No \\ \hline
% \textit{Personalization}   & Limited & Yes & No & Limited & Yes & No \\ \hline
% \end{tabular}
% }%
% % deprecated:
% %\textit{Open Access Focus} & Partial & Partial & No \\ \hline
% \caption{Comparison of scientific search systems across four key features. The rows represent the evaluated features, while the columns categorize different types of search systems. Each cell indicates the extent to which a given feature is supported within a particular category of search system.}
% \label{tab:comparison}
% \end{table}

This section surveys state-of-the-art AI-enhanced scientific discovery tools, identified as four main types based on their core functionality: (1) \textbf{AI-enhanced search}, which retrieves relevant literature from vast repositories; (2) \textbf{graph-based systems}, which map relationships between research concepts and publications; (3) \textbf{paper chat and QA}, which enable interactive exploration of scientific content; and (4) \textbf{recommender systems}, which suggest relevant papers based on user preferences. A detailed overview of these tools is provided in Table~\ref{tab:google_scholar}. Additionally, two traditional scientific discovery tools, namely search engines and benchmarks with leaderboards, are discussed in Appendix \ref{ax:search_engine}.


%This section surveys four categories AI-enhanced scientific discovery tools based on their core functionality. 1) AI-enhanced search, 2) graph-based systems, 3) paper chat and QA, and 4) recommender systems. A detailed overview of all systems is provided in Table~\ref{tab:google_scholar}.
% Table~\ref{tab:comparison} compares those six categories across four key features. 


% \paragraph{Search engines}

% Traditional academic search engines such as \href{https://scholar.google.com}{Google Scholar}, \href{https://www.semanticscholar.org}{Semantic Scholar}, \href{https://xueshu.baidu.com}{Baidu Scholar}, \href{https://science.gov}{Science.gov}, and \href{https://www.base-search.net}{BASE} are characterized by their broad literature coverage, citation tracking capabilities, and keyword-based search functionality. Their primary advantages include extensive indexing of scholarly content, which involves aggregating and organizing vast amounts of academic documents from various sources such as publisher websites, institutional repositories, and open-access archives. This comprehensive indexing spans multiple disciplines and document types, ensuring that users can access a diverse set of resources. Additionally, these platforms offer robust citation analysis features that allow researchers to track citation counts, measure the impact of publications, and explore citation networks to identify influential works and emerging trends within a given field. Another significant advantage is their free access to a wide range of academic resources, such as peer-reviewed journal articles, conference papers, preprints, theses and dissertations, technical reports, books and book chapters, as well as grey literature like white papers, government reports, and institutional research outputs. However, these search engines have certain limitations, such as limited AI-driven filtering options and relatively basic relevance ranking mechanisms compared to more advanced AI-enhanced search tools. 

\paragraph{AI-enhanced Search}

Platforms such as \href{https://elicit.com}{Elicit}, \href{https://consensus.app}{Consensus}, \href{https://openscholar.allen.ai}{OpenScholar}, and \href{https://typeset.io}{SciSpace} leverage AI, including LLMs, to extend beyond traditional search by enabling semantic search, paper summarization, evidence synthesis, and trend analysis. Unlike conventional search engines that rely on keyword matching, these tools use NLP and machine learning to extract key insights, synthesize information to answer research queries \cite{giglou2024llms4synthesis}, and generate structured summaries. Their ability to quickly summarize and categorize findings—such as study outcomes, methodologies, and limitations—helps researchers efficiently compare and interpret literature.


%Platforms such as \href{https://elicit.com}{Elicit}, \href{https://consensus.app}{Consensus}, \href{https://openscholar.allen.ai}{OpenScholar}, and \href{https://typeset.io}{SciSpace} leverage AI, including LLMs, to go beyond traditional search functionalities by offering AI-driven semantic search, paper summarization, evidence synthesis, and trend analysis. Unlike conventional search engines that primarily focus on retrieving relevant documents based on keyword matches, these tools utilize NLP and machine learning algorithms to extract key insights, synthesize information from the relevant papers to answer the research queries \cite{giglou2024llms4synthesis}, and present structured summaries of the papers. One of their key advantages is the ability to generate quick and concise summaries, helping researchers save time and focus on the most relevant aspects of the literature. Additionally, they provide structured insights by categorizing findings into study outcomes, methodologies, and limitations, which simplifies the process of comparing and interpreting research. %Some recent efforts, such as \href{https://sakana.ai/ai-scientist/}{The AI Scientist}~\cite{lu2024aiscientist} even aim to autonomously conduct scientific exploration and experimentation. \todo{SE: The AI scientist is too broad to be cited here, maybe?}

%However, despite their advantages, AI-enhanced search tools have certain limitations, including potential biases in summarization due to algorithmic processing and reliance on the quality and availability of the underlying datasets, which may lead to incomplete or skewed representations of the literature.

\input{table/table_1}

\paragraph{Graph-based Systems}

Graph-based systems such as \href{https://ask.orkg.org}{ORKG ASK} are designed to facilitate structured access to scientific knowledge. Unlike conventional paper search engines, they leverage a KG that organizes research contributions as structured data rather than unstructured text. Such contributions are typically extracted from the abstract, introduction, and result sections \cite{dsouza-etal-2021-semeval, pramanick2024naturenlpanalyzingcontributions}. Those systems enable users to ask complex, domain-specific questions and receive answers synthesized from semantically structured scientific data. They typically use techniques such as KG-based reasoning and retrieval-augmented generation (RAG) to extract relevant information from the KG, providing more interpretable and verifiable answers compared to traditional LLM-based QA systems. \href{https://citespace.podia.com/}{CiteSpace} and \href{https://sci2.cns.iu.edu/user/index.php}{Sci2} are specialized bibliometric analysis and network analysis tools to study the structure and evolution of scientific research. \href{https://citespace.podia.com/}{CiteSpace} focuses on identifying research trends, keyword co-occurrence networks, and citation bursts, using visual analytics to highlight emerging topics and influential papers using graphs. \href{https://sci2.cns.iu.edu/user/index.php}{Sci2} is a more general-purpose tool designed for analyzing scholarly datasets, enabling users to perform network analysis, geospatial mapping, and temporal modeling of scientific literature and collaboration patterns. \href{https://www.connectedpapers.com/}{Connected Papers} is a scientific literature exploration tool designed to help researchers discover related papers based on a given seed paper. Unlike traditional citation-based systems, it builds a graph of papers using a similarity metric derived from co-citation and bibliographic coupling analysis. The platform constructs a network where each node represents a paper, and edges indicate similarity based on shared references and citations rather than direct citation links. This approach allows users to find relevant papers that may not be directly cited but are conceptually related. Graph-based visualizations provide an intuitive way to explore clusters of research, identify foundational and emerging works, and track the evolution of scientific ideas.

\paragraph{Paper Chat and QA}

Paper chat and question-answering (QA) systems such as \href{https://chatgpt.com/}{ChatGPT}, \href{https://chat.deepseek.com}{Deepseek Chat}, \href{https://notebooklm.google}{NotebookLM}, \href{https://www.explainpaper.com/}{ExplainPaper}, \href{https://www.chatpdf.com/}{ChatPDF}, and \href{https://docanalyzer.ai/}{DocAnalyzer.AI} allow users to interact with scientific papers by asking questions and receiving responses based on the document’s content. They typically process a limited number of user-provided PDFs or text from specific websites. The core technology behind them is RAG
~\cite{lewis2020retrieval, asai2024selfrag, kang-etal-2024-taxonomy}, a technique that combines information retrieval with LLMs
to improve accuracy and grounding. A typical RAG system first partitions the document into smaller sections and converts them into vector representations using embedding models. Upon a user query, the system retrieves the most relevant sections based on semantic similarity and passes them as context to an LLM, which then generates a response. This mechanism ensures that answers are directly grounded in the provided documents rather than relying solely on the model’s pre-trained knowledge, enhancing factual reliability and interpretability. Some systems incorporate LLM agents~\cite{tan-etal-2023-multi2claim, cai-etal-2024-mixgr,li-etal-2025-chatcite} that can reason over retrieved information, summarize findings, or extract key insights. These agents can follow multi-step reasoning strategies to provide more nuanced responses, such as synthesizing information from multiple sections or explaining technical terms in simpler language. By anchoring responses to document content, RAG-based systems mitigate hallucinations and make it easier for users to verify claims by checking the referenced passages. The effectiveness of these systems depends on the quality of document chunking, the efficiency of retrieval, and the model’s ability to integrate information into coherent, context-aware answers.

\paragraph{Recommender Systems} 

Scientific paper recommender systems such as \href{https://arxiv-sanity-lite.com/}{Arxiv Sanity}, \href{https://www.scholar-inbox.com/}{Scholar Inbox}, \href{https://researchtrend.ai}{ResearchTrend.ai}, and \href{https://www.researchrabbit.ai/}{Research Rabbit} leverage machine learning and information retrieval techniques to help researchers discover relevant literature. These systems generally fall into two main categories: content-based filtering, collaborative filtering and hybrid approaches. Content-based methods~\cite{amami2016lda, bhagavatula-etal-2018-content} analyze the text of papers to build representations that capture their meaning. Traditional approaches rely on sparse abstract or document representations such as TF-IDF~\cite{sparck1972statistical}, which assigns importance to words based on their frequency and distinctiveness in a corpus. More advanced models use dense abstract or document embeddings derived from neural networks, such as SPECTER~\cite{cohan-etal-2020-specter} or GTE~\cite{li2023towards}, which map papers into a high-dimensional vector space where similar documents are close to each other. The Massive Text Embedding Benchmark (MTEB)~\cite{muennighoff-etal-2023-mteb} ranks many state-of-the-art embedding models on a comprehensive benchmark comprising various different datasets and tasks. These embeddings enable fast similarity searches and improve over simple keyword matching. In contrast, collaborative filtering~\cite{wang2014relational, bansal2016ask} relies on user interactions, such as downloads, bookmarks, and citations, to recommend papers based on the behavior of similar users. One challenge of pure collaborative filtering is the cold start problem, where new papers or users lack sufficient data for recommendations. To mitigate this, many modern systems employ hybrid approaches, such as two-tower architectures~\cite{10.1145/3298689.3346996, covington2016deep, yu2021dual}. These models learn separate representations for papers and users, combining textual embeddings with user interaction data to generate more personalized recommendations. State-of-the-art systems often use a mix of these techniques to balance relevance, novelty, and diversity. The effectiveness of these systems depends on the quality of embeddings, the availability of interaction data, and the efficiency of ranking algorithms that surface the most useful papers.

% \paragraph{Benchmarks and leaderboards}

% Code and Dataset-Focused Search Engines include platforms such as \href{https://portal.paperswithcode.com/}{Papers with Code}, \href{https://github.com/OSU-NLP-Group/ScienceAgentBench}{ScienceAgentBench}, and \href{https://huggingface.co/}{Huggingface}, which are specifically designed to bridge the gap between academic publications and practical implementation by linking research papers with associated code and datasets. These platforms facilitate reproducibility and practical application of research findings by aggregating code repositories, enabling researchers and practitioners to easily explore implementations, compare results, and benchmark their models. A key feature of such platforms is their ability to provide dataset discovery tools, which allow users to identify relevant datasets for specific research problems, fostering collaboration and accelerating experimentation cycles. These search engines are particularly valuable for machine learning practitioners, as they facilitate quick access to ready-to-use codebases, helping them implement cutting-edge research more efficiently. Based on these community-curated leaderboards, some studies have proposed models for constructing leaderboards directly from scientific papers \cite{hou-etal-2019-identification,kardas-etal-2020-axcell,sahinuc-etal-2024-efficient}. 

% However, despite their advantages, these platforms primarily focus on machine learning-related domains, resulting in limited coverage and applicability for researchers working in other scientific fields.


%\todo{SE: are your four classes distinct? Overlapping? Do you think this taxonomy is adequate, with distinctive classes?}


%Describe the state-of-the-art methods and their results, noting any significant qualitative/quantitative differences between them where appropriate.

\begin{comment}
\begin{enumerate}
    \item \url{https://scholar.google.com/}
    \item \url{https://www.semanticscholar.org/}
    \item \url{https://paperswithcode.com/}: 
    \item \url{https://www.base-search.net/}: 
    \item \url{https://typeset.io/}
    \item \url{https://ask.orkg.org/}
    \item \url{https://undermind.ai/home/}
    \item \url{https://openscholar.allen.ai/}
    \item \url{https://consensus.app/}
    \item \url{https://researchtrend.ai/}
    \item \url{https://www.scilit.net/}
    \item \url{https://www.read.enago.com/}
    \item \url{https://elicit.com/}
    \item \url{https://science.gov/}
    \item Baidu Scholar \url{https://xueshu.baidu.com/} - interface in Chinese
    \item \url{https://github.com/Future-House/paper-qa} \cite{lala2023paperqa,skarlinski2024language}
    \item \url{https://notebooklm.google/}
    \item \url{The AI Scientist} \cite{lu2024aiscientist}
    \item \url{ScienceAgentBench} \cite{chen2024scienceagentbench}
    \item \url{Core GPT} \cite{pride2023core}
    \item \url{https://huggingface.co/papers/2410.05080} -- other related references for scientific discovery
\end{enumerate}
\end{comment}

% %%%%%%%%%%%% Moved to appendix  %%%%%%%%%%%%%%%%
\subsubsection{Ethical Concerns}

%Identify and discuss any ethical issues related to the (mis)use of the data or the application of the methods, as well as strategies for mitigations.

The use of AI in scientific search, summarization and comparison raises ethical considerations, particularly in ensuring transparency, accountability, and equity. AI can significantly accelerate the pace of discovery, automate search tasks, and uncover patterns that may elude human researchers, but it also introduces risks and biases. %such as perpetuating biases present in training data, \todo{SE: can you avoid general ethical issues in this subsection and only focus on ones related to your search direction?}
%undermining the integrity of scientific processes (\eg %\todo{SE: be sure to use `e.g.' correctly} 
%authorship and credit assignment), and enabling the misuse of findings. 
Existing dynamics such as the Matthew effect, where well-known researchers receive disproportionate attention, might be reinforced by the AI algorithms, intensifying inequalities. We believe that research should follow a human-centric approach, in which the human researcher is provided with advanced tools but remains fully responsible for executing the research and summarizing the results in research papers. It is also important to develop algorithms to reduce biases by recommending relevant work to researchers based on the \textit{content} of the research, independent of the popularity of the authors. Tools that are able to uncover gaps in the existing literature might even lead to a more uniform allocation of researchers to topics, reducing the bias towards overpopulated areas.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Domains of Application}

%Indicate whether any of the data, methods, ethical concerns, etc. are specific to a given domain (biology, health, computer science, etc.).

The search, summarization and comparison tools discussed in this section are general and apply to all fields of science. The presented benchmarks, however, are specific to the field of computer science in general and artificial intelligence in particular.

\subsubsection{Limitations and Future Directions}

%Summarize the limitations of current approaches; point out any notable gaps in the research and future directions

% ag: not sure what this comment is about, removed it for now
% \todo{SE: low quality/accuracy?}.

Despite the significant advancements in AI-powered scholarly search systems, several limitations persist that hinder their full potential. One of the primary challenges is \textit{data quality and coverage gaps}, as these systems often struggle with handling incomplete, non-standardized, or outdated data sources, which can lead to inaccuracies and inconsistencies in retrieved information. Additionally, \textit{bias in AI models} remains a critical concern, where search and ranking algorithms may introduce biases based on training data, potentially influencing the visibility of certain research areas and limiting the diversity of perspectives presented to users. Another major limitation lies in \textit{scalability and real-time processing}, as efficiently handling large-scale datasets while maintaining low latency and high retrieval accuracy remains a technical challenge. Addressing these limitations opens several promising future directions. One potential avenue is \textit{enhanced personalization} which can be achieved by adapting search engines to user preferences, providing more tailored recommendations based on research interests and behavioral patterns. Lastly, fostering \textit{interdisciplinary collaboration} through the integration of AI-powered search systems with other digital tools, such as data visualization platforms and research management software, could facilitate more comprehensive and insightful research outcomes. Addressing these challenges and exploring future directions will be crucial for realizing the full potential of AI-driven scholarly search and synthesis. 

%\todo{SE: don't forget to cite and discuss your tables and figures. Some content is redundant (need to shorten anyway). Limitations are also discussed above. Another limitations could be independent domains, such as STEM (arxiv, etc.); Paperswithcode seems to address CS papers?. Also, do not forget to address ethics + domains of applications.}
% ag: we mention this as part of the domains of applications. Tables are referenced now.

