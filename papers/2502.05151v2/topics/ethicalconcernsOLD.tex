\subsection{On generative AI as a Tool in Academic Writing}  
%(BK, SG) 

In the following, a brief overview of the current discussions on generative AI and ethics is given. First of all, prevalent topics discussed are identified based on meta studies of recent literature on generative AI and ethics. Then the focus is shifted to ethical considerations/concerns regarding the use of generative AI in academia, including positions from editorial boards of journals. 
Furthermore, taking Willcocks’ eight-point ‘Artificial Imperfections’ test as a starting point, we discuss the needs, expectations and ethical concerns related to the use of LLMs as a supportive tool (i) in academic writing including the potentials and caveats of LLMs in  researching/summarizing the state-of-the art in ever growing bodies of scientific publications, as well as (ii) within peer reviewing.

Baldassarre et al. \cite{baldassarre2023social}, for instance, present a systematic literature review regarding the social impact of generative AI, especially taking into account 71 papers on ChatGPT. They identify the following areas of concern: privacy, inequality, bias, discrimination, and stereotypes. Another literature review on ethics and generative AI conducted by Hagendorff \cite{hagendorff2024mapping} identifies the following topics as becoming increasingly of interest: jailbreaking, hallucination, alignment, harmful content, copyright, models leaking private data, impacts on human creativity. The work also identifies 19 distinct clusters of ethics topics with fairness/bias being the most frequently mentioned, followed by safety, harmful content/toxicity, hallucinations, privacy, interaction risks, security/robustness on ranks two to six, and writing/research on rank 18. 
Ali and Aysan \cite{ali2024ethical} review 364 recent papers on generative AI and ethics published from 2022 to 2024 in different domains including the use of generative AI in scientific research. Regarding academia, the prevalent topics identified as critical are the authenticity of the work, intellectual property and academic integrity. The editors-in-chief of the Journal of Information Technology, for instance, elaborate in \cite{schlagwein2023chatgpt} on the limitations and risks of using generative AI in the production of scientific publications. They refer to the eight-point ‘Artificial Imperfections’ test to illustrate current limitations of generative AI: (the 8 points are quoted from \cite{schlagwein2023chatgpt})
\begin{itemize}
\item	AI is brittle. It provides only narrow and non-human ‘intelligence’.
\item	AI is opaque. Its inner workings are black-boxed.
\item	AI is greedy. It requires large data training sets as well as processing power, memory and energy.
\item	AI is shallow and tone-deaf. It produces decisions and knowledge claims without understanding, feeling, empathising, seeing, creating, reflecting or even learning in any human sense of these terms.
\item	AI is manipulative and hackable. The digitalisation of many decisions makes them amenable to manipulation by corporations and governments at scale, or open to hacking by malicious actors.
\item	AI is biased. It is inherently biased in that it encodes and enshrines any biases found in the training data.
\item	AI is invasive. It commoditises private data.
\item	AI is faking it. It is ‘bullshitting-as-a-service’. AI is not creative; it fakes creativity. It is not emotional; it fakes emotions, etc.
\end{itemize}
Nevertheless, the editors conclude that the use of AI should not be forbidden, however, if it is used, the authors must take full responsibility of the outcome, adhere to the “scientific principle of transparency” and give full and transparent disclosure of the usage of AI in the respective publication, and moreover that “it is then up to the reviewers and editors to assess and make decisions on the specific use of that generative AI in a specific piece of research.”
Similarly, Pu et al. \cite{pu2024chatgpt} in their editorial to iMeta 3(2), 2024 state that AI‐assisted technologies cannot be recognized as authors, the use of generative AI in a scientific manuscript/publication must be transparently disclosed, including the prompts, specific versions of the tools used. The authors are fully responsible for the integrity of their manuscripts. They must address ethical concerns and ensure the accuracy and fairness of AI-generated content, comply with data protection and privacy laws, and also consider copyright and intellectual property issues surrounding AI‐generated content. The use of AI‐generated images and multi-media must be specifically allowed. They explicitly prohibit the use of AI in the reviewing process.

So far so good, the proportion of AI generated text in academic manuscripts is increasing. Recently, a number of studies have been published that examine the use of LLMs in scientific writing. A prevalent approach is to analyse scientific publications with respect to the occurrence of words identified as predominant in LLM generated text, and to compare the occurrence of respective words before and after the launch of ChatGPT in 2022, e.g.,  \cite{gray2024chatgptcontaminationestimatingprevalence, kobak2024delving}.  Liang et al. \cite{liang2024mapping} conducted an analysis across 950,965 papers published in arXiv, bioRxiv, and Nature journals between January 2020 and February 2024 and found a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5\%). Moreover, the authors found that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Cheng et al. \cite{cheng2024have} analyzed a dataset of 45129 preprint manuscripts uploaded to arXiv, bioRxiv, and medRxiv from January 2022 to March 2024 and found that (i) the use of AI in the manuscripts  correlates with the trend of ChatGPT web searches; (ii) it is widespread across scientific domains but highest in computer science and engineering; (iii) the usage varies with language backgrounds and geographic location of the author affiliations, with Italy and China leading (>5\%); and (iv) AI-generated texts are most significantly used in hypothesis formulation and conclusion summarization.
Accordingly, detection of LLM-generated text is an emerging field of research. See for instance \cite{tang2024science, wu2025survey} for surveys. 

Last but not least AI models are on the brink of being used in peer reviewing: Checco et al. 2021 \cite{checco2021ai}, for instance, trained an AI model on 3 300 past conference papers and the associated review scores. When showing unreviewed papers, the AI model could often predict the peer review outcome by only using the textual content of the new manuscript. The authors also discuss explainability methods such as LIME to explain the model decisions on a specific document. This has the potential to uncover biases in the review process.

Summing up, the use of AI models in scientific writing and peer reviewing has become reality. Editors are very well aware of it and try to regulate and curb the use of AI. This requires both trust in the authors' veracity as well as technical means for verification. Accordingly, automatic detection of AI-generated text is an emerging field of research.


