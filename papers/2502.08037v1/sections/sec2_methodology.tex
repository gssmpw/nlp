\section{Methodology}
% \wishlist{Add a figure of result summary to demonstrate the benefits of embedding surgery as well as franken-adapter. (20/10)}
% \wishlist{Add a figure of the training method (embedding tuning, franken-adapter, lora-adaptation).}
% Restate the motivation of this work. Modern LLMs is mainly trained using English data with tiny fraction (~5\%) of multilingual data, so it's multilingual ability is limited. Common approach is to do continual pre-training on data mixture with upsampled multilingual ratios. However, they neglects the fact that the tokenizers used in LLMs are usually trained on data subsampled from the English-dominant pre-training data mixture. This leads to the low frequencies of multilingual tokens and thus, making them less likely to appear as independent tokens in the built vocabulary. These multilingual-unfriendly tokenizers break multilingual texts into too many segments, and thus increasing the difficulty of modeling these texts compared to English. Therefore, a natural question raised is that in addition to increasing the multilingual ratios of model pre-training data, making multilingual-friendly tokenizers is also important for enhancing the multilingual ability of LLMs.

% Modern LLMs are mainly trained using English data with a tiny fraction ($<10\%$) of multilingual data~\citep{dubey2024llama3herdmodels}, so their multilingual ability is limited. A common approach for multilingual enhancement is to do continual pre-training on a data mixture with an increased proportion of multilingual data. However, they neglect the fact that the tokenizers used in LLMs are usually trained on data sub-sampled from the English-dominant pre-training data mixture. This leads to low frequencies of multilingual tokens, making them less likely to appear as complete words in the built vocabulary. As a result, these multilingual-unfriendly tokenizers break multilingual texts into too many segments (Figure~\ref{fig:fertility_comparison}), increasing the difficulty of modeling these texts compared to English~\citep{ahuja-etal-2023-mega}. In this paper, our goal is to demonstrate that in addition to increasing the multilingual ratios of model pre-training data, making multilingual-friendly tokenizers is also important for enhancing the multilingual ability of LLMs. Meanwhile, we also show that switching to these tokenizers with solely embedding tuning on multilingual data is sufficient for efficient cross-lingual transfer of LLMs. 

% \wishlist{The motivations of train multilingual embeddings for each language group (2-3 sentences for explanations (two cases: 1 per language and 1 for all languages, demonstrate their disadvantages).}
Modern LLMs exhibits limited multilingual ability, primarily due to the dominance of English training data and the relatively small multilingual proportion~\citep{dubey2024llama3herdmodels}. Additionally, the tokenizers employed in these LLMs, which are constructed from sub-sampled pre-training corpora, are biased towards English and several high-resource languages. This results in fragmenting texts from long-tail languages into too many tokens (Figure~\ref{fig:fertility_comparison}), thereby degrading the performance and efficiency of processing such languages~\citep{ahuja-etal-2023-mega}. In this paper, we demonstrate that having tokenizers that provide equitable representation for low-resource languages is critical to the effectiveness of our proposed \emph{\ouradapter} approach. 

\begin{figure}
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/fertility_comparison.pdf}
    \vspace{-6mm}
    \caption{The tokenization comparison between using the vanilla and customized multilingual tokenizers on \gemmatwo. Tok. Length refers to the average number of tokens required to represent the same amount of texts.} 
    % \wishlist{Add the tokenizer comparison of Palm and Aya in appendix.}
    %  \wishlist{Add tokenization qualitative examples before and after changing tokenizers (English and a few LRLs.}
    \vspace{-6mm}
    \label{fig:fertility_comparison}
\end{figure}

\subsection{Customized Vocabulary Construction}\label{sec:vocab_construct}
Our strategy involves constructing distinct tokenizers for each language group (\S\ref{sec:languages}).\footnote{Our preliminary findings suggest that employing distinct tokenizers for each language group enhances cross-lingual transfer. Please refer to Appendix Figure~\ref{fig:joint_vs_distinct_tokenizer} for a comparison with the variant of using a single tokenizer for all language groups.} Tailoring tokenizers to specific language groups enhances cross-lingual transfer among geographically related long-tail languages compared to using monolingual tokenizers. Moreover, this approach avoids the shortcomings of a universal tokenizer that treats all low-resource languages uniformly poorly. Based on this, we propose a \emph{Prune-with-Extension} approach for developing tokenizers optimized for language adaptation while maintaining English ability. 
% First, we train separate tokenizers for each language group using the BPE algorithm~\citep{bpe,sennrich-etal-2016-neural}. Next, we employ the tokenizers of target LLMs to identify English tokens. Finally, we create the final vocabulary by merging English and long-tail tokens.
First, we prune the tokenizer of target LLMs by removing non-English tokens. Then the pruned tokenizer is extended through adding new tokens, which are obtained by training tokenizers for low-resource languages using BPE~\citep{bpe,sennrich-etal-2016-neural}.

% \inlinetitle{Reusing English Tokens}
\inlinetitle{Pruning the Tokenizer}
To preserve the pre-trained knowledge embedded in the language model, current approaches often expand the vocabulary by adding new tokens~\citep{fujii2024continual, cui2024efficienteffectivetextencoding}. This, however, can substantially increase pre-training time due to the extra computational cost of the softmax layer~\citep{liang-etal-2023-xlm}.\footnote{Embedding and $\operatorname{softmax}$ layers are typically tied in modern LLMs. LLMs evaluated in this work all follow this setting.} 
% Our proposed method aims to maintain pre-training efficiency while preserving existing knowledge by avoiding vocabulary expansion. 
To avoid this, we first prune the existing tokenizers by retaining only English tokens before adding those from low-resource languages.
Given the predominant English training data for LLMs, we hypothesize that a significant portion of their knowledge is associated with English tokens, and reusing English tokens can effectively retain this knowledge~\citep{garcia-etal-2021-towards}. 
% In our implementation, for a given LLM, we tokenize a set of 20 million English sentences using its tokenizer. We then filter out any non-Latin script tokens, assuming the remaining to be English.
In our implementation, for a given LLM, we identify English tokens by tokenizing a set of 20 million English sentences using its tokenizer, with further filtering through removing non-Latin script tokens.\footnote{40\% of the tokens are discarded: these are non-Latin scripts tokens from high-resource languages, domain-specific tokens (\eg code), and very rare English tokens.}

\inlinetitle{Training Multilingual Tokenizers}
% To construct a multilingual vocabulary for long-tail languages, we use the Next Thousand Languages (NTL) project corpus~\citep{caswell-etal-2020-language, bapna2022building}. 
To get the data for building a multilingual vocabulary for long-tail languages, we sample from the Next Thousand Languages (NTL) corpus~\citep{caswell-etal-2020-language, bapna2022building}.
Our empirical analysis reveals that sampling data for each language up to a maximum of 500K lines from NTL effectively addresses the imbalance between high- and low-resource languages, outperforming temperature-sampling techniques. 
Subsequently, we train a BPE tokenizer using the sampled data to generate a vocabulary whose size aligns with that of the target LLMs.


\inlinetitle{Extending the Pruned Tokenizer}
% For Unigram tokenizers, we merge the English and multilingual tokens into a single multilingual vocabulary~\citep{chung-etal-2020-improving}. For BPE tokenizers, 
We sequentially add the identified English tokens followed by tokens from the newly built multilingual tokenizer. Both types of tokens are added in the same preference order as in their respective tokenizers. The final vocabulary maintains the same size as the original tokenizer, with over 60\% token overlap, resulting in negligible variations in English tokenizations (see  Table~\ref{tab:tokenization_qualitative_examples} in Appendix).
Figure~\ref{fig:fertility_comparison} shows our final vocabulary achieves significant compression rate improvements by consistently producing shorter sequences across languages of a spectrum of resource levels while barely affecting English.

\subsection{Model Training}\label{sec:model_training}

\inlinetitle{Embedding Initialization}
% Our goal is to maximally inherit the pre-training knowledge from the target LLM's embedding layer. We follow \citet{gee-etal-2022-fast} to first copy the token embeddings from the target LLM's embedding layer for overlapped tokens. For new tokens, we use the LLM's original tokenizer to tokenize each of them and use average pooling of embeddings of the tokenized tokens as the initialization.
To maximally inherit the pre-trained knowledge embedded in the target LLM's embedding layer, we adopt a strategy inspired by \citet{gee-etal-2022-fast}. For tokens that overlap between the target LLM's vocabulary and our multilingual vocabulary, we directly copy the corresponding embeddings. For new tokens, we employ the LLM's original tokenizer to decompose them into subtokens and initialize their embeddings using the average of their subtoken embeddings.

\inlinetitle{Language Adaptation}
To adapt the LLMs to new languages, we follow~\citet{artetxe-etal-2020-cross} to only fine-tune the customized embeddings on curated multilingual data $\mathcal{D}_{la}$ while keeping the transformer body frozen (Figure~\ref{fig:franken_adapter} step 2(b)), with the same training objective used in the initial LLM pre-training phase. This is based on the assumption that the pre-trained transformer body encapsulates universal cross-lingual knowledge~\citep{zhao2024how,wendler-etal-2024-llamas,tang-etal-2024-language}, while the embedding layer encodes language-specific information, which suggests embedding tuning should be effective for language adaptation. 
% This makes embedding surgery an effective strategy for significantly enhancing cross-lingual transfer.

\inlinetitle{\ouradapter}
To facilitate zero-shot cross-lingual transfer, \citet{artetxe-etal-2020-cross} fine-tunes the transformer body of PLMs on a specific English downstream task, then incorporates monolingual embeddings tailored for a second language. This approach necessitates task-specific fine-tuning of the transformer and language-specific embedding creation. In contrast, our \ouradapter method instruction-tunes the transformer body of LLMs on a diverse range of English tasks $\mathcal{D}_{it}$~\citep{wei2022finetuned} (Figure~\ref{fig:franken_adapter} step 2(a)). Notably, we employ the LLM's \emph{original} embeddings and keep them frozen in this step.
% \edit{By freezing the embeddings, we ensure that the instruction-following capabilities are concentrated in the transformer body.}
We then integrate the customized embeddings obtained from the language adaptation stage into the instruction-tuned transformer body. This results in the \ouradapter (Figure~\ref{fig:franken_adapter} step 3), a modular framework designed for zero-shot cross-lingual transfer.\footnote{Freezing embeddings makes the instruction tuning and language adaptation processes symmetry. This enhances modularity and improves the parameter compatibility of \ouradapter.}
% \footnote{\edit{This modularity allows the \ouradapter to decouple the transformer body, which specializes in instruction-following, from the embeddings that are language-specific.}}


\inlinetitle{LoRA-Adaptation}
% A potential limitation of the \ouradapter approach is the risk of vulnerability due to the independent training of the instruction-tuned transformer body and pre-trained multilingual embeddings. Our empirical findings indicate that the \ouradapter is well-suited for discriminative tasks but exhibits less controllable behavior in generative tasks. To mitigate this and ensure the assembled model's effectiveness across various tasks, we insert LoRA weights into the self-attention layer of the tuned transformer body. These weights are then fine-tuned on a joint corpus $\mathcal{D}_{mix}=\mathcal{D}_{la}\cup\mathcal{D}_{it}$, while keeping both the transformer body and embeddings frozen.
Since the transformer body and customized embeddings are independently trained, the \ouradapter approach may suffer from incompatible parameters. Our empirical findings indicate that \ouradapter is effective for discriminative tasks but sometimes underperforms an instruction-tuned baseline on generative tasks. To mitigate this and ensure the assembled model's effectiveness across various tasks, we insert LoRA weights into the self-attention layer of the tuned transformer body (Figure~\ref{fig:franken_adapter} step 3). These weights are then fine-tuned on a sub-sampled joint corpus $\mathcal{D}_{mix}=\mathcal{D}_{la}\cup\mathcal{D}_{it}$, while keeping both the transformer body and embeddings frozen.

% \wishlist{Add a discussion section to illustrate the training efficiency of our method compared conventional continued training method (full parameter tuning on NTL and then on FLAN for each language group.}
% \inlinetitle{Training Efficiency}
% \edit{\ouradapter offers a modular framework for composing embeddings tailored to different languages and LLMs post-trained for diverse purposes. For $M$ distinct language groups, our approach requires only $M$ instances of embedding tuning. These embeddings are then integrated into various types of post-trained LLMs, enabling on-the-fly usage with minimal training costs. This is in contrast to conventional continued pre-training method with full-parameter tuning, which requires extensive training on target languages followed by instruction tuning on alignment data, resulting in $2M$ instances of full-parameter tuning. Moreover, this process must be repeated if the instruction tuning is tailored for different task-solving abilities. By contrast, our method requires only one instance of instruction tuning for each target ability and achieves cross-lingual transfer through embedding swapping, thereby eliminating the need for massive redundant training.
% % Consequently, this incurs significantly higher resource demands compared to our method which avoids redundant training through efficient model reuse.
% }
% \wishlist{Add a figure for comparison with full-parameter tuning in appendix.}
% \wishlist{
% Have a separate subsection 2.3: discussion on several advantages (summarize the important points)
% 1. tokenizer make the training and inference more efficient.
% 2. less parameters tuned.
% 3. keep the transformer body untouched, avoid knowledge forgetting.
% 4. list the modular advantage, avoid redundant training.}

\subsection{Discussion}
Compared to the typical continued pre-training approach with full-parameter tuning on target languages, \ouradapter offers several advantages: 1) Customized tokenizers ensure fairer representations for target languages, which not only facilitates cross-lingual transfer but also improves training and inference efficiency. 2) Embedding tuning further enhances language adaptation efficiency by keeping the transformer body intact. This mitigates catastrophic forgetting that is prevalent in traditional methods. 3) The modular nature of \ouradapter enables the reuse of existing models with pre-established capabilities through simple and cost-effective embedding swapping in a fully \emph{post-hoc} manner. This property distinguishes \ouradapter from typical methods that requires costly and separate adaptations to acquire different skills, making it more desirable for effective and efficient cross-lingual transfer in many settings. (See Appendix~\ref{appendix:cpt} for more details with empirical evidence.)
