\section{Related Work}

\inlinetitle{Language Adaptation of LLMs.}
% The coverage of LLMs remains largely confined to English and a limited subset of other languages. 
Progress in LLMs have been dominated by English and other high-resource languages.
A growing body of research has shown progress in extending the applicability of LLMs to a broader range of languages. These methods typically involve continued pre-training on additional monolingual data~\citep{cui2024efficienteffectivetextencoding,zhao2024llamaenglishempiricalstudy} or multilingual instruction-tuning on synthetic data ~\citep{chen-etal-2024-monolingual,ustun-etal-2024-aya,aryabumi2024aya}. Notably, such efforts primarily focus on monolingual transfer or the multilingual adaptation of a few high-resource languages. Furthermore, these techniques often rely on computationally expensive full-parameter tuning, a strategy that tends to cause adapted LLMs to lose previously acquired knowledge due to catastrophic forgetting. \citet{bansal2024llm} augments an LLM with a smaller multilingual-specialized LLM by introducing a limited set of trainable parameters, yielding improved language adaptation while retaining general pre-trained knowledge. In contrast, we demonstrate that embedding tuning with additional multilingual data is sufficient for adapting LLMs to massive languages and creating customized vocabularies tailored to these languages can further enhance cross-lingual transfer.

\inlinetitle{Efficient Methods for Cross-Lingual Transfer.}
Diverse approaches have been proposed to efficiently adapt pre-trained LMs to new languages. These methods include using adapters~\citep{pfeiffer-etal-2020-mad,pfeiffer-etal-2022-lifting,yong-etal-2023-bloom,liu2022fewshot} and sparse fine-tuning~\citep{ansell-etal-2022-composable}. While effective, such parameter-efficient methods have been shown to be less effective in acquiring new knowledge~\citep{biderman2024lora}. Embedding surgery, on the other hand, has proven highly effective for adapting PLMs between languages while keeping the transformer body untouched ~\citep{artetxe-etal-2020-cross,de-vries-nissim-2021-good,marchisio-etal-2023-mini,chen2023improving}. However, its application has been limited to encoder-only PLMs and constrained by monolingual transfer, where one embedding is created per target language. Our \ouradapter approach builds upon this idea but differs significantly in that: we demonstrate its effectiveness with more powerful decoder-only LLMs for much wider range of tasks requiring different skill sets, and extend its capability to support massive cross-lingual transfer, enabling adaptation to a broader range of long-tail languages with a single embedding.

% Tokenizer Manipulation for LLMs.