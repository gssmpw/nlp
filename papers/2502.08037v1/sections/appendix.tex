\section{Experimental Settings}\label{appendix:exp_settings}
\subsection{Training Details}\label{appendix:training_details}
For language adaptation, we use a constant learning rate of $1\times 10^{-4}$ for \palmtwo and $1\times 10^{-5}$ for \gemmatwo and \aya with the Adam optimizer~\citep{kingma2014adam}. The embeddings are trained on a total of 200B tokens,\footnote{English accounts for 30.8\% of the tokens.} with each batch consisting of examples packed to a maximum sequence length of 2K for \palmtwo and 8K for \gemmatwo and \aya. We pre-train the model using the UL2 objectives~\citep{tay2023ul} for \palmtwo and causal language modeling objectives for \gemmatwo and \aya. Language adaptation consumes up to 256 TPU-v5 chips for the largest \gemmatwo-\texttt{27B} and \aya-\texttt{35B} models. The batch size is selected based on the model size and computing resources we have, with batch sizes of 256, 128, and 64 assigned to the \gemmatwo-\texttt{2B}, \texttt{9B}, and \texttt{27B} models, respectively. A similar strategy is applied to the \aya models. For \palmtwo models, we use the same batch size of 2048 for all variants. We choose the best checkpoint based on the performance of \flores development sets corresponding to each target language group. The training time varies with model size, where smaller models complete training within 24 hours while larger models require up to 1 week to finish.
%  Notably, for each LLM, we create new embeddings tailored to each of the three language groups (\ie \sea, \afr, and \ind).

We instruction-tune the transformer body of LLMs on $\mathcal{D}_{it}$ using the same hyper-parameter setting to obtain \texttt{XX-FLAN}, where we sample up to 200M instances from the FLAN mixture to construct $\mathcal{D}_{it}$. We use early stopping to select the best model based on the performance on MMLU~\citep{hendrycks2021measuring} and assemble it with customized embedding to obtain \texttt{XX-FA}. We observe all LLMs converge fast, as indicated by the average performance on MMLU. In most cases, training is completed within 24 hours.

For LoRA-Adaptation, we add LoRA weights to the self-attention module of all transformer layers with a LoRA rank of 64 and exclusively optimize these weights.\footnote{This adds less than 1\% parameters.} We use a learning rate of $5\times 10^{-6}$ for all models with 10\% steps of warm-up. Analogous to embedding tuning, the \flores development set is used for model selection. The training process is computationally efficient, completing within 12 hours even for the largest model.

\input{tables/prompt_formats}
\begin{figure*}[t]
    % \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/franken_adapter_english_benchmark.pdf}
    \vspace{-6mm}
    \caption{Performance on English tasks. For \ouradapter methods, we present the averaged performance with variances across three embeddings (\ie \sea, \afr, \ind). Shading indicates the standard deviations measured over three embeddings.}
    \vspace{-4mm}
    \label{fig:franken_adapter_english_benchmark}
\end{figure*}

\subsection{Evaluation Details}\label{appendix:eval_details}
We use the prompt formats listed in Table~\ref{tab:prompt_formats} for evaluation. For generation tasks, greedy decoding is employed with a maximum sequence length of 256 tokens. For classification tasks, we calculate the logits of each available option (\eg (A), (B)) and select the option with the highest score as the predicted answer.

For evaluating models in Figure~\ref{fig:result_summarization}, we use the same prompt formats shown in Table~\ref{tab:prompt_formats} for \texttt{Aya23-35B}~\citep{aryabumi2024aya}, \texttt{BLOOMZ-7B}~\citep{muennighoff-etal-2023-crosslingual}, and \texttt{Qwen2.5-32B-IT}~\citep{yang2024qwen2technicalreport}. We collect evaluation results of \texttt{ChatGPT-3.5-Turbo} from different papers: \flores from~\citet{robinson-etal-2023-chatgpt}, \sib from~\citet{adelani-etal-2024-sib}, \belebele from~\citet{bandarkar-etal-2024-belebele},\footnote{The results of shn\_Mymr and ory\_Orya are missing.} and \xorqain and \xsumin from~\citet{singh-etal-2024-indicgenbench}.\footnote{We copy the 1-shot results from~\citet{singh-etal-2024-indicgenbench}.} We use the officially-released translations\footnote{\url{https://github.com/facebookresearch/fairseq/tree/nllb}} for obtaining the results in \texttt{NLLB-54B-MOE}~\citep{nllbteam2022languageleftbehindscaling}.\footnote{The results of min\_Arab, ful\_Latn, orm\_Latn, and kon\_Latn are missing, which are excluded when computing the average score in NLLB-54B-MOE.}

\section{Details on English Benchmarking}\label{appendix:english_tasks}
For benchmarking performance in English, we choose ARC-Challenge~\citep{clark2018thinksolvedquestionanswering}, Boolq~\citep{clark-etal-2019-boolq}, DROP~\citep{dua-etal-2019-drop}, GSM8K~\citep{cobbe2021trainingverifierssolvemath}, HellaSwag~\citep{zellers-etal-2019-hellaswag}, MMLU~\citep{hendrycks2021measuring}, PIQA~\citep{bisk2019piqa}, and Winogrande~\citep{winogrande}. We use different number of shots for evaluation following~\citet{team2024gemma}.

Figure~\ref{fig:franken_adapter_english_benchmark} shows the comparison between the instruction-tuned FLAN model and our \ouradapter across various sizes of \gemmatwo. Both of our \ouradapter variants exhibit performance regressions across all tasks compared to the FLAN model, although the performance gap closes as model capacity scales up. We believe that these minor regressions are justifiable in light of the substantial gains achieved in zero-shot cross-lingual transfer.


\input{tables/franken_adapter_results_other_models}
\section{Additional Main Results}\label{appendix:extra_main_results}

\begin{figure}
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=\linewidth]{figures/emb_surgery_results_palm_aya.pdf}
    \vspace{-8mm}
    \caption{\la on \palmtwo (XXS, S) and \aya (8B, 35B). Absolute gains over the pre-trained models are reported.}
    %  \wishlist{Finalize \aya-\afr.}
    \vspace{-6mm}
    \label{fig:palm_aya_results}
\end{figure}
% \boldtitle{Benefits of \la on less multilingual LLMs are more pronounced.}
\subsection{\la on \palmtwo and \aya}\label{appendix:extra_la_results}
We evaluate the generalization ability of language adaptation on two LLMs with varying levels of multilingualism. Among them, \palmtwo-\texttt{S} exhibits the strongest multilingual abilities while \aya models demonstrate limited multilingual performance. Figure~\ref{fig:palm_aya_results} shows that the performance gains from language adaptation decrease as the original multilingual scope of the LLMs expand (\aya $\rightarrow$ \palmtwo-\texttt{XXS} $\rightarrow$ \palmtwo-\texttt{S}). Moreover, larger performance improvements are observed on \aya models when scaling up their size, suggesting that language adaptation may be particularly effective for models with stronger English proficiency.


\subsection{\ouradapter Results on \palmtwo, \aya and \gemmatwo-\texttt{IT}}\label{appendix:extra_fa_results}
Employing the same \ouradapter pipeline, we observe similar patterns of performance improvement across all sizes of the \palmtwo and \aya models, as shown in Table~\ref{tab:franken_adapter_results_other_models}. This demonstrates the broad applicability of our method to various types of LLMs. In addition, we also show the detailed results of \gemmatwo-\texttt{IT} models, as supplementary to Figure~\ref{fig:franken_adapter_vs_emb_surgery_on_it}. The \ouradapter method also performs effectively with off-the-shelf \textsc{It} models that have undergone complex supervised fine-tuning and reinforcement learning. This further underscores the versatility of our proposed approach in integrating pre-trained multilingual embeddings into LLMs that have been instruction-tuned using diverse methodologies for efficient zero-shot cross-lingual transfer.


\begin{figure*}
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/fa_vs_sct.pdf}
    \vspace{-4mm}
    \caption{The difference between our \ouradapter and the continued pre-training (CPT) baseline for zero-shot cross-lingual transfer. The same customized tokenizers are used by CPT. For $M$ language groups and $N$ target skills to be adapted, our \ouradapter avoids redundant training through model composition, requiring only $M$ instances of embedding tuning and $N$ instances of transformer body tuning. By contrast, the CPT baseline requires separate adaptation for each target skill, resulting in a total number of $M+M*N$ instances of full-parameter tuning.}
    % \wishlist{Only have background color for trainable parts. Indicate colored background are trainable while whiteboard is freezing. Compare the number of training instances of these two methods (\ouradapter M Embed + N IT, continued pre-training M * N full parameter tuning). Add the continued pre-training baseline results (2B and 9B on SEA languages).}
    \vspace{-6mm}
    \label{fig:fa_vs_sct}
\end{figure*}

\begin{figure}
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=\linewidth]{figures/fa_vs_cpt.pdf}
    \vspace{-8mm}
    \caption{The performance comparison between \ouradapter (w/ LoRA-Adaptation) and the continued pre-training (CPT) baseline. For English, we report the average across 8 datasets. For the rest of multilingual datasets, we report scores of the \sea subsets.}
    \vspace{-6mm}
    \label{fig:fa_vs_cpt_sea}
\end{figure}


\section{Comparison to Continued Pre-training}\label{appendix:cpt}
We compare our \ouradapter approach with a standard continued pre-training (CPT) baseline. Figure~\ref{fig:fa_vs_sct} illustrates the training process differences between these two methods. Specifically, we first pre-train the entire model using our curated multilingual dataset, $\mathcal{D}_{la}$, followed by instruction tuning the model with English alignment data, $\mathcal{D}_{mix}$. Both stages are performed using full-parameter tuning. To ensure a fair comparison, we adopt the same customized tokenizer and embedding initialization techniques to exclude the influence of tokenization.

Figure~\ref{fig:fa_vs_cpt_sea} reports the results of CPT and \ouradapter based on \gemmatwo. CPT demonstrates significant declines in English proficiency across all model scales, indicating  catastrophic forgetting of knowledge. By contrast, \ouradapter exhibits only minor regressions, underscoring the advantages of model composition that reuses the knowledge of existing models. For zero-shot cross-lingual transfer, \ouradapter generally outperforms CPT on \belebele and \sib, both of which rely on knowledge transfer from English. However, on \flores, CPT achieves substantially better performance than \ouradapter. This outcome is unsurprising, as CPT employs full fine-tuning of the model, thereby using its greater capacity to learn from parallel data within the multilingual pre-training dataset $\mathcal{D}_{la}$. Besides, CPT experiences significant performance drop on GSM8K-NTL compared to \ouradapter (29.8\% \versus 40.4\% on 9B and 41.0\% \versus 44.9\% on 27B). Overall, \ouradapter effectively mitigates the knowledge-forgetting challenges inherent in conventional language adaptation approaches and offers a more robust solution for transferring capabilities across languages.

\section{Supplementary Analysis}\label{appendix:supplementary_analysis}

\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=\linewidth]{figures/fertility_comparison_palm_aya.pdf}
    \vspace{-6mm}
    \caption{The tokenization comparison between using the vanilla and customized multilingual tokenizers on \gemmatwo. Tok. Length refers to the average number of tokens required to represent the same amount of texts.}
    \vspace{-4mm}
    \label{fig:fertility_comparison_palm_aya}
\end{figure}
% \inlinetitle{Tokenization fertilities of customized vocabularies on \palmtwo and \aya.}
\inlinetitle{Additional tokenization results on the customized vocabularies.}
We present additional results on tokenization fertility using customized vocabularies developed for \palmtwo and \aya. Figure~\ref{fig:fertility_comparison_palm_aya} shows similar patterns as those reported for \gemmatwo, where the fertility tokenization for both MRLs and LRLs shows a substantial decrease, while the English tokenization remains roughly unchanged. This effect is particularly pronounced in \aya, where we observe over $\times$3 reduction in fertility for \sea and \ind languages. In Table~\ref{tab:tokenization_qualitative_examples}, we also show a few tokenized examples for low-resource languages. We find that the customized tokenizer produces more meaningful tokens and avoids overtokenization.

\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=\linewidth]{figures/joint_vs_distinct_tokenizer.pdf}
    \vspace{-8mm}
    \caption{Performance comparison between employing distinct tokenizers for each language group and using a single tokenizer for all language groups (\ie Joint Tokenizer). The averaged performance of five tasks included in Table~\ref{tab:franken_adapter_results} is reported.}
    \vspace{-4mm}
    \label{fig:joint_vs_distinct_tokenizer}
\end{figure}
\inlinetitle{A joint customized tokenizer for all language groups is inferior to language-group specific tokenizers.}
In this ablation, we examine the effects of using distinct tokenizers for each language group in our experiments. For comparison, we create a joint tokenizer for the \sea, \afr, and \ind language groups by merging their tokenizer training corpora. The joint tokenizer maintains the same vocabulary size as the distinct tokenizers. We first conduct language adaptation with embedding tuning using the joint tokenizer on the combined pre-training dataset for the three language groups. This is followed by applying the standard \ouradapter method with LoRA-Adaptation. Importantly, this approach creates a single model, which is subsequently evaluated across all \sea, \afr, and \ind languages.

As shown in Figure~\ref{fig:joint_vs_distinct_tokenizer}, the use of a joint tokenizer significantly improves the cross-lingual transfer capabilities of \gemmatwo models across all model sizes. However, this approach underperforms compared to our primary setup, which employs distinct tokenizers for each language group. The performance difference is particularly pronounced in 2B models. This discrepancy arises because the use of a joint tokenizer reduces the capacity allocated to each language, as the total vocabulary size remains fixed. Consequently, this results in an approximate 10\% increase in fertility across languages at all resource levels, potentially hindering language learning. This finding aligns with the trends observed in Figure~\ref{fig:effects_customized_vocabulary}, where smaller models are more impacted by changes in fertility.

\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=\linewidth]{figures/fertility_vs_performance_palm2.pdf}
    \vspace{-8mm}
    \caption{Correlation between the performance of language adaptation on \palmtwo-\texttt{XXS} with tokenizer fertility. Normalized \texttt{ChrF++} on \textsc{Flores}-\sea are reported. {$\bLozenge$} and {\pfix}  indicate the original and customized tokenizers in \palmtwo.}
    \vspace{-6mm}
    \label{fig:fertility_vs_performance_palm2}
\end{figure}
\inlinetitle{Tokenizer fertility is also inversely correlated to downstream performance in \palmtwo.}
We replicate the analysis shown in Figure~\ref{fig:fertility_vs_performance} using \palmtwo-\texttt{XXS}, with results shown in Figure~\ref{fig:fertility_vs_performance_palm2}. We observe patterns analogous to those reported for \gemmatwo-\texttt{2B}, wherein reduced tokenizer fertility is generally associated with improved downstream performance and this relationship is particularly pronounced in LRLs and languages written in non-Latin script.

\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=\linewidth]{figures/data_ablation_on_pretraining.pdf}
    \vspace{-8mm}
    \caption{Pre-training data ablation for language adaptation. $\mathcal{D}_{ntl}$: multilingual data sampled from the NTL corpus; $\mathcal{D}_{doc}$: multilingual data sampled Wikipedia and mC4; $\mathcal{D}_{la}$: our final data mixture for language adaptation, \ie $\mathcal{D}_{la}=\mathcal{D}_{ntl}\cup\mathcal{D}_{doc}$; $\mathcal{D}_{mono}$: monolingual data by excluding parallel sentences from $\mathcal{D}_{la}$. \sea languages results on \gemmatwo-\texttt{2B} are reported.}
    \vspace{-4mm}
    \label{fig:data_ablation_on_pretraining}
\end{figure}
\inlinetitle{Long-tail NTL and document-level data are both important for language adaptation.}
We perform language adaptation on \gemmatwo-\texttt{2B} in \sea languages with different data mixture. Figure~\ref{fig:data_ablation_on_pretraining} reveals that improved performance in long-tail languages primarily stems from the inclusion of NTL data, while document-level data  plays a crucial role in preserving knowledge for high-resource languages. The significance of incorporating document-level data is further underscored by the results on \belebele, where the removal of $\mathcal{D}_{doc}$ leads to a substantial performance decline across all resource levels. This finding highlights the importance of $\mathcal{D}_{doc}$ in maintaining the ability of LLMs in processing various types of texts. In addition, excluding parallel data from the training mixture (\ie, $\mathcal{D}_{mono}$) leads to a significant decline in performance on translation tasks. Moreover, the performance of LRLs on \belebele also declines substantially, indicating the critical role of parallel data in enhancing tasks beyond translation through facilitated cross-lingual transfer~\citep{anil2023palm}.

\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.25cm}
    \centering
    \includegraphics[width=\linewidth]{figures/data_ablation_on_lora_adaptation.pdf}
    \vspace{-8mm}
    \caption{Training data ablation for Lora-Adaptation. $\mathcal{D}_{la}$: multilingual data for embedding tuning; $\mathcal{D}_{it}$: the FLAN mixture for instruction-tuning; $\mathcal{D}_{mix}$: a combination of equally sub-sampled $\mathcal{D}_{la}$ and $\mathcal{D}_{it}$. Averaged \sea language results (normalized score \versus \ouradapter) on \gemmatwo-\texttt{2B} are reported.}
    \vspace{-6mm}
    \label{fig:data_ablation_on_lora_adaptation}
\end{figure}
\inlinetitle{Both multilingual and instruction data are important for LoRA-Adaptation}
We investigate the impact of employing multilingual $\mathcal{D}_{la}$ and instruction-tuning data $\mathcal{D}_{it}$ in LoRA-Adaptation. As shown in Figure~\ref{fig:data_ablation_on_lora_adaptation}, the removal of either $\mathcal{D}_{la}$ or $\mathcal{D}_{it}$ harms the performance compared to the vanilla \ouradapter model on \flores or \belebele, while combining both datasets enables the adapted LLM to achieve the best overall results.

\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.25cm}
    \centering
    \includegraphics[width=\linewidth]{figures/ablation_on_emb_init.pdf}
    \vspace{-8mm}
    \caption{Ablations on embedding initialization methods. \textsc{Flores}-\sea language performance of language adaptation on \gemmatwo-\texttt{2B} is reported. \texttt{Max Pooling}: for each new token in the customized vocabulary, we use the original tokenizer to tokenize it and apply max pooling over the embeddings of the corresponding subtokens as the initialization.}
    \vspace{-5mm}
    \label{fig:ablation_on_emb_init}
\end{figure}
\inlinetitle{Employing the original embeddings for initialization is essential to language adaptation.}
In Figure~\ref{fig:ablation_on_emb_init}, we show that without initializing the customized embeddings using the original embeddings from the LLM, the language adaptation does not perform well at all, yielding \texttt{ChrF++} scores below 10 even for HRLs. In contrast, initializing with the original embeddings significantly improves the effectiveness of language adaptation, where the average pooling method slightly outperforms the max pooling variant.

\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.25cm}
    \centering
    \includegraphics[width=\linewidth]{figures/emb_surgery_lora_fpt.pdf}
    \vspace{-8mm}
    \caption{The effects of using customized vocabularies with different proportions of tuned parameters. The averaged score on \sea languages of \flores is reported.}
    \vspace{-6mm}
    \label{fig:emb_surgery_lora_fpt}
\end{figure}
\inlinetitle{The benefits of employing customized vocabulary decrease with more tuned parameters.}
We study the effects of employing customized embeddings with varying ratios of tuned parameters. As shown in Figure~\ref{fig:emb_surgery_lora_fpt}, the benefits of using customized embeddings diminish as the number of tuned parameters increases (Emb $\rightarrow$ Emb+LoRA $\rightarrow$ Full). Specifically, on \gemmatwo-\texttt{2B} model, customized embeddings show no advantage when full-parameter tuning is employed. This phenomenon arises because increasing the number of tuned parameters allocates greater model capacity for language adaptation, which simplifies the adaptation process compared to relying solely on embedding tuning. Nonetheless, full-parameter tuning could exacerbate the problem of catastrophic forgetting, while embedding tuning provides a safer alternative. Furthermore, the use of customized embeddings amplifies the advantages of embedding tuning, making it a promising technique.

\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/fixed_context_window_comparison.pdf}
    \vspace{-5mm}
    \caption{N-shot prompting results on \textsc{Flores}-\sea by varying context windows. The maximum average number of shots that fit in each window is indicated.}
    \vspace{-4mm}
    \label{fig:fixed_context_window_comparison}
\end{figure}
\inlinetitle{Customized vocabulary is more inference efficient.}
Figure~\ref{fig:fixed_context_window_comparison} indicates that employing the customized vocabulary can consistently leads to better results under varying context window sizes. Additionally, the customized vocabulary makes the model more inference and prompt efficient, achieving the same performance while needing roughly 10$\times$ fewer in-context learning examples examples and tokens.

\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=\linewidth]{figures/limited_multilinguality_llms.pdf}
    \vspace{-8mm}
    \caption{Learning curves of language adaptation on LLMs with limited multilingual abilities. Averaged results on \sea languages of \flores are reported.}
    \vspace{-6mm}
    \label{fig:limited_multilinguality_llms}
\end{figure}

\inlinetitle{Multilingual warmup is necessary for language adaptation on LLMs with limited multilingual abilities.}
We investigate whether embedding tuning with customized embeddings is a universal technique for the language adaptation of any types of LLMs. As shown in Figure~\ref{fig:limited_multilinguality_llms}, simply performing language adaptation on \texttt{Gemma-2B}, a model with very limited multilingual capabilities, does not successfully adapt it to various languages. In contrast, when a multilingual continued pre-training process is conducted prior to language adaptation, where the document-level data $\mathcal{D}_{doc}$ is used to warm up the LLM, we observe consistent improvements throughout the training process. This suggests that, for LLMs, a good initial multilingual ability is essential for the success of language adaptation.


\section{Results by Language}\label{appendix:detailed_lang_results}
Table~\ref{tab:dataset_by_language} presents an overview of languages available across our evaluation benchmarks. We show the per-language results on each task for both language adaptation
% (Tables~\ref{tab:flores_results},~\ref{tab:gsm_8k_ntl_detailed_results},~\ref{tab:belebele_results})
(Tables~\ref{tab:flores_results} --~\ref{tab:belebele_results})
% and \ouradapter (Tables~\ref{tab:franken_adapter_belebele_results},~\ref{tab:franken_adapter_sib200_results},~\ref{tab:franken_adapter_flores200_results},~\ref{tab:franken_adapter_xorqa_results},~\ref{tab:franken_adapter_xsumin_results}).
and \ouradapter (Tables~\ref{tab:franken_adapter_belebele_results} --~\ref{tab:franken_adapter_xorqa_results}).


\onecolumn

\input{tables/tokenization_qualitative_examples}

\input{tables/dataset_by_language}

% \clearpage
\input{tables/flores_results}
\input{tables/belebele_results}

\input{tables/franken_adapter_belebele_results}
\input{tables/franken_adapter_sib200_results}
\input{tables/franken_adapter_flores200_results}
\input{tables/franken_adapter_gsm8kntl_results}
\input{tables/franken_adapter_xsum_results}
\input{tables/franken_adapter_xorqa_results}
