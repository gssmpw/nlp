\section{Experiment setup}
% \subsection{Pre-training Data and Languages}
\subsection{Pre-training Data}
% \inlinetitle{Data Mixture}
The data $\mathcal{D}_{la}$ for embedding training is a mixed corpus with 65\% sentence-level and 35\% document-level texts. The sentence-level data is exclusively from the Next Thousand Languages (NTL) corpora~\citep{caswell-etal-2020-language, bapna2022building}, which provides web-crawled monolingual sentences and translation pairs for over 1000 languages. For document-level texts, we sample data from multilingual Wikipedia and mC4~\citep{xue-etal-2021-mt5}. We use UniMax sampling~\citep{chung2023unimax} with $N=5$ to up-sample low-resource languages. Additionally, we consider English as a high-resource language and always include it in the training mixture to prevent catastrophic forgetting.

We take FLAN~\citep{wei2022finetuned} as the instruction tuning data $\mathcal{D}_{it}$. 
% For LoRA-Adaptation, we create $\mathcal{D}_{mix}$ by combing $\mathcal{D}_{la}$ and $\mathcal{D}_{it}$ equally\footnote{We empirically find this results in good downstream performance (Figure~\ref{fig:data_ablation_on_lora_adaptation}), although thoroughly testing how different sampling ratios from each kind of data affect final performance would be beneficial, which we leave as future work.} and tune the model on 10\% of $\mathcal{D}_{mix}$.
The data $\mathcal{D}_{mix}$ used in LoRA-Adaptation consists of a 10\% sample of $\mathcal{D}_{it}$, combined with an equal number of instances from $\mathcal{D}_{la}$.\footnote{This ensures the instruction-following ability isn't forgotten. Please refer to Appendix Figure~\ref{fig:data_ablation_on_lora_adaptation} for detailed analysis.}

% \inlinetitle{Languages}
\subsection{Languages}\label{sec:languages}
We select languages from three families based on geographic relations: South East Asian (\sea), African (\afr), and Indic (\ind). 
% Following the NTL classification, we include 212 languages from \sea, 392 from \afr, and 170 from the \ind family.
This results in 212 languages from \sea, 392 from \afr, and 170 from \ind. Each regional dataset is processed separately, with a tailored tokenizer, language-adapted embeddings and LoRA update parameters.

% \subsection{Models and Training Details}

\subsection{Models}
% We evaluate our method on two open-source LLMs: \gemmatwo (2B, 9B, 27B)~\citep{team2024gemma} and ; and one 
Our evaluation is focused on \gemmatwo (2B, 9B, 27B)~\citep{team2024gemma}. We also test the generalization ability of our method in two LLMs with varying degrees of multilinguality: \aya (8B, 35B)~\citep{aryabumi2024aya} and \palmtwo (XXS, S)~\citep{anil2023palm}.

As shown in Figure~\ref{fig:franken_adapter}, we end up having four types of models: (i) \texttt{$*$-FLAN} (step-2a): models that undergo instruction-tuning. (ii) \texttt{Lang-Adapt} (step-2b): the LLMs after language adaptation with embedding tuning. (iii) \ouradapter (step-3 Combine): model denoted as \texttt{$*$-FA} is constructed by combining the transformer body of \texttt{$*$-FLAN} with the embeddings from \texttt{Lang-Adapt}. (iv): \texttt{LoRA-Adapt} (step-3 LoRA Tuning): \ouradapter models with the LoRA-Adaptation process. Detailed training procedures for each model type are in Appendix~\ref{appendix:training_details}.

% \inlinetitle{Training Details}
% For embedding training, we use a constant learning rate of $1\times 10^{-4}$ for \palmtwo and $1\times 10^{-5}$ for \gemmatwo and \aya with the Adam optimizer~\citep{kingma2014adam}. The embeddings is trained on a total of $\sim$200B tokens, with each batch consisting of examples packed to a maximum sequence length of 2K for \palmtwo and 8K for \gemmatwo and \aya. We follow the original papers to pre-train the model using the UL2 objectives~\citep{tay2023ul} for \palmtwo and causal language modeling objectives for \gemmatwo and \aya. Notably, for each LLM, we create new embeddings tailored to each of the three language groups (\ie \sea, \afr, and \ind).
  
% We instruction-tune the transformer body of LLMs on $\mathcal{D}_{it}$ with the same hyperparameters as above to obtain \texttt{Model-FLAN}. We use early stopping to select the best model based on the performance on MMLU~\citep{hendrycks2021measuring} and assemble it with customized embedding to obtain \texttt{Model-FA}. For LoRA-Adaptation, we set LoRA rank to 64 and use a learning rate of $5\times 10^{-6}$ for all models with 10\% steps of warm-up.

\subsection{Evaluation Tasks}
For LLMs after language adaptation, we adopt the \emph{five-shot} prompting strategy. In contrast, \ouradapter is evaluated in a \emph{zero-shot} setting, given it has been instruction tuned. We also evaluate \ouradapter using a compiled English benchmark (Appendix~\ref{appendix:english_tasks}) to examine potential regressions in general English ability.

\textbf{\belebele~\citep{bandarkar-etal-2024-belebele}}
is a multiple-choice reading comprehension (MRC) dataset 
with 122 languages.
% with 122 languages of varying resource levels.
The dataset contains 900 instances created from short passages sampled from \flores. 
We evaluation on all the 15 \sea, 25 \afr, and 19 \ind languages. 
We follow the original paper to use the \texttt{Accuracy} metric and sample five-shot prompts from the English training dataset. 

\textbf{\sib~\citep{adelani-etal-2024-sib}}
is a topic classification task based on texts sampled from \flores. 
% It was created by annotating the English samples and extending the annotation to the instances of the remaining languages. 
We select all the 25 \sea, 45 \afr and 26 \ind languages for evaluation with zero-shot prompting and use the \texttt{Accuracy} as the metric for performance comparison.

\textbf{\flores~\citep{goyal-etal-2022-flores}}
is a machine translation dataset with 200 languages. 
% The data was extracted from 842 English Wikipedia articles with diverse topics and domains, which have been translated into 200 languages by professional translators. 
We evaluate on all the 23 \sea, 42 \afr, and 21 \ind languages using \texttt{ChrF++} and sample five-shot prompts from the dev set for few-shot evaluation.

\textbf{GSM8K-NTL~\citep{shi2023language, bansal2024llm}}
is a silver benchmark created by automatically translating the English GSM8K dataset into 25 low-resource languages. For evaluation, we focus on 5 \sea, 5 \afr, and 10 \ind languages and follow~\citet{bansal2024llm} to use the \texttt{Accuracy} metric and fixed five-shot prompts.

\textbf{\textsc{IndicGenBench}~\citep{singh-etal-2024-indicgenbench}}
is a human-curation benchmark across 29 Indic languages. We evaluate on the cross-lingual question-answering dataset~\textsc{XorQA-In} and the cross-lingual summarization dataset~\textsc{XSum-In} with zero-shot prompting. The token-level \texttt{F1} and \texttt{ChrF} scores are reported in~\textsc{XorQA-In} and~\textsc{XSum-In}, respectively.

\begin{figure}
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.5cm}
    \centering
    \includegraphics[width=\linewidth]{figures/emb_surgery_results_gemma2.pdf}
    \vspace{-6mm}
    \caption{\flores \textsc{En-Xx} and \belebele \la results across all sizes of \gemmatwo models with 5-shot prompting. Absolute gains over the pre-trained models are reported. $\#P$: fraction of tuned parameters (\ie embedding parameters).}
    \vspace{-6mm}
    \label{fig:gemma_results}
\end{figure}

\subsection{Main Results}
We first present the results of \la through embedding tuning on its own (\S\ref{sec:model_training} -- Language Adaptation) in \S\ref{sec:embed_surgery_results}. Then we evaluate the application of \ouradapter alongside LoRA adaptation (\S\ref{sec:model_training} -- \ouradapter \& LoRA-Adaptation) in \S\ref{sec:franken_adapter_results}.

\input{tables/gsm8k_ntl_results}

\subsubsection{\la Results}\label{sec:embed_surgery_results}

\boldtitle{\la improves cross-lingual transfer.}
Figure~\ref{fig:gemma_results} shows the absolute gains of language adaptation on \gemmatwo models. When evaluating across three language groups, we observe that language adaptation consistently outperforms vanilla \gemmatwo models, demonstrating a significant performance advantage. The performance gains are particularly more pronounced on medium and low-resource languages, and this trend becomes increasingly apparent as model size scales. Similar findings are observed for \aya and \palmtwo, as presented in Appendix~\ref{appendix:extra_la_results}.

\input{tables/franken_adapter_results}


\boldtitle{\la helps preserve LLMs' general knowledge.}
% Table~\ref{tab:gsm_8k_ntl_results} shows that embedding surgery significantly enhances the reasoning abilities of LLMs on low-resource languages. Compared to CALM~\citep{bansal2024llm}, which imparts new skills to LLMs by augmenting a specialized LLM, our method achieves more substantial improvements on \palmtwo-\texttt{S} without incurring extra inference costs. Moreover, our approach shows superior performance (+4\%) when comparing to \palmtwo-\texttt{S-NTL} that was full-parameter tuned on NTL. This suggests that embedding surgery is more effective in transferring language knowledge to LLMs while preserving general knowledge. Notably, we observe boosted gains on \gemmatwo, indicating the significant potential of embedding surgery for enhancing the low-resource language reasoning abilities of high-capacity LLMs.
We evaluate whether language adaptation improves the transfer of English reasoning in LLMs to other languages. Table~\ref{tab:gsm_8k_ntl_results} shows LLMs with language adaptation consistently improves the mathematical reasoning ability across various low-resource languages. Compared to CALM~\citep{bansal2024llm}, a form of adapter enabling model composition, our method achieves more substantial improvements over \palmtwo-\texttt{S} with only embedding tuning and incurring no extra inference costs.\footnote{Our approach, however, requires more training as embedding tuning must be repeated for different linguistic regions.} Moreover, our approach shows superior performance (+4\%) compared to \palmtwo-\texttt{S-NTL} that was full-parameter tuned on NTL.\footnote{Both methods use the same training dataset. However, NTL training did not create region-specific models by splitting training data, potentially diminishing its effectiveness due to the curse of multilinguality~\citep{conneau-etal-2020-unsupervised}, which arises when a single model is trained on too many languages.} Overall, our results suggest that language adaptation offers an effective alternative to CALM and full-parameter tuning. We observe performance improvements with more modern models, with particularly pronounced gains in \gemmatwo.

\subsubsection{\ouradapter Results}\label{sec:franken_adapter_results}

\boldtitle{\ouradapter enhances mathematical reasoning performance in target languages on the fly.}
In Table~\ref{tab:gsm_8k_ntl_fa_results}, we show that \ouradapter consistently outperforms the instruction-tuned baseline for zero-shot evaluation on mathematical reasoning tasks, with an average improvement of up to 8\% across 20 low-resource languages. Moreover, \ouradapter further advances performance by incorporating LLMs with enhanced mathematical reasoning ability (\eg the $\texttt{IT}$ models aligned via reinforcement learning).\footnote{We observe LLMs generate Chain-of-Thought steps more often in English than in the question language, and we hypothesize that the customized embeddings in \ouradapter enable the model to better comprehend the questions.}

\ouradapter can also enhance the cross-lingual performance of LLMs instruction-tuned for reasoning. During the instruction-tuning stage (Figure~\ref{fig:franken_adapter} step 2(a)), we employ a mathematical instruction dataset, \texttt{WebInstruct}~\citep{yue2024mammoth}, for model training, and obtain the composed model by reusing the embeddings from the language adaptation stage. Table~\ref{tab:gsm_8k_ntl_fa_results} shows \ouradapter consistently improves performance in target languages. These results underscore its versatility by facilitating cross-lingual transfer in models post-trained for a specific domain.

% \boldtitle{\ouradapter is generally more robust to classification than generation tasks.}
\boldtitle{\ouradapter is more effective for classification tasks.}
As shown in Table~\ref{tab:franken_adapter_results}, \ouradapter consistently improves the performance on classification tasks across all model sizes with up to 10\% absolute gains. By contrast, for the generation tasks, the method's behavior is inconsistent, often leading to performance degradation. We attribute this phenomenon to intrinsic difficulty: classification tasks are generally easier as the solution space is typically small compared to generation tasks, making them more robust to embedding changes. However, the embedding layer is used for both text encoding and decoding in generation, and the auto-regressive generation paradigm makes the model sensitive to embedding changes due to error propagation accumulating over time steps.


\boldtitle{LoRA-Adaptation connects both worlds.}
In Table~\ref{tab:franken_adapter_results}, we show that by applying cost-effective LoRA-Adaptation, the two components within the \ouradapter can cooperate better, leading to significant gains on generation tasks. This results in an average improvement of 5.4\% on the largest 27B variant. The results demonstrate the practical use of our \ouradapter method for efficient zero-shot cross-lingual transfer of instruction-tuned LLMs. See Appendix~\ref{appendix:extra_fa_results} and Table~\ref{tab:franken_adapter_results_other_models} for additional results on \aya and \palmtwo. Furthermore, these improvements come at minimal cost on English proficiency (See Appendix~\ref{appendix:english_tasks}) and the same findings are observed for \aya \ \etc (See Table~\ref{tab:franken_adapter_results_other_models})

\subsection{Ablation Analysis}

\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=\linewidth]{figures/effects_customized_vocabulary.pdf}
    \vspace{-8mm}
    \caption{Ablations on tokenizers for language adaptation. Macro-averaged scores on \sea subset of \flores and \belebele are reported. $\Delta_{TF}$: \% tokenizer fertility reduction.}
    % \wishlist{Use same colours for each category and dashed line for the vanilla/pt models. Keep them consistent for different figures.}
    \vspace{-6mm}
    \label{fig:effects_customized_vocabulary}
\end{figure}

\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=\linewidth]{figures/effects_customized_vocabulary_franken_adapter.pdf}
    \vspace{-9.5mm}
    \caption{\ouradapter result comparison when using original and customized tokenizers. The macro-averaged scores on \sea subset of \belebele, \sib and \flores are reported. }
    \vspace{-4mm}
    \label{fig:effects_customized_vocabulary_franken_adapter}
\end{figure}

\boldtitle{Customized vocabulary amplifies the benefits of training on multilingual data.}
% We study the influence of multilingual pre-training data and customized vocabulary.
We decouple the effects of multilingual data in language adaptation from the change in vocabularies.
Figure~\ref{fig:effects_customized_vocabulary} shows that simple continued training on additional multilingual data with embedding tuning can significantly improve the performance in medium and low-resource languages, indicating LLMs are under-fitting to these languages. Based on this, employing a customized vocabulary that is fairer to these languages (Figure~\ref{fig:fertility_comparison}) can amplify the benefits of embedding tuning on multilingual data. This enhanced learning process facilitates better knowledge acquisition~\citep{zhang-etal-2022-robust,hofmann-etal-2022-embarrassingly}. Moreover, the improvements are more pronounced in smaller models, highlighting the importance of effective tokenization for these models to adapt well to low-resource languages. 
% In Figure~\ref{fig:effects_customized_vocabulary_franken_adapter}, we further show the employment of customized vocabularies leads to consistent performance gains under the \ouradapter framework, underscoring their importance for more effective cross-lingual transfer.
In addition, Figure~\ref{fig:effects_customized_vocabulary_franken_adapter} indicates that the importance of customized vocabulary is also apparent in the \ouradapter setting.

\input{tables/latency_compare}
\boldtitle{Customized vocabulary improves latency.}
We evaluate latency by measuring the number of texts processed per second by LLMs. We use the passages from all \sea, \afr, and \ind languages in \belebele as test texts. For comparison, we test \ouradapter models integrated with embeddings trained using either the original \gemmatwo tokenizer or our customized one. Table~\ref{tab:latency_compare} shows that employing customized tokenizer consistently improves latency, particularly in MRLs and LRLs. This trend becomes increasingly pronounced as model size scales, highlighting the importance of customized tokenizers in achieving low-latency processing for long-tail languages in larger LLMs.

\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=\linewidth]{figures/fertility_vs_performance.pdf}
    \vspace{-8.5mm}
    \caption{Correlation between the results of language adaptation on \gemmatwo-\texttt{2B} with tokenizer fertility. Normalized \texttt{ChrF++} on \textsc{Flores}-\sea is reported. {$\bLozenge$} and {\pfix}  indicate the original and our customized tokenizers used in the other settings, respectively.}
    % \fan{Should we indicate leftmost and rightmost dots for HRLs and LRLs are from the original \gemmatwo tokenizer?}
    \vspace{-5.5mm}
    \label{fig:fertility_vs_performance}
\end{figure}

\boldtitle{Tokenization fertility is inversely correlated to downstream performance.}
We study how tokenization fertility (the average number of tokens produced per word) affects the LLM's performance across languages. 
To ablate this effect, we generate several re-sampled replicates of our tokenizer training datasets with different levels of priority given to high \versus lower resource languages. Specifically, we use temperature sampling to manipulate the training sentences of each language for building different tokenizer training data, and train tokenizers with varying fertilities. We then do language adaptation for each of these tokenizers and analyze the downstream performance. As shown in Figure~\ref{fig:fertility_vs_performance}, we find that the performance is inversely correlated to tokenization fertility, but the correlation is not uniform across languages. Notably, slight reductions in fertility lead to significant performance improvements in low-resource languages (\eg \textsc{Ace}) while high-resource languages are less sensitive to fertility changes.
Furthermore, Latin-script languages generally benefit more from fertility reductions compared to those in non-Latin scripts.
% \edit{Languages written in non-Latin scripts also favor low fertility regardless of resource levels (\eg \textsc{Th} and \textsc{Lo}).}\fan{\trevor{is your point that the blue and red points are roughly on the same line? I'm a bit puzzled, can discuss.} Need to discuss on this.} However, for these languages, monotonically reducing fertility is not always beneficial, with the performance saturating after reaching a certain threshold.
Please refer to Appendix Figure~\ref{fig:fertility_vs_performance_palm2} which reports similar findings for \palmtwo-\texttt{XXS}.

\boldtitle{Pruning with extension outperforms other tokenizer construction methods.}
We study the impact of tokenizer construction methods on the performance of language adaptation. We compare our \emph{Prune-with-Extension} method (\S\ref{sec:vocab_construct}) against two variants: 1) \emph{Scratch}, which trains tokenizers from scratch on English and target languages; and 2) \emph{Extension}, which appends target language tokens to existing tokenizers without pruning. This adds an additional of 34\% embedding parameters. Figure~\ref{fig:ablation_on_tokenizer_building} shows our \emph{Prune+Extension} method achieves the best overall results. Notably, the performance differences on \textsc{Flores} are minimal across the methods. We attribute this to the relatively lower intrinsic complexity of \textsc{Flores} compared to \belebele that demands reasoning ability most likely transferred from English. This claim is supported by the substantial improvements in English on \belebele when switching to our approach that preserves original English tokens. Moreover, we observe that tokenizer built from scratch exhibits significantly fewer overlapping English tokens with the original tokenizer compared to the other methods. This signifies the importance of retaining English tokens to preserve pre-training knowledge, which is vital for the success of language adaptation.\footnote{It's an open question of whether other facets of the tokenizer need to be retained to preserve other behaviours, e.g., markup tokens to facilitate code understanding.} Our method also surpasses \emph{Extension}, indicating that removing irrelevant tokens is beneficial while avoid introducing extra parameters.\footnote{We suspect large vocabulary could increase ambiguity in output $\operatorname{softmax}$. Evidence also reveals that large vocabularies are not optimal for smaller LLMs~\citep{tao2024scaling}.}

\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=\linewidth]{figures/ablation_on_tokenizer_building.pdf}
    \vspace{-9mm}
    \caption{Ablations on tokenizer building methods. We report \sea language adaptation on \gemmatwo-\texttt{2B}.}
    \vspace{-4.5mm}
    \label{fig:ablation_on_tokenizer_building}
\end{figure}
\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=\linewidth]{figures/franken_adapter_vs_emb_surgery_on_it.pdf}
    \vspace{-9mm}
    \caption{Comparing applying language adaptation and \ouradapter on \gemmatwo-\texttt{IT} models. The averaged scores in five evaluation benchmarks (Table~\ref{tab:franken_adapter_results}) are reported.}
    \vspace{-6mm}
    \label{fig:franken_adapter_vs_emb_surgery_on_it}
\end{figure}

\boldtitle{\la on IT LLMs represents a viable alternative to \ouradapter .}
LLMs are typically released in both pre-trained (\textsc{Pt}) and instruct (\textsc{It}) versions. These versions provide two distinct pathways for achieving zero-shot cross lingual transfer: 1) employing our \ouradapter pipeline, which uses the \textsc{Pt} model for language adaptation and subsequently integrates the new embeddings into the \textsc{It} model through LoRA-Adaptation; 2) employing the \textsc{It}+Lang-Adapt approach, which performs one-step language adaptation directly on \textsc{It} model with the same training costs as on \textsc{Pt} model. Figure~\ref{fig:franken_adapter_vs_emb_surgery_on_it} shows a comparison of these two strategies. Our analysis reveals that for smaller model sizes, \ouradapter outperforms \textsc{It}+Lang-Adapt in downstream tasks. However, as model size increases, \textsc{It}+Lang-Adapt emerges as the more effective solution. 
% Nonetheless, the \ouradapter pipeline demonstrates greater flexibility by enabling the integration of pre-trained embeddings into various \textsc{It} models with minimal computational overhead via LoRA-Adaptation. In contrast, the \textsc{It}+Emb Surgery would be significantly more expensive if language adaptation of several \textsc{It} models all stemming from one common \textsc{Pt} model is required.
Overall, the \textsc{It}+Lang-Adapt method provides a highly effective alternative to \ouradapter for zero-shot cross-lingual transfer involving a single model. However, when dealing with multiple \textsc{It} models derived from a common \textsc{Pt} model, \ouradapter offers greater modularity by avoiding costly embedding tuning across individual models.