\section{Introduction}
% \wishlist{Change Embedding surgery to Language Adaptation throughout the paper.}

% 1. Firstly, talk about the background of adapting LLMs to other languages. Point the shortcomings of these methods: 1) most of them focus on one language with simple tokenizer extension; 2) requires full-parameter tuning, which is expensive and is risky of forgetting pre-trained knowledge; 3) focus on several high-resource languages instead of massive transfer to low-resource languages.
% 2. Secondly, talk about the efficient cross-lingual transfer methods, \ie the "On the Cross-lingual Transferability of Monolingual Representations" and related papers. Mention that they only did for pre-trained LMs with only understanding ability and also focus on zero-shot monolingual transfer (one embedding per language). How this applies to LLMs with both language understanding and generation abilities is unknown, and how to achieve efficient cross-lingual transfer to massive languages at the same time is also unclear.
% 3. Introduce our method: 1) how to do embedding surgery 2) how to do franken-adapter. and summarize our findings and contributions.

Large Language Models (LLMs) have transformed the field of natural language processing through pre-training on extensive web-scale corpora~\citep{gpt3, geminiteam2024geminifamilyhighlycapable}. Despite these advancements, their success has been primarily centered on English, leaving the multilingual ability less explored. While the multilingual potential of LLMs has been demonstrated across multiple languages~\citep{shi2023language}, their practical applications remain largely confined to a limited set of high-resource languages. This limitation reduces their utility for users speaking under-represented languages~\citep{ahia-etal-2023-languages}.

\begin{figure}[t]
    % \setlength{\abovecaptionskip}{-0.0001cm}
    % \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=\linewidth]{figures/sota_comparison.pdf}
    \vspace{-9mm}
    \caption{Zero-shot performance comparison between our best model (\gemmatwo-\texttt{27B}-\texttt{\ouradapter-LoRA}) and state-of-the-art LLMs on five benchmarks.}
    \vspace{-4mm}
    \label{fig:sota_comparison}
\end{figure}
\begin{figure}[t]
    % \setlength{\abovecaptionskip}{-0.0001cm}
    % \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/result_summarization.pdf}
    \vspace{-5mm}
    \caption{Result summary across diverse benchmarks. Scores are normalized \versus Pre-trained (top) and instruction-tuned (bottom) LLMs. All scores are macro-averaged across all sizes of \gemmatwo.}
    \vspace{-6mm}
    \label{fig:result_summarization}
    % Numbers for language Adaptation
    % pre-trained = [53.5, 25.8, 19.4]
    % emb_surgery = [56.6, 34.4, 31.3]
    % Numbers for Franken-Adapter
    % flan/instruction_tuning = [58.3, 67.2, 23.2, 34.6, 12.1]
    % franken_adapter = [63.5, 72.6, 23.3, 35.4, 12.2]
    % franken_adapter+lora = [65.4, 72.0, 33.7, 38.8, 15.8]
\end{figure}

A widely adopted approach to multilingual adaptation involves continued pre-training on additional data in target languages by using pre-trained LLMs as initialization~\citep{fujii2024continual, zheng-etal-2024-breaking}. This paradigm typically requires full-parameter tuning on vast data, making the adaptation of a new LLM to accommodate every language prohibitively costly. Moreover, such adaptation poses a significant risk of catastrophic forgetting, whereby the LLM loses previously acquired knowledge from the initial pre-training phase~\citep{luo2024empiricalstudycatastrophicforgetting, shi2024continuallearninglargelanguage}. Although alternative methods such as adapters~\citep{pfeiffer-etal-2021-unks} or LoRA~\citep{hu2022lora} offer more efficient solutions for language adaptation, their capacity to acquire new knowledge remains limited~\citep{biderman2024lora}. 
Model composition \citep{bansal2024llm} can achieve cross-lingual skill transfer by combining a general LLM with a smaller specialist model, to realize synergies over both models' capabilities. However, it requires decoding on the two composed models, which introduces extra inference costs.
These challenges underscore the importance of finding new methods to adapt LLMs to new languages efficiently and effectively.
%\edit{\citet{bansal2024llm} proposes CALM, which augments an LLM with a smaller, multilingual-specialized LLM to enhance translation and math reasoning in low-resource languages. While effective, this composition method introduces extra overhead due to the need for inference across two models.


\citet{artetxe-etal-2020-cross} propose an efficient zero-shot cross-lingual transfer method that adapts English pre-trained language models (PLMs) to unseen languages by learning new embeddings while keeping the transformer layers fixed, hypothesizing that monolingual models learn semantic linguistic abstractions
that generalize across languages. Despite showing promising results competitive to full pre-training, this \emph{embedding surgery} paradigm still has several shortcomings: its reliance on `outdated` encoder-only PLMs limits the applicability, and its monolingual transfer strategy (\ie one embedding per language) reduces its efficiency. These limitations prompt important questions: 1) Can this approach be extended to modern decoder-only LLMs? 2) Can we achieve efficient cross-lingual transfer to many languages and avoid the creation of monolingual embeddings?

\begin{figure*}
    \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/franken_adapter.pdf}
    \vspace{-4mm}
    % \caption{The overview of our training strategies for doing embedding surgery to LLMs: 1) freeze the transformer body and learn new multilingual embeddings to adapt LLMs to a target language group; 2) freeze the original embeddings of LLMs and instruction-tune the transformer body; 3) combine new embeddings with instruction-tuned transformer body for zero-shot cross-lingual transfer; 4) connect the instruction-tuned transformer body and customized embeddings using LoRA weights for enhanced yet efficient cross-lingual transfer.}
    \caption{Overview of our \ouradapter pipeline: 1) pre-train a LLM on English-dominant data; 2a) freeze the original embeddings of LLMs and instruction-tune the transformer body using English alignment data; 2b) learn new multilingual embeddings by freezing the transformer body for target language adaptation of LLMs; 3) combine new embeddings with instruction-tuned transformer body as the \emph{\ouradapter} and further perform LoRA tuning to connect the combined components for enhanced cross-lingual transfer.}
    %  \wishlist{Use \emph{Language adaptation} instead of \emph{Embedding surgery}. step1: pre-train a LLM. Filp step 2 and step 3, use (2a) and 2(b) since they can be done in parallel. Combine Franken-Adapter and LoRA Adaptation into one step 3. (Combine + LoRA Adaptation). Add numbers to each process corresponding to the ones in title.}
    \vspace{-5mm}
    \label{fig:franken_adapter}
\end{figure*}


% In this work, we empirically evaluate the framework within decoder-only LLMs through \emph{multilingual embedding surgery}. 
In this work, we explore \emph{multilingual embedding surgery} on decoder-only LLMs and demonstrate its effectiveness in improving cross-lingual transfer.
% As shown in Figure~\ref{fig:franken_adapter}, our method starts with an existing LLM primarily pre-trained on English data, which we adapt to a target language group\footnote{We focus on three language groups: South East Asia (\sea), Indic (\ind), and African (\afr).} by learning new multilingual embeddings while freezing the transformer body. Meanwhile, we employ English alignment data to instruction-tune the transformer body, keeping the original embeddings fixed. Subsequently, the newly trained multilingual embeddings are combined with the instruction-tuned transformer body for efficient zero-shot cross-lingual transfer, which we designate as the \emph{\ouradapter}. Moreover, we incorporate a cost-effective LoRA-based adaptation to enable seamless interaction between the assembled components within \emph{\ouradapter}.
As shown in Figure~\ref{fig:franken_adapter}, our method starts with two parallel training from an existing LLM primarily pre-trained on English data: 1) We adapt it to a target language group\footnote{We focus on three language groups: South East Asia (\sea), Indic (\ind), and African (\afr).} by learning new multilingual embeddings while freezing the transformer body; 2) We employ English alignment data to instruction-tune the transformer body, keeping the original embeddings fixed. After this, the newly trained multilingual embeddings are combined with the instruction-tuned transformer body for efficient zero-shot cross-lingual transfer, which we designate as \emph{\ouradapter}. Optionally, we can further enhance the compatibility of the composed components within \emph{\ouradapter} via a cost-effective LoRA-based adaptation. 

We show that a single \emph{language adaptation} step, employing the customized embeddings with pre-trained LLM, can significantly enhance the multilingual performance across diverse tasks (Figure~\ref{fig:result_summarization} top). Furthermore, the \emph{\ouradapter} framework, which integrates new embeddings into instruction-tuned LLMs, enables zero-shot cross-lingual transfer straight away, and can further benefit from cost-effective LoRA adaptation (Figure~\ref{fig:result_summarization} bottom). 
In summary, our contributions are:
\begin{compactenum}
    \item We demonstrate that embedding tuning is effective for language adaptation of LLMs, and systematically evaluate the critical role of tokenizers in this process. Our results show large performance improvement on low-resource languages when using customized tokenizers.
    \item Our \ouradapter approach provides a modular framework for efficient zero-shot cross-lingual transfer of LLMs via embedding surgery. Notably, we show that our best model outperforms benchmark LLMs at comparable sizes across diverse tasks (Figure~\ref{fig:sota_comparison}).
    %effectively bridging the language gap.
\end{compactenum}

