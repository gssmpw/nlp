\begin{table*}[t]
\setlength{\belowcaptionskip}{-0.3cm}
\setlength{\tabcolsep}{6pt}
\footnotesize
\centering
\vspace{-2.5mm}
\caption{\ouradapter performance with zero-shot prompting. The best and second-best results are marked in \textbf{bold} and \underline{underlined}. \textcolor{red!80}{Red} values indicate instances where \ouradapter hurts the performance. English results are excluded when computing the average.}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|ccc|ccc|ccc|cc|cc|c}
    \toprule
    \bf Task Type & \multirow{4}{*}[-1ex]{\rotatebox[origin=c]{90}{\bf \textsc{English}}} & \multicolumn{6}{c|}{\bf \textsc{Classification}} & \multicolumn{7}{c|}{\bf \textsc{Generation}} \\
    \cmidrule(lr){3-8} \cmidrule(lr){9-15}
    \multirow{2}{*}{\bf Eval. Metric} & & \multicolumn{3}{c|}{\bf \belebele} & \multicolumn{3}{c|}{\bf \sib} & \multicolumn{3}{c|}{\bf \flores} & \multicolumn{2}{c|}{\bf \textsc{XorQA-In}} & \multicolumn{2}{c|}{\bf \textsc{XSum-In}} & \multirow{4}{*}[2ex]{\bf Avg.} \\
    & & \multicolumn{3}{c|}{\bf \texttt{Accuracy}} & \multicolumn{3}{c|}{\bf \texttt{Accuracy}} & \multicolumn{3}{c|}{\bf \texttt{ChrF++}} & \multicolumn{2}{c|}{\bf \texttt{Token-F1}} & \multicolumn{2}{c|}{\bf \texttt{ChrF}} \\
    \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15}
    \bf Model & & \sea & \afr & \ind & \sea & \afr & \ind & \sea & \afr & \ind & \ind & \textsc{En} & \ind & \textsc{En} \\
    \midrule
    \gemmatwo-\texttt{2B-FLAN} & \bf 73.0 & 52.0 & 36.0 & 50.2 & 65.9 & 47.3 & 67.8 & 27.8 & 9.6 & 20.7 & 7.6 & 47.8 & 3.7 & \bf 36.6 & 31.7 \\ 
    \gemmatwo-\texttt{2B-FA} & \underline{69.8} & \underline{62.0} & \underline{40.2} & \underline{54.4} & \underline{72.9} & \bf 57.9 & \underline{70.5} & \textcolor{red!80}{27.0} & \underline{11.0} & \textcolor{red!80}{17.2} & \underline{8.6} & \underline{54.1} & \underline{6.6} & \underline{\textcolor{red!80}{34.1}} & \underline{35.3} \\
    {\pfix}  \texttt{LoRA-Adapt} & 69.7 & \bf 63.3 & \bf 43.7 & \bf 55.8 & \bf 73.5 & \underline{47.8} & \bf 70.7 & \bf 37.1 & \bf 18.3 & \bf 32.5 & \bf 9.8 & \bf 60.5 & \bf 9.9 & \textcolor{red!80}{31.9} & \bf 38.7 \\
    % SEA+AFR+IND tokenizer
    % \gemmatwo-\texttt{2B-FA-SAI} & 68.3 & 58.4 & 38.8 & 53.1 & 70.6 & 54.3 & 71.8 & 24.8 & 9.3 & 17.7 & 9.7 & 52.9 & 7.3 & 34.6 & 34.5 \\
    % {\pfix}  \texttt{LoRA-Adapt} & 69.6 & 59.6 & 41.8 & 54.4 & 69.5 & 54.4 & 72.4 & 33.7 & 15.9 & 31.9 & 9.7 & 54.3 & 10.2 & 32.2 & 37.4 \\
    \midrule
    \gemmatwo-\texttt{9B-FLAN} & \bf 83.5 & 70.6 & 49.9 & 68.9 & 74.4 & 61.0 & 79.1 & 32.0 & 12.6 & 27.8 & 9.8 & 60.3 & 15.1 & \bf 37.5 & 42.0 \\ 
    \gemmatwo-\texttt{9B-FA} & 82.3 & \underline{78.1} & \underline{57.5} & \underline{71.2} & \bf 78.6 & \underline{68.8} & \underline{79.8} & \underline{35.9} & \underline{16.5} & \underline{31.4} & \bf 12.5 & \bf 65.2 & \underline{15.2} & \underline{\textcolor{red!80}{37.1}} & \underline{45.5} \\
    {\pfix}  \texttt{LoRA-Adapt} & \underline{82.5} & \bf 78.4 & \bf 60.2 & \bf 73.1 & \underline{78.5} & \bf 69.2 & \bf 80.1 & \bf 40.0 & \bf 25.6 & \bf 40.5 & \underline{12.1} & \underline{64.1} & \bf 17.4 & \textcolor{red!80}{37.0} & \bf 47.3 \\
    % SEA+AFR+IND tokenizer
    % \gemmatwo-\texttt{9B-FA-SAI} & 82.5 & 75.8 & 55.7 & 71.4 & 79.1 & 67.8 & 80.7 & 34.3 & 15.1 & 32.4 & 12.0 & 65.6 & 14.9 & 37.7 & 45.1 \\
    % {\pfix}  \texttt{LoRA-Adapt} & 82.0 & 76.1 & 57.5 & 72.7 & 78.3 & 67.7 & 81.3 & 38.6 & 22.5 & 38.8 & 13.3 & 66.2 & 16.9 & 37.7 & 47.0 \\
    \midrule
    \gemmatwo-\texttt{27B-FLAN} & \bf 84.3 & 71.9 & 52.6 & 72.9 & 72.6 & 60.2 & 76.4 & 33.2 & \underline{15.2} & \underline{29.6} & \bf 20.4 & \underline{61.7} & \underline{15.0} & \bf 37.6 & \underline{43.7} \\ 
    \gemmatwo-\texttt{27B-FA} & \underline{83.6} & \underline{78.8} & \underline{56.3} & \underline{73.1} & \underline{78.1} & \underline{66.1} & \underline{78.5} & \underline{33.7} & \textcolor{red!80}{13.2} & \textcolor{red!80}{23.9} & \textcolor{red!80}{15.2} & \textcolor{red!80}{59.3} & \textcolor{red!80}{12.4} & \textcolor{red!80}{36.8} & \textcolor{red!80}{43.5} \\
    {\pfix}  \texttt{LoRA-Adapt} & 83.4 & \bf 79.5 & \bf 60.4 & \bf 74.3 & \bf 79.5 & \bf 68.3 & \bf 80.0 & \bf 42.5 & \bf 25.7 & \bf 40.6 & \underline{\textcolor{red!80}{16.4}} & \bf 70.1 & \bf 18.0 & \underline{\textcolor{red!80}{36.9}} & \bf 49.1 \\
    % SEA+AFR+IND tokenizer
    % \gemmatwo-\texttt{27B-FA-SAI} & 84.0 & 77.1 & 56.3 & 74.8 & 76.8 & 63.5 & 78.8 & 35.3 & 16.6 & 29.8 & 19.1 & 54.8 & 16.0 & 38.3 & 44.7 \\
    % {\pfix}  \texttt{LoRA-Adapt} & 83.8 & 77.6 & 59.2 & 74.7 & 77.1 & 67.0 & 80.3 & 39.6 & 22.5 & 39.8 & 21.5 & 67.6 & 17.9 & 37.2 & 48.5 \\
    \bottomrule
    \multicolumn{13}{c}{}\vspace{-9mm}
    \end{tabular}
    }
\label{tab:franken_adapter_results}
\end{table*}

% Serial Full parameter tuning (first in NTL then in FLAN+NTL (50%/50%)
% SEA:
% Gemma2-2B
% FLORES: 44.2 SIB-200: 66.9 BELEBELE: 55.4
% English
% MMLU: 43.1
% GSM: 30.0
% drop: 50.0
% winogrand: 67.1
% arc: 60.0
% hellaswag: 78.7
% piqa: 74.8
% boolq: 78.1
% avg: 59.7

% Gemma2-9B
% FLORES: 45.2 SIB-200: 83.0 BELEBELE: 76.3 GSM8K-NTL-SEA: 29.8 Avg: 68.2
% English
% MMLU: 59.1
% GSM: 40.4
% drop: 67.3
% winogrand: 78.0
% arc: 83.7
% hellaswag: 88.7
% piqa: 83.1
% boolq: 84.0
% avg: 73.0

% Gemma2-27B
% FLORES: 45.4 SIB-200: 72.6 BELEBELE: 81.2 GSM8K-NTL-SEA: 41.0 En Avg: 81.8
% English:
% drop: 75.6
% GSM: 56.0
% boolq: 90.6
% piqa: 88.6
% hellaswag: 94.6
% arc: 89.5
% winogrand: 89.5
% mmlu: 69.8