%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

\input{def}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
% \usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\ouradapter: Cross-Lingual Adaptation of LLMs by Embedding Surgery}

\begin{document}

\twocolumn[
\icmltitle{\ouradapter: Cross-Lingual Adaptation of LLMs by Embedding Surgery}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Fan Jiang}{equal,unimelb}
\icmlauthor{Honglin Yu}{gra}
\icmlauthor{Grace Chung}{gra}
\icmlauthor{Trevor Cohn}{gra}
\end{icmlauthorlist}

\icmlaffiliation{unimelb}{The University of Melbourne}
\icmlaffiliation{gra}{Google}

\icmlcorrespondingauthor{Fan Jiang}{fan.jiang1@student.unimelb.edu.au}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
The capabilities of Large Language Models (LLMs) in low-resource languages lag far behind those in English, making their universal accessibility a significant challenge.
% While prior work has shown feasibility of efficient language adaptation by learning dedicated embeddings, such methods have largely been limited to small-scale, encoder-only models designed for language understanding tasks.
To alleviate this, we present \emph{\ouradapter}, a modular language adaptation approach for decoder-only LLMs with embedding surgery. Our method begins by creating customized vocabularies for target languages and performing language adaptation through embedding tuning on multilingual data. These pre-trained embeddings are subsequently integrated with LLMs that have been instruction-tuned on English alignment data to enable zero-shot cross-lingual transfer. 
% Our extensive experiments with \gemmatwo shows significant improvements across 96 languages, spanning both discriminative and generative tasks, with minimal regressions in English proficiency. 
Our experiments on \gemmatwo models with up to 27B parameters demonstrate improvements of up to 20\% across 96 languages, spanning both discriminative and generative tasks, with minimal regressions ($<$1\%) in English. Further in-depth analysis reveals the critical role of customizing tokenizers in enhancing language adaptation, while boosting inference efficiency.
% (up to 28\% faster in the prefilling stage for underrepresented languages). 
Additionally, we show the versatility of our method by achieving a 14\% improvement over a math-optimized LLM across 20 languages, offering a modular solution to transfer reasoning abilities across languages post hoc.
% \wishlist{
% 1. emphasize model size up to 27B.
% 2. improve inference latency up to xxx.
% 3. critical role of customized tokenizer.
% 4. math cross-lingual transfer. general approach to transfer reasoning capabilities across languages in post hoc manners.
% }
\end{abstract}

\input{sections/sec1_intro}
\input{sections/sec2_methodology}
\input{sections/sec3_experiments}
\input{sections/sec4_related_work}

\section{Conclusion}
In this work, we introduce \ouradapter, a novel approach that facilitates efficient zero-shot cross-lingual transfer across a wide range of languages with embedding surgery. Our empirical findings reveal that embedding tuning on multilingual data suffices for the effective language adaptation of LLMs. Moreover, this adaptation can be further optimized through the implementation of customized vocabularies tailored to target languages. We also highlight the adaptability of these embeddings, which can be integrated into any instruction-tuned LLMs to enable cross-lingual transfer with minimal training costs.

\section*{Impact Statement}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.
This work presents novel insights into the language adaptation and cross-lingual transfer of LLMs, which have the potential to change the ways in which LLMs are trained and adapted to new languages. We believe \ouradapter can serve as a versatile modular framework to expand the capabilities of LLMs beyond language, such as domain adaptations for enhanced coding and mathematical skills. There might also be safety concerns arising from the disruption of alignment abilities due to the changes to the embeddings, and how to align the model effectively and efficiently to reject unsafe queries after the \ouradapter pipeline which would need to be carefully addressed.

\section*{Acknowledgment}
The authors would thank Isaac Caswell for the valuable comments and feedback. The authors would also thank Daniel Formoso for helping support the research.

\bibliography{anthology,custom}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix
\section*{Overview of Appendix}
Our supplementary includes the following sections:
\begin{itemize}
    \item Section~\ref{appendix:exp_settings}: Experimental Settings, including implementation details for training and evaluation.
    \item Section~\ref{appendix:english_tasks}: Detailed result comparison between instruction-tuned models and \ouradapter in English tasks.
    \item Section~\ref{appendix:extra_main_results}: Additional \la and \ouradapter results on \palmtwo, \aya and \gemmatwo-\texttt{IT} models.
    \item Section~\ref{appendix:cpt}: Results of the continued pre-training baseline for language adaptation.
    \item Section~\ref{appendix:supplementary_analysis}: Supplementary analysis including additional data ablations.
    \item Section~\ref{appendix:detailed_lang_results}: Results by language for both language adaptation and \ouradapter.
\end{itemize}

\input{sections/appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
