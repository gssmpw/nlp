\typeout{A Survey of Prompt Engineering}

% These are the instructions for authors for IJCAI-24.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai24.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'


% \usepackage{forest}

\usepackage{ijcai24}
\usepackage{enumitem}
% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{algorithm}
% \usepackage{graphicx}
\usepackage{multirow}
\usepackage{algorithmic}

\urlstyle{same}
\usepackage{xcolor}
\definecolor{mygreen}{RGB}{0,150,0}
\definecolor{myred}{RGB}{255,0,0}
% \usepackage[symbol]{footmisc}
\usepackage[switch]{lineno}

%-----Vinija
\usepackage[draft,textsize=footnotesize,textwidth=15mm]{todonotes}
\newcommand\vj[1]{\todo[author=VJ,color=magenta!40]{#1}}
\newcommand\vjil[1]{\todo[author=VJ,color=magenta!40,inline]{#1}}
\newcommand\vjb[1]{\textcolor{magenta}{#1}}
\newcommand{\chirag}[1]{{\color{blue}[[Chirag: #1]]}}
\newcommand{\eg}{{\textit{e.g., }\xspace}}
\newcommand{\ie}{{\textit{i.e., }\xspace}}

%%%%%%%%%Color%%%%%%%%%%%%%%%%

\definecolor{paired-light-blue}{RGB}{198, 219, 239}
\definecolor{paired-dark-blue}{RGB}{49, 130, 188}
\definecolor{paired-light-orange}{RGB}{251, 208, 162}
\definecolor{paired-dark-orange}{RGB}{230, 85, 12}
\definecolor{paired-light-green}{RGB}{199, 233, 193}
\definecolor{paired-dark-green}{RGB}{49, 163, 83}
\definecolor{paired-light-purple}{RGB}{218, 218, 235}
\definecolor{paired-dark-purple}{RGB}{117, 107, 176}
\definecolor{paired-light-gray}{RGB}{217, 217, 217}
\definecolor{paired-dark-gray}{RGB}{99, 99, 99}
\definecolor{paired-light-pink}{RGB}{222, 158, 214}
\definecolor{paired-dark-pink}{RGB}{123, 65, 115}
\definecolor{paired-light-red}{RGB}{231, 150, 156}
\definecolor{paired-dark-red}{RGB}{131, 60, 56}
\definecolor{paired-light-yellow}{RGB}{231, 204, 149}
\definecolor{paired-dark-yellow}{RGB}{141, 109, 49}

\definecolor{bg1}{HTML}{FF9966}
\definecolor{bg2}{HTML}{CCE5FF}
\definecolor{bg3}{HTML}{FFCC99}
\definecolor{bg4}{HTML}{FFC107}
\definecolor{bg5}{HTML}{FFCCCC}
\definecolor{bg6}{HTML}{D5E8D4}
\definecolor{bg7}{HTML}{eeeeee}
\definecolor{bg8}{HTML}{cdeb8b}
\definecolor{bg9}{HTML}{dae8fc}
\definecolor{bg10}{HTML}{a2e6eb}

\definecolor{bg31}{HTML}{FFCDD2} % light pink

\definecolor{bg32}{HTML}{F8BBD0}

\definecolor{bg33}{HTML}{E1BEE7} % lavender

\definecolor{bg34}{HTML}{D7CCC8} % light tan

\definecolor{bg35}{HTML}{B2DFDB} % light teal

\definecolor{bg36}{HTML}{A5D6A7} % light green

\definecolor{bg37}{HTML}{FFF9C4} %light yellow

\definecolor{bg38}{HTML}{FFECB3} % peach

\definecolor{bg111}{HTML}{CB6843}

\definecolor{bg112}{HTML}{D77C5C}

\definecolor{bg113}{HTML}{E28E6E}
\definecolor{bg114}{HTML}{E89F7D}
\definecolor{bg115}{HTML}{EDAE8A}
\definecolor{bg116}{HTML}{F0BA95}
\definecolor{bg117}{HTML}{F3C29F}
\definecolor{bg118}{HTML}{F6CCAA}
\definecolor{bg119}{HTML}{F8D5B3}
\definecolor{bg120}{HTML}{FADCBD}
\definecolor{bg121}{HTML}{FCE6C7}

\definecolor{bg39}{HTML}{FFE0B2} % apricot

\definecolor{bg40}{HTML}{3CB371} % blush pink

\definecolor{bg43}{HTML}{ffe5d9}

\definecolor{bg15}{HTML}{7FFFD4}

\definecolor{bg17}{HTML}{F0FFFF}

\definecolor{bg18}{HTML}{F5FFFA}

\definecolor{bg19}{HTML}{F8F8FF}

\definecolor{bg20}{HTML}{FFFFFF}

\definecolor{bg21}{HTML}{E1F5FE}

\definecolor{bg22}{HTML}{B3E5FC}

\definecolor{bg23}{HTML}{81D4FA}

\definecolor{bg24}{HTML}{4FC3F7}

\definecolor{bg25}{HTML}{29B6F6}

\definecolor{bg26}{HTML}{03A9F4}

\definecolor{bg27}{HTML}{039BE5}

\definecolor{bg28}{HTML}{0288D1}

\definecolor{bg29}{HTML}{0277BD}

\definecolor{bg30}{HTML}{01579B}

\definecolor{bg16}{HTML}{FFCC99} 


\definecolor{pg51}{HTML}{E8F5E9} % pale green
\definecolor{pg52}{HTML}{C8E6C9} % honeydew green
\definecolor{pg53}{HTML}{B9F6CA} % light mint green
\definecolor{pg54}{HTML}{A9DFBF} % pale sage green
\definecolor{pg55}{HTML}{BCF5A6} % lemon green

\definecolor{pg56}{HTML}{BEF1CE} % seashell green
\definecolor{pg57}{HTML}{CEF6EC} % icy green
\definecolor{pg58}{HTML}{B7F0B1} % feijoa green
\definecolor{pg59}{HTML}{B1F2B5} % pastel light green
\definecolor{pg60}{HTML}{9DF3C4} % greenish cyan

\definecolor{pg61}{HTML}{DEF7E0} % pale green
\definecolor{pg62}{HTML}{E8F8DC} % greenish beige

\definecolor{pg63}{HTML}{EBF7E7} % seafoam green
\definecolor{pg64}{HTML}{F0FDF4} % pale turquoise

\definecolor{pg65}{HTML}{F1FEE7} % mint cream
\definecolor{pg66}{HTML}{F7FFF6} % foam green
\definecolor{pg67}{HTML}{FCFFE7} % pale spring bud
\definecolor{pg68}{HTML}{F4FFD2} % light lime green
\definecolor{pg69}{HTML}{EEFFE2} % tea green
\definecolor{pg70}{HTML}{E3FDF5} % tropical green


\definecolor{connect-color}{RGB}{0,0,0}
\definecolor{middle-color}{RGB}{255,255,255}
% \definecolor{leaf-color}{RGB}{166,208,153}
\definecolor{leaf-color}{RGB}{173,216,230}
% \definecolor{line-color}{RGB}{166,208,153}
\definecolor{line-color}{RGB}{25,25,112}

%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{subcaption}
% \usepackage[round]{natbib}
\usepackage{listings}
\usepackage{tcolorbox}
\usepackage{xspace}
\usepackage{makecell}
\usepackage{soul}               % to strikethrough
\setstcolor{red}            %to strikethrough in red
\usepackage{tikz}

\usepackage{todonotes}
\newcommand\ac[1]{\todo[author=AC,color=blue!40]{#1}}
\newcommand\acil[1]{\todo[author=AC,color=blue!40,inline]{#1}}
\newcommand\acb[1]{\textcolor{blue}{#1}}
\newcommand\act[1]{\textcolor{blue}{#1}}

\usepackage{longtable}
\usepackage{tablefootnote}
% \usepackage{forest}
\usepackage[edges]{forest}
\definecolor{hidden-draw}{RGB}{20,68,106}
\definecolor{hidden-pink}{RGB}{255,245,247}
\definecolor{red}{RGB}{255,0,0}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

% temp for marco's comments
\newcommand{\asli}[1]{{\color{purple}\textbf{AC: #1}}}
\newcommand{\chris}[1]{{\color{olive}\textbf{CN: #1}}}
\newcommand{\jane}[1]{{\color{brown}\textbf{JY: #1}}}
\newcommand{\greg}[1]{{\color{violet}\textbf{GM: #1}}}
\newcommand{\egrave}[1]{{\color{orange}\textbf{EG: #1}}}

\definecolor{hidden-draw}{RGB}{0,0,0}
\definecolor{hidden-pink}{RGB}{255,182,193}

%%%%%%%%%%%%%%%%%

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% \usepackage[draft,textsize=footnotesize,textwidth=15mm]{todonotes}
% \newcommand\vr[1]{\todo[author=VR,color=orange!40]{#1}}
% \newcommand\vril[1]{\todo[author=VR,color=orange!40,inline]{#1}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.





% \documentclass{article}
\usepackage{forest}    % Required for the forest environment
\usepackage{hyperref}  % Required for hyperlinks
\usepackage{tikz}      % Required for styling nodes

% Define styles for different types of nodes
\tikzset{
    root style/.style={
        draw,
        rounded corners,
        fill=blue!30, % Color for the root node
        align=center,
        font=\bfseries
    },
    child style/.style={
        draw,
        rounded corners,
        fill=green!30, % Color for child nodes
        align=center,
        font=\bfseries
    },
    grandchild style/.style={
        draw,
        rounded corners,
        fill=red!30, % Color for grandchild nodes
        align=center,
        font=\bfseries
    }
}

\tikzset{
  my-box/.style={
    rectangle,
    draw=hidden-draw,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=40em,
    inner sep=2pt,
    align=center,
    % fill opacity=.5,
    line width=0.8pt,
  },
  leaf/.style={
    my-box,
    minimum height=1.5em,
    % fill=hidden-pink!80,
    text=black,
    align=center,
    font=\normalsize,
    inner xsep=2pt,
    inner ysep=4pt,
    line width=0.8pt,
  }
}
% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2024.0)
}

\title{The Multilingual Mind: A Survey of Multilingual Reasoning  in Language Models}


% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse

\author{
    Akash Ghosh$^1$,  
    Debayan Datta$^1$,  
    Sriparna Saha$^1$,  
    Chirag Agarwal$^2$\textsuperscript{*} \\
    \affiliations
    $^1$Department of Computer Science and Engineering, Indian Institute of Technology Patna\\
    $^2$University of Virginia\\
    \emails
    \texttt{\{akash\_2321cs19, sriparna\}@iitp.ac.in}, 
    \texttt{debayan.datta0206@gmail.com}, 
    \texttt{chiragagarwal@virginia.edu}
}

% \fi



% %%%% ijcai24.tex

% \typeout{IJCAI--24 Instructions for Authors}

% % These are the instructions for authors for IJCAI-24.

% \documentclass{article}
% \pdfpagewidth=8.5in
% \pdfpageheight=11in

% % The file ijcai24.sty is a copy from ijcai22.sty
% % The file ijcai22.sty is NOT the same as previous years'
% \usepackage{ijcai24}

% % Use the postscript times font!
% \usepackage{times}
% \usepackage{soul}
% \usepackage{url}
% \usepackage[hidelinks]{hyperref}
% \usepackage[utf8]{inputenc}
% \usepackage[small]{caption}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amsthm}
% \usepackage{xcolor}
% \usepackage{booktabs}
% \usepackage{algorithm}
% \usepackage{algorithmic}
% \usepackage[switch]{lineno}
% \usepackage{amssymb}% http://ctan.org/pkg/amssymb
% \usepackage{pifont}% http://ctan.org/pkg/pifont
% \newcommand{\cmark}{\ding{51}}%
% \newcommand{\xmark}{\ding{55}}%
\def\headline#1{\hbox to \hsize{\hbox{#1}\hrulefill}}

% % Comment out this line in the camera-ready submission
% \linenumbers

% \urlstyle{same}

% % the following package is optional:
% %\usepackage{latexsym}

% % See https://www.overleaf.com/learn/latex/theorems_and_proofs
% % for a nice explanation of how to define new theorems, but keep
% % in mind that the amsthm package is already included in this
% % template and that you must *not* alter the styling.
% \newtheorem{example}{Example}
% \newtheorem{theorem}{Theorem}

% % Following comment is from ijcai97-submit.tex:
% % The preparation of these files was supported by Schlumberger Palo Alto
% % Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% % Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% % Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% % preparation.

% % These instructions can be modified and used in other conferences as long
% % as credit to the authors and supporting agencies is retained, this notice
% % is not changed, and further modification or reuse is not restricted.
% % Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% % contacts for providing assistance without their prior permission.

% % To use for other conferences, change references to files and the
% % conference appropriate and use other authors, contacts, publishers, and
% % organizations.
% % Also change the deadline and address for returning papers and the length and
% % page charge instructions.
% % Put where the files are available in the appropriate places.


% % PDF Info Is REQUIRED.

% % Please leave this \pdfinfo block untouched both for the submission and
% % Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
% \pdfinfo{
% /TemplateVersion (IJCAI.2024.0)
% }

% \title{A Systematic Survey of Prompt Engineering}


% % Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% % Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
% \author{
% First Author$^1$
% \and
% Second Author$^2$\and
% Third Author$^{2,3}$\And
% Fourth Author$^4$
% \affiliations
% $^1$First Affiliation\\
% $^2$Second Affiliation\\
% $^3$Third Affiliation\\
% $^4$Fourth Affiliation
% \emails
% \{first, second\}@example.com,
% third@other.example.com,
% fourth@example.com
% }
% \fi

\begin{document}

\maketitle

\begin{abstract}
\looseness=-1 While reasoning and multilingual capabilities in Language Models (LMs) have achieved remarkable progress in recent years, their integration into a unified paradigm—multilingual reasoning—is at a nascent stage. Multilingual reasoning requires language models to handle logical reasoning across languages while addressing misalignment, biases, and challenges in low-resource settings. This survey provides the first in-depth review of multilingual reasoning in LMs. In this survey, we provide a systematic overview of existing methods that leverage LMs for multilingual reasoning, specifically outlining the challenges, motivations, and foundational aspects of applying language models to reason across diverse languages. We provide an overview of the standard data resources used for training multilingual reasoning in LMs and the evaluation benchmarks employed to assess their multilingual capabilities. Next, we analyze various state-of-the-art methods and their performance on these benchmarks. Finally, we explore future research opportunities to improve multilingual reasoning in LMs, focusing on enhancing their ability to handle diverse languages and complex reasoning tasks.

\begin{comment}
Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in prompt engineering, categorized by application area. For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized. We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique. This systematic analysis enables a better understanding of this rapidly developing field and facilitates future research by illuminating open challenges and opportunities for prompt engineering.
\end{comment}
% Prompt engineering has become a pivotal strategy in enhancing the functionality of large language models (LLMs) and vision language models (VLMs) through the incorporation of task-specific hints known as prompts. 

% By enabling models to generate predictions based solely on prompts without altering model parameters, prompt engineering facilitates the seamless integration of large pre-trained models into practical real-world applications. These prompts can take the form of manually crafted natural language instructions or automatically generated vector representations. Despite the considerable body of research on prompt engineering, a systematic overview of these techniques is currently lacking. This paper aims to fill this void by presenting a comprehensive survey of recent advancements in prompt engineering, organized by application area. Each prompt technique type is concisely summarized, covering prompting methods, applications, datasets, and a discussion of their respective advantages and disadvantages. The paper also incorporates a taxonomy diagram and a table summarizing the datasets and models utilized in the reviewed literature.

% Prompt engineering has emerged as a crucial technique for improving the capabilities of large language models (LLMs) and vision language models (VLMs) with task-specific hints, known as prompts. Prompt engineering empower the models to make predictions solely based on prompts without updating model parameters, facilitating the seamless integration of large pre-trained models into real-world applications. These prompts can be manually crafted as natural language instructions or automatically generated as vector representations. Despite significant research in prompt engineering, there is a dearth of a systematic overview of these techniques. This paper seeks to address this gap by providing a comprehensive survey of recent advancements in prompt engineering, categorized by application area. For each type of prompt technique, the paper delivers a succinct summary encompassing prompting methods, applications, datasets and discussing their advantages and disadvantages. Additionally, the paper includes a taxonomy diagram and a table summarizing the datasets and models employed in the surveyed literature.



% This survey elucidates foundational principles of prompt
% engineering, such as role-prompting, one-shot, and few-shot prompting, as well as
% more advanced methodologies such as the chain-of-thought and tree-of-thoughts
% prompting. 1

% Prompt engineering is a burgeoning area of research focused on designing, refining, and implementing prompts or instructions to guide the output of large language models (LLMs) and sision language models (VLMs) for various tasks.

% This review paper provides a comprehensive overview of the latest developments in prompt engineering methods and their applications. We begin with an introduction to foundation models and the use of prompts to elicit desired behaviors. We then describe different prompting strategies such as demonstrations, examples, natural language instructions, and few-shot learning. Key prompt engineering techniques covered include prefix tuning, in-context learning, chain of thought prompting, and prompt programming. For each technique, we explain the underlying methodology, highlight innovative applications across domains like question answering, search, and dialogue, and discuss strengths as well as limitations. We also compare prompting approaches and provide guidelines on selecting suitable techniques based on use case requirements and model architecture. The review synthesizes learnings from the latest research to provide both a conceptual framework and practical guidance for prompt engineering. It concludes with an analysis of current challenges and an outlook on promising future directions in this rapidly advancing field.[claude]

% Prompt engineering enhances a large pre-trained model by incorporating task-specific cues, referred to as prompts, to tailor the model for novel tasks. These prompts can be manually crafted as natural language instructions or automatically generated, taking the form of natural language instructions or vector representations. This approach empowers the model to make predictions solely based on prompts without updating model parameters, facilitating the seamless integration of large pre-trained models into real-world applications. Despite significant research in prompt engineering in natural language processing, there is a dearth of a systematic overview specifically focused on pre-trained natural language models. This paper seeks to address this gap by providing a comprehensive survey of recent advancements in prompt engineering, categorized by application area. For each type of prompt technique, the paper delivers a succinct summary encompassing prompting methods and applications and discussing their advantages and disadvantages. Additionally, the paper includes a taxonomy diagram and a table summarizing the datasets and models employed in the surveyed literature. [cg]

% Prompt engineering is a technique that involves augmenting a large pre-trained model with task-specific hints, known as prompts, to adapt the model to new tasks. Prompts can be created manually as natural language instructions or generated automatically as either natural language instructions or vector representations. Prompt engineering enables the ability to perform predictions based solely on prompts without updating model parameters, and the easier application of large pre-trained models in real-world tasks. In past years, Prompt engineering has been intensively studied in natural language processing. 
% However, there is currently a lack of a systematic overview of prompt engineering on pre-trained natural language models. This paper aims to provide a comprehensive survey of latest research in prompt engineering based on the application area. For each type of prompt techniques, a brief summary, prompting methods, prompting-based applications, and the advantages and disadvantages are summarized and discussed. Furthermore, a taxonomy diagram and a table summarizes the dataset and employed models.

\end{abstract}
% \section{Motivation}


\begin{comment}
\begin{figure}
  \centering
     \includegraphics[width =0.50\textwidth, height=3cm]{ijcai-review-page-4.drawio.pdf}\hfill
     % \includegraphics[width=1\textwidth, height=4cm]{IJCAI-review-Page-4.drawio.pdf}
     \caption{Visual breakdown of prompt engineering components: LLMs trained on extensive data, instruction and context as pivotal elements shaping the prompt, and a user input interface.}
  % \caption{Overview of the Prompt Engineering}
  \label{fig:proposedfedbndp}
\end{figure}
\end{comment}

\section{Introduction}
\label{sec:intro}
\begin{flushright}
    \normalsize
    \textit{If we spoke a different language, we would perceive a somewhat different world.}\vspace{-0.05in}
    \headline{~~~~}
    \vspace{-0.05in}
    \textit{~~~Ludwig Wittgenstein}
\end{flushright}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{LLM_sketch.drawio.pdf}
    \caption{Overview of different aspects covered in our survey.}
    \vspace{-0.15in}
    \label{fig:half_page_image}
\end{figure}

% such as GPT-4~\citep{achiam2023gpt}, LLaMA~\citep{touvron2023llama}, and mT5~\citep{xue2020mt5}
Large Language Models (LLMs) have emerged as transformative tools in natural language processing, demonstrating state-of-the-art performance in language generation, translation, and summarization. These models, trained on vast corpora, excel in generating human-like text and understanding diverse linguistic contexts.
% This extension is vital in an increasingly interconnected world where equitable AI solutions are essential.
Despite their success in language generation, LLMs often face significant challenges in addressing \textit{underrepresented languages} and \textit{reasoning}. 

% , promoting global inclusivity,
While the development of Multilingual LLMs~\citep{qin2024multilingual,huang2024survey} extends LLM's capabilities in addressing multiple languages and catering to the needs of linguistically diverse communities, their proficiency in generation stems from training on large-scale corpora optimized for next-word prediction rather than logical inference~\citep{ramji2024inductive}. Consequently, while they produce fluent and contextually appropriate responses, they frequently struggle with complex reasoning tasks, particularly those requiring multi-step logic or nuanced understanding~\citep{patel2024multi}. These limitations become even more pronounced in multilingual settings due to key technical problems like cross-lingual misalignment, biases in training data, and the scarcity of resources for low-resource languages.

\looseness=-1 Reasoning is formally defined as the process of drawing logical conclusions based on available information, enabling individuals and systems to solve problems and make complex decisions. 
% In artificial intelligence (AI), reasoning is a capability that extends systems' ability to perform beyond simple tasks, allowing for in-depth comprehension, adaptability, and robust problem-solving.
Recent advancements have sought to bridge this gap by enhancing reasoning capabilities in LLMs using Chain-of-Thought (CoT) prompting~\citep{wei2022chain}, fine-tuning~\citep{lobo2024impact}, and hybrid modeling~\citep{yao2024hdflow}, especially in high-resource languages like English. However, reasoning in multilingual contexts remains a relatively unexplored domain. Existing efforts predominantly focus on a handful of high-resource languages, leaving low-resource and typologically distant languages underrepresented. The lack of robust benchmarks, diverse training corpora, and alignment strategies further impede progress in this vital area.

Multilingual reasoning, which combines logical inference with multilingual capabilities, is essential for creating AI systems that can operate effectively across diverse linguistic and cultural contexts~\citep{shi2022language}. Such systems hold immense potential for global applications, from multilingual education to culturally adaptive healthcare, ensuring inclusivity and fairness. The motivation for this survey arises from the urgent need to address these challenges and provide a systematic exploration of methods, resources, and future directions for multilingual reasoning in LLMs. The different aspects covered in this survey are shown in Figure~\ref{fig:half_page_image} and 
\begin{figure*}[htp]
    \centering
    \includegraphics[width=0.9\textwidth, height=10cm]{figures/tree_structure.pdf} % Replace with your PDF file name
    \vspace{-0.25in}
    \caption{\textbf{Taxonomy tree of current Multilingual Research.} The thrusts for improving multilingual reasoning mainly include representation learning, fine-tuning, prompting, and model editing. With the emergence of multilingual LLMs, while initial research focused on naive prompting, recent works propose several alignment, editing, and fine-tuning strategies to improve reasoning in multilingual LLMs.}
    \vspace{-0.1in}
    \label{fig:taxonomy_diagram}
\end{figure*}
the contributions are as follows:

\noindent\textbf{1) Comprehensive Overview:} We systematically review existing methods that leverage LLMs for multilingual reasoning, outlining challenges, motivations, and foundational aspects of applying reasoning to diverse languages.

\noindent\textbf{2) Training Corpora and Evaluation Benchmarks:} We analyze the strengths, limitations, and suitability of existing multilingual corpora and evaluation benchmarks in assessing the reasoning capabilities of LLMs for diverse linguistic tasks.
% We summarize the standard multilingual training corpora and evaluation benchmarks used to assess reasoning capabilities in LLMs, analyzing their strengths, limitations, and suitability for diverse linguistic tasks.\par

\noindent\textbf{3) Analysis of State-of-the-Art Methods:} We evaluate the performance of various state-of-the-art techniques, including CoT prompting, instruction tuning, and cross-lingual adaptations, on multilingual reasoning benchmark tasks.


\noindent\textbf{4) Future Research Directions:} We identify key challenges and provide actionable insights for advancing multilingual reasoning, focusing on adaptive alignment strategies, culturally aware benchmarks, and methods for low-resource languages.\par

\section{Multilingual Reasoning in LLMs}
\label{sec:multilingual-llms}
% \chirag{Write some introductory lines here to describe what we are talking about in this section}
% \textcolor{red}{
\looseness=-1 Recent advancements in large language models have improved reasoning in mathematics and logical reasoning. However, extending these abilities across languages introduces several challenges, including consistency, low-resource adaptation, and cultural integration. Below, we describe the preliminaries and key characteristics of multilingual reasoning, focusing on challenges and desiderata for cross-lingual inference.
% \textcolor{red}{The di}
% }





\subsection{Preliminaries}
\label{sec:prelim}

\textbf{Large Language Models (LLMs).} 
LLMs are transformer-based~\citep{vaswani2017attention} neural network architectures designed to model the probability of a sequence of tokens. Formally, LLMs are trained to predict the probability of a word (or sub-word token) given the preceding words in a sequence \( X = \{x_1, x_2, \dots, x_n\} \), formalized as:
$$
P(X) = \prod_{i=1}^n P(x_i \mid x_1, x_2, \dots, x_{i-1}),
$$
where $P(X)$ is the probability of the entire sequence and $P(x_i | x_1, x_2, \dots, x_{i-1})$ is the conditional probability of the i$^{th}$ token given the preceding tokens.

\begin{comment}
\textbf{Reasoning:} Reasoning enables AI to draw logical conclusions \( C \) from premises \( P \) using a mapping function:
\[
C = f(P).
\]
Reasoning types include:
\begin{itemize}
    \item \textbf{Deductive}: From general premises to specific conclusions.
    \item \textbf{Inductive}: Generalizing patterns from specific examples.
    \item \textbf{Abductive}: Inferring the most plausible explanation for observations.
\end{itemize}
\end{comment}

\noindent\textbf{Reasoning.} One of the key reasons behind the success of LLMs in mathematic and logical tasks is their reasoning capabilities. Formally, reasoning enables LLMs to draw logical conclusions \( C \) from premises \( P \) using a mapping function: $C = f(P)$. To this end, there are different types of reasoning strategies that an LLM can employ:
% Types of Reasoning in AI
% LLMs employ multiple reasoning strategies:
% \begin{itemize}
% \item

\noindent\textbf{a) Deductive Reasoning:} It derives specific conclusions from general premises. If a given set of premises $P_i$ are true, the conclusion $C$ must be true.
    \[
    P_1, P_2, ..., P_n \Rightarrow C,
    \]
% \item

\noindent\textbf{b) Inductive Reasoning:} Generalizes patterns from specific instances, leading to probabilistic conclusions.
    \[
    P_1, P_2, ..., P_n \Rightarrow C_{\text{probabilistic}}
    \]
% \item
\textbf{c) Abductive Reasoning:} Infers the most plausible explanation ($H_{\text{best}}$) for given observation $O$.
    \[
    O \Rightarrow H_{\text{best}}
    \]
% \item
\textbf{d) Analogical Reasoning:} Identifies relationships between domains and transfers knowledge.
    \[
    A : B \approx C : D
    \]
% \item
\textbf{e) Commonsense Reasoning:} Uses real-world knowledge for intuitive decision-making.
% \end{itemize}

% \chirag{I don't think the text in blue below is giving any motivation about multilingual reasoning in general. It is more directed towards multilingualism in LLMs.}
% \chirag{
% \noindent\textbf{Linguistic Diversity.} Multilingual reasoning ensures LLMs effectively handle diverse linguistic structures and support over 7,000 global languages for inclusivity and equity. This is specifically useful for applications in healthcare, where multilingual LLMs can interpret symptoms in one language (\eg Hindi) and provide actionable advice in another (\eg English). Further, in education, it enables explanations in native underrepresented languages like Swahili or Tamil.
% % \textit{Applications:} In healthcare, multilingual reasoning interprets symptoms in one language (e.g., Hindi) and provides actionable advice in another (e.g., English). In education, it enables explanations in native languages like Swahili or Tamil. \par
% \noindent\textbf{Low-Resource Language Challenges:} Addresses the scarcity of training data in underrepresented languages, enabling AI systems to function in low-resource scenarios. \par
% \textit{Applications:} During disaster responses, it analyzes emergency communications in languages like Haitian Creole. In legal systems, it reasons through multilingual legal documents to ensure equitable outcomes. \par
% \textbf{Cultural Contextualization:} Multilingual reasoning incorporates cultural norms and idiomatic expressions, enabling AI to adapt to regional differences. \par
% \textit{Applications:} In financial analysis, it considers regional economic factors, such as investment risks in Japan versus Brazil. During pandemics, it tailors public health advice to cultural attitudes. \par
% \textbf{Cross-Lingual Consistency:} Ensures logical coherence across languages, supporting global organizations and collaborations. \par
% \textit{Applications:} For policy enforcement, it reasons through multilingual regulations like GDPR. In scientific research, it enables reasoning across multilingual papers, fostering innovation. \par}
\subsection{Desiderata in Multilingual Reasoning}
\label{sec:desiderata} 
Here, we describe desiderata that lay the foundation for multilingual reasoning capabilities in LLMs. Multilingual reasoning refers to the capability of models to perform logical inference, problem-solving, and decision-making across multiple languages while maintaining consistency and cultural contextualization. Let \( L {=} \{l_1, l_2, \dots, l_m\} \) represent a set of \( m \) languages, and let \( P_l \) and \( C_l \) denote the premise and conclusion in a given language \( l_i \). For a multilingual reasoning model \( M \), the task can be defined as: $M(P_{l_{i}}) \to C_{l_{i}}, \quad \forall l_{i} \in L,$
where \( M \) must satisfy the following key desiderata:\vspace{0.05in}

\noindent\textbf{1. Consistency:} A model should make logically equivalent conclusions across languages for semantically equivalent premises, \ie
\[
C_{l_i} \approx C_{l_j}, \quad \text{if } P_{l_i} \equiv P_{l_j}, \quad \forall l_i, l_j \in L,
\]
where \( \equiv \) indicates semantic equivalence of premises across languages. Consistency ensures that logical conclusions remain invariant of the input language.\vspace{0.05in}

\noindent\textbf{2. Adaptability:} For languages \( l_k \in L_{\text{low-resource}} \), the model must generalize effectively using cross-lingual transfer from high-resource languages and perform robust reasoning, \ie 
\[
\forall l_k \in L_{\text{low-resource}}, \quad M(P_{l_k}) \to C_{l_k},
\]
where transfer is performed from \( l_k \in L_{\text{high-resource}} \).\vspace{0.05in}

\noindent\textbf{3. Cultural Contextualization:} Reasoning should consider cultural and contextual differences inherent to each language, \ie for a context \( c_{l_i} \) specific to language \( l_i \), the conclusion \( C_{l_i} \) should adapt accordingly:
\[
C_{l_i} = f(P_{l_i}, c_{l_i}), \quad \forall l_i \in L,
\]
where \( f \) is a mapping function that integrates linguistic reasoning with cultural nuances, critical for tasks like healthcare, policy enforcement, and education, where culturally informed reasoning is required.\vspace{0.05in}

\noindent\textbf{4. Cross-Lingual Alignment:} 
% The model must align reasoning processes across typologically~\chirag{what is typological?} diverse languages. 
% \textcolor{red}{
The model must align reasoning processes across typologically diverse languages, where typology refers to linguistic differences in syntax, morphology, and structure (\eg word order variations between English and Japanese). Given the typological variations \( T_{l_i} \) and \( T_{l_j} \) for languages \( l_i \) and \( l_j \), alignment ensures that reasoning remains consistent and coherent across languages, regardless of their structural differences, \ie\vspace{0.02in}
% } 
% Given typological variations \( T_{l_i} \) and \( T_{l_j} \) for languages \( l_i \) and \( l_j \), alignment ensures:
$$
\text{if } P_{l_i} \equiv P_{l_j}, \quad M(P_{l_i}) \approx M(P_{l_j}), \quad \forall l_i, l_j \in L.
$$

\noindent Next, we highlight existing works that propose different training corpus and benchmarks for multilingual reasoning in Sec.~\ref{sec:dataset} and then describe previously proposed techniques to improve multilingual reasoning of LLMs in Sec.~\ref{sec:method}. The different thrusts of multilingual reasoning research are shown in Figure~\ref{fig:taxonomy_diagram}.



\section{Multilingual Reasoning  Datasets}
\label{sec:dataset}
\looseness=-1 The role of multilingual datasets in reasoning tasks is crucial for developing robust and linguistically diverse LLMs. Models trained on monolingual corpora often exhibit language biases~\citep{lyu2024regional}, limiting their reasoning capabilities across non-English languages. Therefore, multilingual datasets play a vital role in ensuring equitable model performance, particularly in high-stakes domains such as science, law, and healthcare~\citep{hendrycks2020measuring}. Additionally, we need robust benchmarks to evaluate the effectiveness of various LLMs and techniques in handling critical domain-specific reasoning queries across both high-resource and low-resource languages~\citep{xu2024cruxeval, rasiah2024one}. Below, we analyze the datasets based on training datasets~(Sec.~\ref{sec:corpus}), and evaluation benchmarks~(Sec.~\ref{sec:eval-bench}), comprising domains, tasks, and language distribution in current multilingual reasoning datasets.

\subsection{Training Corpus}
\label{sec:corpus}
The best strategy to equip a language model with a specific type of reasoning is to train the model on it. However, the training objective differs based on the use case, domain, and the language in which the model needs to be adapted. For example, to perform some form of mathematical reasoning (\eg GSM8K~\citep{cobbe2021training} and OpenMathQA~\citep{amini2019mathqa} are large datasets that are used to improve mathematical reasoning) in a particular language, it needs to be trained with mathematical reasoning datasets, which will differ if we want to adapt the model for legal reasoning. 
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/language.pdf}
    \vspace{-0.2in}
    \caption{\looseness=-1\textbf{Language distribution across training corpora and benchmarks for multilingual reasoning.} The \texttt{y}-axis denotes the number of training corpora/benchmark datasets that include a given language (\texttt{x}-axis). We observe a long-tail distribution denoting that current datasets predominantly cover languages like Chinese, English, French, and German, highlighting the need for benchmarks that represent long-tail languages.}
    \label{fig:four_plots}
\end{figure*}
% \textcolor{red}{I have commented on that part and edited the remaining section.}
\begin{comment}In most of the previous multilingual reasoning works, researchers have fine-tuned the reasoning dataset based on their use case~\chirag{clarify this statement}.\end{comment} 
% For example, datasets like GSM8K~\citep{cobbe2021training} and OpenMathQA~\citep{amini2019mathqa} are large datasets that are used to enhance the mathematical reasoning of language models in multilingual settings. 
While most training corpora are predominantly based on mathematical reasoning, XCSQA~\citep{zhu2024power} and MultiNLI~\citep{williams2017broad} are used for enhancing logical and coding reasoning and sPhinX~\citep{ahuja2024sphinx} is developed to translate instruction-response pairs into 50 languages for fine-tuning. In addition, there are cases where translation datasets like OPUS~\citep{tiedemann2012opus}, FLORES-200~\citep{goyal2022flores}, and LegoMT~\citep{yuan2022legomt} are used to map the multilingual representation into the representation space of LLMs.
Further,~\citet{ponti2020xcopa} introduced XCOPA, showing that multilingual pre-training and zero-shot fine-tuning under-perform compared to translation-based transfer. We argue that, moving forward, selecting the appropriate dataset and training methodology is crucial for optimizing a model's performance in specialized reasoning tasks.

\subsection{Evaluation Benchmark}
\label{sec:eval-bench}
% \chirag{This section needs complete revision. We are essentially describing the figure in the text. We should talk about the motivation and problem of eval benchmarks in multilingual reasoning and how these proposed benchmarks solve those problems.}
\looseness=-1 Evaluation benchmarks are fundamental to advancing the field of multilingual reasoning as they provide a consistent and systematic framework to assess the performance of models across diverse reasoning tasks, such as logical inference, mathematical problem-solving, and cross-lingual understanding. Each reasoning task and domain presents unique challenges, making it crucial to have tailored benchmarks that reflect the specific requirements and complexities of those tasks. Below, we analyze the evaluation benchmarks on three key aspects namely domain~(Fig.~\ref{fig:pie-chart-1}), task~(Fig.~\ref{fig:pie-chart-2}), and languages~(Fig.~\ref{fig:four_plots}) in detail.
% We have analyzed each of these subcategories in depth in the below sections.
\begin{comment}
In Figure~\ref{fig:domain-distribution}, we present the distribution of evaluation benchmarks across various reasoning tasks, where the tasks are categorized into different reasoning types, and the portion of benchmarks dedicated to each reasoning type is indicated in percentages. The largest portion, covering 24\%, is dedicated to \textit{logical reasoning}, with datasets like XCSQA~\cite{}, STREET~\cite{}, XStoryCloze~\cite{}, and XCOPA~\cite{} contributing to this category. Following this, \textit{knowledge reasoning} accounts for 20\% of the benchmarks, with datasets such as Reasoning by Equivalence~\cite{}, XNLI~\cite{}, Reasoning by Inheritance~\cite{}, and mRewardBench~\cite{} included in this category. \textit{Mathematical reasoning} constitutes 16\% of the benchmarks, represented by datasets like MSVAMP~\cite{}, MGSM~\cite{}, and IndiMathQA~\cite{}. \textit{Common sense reasoning} and \textit{compositional reasoning} each account for 8\%, with datasets such as BBH-Hard~\cite{} and mTEMPREASON~\cite{} for compositional reasoning, and BBH-Hard~\cite{} alone for common sense reasoning. Similarly, \textit{code reasoning} also covers 8\%, with no specific datasets mentioned in this figure. Multilingual reasoning and temporal reasoning each represent 8\%, supported by datasets such as CRUXEval~\cite{} and BBH-Hard~\cite{} for multilingual reasoning and MCR~\cite{} for temporal reasoning. Finally, tabular reasoning accounts for 4\%, with datasets like MCR being the key contributors.
\end{comment}

% \begin{figure*}[htp]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \includegraphics[width=\textwidth]{piechart for domains with legend (4).pdf}
%         \caption{Plot 1}
%         \label{fig:plot1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.48\textwidth}
%         \includegraphics[width=\textwidth]{figures/pie_chart_1.pdf}
%         \caption{Plot 2}
%         \label{fig:plot2}
%     \end{subfigure}
%     \caption{A set of four horizontally aligned plots spanning both columns.}
%     \label{fig:domain-distribution}
% \end{figure*}

% \begin{figure*}[htp]
%     \centering
%     \begin{subfigure}[b]{0.35\textwidth}
%         \includegraphics[width=\textwidth]{figures/pie_chart_2.pdf}
%         \caption{Plot 1}
%         \label{fig:plot1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.35\textwidth}
%         \includegraphics[width=\textwidth]{figures/pie_chart_1.pdf}
%         \caption{Plot 2}
%         \label{fig:plot2}
%     \end{subfigure}
%     \caption{A set of four horizontally aligned plots spanning both columns.}
%     \label{fig:domain-distribution}
% \end{figure*}
% \begin{comment}
% \end{comment}
\subsubsection{Domains and Tasks Covered}
\label{sec:domain}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/pie_chart_1.pdf}
    \vspace{-0.05in}
    \caption{\textbf{Distribution of multilingual reasoning datasets.} We observe that datasets predominantly comprise logical, commonsense, and math reasoning datasets, and the community needs more benchmarks to include temporal, compositional, and tabular reasoning.}
    \label{fig:pie-chart-1}
\end{figure}
% \chirag{Same as the above section. Don't describe the figure in the text -- we will do that in the figure captions.}
% \textcolor{red}{
Multilingual reasoning in language models spans multiple domains, each with its own set of complexities and requirements. Understanding these differences is essential for developing language models that can effectively adapt to various applications. For instance,~\citet{cobbe2021training} highlighted that mathematical reasoning requires structured multi-step logic and datasets like GSM8K. While~\citet{ponti2020xcopa} showed that causal reasoning in XCOPA relies on cross-lingual consistency and commonsense inference,~\citet{tiedemann2012opus} noted that multilingual reasoning introduces typological challenges. These studies emphasize the need for tailored approaches to address the specific demands of each task and domain. Hence, it is crucial to \textbf{build reliable and robust benchmarks for all domains and tasks} to develop more robust techniques tailored to handle the complexity of a particular domain and task. Figures~\ref{fig:pie-chart-1}-\ref{fig:pie-chart-2} show the distribution of datasets across various domains and tasks, highlighting the need to develop more comprehensive benchmarks across multiple domains. Currently, tasks such as math, legal, and commonsense reasoning dominate multilingual evaluation benchmarks, collectively accounting for \textbf{54\%} of the total (see Fig.~\ref{fig:pie-chart-2}). In contrast, domains like science, visual reasoning, tabular reasoning, temporal reasoning, and ethics are underrepresented, covering only \textbf{35\%}. Furthermore, while commonsense reasoning makes a significant contribution to existing works, other reasoning tasks lag. Notably, \textbf{crucial domains such as finance and healthcare still lack dedicated evaluation benchmarks} for multilingual reasoning, highlighting a significant gap in the field. 
% \textcolor{red}{The distribution of domains and tasks covered in different datasets are shown in Figure-\ref{fig:pie-chart-1} and Figure-\ref{fig:pie-chart-2} respectively.}
\begin{comment}
with the percentage of coverage for each domain and examples of corresponding datasets. The maths and legal domain and the logical domain each cover the largest portion at 18.9\% , 18.9\% and . Maths is supported by datasets such as MSVAMP, MGSM, mCOT, XCOT, and MathInstruct, while legal reasoning includes datasets like court view generation, criticality prediction, judgment prediction, etc.
The command sense domain accounts for 16.2\% and logical which covers 13.5\% consists of datasets like MULTINLI, mREWARDBENCH, Reasoning by Equivalence, and XNLI. The visual domain contributes 8.1\%, represented by datasets like NLVR2 and MARVL. Smaller portions include temporal (5.4\%), with BBH-Hard and mTEMPREASON; tabular reasoning (5.4\%) with Tabular (BBH-Hard), compositional represented by MCR; and ethics (2.7\%) supported by the Moral Judgment Dataset. 
\end{comment}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/pie_chart_2.pdf}
    \vspace{-0.05in}
    \caption{\looseness=-1\textbf{Distribution of domains in multilingual reasoning datasets.} While legal, commonsense, and math domain dataset cover up to 54\% of current multilingual reasoning research, other under-explored domains include ethics, science, visual, and compositional.}
    \label{fig:pie-chart-2}
\end{figure}
\subsubsection{Languages Covered}
\label{sec:language}
% \chirag{Every subsection in Sec 3 should start with why those are important. For instance, we should start this subsection with why language coverage is important. Then talk about where existing works lie and end with what are the remaining gaps.}
% \textcolor{red}{
% Figure 3 and  4 represent the distribution of the datasets based on human and coding languages, respectively.
\begin{figure*}
    \centering
    % First row of plots
\includegraphics[width=0.96\textwidth]{figures/prompts.pdf}
    \vspace{-0.1in}
    \caption{\textbf{Taxonomy of Multilingual Reasoning Methods.} A categorization of approaches for enhancing multilingual reasoning in language models, covering (A) Representation Alignment, (B) Finetuning, (C) Prompting, and (D) Model Editing.}
    \vspace{-0.1in}
    \label{fig:method}
\end{figure*}
Comprehensive language coverage is vital for multilingual reasoning, ensuring inclusivity and balanced performance across high-resource and low-resource linguistic communities. Based on languages, current benchmarks can be primarily classified into human languages and coding languages. Benchmarks like XNLI~\citep{conneau2018xnli}, mCSQA~\citep{sakai2024mcsqa}, and MARC~\citep{keung2020multilingual} predominantly focus on high-resource languages like English, Chinese, French, and Spanish. While some efforts include low-resource languages like Swahili (XCOPA~\citep{ponti2020xcopa}), Haitian (M4U~\citep{wang2024m4u}), and Nepali (mMMLU~\citep{hendrycks2020measuring}), \textbf{their representation remains minimal} and research in these languages remain at a nascent stage. Typologically distant and underrepresented languages, such as Kannada, Gujarati (xSTREET~\citep{li2024eliciting}), and Quechua, are rarely included, \textbf{further widening linguistic inequities.} Datasets like FLORES-200 attempt to balance high- and low-resource languages but fail to achieve comprehensive coverage. We note that to ensure LLMs perform effectively across diverse linguistic and cultural contexts, it is critical to include a broader range of low-resource and endangered languages~\citep{goyal2022flores, amini2019mathqa}. The complete distribution of human languages across benchmarks is shown in Figure~\ref{fig:four_plots}. Finally, only three benchmarks (BBH-Hard, CRUXEval, and TCC) incorporate coding languages across multiple languages. 
% BBH-Hard covers C++, Go, Java, JavaScript, Python, CRUXEval includes C++, Go, Java, JavaScript, Julia, Lua, Perl, PHP, Python, R, and TCC spans C\#, C++, Rust, Scala, Shell, Swift, TypeScript.
% }
\begin{comment}
For human languages, different datasets cover a wide range of languages, including high-resource languages such as Arabic (ar), Hebrew (he), Chinese (zh), Japanese (ja), Korean (ko), French (fr), German (de), and Spanish (es), which appear in benchmarks like XNLI, mCSQA, XCOPA, and MARC. Apart from English (en), commonly included languages across multiple datasets are Chinese (zh), French (fr), German (de), and Spanish (es). Some datasets also include very low-resource languages, such as Nepali (ne) and Quechua (qu) in mMMLU, Swahili (sw) in XCOPA, Kannada (kn) and Gujarati (gu) in xSTREET, and Haitian Creole (ht) in M4U, ensuring a balance between high-resource and low-resource languages for comprehensive evaluation. Different coding datasets cover a variety of programming languages, including widely used ones such as Python, Java, JavaScript, C++, and PHP, featured in benchmarks like CRUXEval and BBH-Hard. Other languages include C\#, R, Rust, TypeScript, Swift, Scala, Ruby, Lua, Shell, Perl, Go, Julia, Racket, and D, covering multiple programming paradigms. These datasets also include translated code comments, supporting multilingual understanding and enhancing model evaluation across diverse programming languages and documentation.
\end{comment}

\section{Methods}
\label{sec:method}
% \chirag{Will take a pass of the method section tomorrow}
Multilingual reasoning within language models has garnered significant attention in recent years, leading to the development of diverse techniques for enhancing their capabilities across diverse languages. Prior works have explored various directions to improve multilingual reasoning. Building upon this body of work (see Figure~\ref{fig:method}), we identify four primary thrusts, \textit{viz.} representation alignment, fine-tuning, prompting, and model editing, that collectively contribute to advancing multilingual reasoning in language models.\par

\noindent\textbf{a) Representation Alignment.} 
% \chirag{Again, this reads as if we are picking individual papers independently and writing a TL;DR for each one of them.}
% \chirag{We shouldn't directly start describing previous papers. The first 1-2 lines should be about what is broadly representation alignment and how it can solve the multilingual reasoning problem} 
% \textcolor{red}{
Multilingual reasoning requires consistent representations across languages, but LLMs often struggle due to imbalanced training data. Representation alignment ensures that equivalent concepts share similar embeddings, reducing inconsistencies in cross-lingual inference, essential for reasoning and multilingual generalization.
% }
\citet{li2024improving} employs Multilingual Contrastive Learning to align multilingual sentence representations by treating translation pairs as positive samples and pulling their embeddings closer, bridging representation gaps between languages, enhancing cross-lingual reasoning and generation capabilities. \textbf{Multilingual Alignment Learning} is another form of representation alignment that ensures semantic consistency across languages by aligning their representations for improved multilingual performance, where prior works leverage external multilingual models to enhance reasoning by aligning multilingual capabilities with LLMs~\citep{huang2024mindmerger}, bridge multilingual encoders with LLMs using minimal parameters to achieve effective alignment without supervision~\citep{yoon2024langbridge}, and evaluate the semantic similarity between English and other languages within LLM embeddings to measure alignment quality~\citep{kargaran2024mexa}. Furthermore, an exciting new direction in representation alignment is \textbf{Multilingual Compositional Learning} which constructs compositional representations by combining equivalent token embeddings across multiple languages~\citep{arora2024towards}.
% combines representations across languages to create unified embeddings, where  showed improved reasoning consistency by constructing compositional representations that integrate equivalent token embeddings across multiple languages.

\noindent\textbf{b) Finetuning.} It 
% focuses on aligning a model's representations across multiple languages to ensure consistent performance and understanding in multilingual scenarios. This paradigm
leverages cross-lingual data and tasks to fine-tune models for enhanced reasoning and comprehension, leading to numerous innovative approaches. One such approach, exemplified by LinguaLIFT~\citep{zhang2024lingualift}, uses code-switched fine-tuning along with language alignment layers to effectively bridge the gap between English and languages with fewer resources, helping maintain the nuance and context across linguistic boundaries. Similarly, QuestionAlign~\citep{zhu2024power} takes a step further by aligning questions and responses in multiple languages, thereby enhancing cross-lingual understanding and consistency in reasoning. While these methods have leaned towards extensive fine-tuning, SLAM~\citep{fan2025slam} introduces a more parameter-efficient strategy, which selectively tunes only layers critical for multilingual comprehension, \textbf{significantly lowering the computational demands} while still maintaining or even enhancing the model's reasoning capabilities. Translation has also been harnessed as a powerful tool for knowledge transfer in multilingual settings. TransLLM~\citep{geng2024not}, for instance, focuses on translation-aware fine-tuning to align different languages, thereby improving the model's ability to perform reasoning across languages. This method not only enhances language understanding but also adapts the model for various cross-lingual tasks. For those aiming at more complex reasoning tasks, \textbf{reasoning-focused fine-tuning} has proven beneficial. The Multilingual Chain-of-Thought (mCoT) Instruction Tuning method~\citep{lai2024mcot} utilizes a dataset specifically curated for reasoning across languages (mCoT-MATH) and combines CoT reasoning with instruction tuning to boost consistency and logical problem-solving in multiple languages. In addition, preference-based techniques to align reasoning outputs across languages emphasizes the use of language imbalance as a reward signal in models like DPO (Direct Preference Optimization) and PPO (Proximal Policy Optimization)~\citep{she2024mapo}. Finally, an interesting direction moving forward is the use of curriculum-based and retriever-based fine-tuning techniques to enhance multilingual reasoning~\citep{anand2024multilingual,bajpai2024multilingual}.
% For example, LinguaLIFT~\citep{zhang2024lingualift} employs code-switched fine-tuning and language alignment layers to bridge the gap between English and low-resource languages. Similarly, QuestionAlign~\citep{zhu2024power} enhances multilingual reasoning by aligning questions and responses across languages to improve cross-lingual understanding and reasoning consistency. While the aforementioned approaches predominantly focus on naive fine-tuning, SLAM~\citep{fan2025slam} proposes a parameter-efficient approach that selectively tunes layers responsible for multilingual comprehension, significantly reducing computational costs while preserving reasoning capabilities. Further, several works have proposed translation-aware fine-tuning that leverages translation as a key mechanism to transfer knowledge and improve multilingual reasoning capabilities. For instance, methods like TransLLM~\citep{geng2024not} use translation-focused fine-tuning to align languages, enhance reasoning, and adapt models for cross-lingual tasks effectively. Finally, reasoning-focused fine-tuning methods have been shown to enhance a model's ability to perform complex reasoning tasks by fine-tuning on datasets specifically designed for reasoning across domains or languages. The Multilingual Chain-of-Thought (mCoT) Instruction Tuning method~\citep{lai2024mcot} leverages CoT reasoning and instruction tuning on a multilingual dataset (mCoT-MATH) to improve reasoning consistency and logical problem-solving across multiple languages, where multilingual preference optimization (MAPO)~\citep{she2024mapo} uses preference-based methods to align multilingual outputs, language imbalance driven rewarding utilizes language imbalance as a reward signal and applies DPO to improve alignment and MAPO optimizes reasoning preferences across languages using alignment-based optimization with DPO and PPO. A few recent works, such as \citep{anand2024multilingual,bajpai2024multilingual}, demonstrate that curriculum-based fine-tuning and retriever-based fine-tuning can enhance multilingual reasoning for downstream tasks.

\par
\begin{comment}
\begin{figure*}[htp]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth,height=3.5cm]{plot_mgsm1 (1).pdf}
        \caption{Plot 1}
        \label{fig:plot1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth , height=3.5cm]{plot_msvamp1 (1).pdf}
        \caption{Plot 2}
        \label{fig:plot2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth , height=3.5cm]{plot_xcsqa1.pdf}
        \caption{Plot 3}
        \label{fig:plot3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth , height=3.5cm]{plot_xnli1.pdf}
        \caption{Plot 4}
        \label{fig:plot4}
    \end{subfigure}
    \caption{A set of four horizontally aligned plots.}
    \label{fig:four_plots}
\end{figure*}
\end{comment}

\looseness=-1\noindent\textbf{c) Prompting.} 
% \chirag{We shouldn't directly start describing previous papers. The first 1-2 lines should be about what is broadly representation alignment and how it can solve the multilingual reasoning problem} 
% \textcolor{red}{
Prompting has emerged as a key technique for enhancing how LLMs adapt and reason across different languages. By guiding the model through specific strategies, prompting not only facilitates dynamic adaptation but also addresses the challenges posed by data imbalances, thereby enhancing cross-lingual consistency, logical alignment, and the robustness of reasoning.
% Prompting enables dynamic adaptation in multilingual reasoning by guiding LLMs to structure inference effectively across languages. It helps mitigate data imbalances through strategies like direct input, translation-based prompting, and reasoning chains, improving cross-lingual consistency, logical alignment, and reasoning robustness.
% } 
One effective method is Direct Multilingual Input Prompting, as explored in the study by~\citet{sakai2024mcsqa}. Here, the model directly processes inputs in various native languages without any need for translation, preserving the original linguistic nuances. This approach was notably applied in the paper ``Do Moral Judgements'' by~\citet{khandelwal2024moral}, where moral scenarios were presented in their native languages to assess the model's reasoning capabilities directly.
% Direct Multilingual Input Prompting~\citep{sakai2024mcsqa} uses native multilingual inputs without translation. The paper Do Moral Judgements~\citep{khandelwal2024moral} applies this by presenting moral dilemmas directly in multiple languages to evaluate reasoning. 
Another strategy, Translation-Based Prompting~\citep{liu2024translation} uses translation to convert multilingual inputs into a pivot language for processing, where tasks are translated into English for reasoning and translated back to the target language for evaluation~\citep{wang2024m4u,zhao2024large}, where
% English CoT in multilingual reasoning refers to a prompting strategy where reasoning steps are performed in English regardless of the input language~\citet{wang2024m4u} and \citet{zhao2024large} are few works in this direction.
one can also generate diverse CoT with Negative Rationales (d-CoT-nR) by incorporating both correct and incorrect reasoning paths to refine multilingual reasoning capabilities~\citep{payoungkhamdee2024empirical}.
% ~\citet{payoungkhamdee2024empirical} uses Advanced English CoT by generating diverse reasoning paths in English and leveraging them for multilingual reasoning enhancement. It uses diverse CoT with Negative Rationales (d-CoT-nR) by incorporating both correct and incorrect reasoning paths to refine multilingual reasoning capabilities.

\looseness=-1\noindent\textbf{d) Model Editing.} Model (or knowledge) editing is a growing and exciting research area that aims to modify/update the information stored in a model. Formally, model editing strategies update pre-trained models for specific input-output pairs without retraining them and impacting the baseline model performance on other inputs. Multilingual Precision Editing involves making highly specific updates to model knowledge while ensuring minimal impact on unrelated information. Multilingual knowledge Editing with neuron-Masked Low-Rank Adaptation (MEMLA)~\citep{xie2024memla} enhances multilingual reasoning by leveraging neuron-masked LoRA-based edits to \textbf{integrate knowledge across languages and improve multi-hop reasoning} capabilities. Further, Multilingual Translation Post-editing refines translations by correcting errors in multilingual outputs for better alignment and fluency, where we can enhance multilingual reasoning by incorporating auxiliary translations into the post-editing process, enabling LLMs to improve semantic alignment and translation quality across languages~\citep{lim2024mufu}. 
% For instance, the MUltilingual FUsed learning (Mufu)~\citep{lim2024mufu} approach enhances multilingual reasoning by incorporating auxiliary translations into the post-editing process, enabling LLMs to improve semantic alignment and translation quality across languages.\par
% \chirag{Some lines to introduce the section}
% \chirag{Write the metric equations wherever relevant}
% \textcolor{red}{Rewritten this section based on the feedback}

\section{Evaluation Metrics and Benchmarks}
\looseness=-1 Evaluating multilingual reasoning in LLMs requires standardized metrics to ensure logical consistency and cross-lingual coherence. Unlike traditional NLP, it must address inference errors, translation drift, and reasoning stability across languages.

\subsection{Metrics}
Here, we categorize the key evaluation metrics for multilingual reasoning, along with their formal definitions:\vspace{0.015in}

\noindent\textbf{1) Accuracy-Based Metrics.}
These metrics assess overall correctness in reasoning and multilingual benchmarks: i) \textit{General Accuracy} measures the proportion of correct outputs over total samples and ii) \textit{Zero-Shot Accuracy}, which evaluates model performance on unseen tasks or categories without fine-tuning.\vspace{0.015in}
% \begin{itemize}
%     \item \textbf{General Accuracy ($A$)}: Measures the proportion of correct outputs over total samples.
%     \begin{equation}
%         A = \frac{\text{Correct Predictions}}{\text{Total Predictions}}
%     \end{equation}
%     \item \textbf{Zero-Shot Accuracy ($A_{ZS}$)}: Evaluates model performance on unseen tasks or categories without fine-tuning.
% \end{itemize}

\noindent\textbf{2) Reasoning and Consistency Metrics.} These metrics evaluate logical inference and multi-step reasoning ability: i) \textit{Reasoning Accuracy}: Assesses correctness in logical and step-by-step reasoning tasks and ii) \textit{Path Consistency}: Measures coherence between reasoning steps in CoT prompting.\vspace{0.015in}
% \begin{itemize}
%     \item \textbf{Reasoning Accuracy ($A_R$)}: Assesses correctness in logical and step-by-step reasoning tasks.
%     \item \textbf{Reasoning Path Consistency ($C_R$)}: Measures coherence between reasoning steps in chain-of-thought (CoT) prompting.
% \end{itemize}

\noindent\textbf{3) Translation and Cross-Lingual Metrics.} To ensure multilingual reasoning consistency, models must preserve meaning across languages: i) \textit{Translation Success Rate (TSR)}: Measures correctness and semantic preservation in multilingual translations as the ratio of accurate translations and total translations
% $$\text{TSR} = \frac{\text{Accurate Translations}}{\text{Total Translations}}$$ 
% \begin{equation}
%     \text{TSR} = \frac{\text{Accurate Translations}}{\text{Total Translations}}
% \end{equation}
and ii) \textit{Cross-Lingual Consistency}: Evaluates whether logically equivalent statements yield \textit{consistent reasoning outputs} across different languages.\vspace{0.015in}
% \begin{itemize}
%     \item \textbf{Translation Success Rate (TSR)}: Measures correctness and semantic preservation in multilingual translations.
%     \begin{equation}
%         TSR = \frac{\text{Accurate Translations}}{\text{Total Translations}}
%     \end{equation}
%     \item \textbf{Cross-Lingual Consistency ($C_L$)}: Evaluates whether logically equivalent statements yield \textit{consistent reasoning outputs} across different languages.
% \end{itemize}

\noindent\textbf{4) Perplexity and Alignment Metrics.} These metrics quantify \textit{semantic alignment} and measure whether embeddings across languages remain consistent: i) \textit{Perplexity-Based Alignment ($P_{\text{align}}$)}:
\begin{equation}
    P_{\text{align}} = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(x_i)\right),
\end{equation}
where $P(x_i)$ is the model's probability of predicting token $x_i$. Lower perplexity indicates better alignment 
and ii) \textit{Semantic Alignment Score ($S_{\text{align}}$)} that measures the cosine similarity between multilingual sentence embeddings:
\begin{equation}
    S_{\text{align}} = \frac{E_l \cdot E_t}{\|E_l\| \|E_t\|},
\end{equation}
\looseness=-1 where $E_l$ and $E_t$ are sentence embeddings in different languages.
% \begin{itemize}
%     \item \textbf{Perplexity-Based Alignment Score ($P_{align}$)}:
%     \begin{equation}
%         P_{align} = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(x_i)\right)
%     \end{equation}
%     where $P(x_i)$ is the model's probability of predicting token $x_i$. Lower perplexity indicates \textbf{better alignment}.
%     \item \textbf{Semantic Alignment Score ($S_{align}$)}: Measures the cosine similarity between multilingual sentence embeddings:
%     \begin{equation}
%         S_{align} = \frac{E_l \cdot E_t}{\|E_l\| \|E_t\|}
%     \end{equation}
%     where $E_l$ and $E_t$ are embeddings for sentences in different languages.
% \end{itemize}

\subsection{Performance on Benchmarks}
Here, we discuss the performance of the aforementioned methods on standard mathematical reasoning benchmarks (MGSM~\citep{shi2022language} and MSVAMP~\citep{chen2023breaking}), common sense reasoning (xCSQA~\citep{lin2021xcsqa}), and logical reasoning (xNLI~\citep{conneau2018xnli})\footnote{We only cover benchmarks analyzed by more than four papers.}. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/figure_7.pdf}
    \vspace{-0.1in}
    \caption{Accuracy trends of various methodologies on multilingual reasoning benchmarks, including \colorbox{blue!40}{MGSM}, \colorbox{red!40}{MSVAMP}, \colorbox{purple!40}{XNLI}, and \colorbox{green!40}{XCSQA}. The x-axis represents the date of paper submission on arXiv, while the y-axis indicates percentage accuracy.}
    \vspace{-0.1in}
    \label{fig:benchmark}
\end{figure}
In Fig.~\ref{fig:benchmark}, the \texttt{x}-axis is the timestamp of their arXiv submission, and the \texttt{y}-axis is the predictive performance (accuracy) of the methods on those benchmarks. Next, we describe the four most popular benchmarks and detail the performance of state-of-the-art reasoning techniques on them, highlighting existing gaps in language models that limit their reasoning performance.\par
\noindent\textbf{MGSM} tests multilingual arithmetic reasoning in LLMs with 250 translated math problems in ten diverse languages. 
% Although, MAPO and MindMerger excelled with preference optimization and leveraging external reasoning, QAlign2MonoReason and LinguaLIFT struggled due to weak alignment. 
While recent trends suggest that advanced post-training techniques like MAPO are crucial for strong performance, fine-tuning strategies may be more impactful than stronger reasoning architectures or relying on the model's English expertise to improve multilingual performance. \par

\looseness=-1\noindent\textbf{MSVAMP} is an out-of-domain multilingual mathematical reasoning dataset comprising 10,000 problems across ten languages and serves as a comprehensive test bed to evaluate LLMs' generalization in multilingual mathematical contexts. We find that advanced preference optimization (MAPO) achieves much stronger performance than CoT-based fine-tuning, suggesting advanced fine-tuning techniques are a better direction to beat the current best in this benchmark.\par
\noindent\textbf{xCSQA} is a multilingual extension of the CommonsenseQA dataset, encompassing 12,247 multiple-choice questions translated into 15 languages, designed to assess LLMs' cross-lingual commonsense reasoning capabilities. The current trend in this benchmark shows that stronger fine-tuning strategies like two-step fine-tuning (LingualLIFT) or preference optimization (MAPO) show better performance than selectively fine-tuning specific layers as in SLAM.\par
\noindent\textbf{xNLI} evaluates cross-lingual sentence inference across 15 languages, including low-resource ones. Recent studies suggest that LLM integration with external models \citep{huang2024mindmerger} and multilingual alignment followed by fine-tuning \citep{zhang2024lingualift} outperform contrastive learning methods like TCC \citep{chia2023contrastive}, highlighting the need for more structured multilingual adaptation strategies.


















\begin{comment}
\section{Evaluation Metrics And Benchmarks}
\chirag{Some lines to introduce the section}


\subsection{Metrics}
\label{sec:metric}

Here, we describe some of the commonly used evaluation metrics to quantify the performance of multilingual reasoning performance of LLMs. To avoid the high cost and subjectiveness of human evaluation, prior research has proposed a variety of metrics, which we categorize as:

\chirag{Write the metric equations wherever relevant}

\noindent\textbf{1. Accuracy-Based Metrics.}
Accuracy measures overall correctness across tasks, including reasoning and multilingual benchmarks. Zero-Shot Accuracy evaluates performance on unseen tasks or categories without task-specific fine-tuning.

\noindent\textbf{2. Reasoning and Consistency.}
Reasoning Accuracy focuses on logical and multi-step reasoning tasks, especially in chain-of-thought (CoT) contexts. Reasoning Path Consistency measures the coherence of logical reasoning steps within a task.

\noindent\textbf{3. Translation and Cross-Lingual.}
Translation Success Rate (TSR) assesses the correctness and meaning preservation of multilingual translations. Cross-lingual consistency evaluates reasoning stability and coherence across multiple languages.

\noindent\textbf{4. Perplexity and Alignment.}
PPL-Based Alignment Score determines how well a model aligns input-output pairs, with lower perplexity indicating better alignment. Alignment Score quantifies semantic similarity between multilingual representation pairs (e.g., translation pairs).

\begin{figure*}[htp]
    \centering
    \includegraphics[width=0.81\textwidth]{figures/figure_7.pdf}
    % % First row
    % \begin{subfigure}[b]{0.40\textwidth}
    %     \includegraphics[width=\textwidth, height=2cm]{plot_msvamp2.pdf}
    %     \caption{Plot 1}
    %     \label{fig:plot1}
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}[b]{0.40\textwidth}
    %     \includegraphics[width=\textwidth, height=2cm]{plot_mgsm2.pdf}
    %     \caption{Plot 2}
    %     \label{fig:plot2}
    % \end{subfigure}
    % \vspace{0.5cm}
    % % Second row
    % \begin{subfigure}[b]{0.40\textwidth}
    %     \includegraphics[width=\textwidth, height=2.5cm]{plot_xcsqa2.pdf}
    %     \caption{Plot 3}
    %     \label{fig:plot3}
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}[b]{0.40\textwidth}
    %     \includegraphics[width=\textwidth, height=2.5cm]{plot_xnli2.pdf}
    %     \caption{Plot 4}
    %     \label{fig:plot4}
    % \end{subfigure}
    \caption{A set of four plots arranged in two rows.}
    \label{fig:benchmark}
\end{figure*}


\end{comment}
\section{Future Directions}
\label{sec:future}
As we rapidly develop the new generation of reasoning models, our community must ensure that the models remain unbiased towards languages from underrepresented regions around the world. Looking forward, we call on the community to put their collective efforts into some of the following future directions:\vspace{0.03in}

\looseness=-1\noindent\textbf{1. Multilingual Alignment and Reasoning Transfer.} A key challenge in multilingual reasoning is the lack of data in different languages. One promising solution is to leverage existing large datasets and transfer/distill their knowledge in the representation space, as demonstrated by \citep{yoon2024langbridge,huang2024mindmerger}. Future research should focus on improving cross-lingual knowledge transfer techniques, enabling models to use high-resource languages as a bridge to enhance reasoning consistency in \textit{low-resource languages}.
% \vspace{0.03in}
% \noindent\textbf{2. Low resource Representation.} Reasoning in underrepresented languages is constrained by limited training data. 
Another direction is to generate synthetic datasets using techniques like back-translation and data augmentation, tailored specifically for reasoning tasks.\vspace{0.03in}
% Additionally, few-shot and zero-shot learning, combined with advanced prompting strategies, can reduce reliance on large annotated datasets, enhancing  model performance in low-resource languages.

\begin{comment}
\noindent\textbf{4. Ethical and Culturally Aware Reasoning.}\chirag{This appears to be a problem with multilingual models, not reasoning. I suggest keeping this ONLY if we can come up with examples demonstrating the NEED for ethical and culturally aware reasoning.}
Multilingual reasoning systems must incorporate cultural and ethical awareness to ensure fairness and mitigate biases. This can be achieved by building datasets that represent diverse cultural norms and linguistic variations. Additionally, models should include cultural and ethical reasoning layers to tailor their outputs to regional contexts while maintaining logical consistency.\par
\end{comment}
\noindent\textbf{2. Explainable and Interpretable Reasoning.}
% ~\chirag{Doesn't multilingual reasoning implicitly mean this?} 
Ensuring faithful and interpretable reasoning in multilingual LLMs is challenging due to linguistic diversity, translation ambiguities, and reasoning inconsistencies. Studies on English CoT reasoning~\citep{tanneru2024hardness,lobo2024impact} highlight faithfulness issues, which become more severe when extended to low-resource languages. Causal reasoning can enhance cross-lingual alignment, improving interpretability by uncovering cause-effect relationships across languages. 
% Additionally, uncertainty robustness is essential for mitigating reasoning errors, particularly in languages with scarce training data. 
Future research should focus on integrating causal reasoning and multilingual CoT frameworks to ensure logical coherence, transparency, and trust in multilingual AI systems.\par

\noindent\textbf{3. Advanced Training and Inference Techniques.} While recent advancements in multilingual reasoning have introduced reasoning-aware fine-tuning and multilingual preference optimization techniques, further efforts are needed to improve training paradigms. Notably, \citet{wu2024reuse} demonstrates that reward signals can be effectively transferred across languages, paving the way for post-training RL methods that improve reasoning in low-resource languages. Additionally, efficient inference-time scaling and agentic frameworks remain under-explored, despite emerging techniques~\citep{khanov2024args,chakraborty2024transfer} gaining traction and multi-agent frameworks~\citep{guo2024large} enabling LLMs to simulate agent interactions or refine their reasoning by learning from self-generated reasoning paths.
% Multi-agent frameworks~\citep{guo2024large} enable LLMs to simulate agent interactions or refine their reasoning by learning from self-generated reasoning paths, improving their ability to tackle complex reasoning scenarios, crucial in a multilingual context.  
% Finally, advanced prompting strategies should be further investigated to enhance cross-lingual generalization and reasoning efficiency.
\vspace{0.03in}

\looseness=-1\noindent\textbf{4. Unified Evaluation Metrics.} A comprehensive evaluation framework is a crucial missing component for assessing multilingual reasoning capabilities. Metrics should measure logical consistency, cultural adaptability, and robustness, considering real-world and adversarial multilingual settings.
% and adversarial scenarios. 
% Establishing such a framework would ensure that LLMs remain reliable, interpretable, and effective across diverse linguistic and reasoning challenges.
\vspace{0.03in}

\noindent\textbf{5. Benchmarks:}
As multilingual reasoning advances, robust evaluation benchmarks are essential. As reasoning is highly domain-specific in nature, developing targeted benchmarks is crucial, especially in high-stakes fields like healthcare, law, and finance, where accuracy directly affects decision-making.\vspace{0.03in}

% \noindent\textbf{6. Efficient Reasoning Models.} An emerging direction in reasoning research is enhancing resource efficiency in reasoning-aware models. Recent works like \cite{ning2024can} propose strategies for more efficient reasoning, reducing computational costs while maintaining logical consistency. However, this area remains largely unexplored in multilingual settings, offering a key opportunity to develop scalable reasoning models that generalize across languages with minimal resources.

\begin{comment}
\noindent\textbf{10. Responsible and Safe Reasoning Outputs.} Ensuring the safety of multilingual reasoning systems is vital, particularly for high-stakes applications. Reward-based optimization techniques can help mitigate harmful or toxic outputs by encouraging safer responses. Models must also be designed to resist adversarial inputs and misinformation, ensuring trustworthy reasoning in domains such as healthcare, law, and education.
\end{comment}

\looseness=-1\noindent\textbf{6. Multimodal Multilingual Reasoning.} While there are a few works on visual reasoning in the multilingual context~\citep{das2024exams}, multimodal reasoning (integrating tables, text, image, audio and video) remains largely unexplored. Advancing this area could enable models handle complex tasks in low-resource languages and incorporate cross-modal reasoning.
% By incorporating cross-modal reasoning, systems can provide holistic insights to provide better reasoning before coming to any conclusion.
% \vspace{0.03in}

% \chirag{Integrate this into one or more of the above six directions.}
% Recent advancements in reasoning techniques for LLMs offer promising directions for multilingual reasoning. 
% Further, reverse thinking techniques, which have proven effective in enhancing logical inference, could be explored to improve multilingual reasoning consistency and robustness.
% Additionally, prior works have shown that LLMs can refine their reasoning by learning from self-generated reasoning paths, presenting a way to mitigate data scarcity in low-resource languages.


\section{Conclusion}
Multilingual reasoning in Large Language Models (LLMs) is a rapidly evolving field, addressing critical challenges like cross-lingual alignment, low-resource language gaps, and cultural adaptation. Our survey highlights advancements in fine-tuning, prompting, and representation learning while identifying gaps in scalability, ethical reasoning, and domain-specific applications. It serves as a call to action for the LLM and reasoning research and development community to focus on advanced alignment techniques, culturally aware reasoning, scalable architectures, and responsible AI outputs. By breaking language barriers and fostering inclusivity, multilingual reasoning has the potential to create globally impactful AI systems. This survey provides a foundation for advancing research in this transformative domain.





% This survey paper aims to serve as a foundational resource that systematically categorizes over 29 distinct prompt engineering techniques based on their targeted functionalities, inspiring further research and empowering innovators in the evolving landscape of prompt engineering.


% \subsubsection{Chain-of-Symbol (CoS) Prompting}
% \label{cos}
% LLMs often struggle with tasks involving complex spatial relationships due to their reliance on natural language, which is susceptible to ambiguity and biases. To overcome this limitation, \cite{hu2023chainofsymbol} introduce CoS, employing condensed symbols instead of natural language. CoS provides distinct advantages: clear and concise prompts, heightened spatial reasoning for LLMs, and improved human interpretability. While CoS is exciting, challenges remain scalability, generalizability, integration with other techniques, and interpretability of LLM reasoning based on symbols. Notably, the implementation of CoS significantly elevates ChatGPT's performance, boosting accuracy from 31.8\% to an impressive 92.6\% on Brick World tasks. Moreover, CoS achieves up to a 65.8\% reduction in prompt tokens, streamlining the process while maintaining high accuracy.




% LLMs often struggle with tasks involving complex spatial relationships due to their reliance on natural language, prone to ambiguity and biases. To address this limitation the authors in \cite{hu2023chainofsymbol} introduce Chain-of-Symbol Prompting (CoS), which replaces natural language with condensed symbols in prompting; CoS offers several advantages: clear and compact prompts, enhanced spatial reasoning for LLMs, and improved human interpretability. While CoS is exciting, challenges remain: scalability, generalizability, integration with other techniques, and interpretability of LLM reasoning based on symbols. The implementation of CoS significantly boosts ChatGPT's performance, increasing accuracy from 31.8\% to 92.6\% on Brick World tasks. Additionally, COS reduces the number of tokens in prompts by up to 65.8\%, streamlining the process while maintaining high accuracy levels.


% Chain-of-Symbol Prompting (CoS) emerges as a novel technique in the vast landscape of prompt engineering, aiming to address a crucial limitation of Large Language Models (LLMs) - their struggle with spatial reasoning. 

% This paper presents the first comprehensive survey on the
% progress of prompt engineering. We thoroughly summarize the existing prompt engineering techniques, which cover various application domains and highlight the advantages and disadvantages. Besides, we summarize the datasets and results for each of the prompt techniques. Furthermore, we have added a  daigrams and a table to highlight the important points. We hope this survey can give a brife overview of the prompt techniques.


% Even with impressive advancements, the most advanced language models still find complex multi-step reasoning challenging. \cite{zheng2023take} presents a Step-Back prompting technique empowering LLMs, such as PaLM-2L, to engage in abstraction, extracting high-level concepts and fundamental principles from specific instances. Consequently, this leads to a substantial improvement in the models' reasoning capabilities. Step-Back prompting involves a two-step procedure consisting of Abstraction and Reasoning. A comprehensive experiment utilizing Step-Back Prompting on PaLM-2L models, tackling diverse and demanding reasoning-intensive tasks such as STEM, Knowledge QA, and Multi-Hop Reasoning. Notably, this technique yields significant performance boosts, improving results in tasks such as MMLU Physics and Chemistry by 7\% and 11\%, TimeQA by 27\%, and MuSiQue by 7\%.


% \input{taxo.tex}
% \include{taxo.tex}


% \subsection{Length of Papers}


% All paper {\em submissions} to the main track must have a maximum of seven pages, plus at most two for references / acknowledgements / contribution statement/ethics statement.

% The length rules may change for final camera-ready versions of accepted papers and
% differ between tracks. Some tracks may disallow any contents other than the references in the last two pages, whereas others allow for any content in all pages. Similarly, some tracks allow you to buy a few extra pages should you want to, whereas others don't.

% If your paper is accepted, please carefully read the notifications you receive, and check the proceedings submission information website\footnote{\url{https://proceedings.ijcai.org/info}} to know how many pages you can use for your final version. That website holds the most up-to-date information regarding paper length limits at all times.


% \subsection{Word Processing Software}

% As detailed below, IJCAI has prepared and made available a set of
% \LaTeX{} macros and a Microsoft Word template for use in formatting
% your paper. If you are using some other word processing software, please follow the format instructions given below and ensure that your final paper looks as much like this sample as possible.

% \section{Style and Format}

% \LaTeX{} and Word style files that implement these instructions
% can be retrieved electronically. (See Section~\ref{stylefiles} for
% instructions on how to obtain these files.)

% \subsection{Layout}

% Print manuscripts two columns to a page, in the manner in which these
% instructions are printed. The exact dimensions for pages are:
% \begin{itemize}
%     \item left and right margins: .75$''$
%     \item column width: 3.375$''$
%     \item gap between columns: .25$''$
%     \item top margin---first page: 1.375$''$
%     \item top margin---other pages: .75$''$
%     \item bottom margin: 1.25$''$
%     \item column height---first page: 6.625$''$
%     \item column height---other pages: 9$''$
% \end{itemize}

% All measurements assume an 8-1/2$''$ $\times$ 11$''$ page size. For
% A4-size paper, use the given top and left margins, column width,
% height, and gap, and modify the bottom and right margins as necessary.

% \subsection{Format of Electronic Manuscript}

% For the production of the electronic manuscript, you must use Adobe's
% {\em Portable Document Format} (PDF). A PDF file can be generated, for
% instance, on Unix systems using {\tt ps2pdf} or on Windows systems
% using Adobe's Distiller. There is also a website with free software
% and conversion services: \url{http://www.ps2pdf.com}. For reasons of
% uniformity, use of Adobe's {\em Times Roman} font is strongly suggested.
% In \LaTeX2e{} this is accomplished by writing
% \begin{quote}
%     \mbox{\tt $\backslash$usepackage\{times\}}
% \end{quote}
% in the preamble.\footnote{You may want to also use the package {\tt
%             latexsym}, which defines all symbols known from the old \LaTeX{}
%     version.}

% Additionally, it is of utmost importance to specify the {\bf
%         letter} format (corresponding to 8-1/2$''$ $\times$ 11$''$) when
% formatting the paper. When working with {\tt dvips}, for instance, one
% should specify {\tt -t letter}.

% \subsection{Papers Submitted for Review vs. Camera-ready Papers}
% In this document, we distinguish between papers submitted for review (henceforth, submissions) and camera-ready versions, i.e., accepted papers that will be included in the conference proceedings. The present document provides information to be used by both types of papers (submissions / camera-ready). There are relevant differences between the two versions. Find them next.

% \subsubsection{Anonymity}
% For the main track and some of the special tracks, submissions must be anonymous; for other special tracks they must be non-anonymous. The camera-ready versions for all tracks are non-anonymous. When preparing your submission, please check the track-specific instructions regarding anonymity.

% \subsubsection{Submissions}
% The following instructions apply to submissions:
% \begin{itemize}
% \item If your track requires submissions to be anonymous, they must be fully anonymized as discussed in the Modifications for Blind Review subsection below; in this case, Acknowledgements and Contribution Statement sections are not allowed.

% \item If your track requires non-anonymous submissions, you should provide all author information at the time of submission, just as for camera-ready papers (see below); Acknowledgements and Contribution Statement sections are allowed, but optional.

% \item Submissions must include line numbers to facilitate feedback in the review process . Enable line numbers by uncommenting the command {\tt \textbackslash{}linenumbers} in the preamble \footnote{New in IJCAI--24}.

% \item The limit on the number of  content pages is \emph{strict}. All papers exceeding the limits will be desk rejected.
% \end{itemize}

% \subsubsection{Camera-Ready Papers}
% The following instructions apply to camera-ready papers:

% \begin{itemize}
% \item Authors and affiliations are mandatory. Explicit self-references are allowed. It is strictly forbidden to add authors not declared at submission time.

% \item Acknowledgements and Contribution Statement sections are allowed, but optional.

% \item Line numbering must be disabled. To achieve this, comment or disable {\tt \textbackslash{}linenumbers} in the preamble.

% \item For some of the tracks, you can exceed the page limit by purchasing extra pages.
% \end{itemize}

% \subsection{Title and Author Information}

% Center the title on the entire width of the page in a 14-point bold
% font. The title must be capitalized using Title Case. For non-anonymous papers, author names and affiliations should appear below the title. Center author name(s) in 12-point bold font. On the following line(s) place the affiliations.

% \subsubsection{Author Names}

% Each author name must be followed by:
% \begin{itemize}
%     \item A newline {\tt \textbackslash{}\textbackslash{}} command for the last author.
%     \item An {\tt \textbackslash{}And} command for the second to last author.
%     \item An {\tt \textbackslash{}and} command for the other authors.
% \end{itemize}

% \subsubsection{Affiliations}

% After all authors, start the affiliations section by using the {\tt \textbackslash{}affiliations} command.
% Each affiliation must be terminated by a newline {\tt \textbackslash{}\textbackslash{}} command. Make sure that you include the newline after the last affiliation, too.

% \subsubsection{Mapping Authors to Affiliations}

% If some scenarios, the affiliation of each author is clear without any further indication (\emph{e.g.}, all authors share the same affiliation, all authors have a single and different affiliation). In these situations you don't need to do anything special.

% In more complex scenarios you will have to clearly indicate the affiliation(s) for each author. This is done by using numeric math superscripts {\tt \$\{\^{}$i,j, \ldots$\}\$}. You must use numbers, not symbols, because those are reserved for footnotes in this section (should you need them). Check the authors definition in this example for reference.

% \subsubsection{Emails}

% This section is optional, and can be omitted entirely if you prefer. If you want to include e-mails, you should either include all authors' e-mails or just the contact author(s)' ones.

% Start the e-mails section with the {\tt \textbackslash{}emails} command. After that, write all emails you want to include separated by a comma and a space, following the order used for the authors (\emph{i.e.}, the first e-mail should correspond to the first author, the second e-mail to the second author and so on).

% You may ``contract" consecutive e-mails on the same domain as shown in this example (write the users' part within curly brackets, followed by the domain name). Only e-mails of the exact same domain may be contracted. For instance, you cannot contract ``person@example.com" and ``other@test.example.com" because the domains are different.


% \subsubsection{Modifications for Blind Review}
% When submitting to a track that requires anonymous submissions,
% in order to make blind reviewing possible, authors must omit their
% names, affiliations and e-mails. In place
% of names, affiliations and e-mails, you can optionally provide the submission number and/or
% a list of content areas. When referring to one's own work,
% use the third person rather than the
% first person. For example, say, ``Previously,
% Gottlob~\shortcite{gottlob:nonmon} has shown that\ldots'', rather
% than, ``In our previous work~\cite{gottlob:nonmon}, we have shown
% that\ldots'' Try to avoid including any information in the body of the
% paper or references that would identify the authors or their
% institutions, such as acknowledgements. Such information can be added post-acceptance to be included in the camera-ready
% version.
% Please also make sure that your paper metadata does not reveal
% the authors' identities.

% \subsection{Abstract}

% Place the abstract at the beginning of the first column 3$''$ from the
% top of the page, unless that does not leave enough room for the title
% and author information. Use a slightly smaller width than in the body
% of the paper. Head the abstract with ``Abstract'' centered above the
% body of the abstract in a 12-point bold font. The body of the abstract
% should be in the same font as the body of the paper.

% The abstract should be a concise, one-paragraph summary describing the
% general thesis and conclusion of your paper. A reader should be able
% to learn the purpose of the paper and the reason for its importance
% from the abstract. The abstract should be no more than 200 words long.

% \subsection{Text}

% The main body of the text immediately follows the abstract. Use
% 10-point type in a clear, readable font with 1-point leading (10 on
% 11).

% Indent when starting a new paragraph, except after major headings.

% \subsection{Headings and Sections}

% When necessary, headings should be used to separate major sections of
% your paper. (These instructions use many headings to demonstrate their
% appearance; your paper should have fewer headings.). All headings should be capitalized using Title Case.

% \subsubsection{Section Headings}

% Print section headings in 12-point bold type in the style shown in
% these instructions. Leave a blank space of approximately 10 points
% above and 4 points below section headings.  Number sections with
% Arabic numerals.

% \subsubsection{Subsection Headings}

% Print subsection headings in 11-point bold type. Leave a blank space
% of approximately 8 points above and 3 points below subsection
% headings. Number subsections with the section number and the
% subsection number (in Arabic numerals) separated by a
% period.

% \subsubsection{Subsubsection Headings}

% Print subsubsection headings in 10-point bold type. Leave a blank
% space of approximately 6 points above subsubsection headings. Do not
% number subsubsections.

% \paragraph{Titled paragraphs.} You should use titled paragraphs if and
% only if the title covers exactly one paragraph. Such paragraphs should be
% separated from the preceding content by at least 3pt, and no more than
% 6pt. The title should be in 10pt bold font and to end with a period.
% After that, a 1em horizontal space should follow the title before
% the paragraph's text.

% In \LaTeX{} titled paragraphs should be typeset using
% \begin{quote}
%     {\tt \textbackslash{}paragraph\{Title.\} text} .
% \end{quote}

% \subsection{Special Sections}



% \begin{center}
% \begin{table*}[h!]
% \centering
% \footnotesize
% % \scriptsize
% \scalebox{0.85}{
% \begin{tabular}{|l|c|c|c|c|}
% \hline 
% Category & \begin{tabular}[c]{@{}c@{}}Prompting Technique \end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluated \\ LLM(s) \end{tabular} & Dataset(s) & \begin{tabular}[c]{@{}c@{}}Task(s) \end{tabular} \\
% \hline
% \begin{tabular}[c]{@{}c@{}}New Tasks Without \\Training Data \end{tabular}  & Zero-shot Prompting  & - & - & - \\ 
% \cline{2-5}

% & Few-shot Prompting  & - & - & - \\ 
% \hline

%  & Chain-of-Thought~\cite{wei2022chain} & \begin{tabular}[c]{@{}c@{}}PaLM 540B, \\ code-davinci-002, \\text-davinci-002,\\LaMDA 137B  \end{tabular}  
%  & \begin{tabular}[c]{@{}c@{}}Arithmetic Reasoning[\\ GSM8K, \\SVAMP,\\ASDiv, \\ AQuA, \\MAWPS] \\ Commonsense \\ Reasoning [\\ CSQA,\\StrategyQA,\\ Date, \\Sports, \\SayCan] \\ Symbolic\\ Reasoning [ \\Letter, \\ Coin] \end{tabular}  & \\ 
 
%  \cline{2-5}
% %  arithmetic(GSM8K ,SVAMP ,ASDiv ,AQuA ,MAWPS)
% % commonsense reasoning (CSQA ,StrategyQA, Date, Sports, SayCan)
% % symbolic reasoning(Letter, Coin) 
 
%  & \begin{tabular}[c]{@{}c@{}}Automatic \\ Chain-of-Thought~\cite{zhang2022automatic} \end{tabular} & \begin{tabular}[c]{@{}c@{}}PaLM 540B, \\GPT-3,\\LaMDA 137B \end{tabular} & \begin{tabular}[c]{@{}c@{}}Arithmetic Reasoning[\\ GSM8K, \\SVAMP,\\ASDiv, \\ AQuA, \\MAWPS] \\ Commonsense \\ Reasoning [\\ CSQA,\\StrategyQA,\\ Date, \\Sports, \\SayCan] \\ Symbolic\\ Reasoning [ \\Letter, \\ Coin] \end{tabular}  & \\ 
%  \cline{2-5}

% Reasoning and Logic   & Self-Consistency~\cite{wang2022self} &
% \begin{tabular}[c]{@{}c@{}} UL2-20B,\\PaLM 540B,\\code-davinci-002,\\
% text-davinci-002,\\ LaMDA 137B \end{tabular} & \begin{tabular}[c]{@{}c@{}}Arithmetic Reasoning[\\ GSM8K, \\SVAMP,\\ASDiv, \\ AQuA, \\MAWPS] \\ Commonsense \\ Reasoning [\\ CSQA,\\StrategyQA,\\ Date, \\Sports, \\SayCan] \\ Symbolic\\ Reasoning [ \\Letter, \\ Coin] \end{tabular}  &  \\ 
%   \cline{2-5}
  
% & Tree-of-Thoughts~\cite{yao2023tree} & GPT-4 & \begin{tabular}[c]{@{}c@{}} Game of 24,\\ Creative Writing,\\ Mini Crosswords \end{tabular} & \\
% \cline{2-5}

% & Graph-of-Thought~\cite{yao2023beyond}  & \begin{tabular}[c]{@{}c@{}} GPT-3, \\GPT-3.5,\\GPT-4,\\code-davinci-002,\\T5(base \& large) \end{tabular}& \begin{tabular}[c]{@{}c@{}} GSM8K,\\ ScienceQA\end{tabular} &  \\ 
% \cline{2-5}

% & System 2 Attention~\cite{weston2023system}  & \begin{tabular}[c]{@{}c@{}} GPT-4, \\ Llama 2-70B\end{tabular}& \begin{tabular}[c]{@{}c@{}} QA,\\ math word problems\\longform \end{tabular} &  \\ 

%   \hline
%  & Chain-of-Verification~\cite{dhuliawala2023chain} & \begin{tabular}[c]{@{}c@{}} Llama 2 70B Chat,\\ Llama 65B \end{tabular} & \begin{tabular}[c]{@{}c@{}} Wikidata,\\ QUEST,\\ MultiSpanQA\end{tabular}&  \\  
%  \cline{2-5}
 
%  &  ReAct Prompting~\cite{yao2022react} & \begin{tabular}[c]{@{}c@{}}PaLM-540B,\\ GPT-3\end{tabular} & \begin{tabular}[c]{@{}c@{}} HotpotQA,\\ Fever,\\ALFWorld,\\ WebShop\end{tabular}&  \\  
%  \cline{2-5}
 
% Reduce Hallucination & \begin{tabular}[c]{@{}c@{}}Retrieval Augmented \\ Generation~\cite{lewis2020retrieval} \end{tabular} & \begin{tabular}[c]{@{}c@{}}T5-11B, \\RAG-Token,\\ RAG-Seq.,\\REALM,\\DPR \end{tabular} &  \begin{tabular}[c]{@{}c@{}} Natural Questions,\\ TriviaQA,\\WebQuestions \\and CuratedTrec,\\MSMARCO NLG\\ task v2.1,\\SearchQA, \\Fever \end{tabular} & \\  
% \cline{2-5}
% \end{tabular}
% }
% \caption{A comparative summary of the literature and the proposed model}
% \label{tab:comparison}
% \end{table*}
% \end{center}



% \begin{center}
% \begin{table*}[h!]
% \centering
% \footnotesize
% % \scriptsize
% \scalebox{0.85}{
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% Category & \begin{tabular}[c]{@{}c@{}}Prompting Technique \end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluated \\ LLM(s) \end{tabular} & Dataset(s) & \begin{tabular}[c]{@{}c@{}}Task(s) \end{tabular} \\
% \hline

%  & Chain-of-Note~\cite{yu2023chainofnote} & \begin{tabular}[c]{@{}c@{}} Llama 2,\\ DPR \end{tabular} & \begin{tabular}[c]{@{}c@{}} NQ,\\ TriviaQA, \\WebQ \end{tabular} &  \\ \cline{2-5}
 
%  & Chain-of-Knowledge~\cite{li2023chainofknowledge} & gpt-3.5-turbo-0613 & \begin{tabular}[c]{@{}c@{}} Factual [\\ FEVER,\\ HotpotQA,\\FeTaQA]\\
% Medical [\\MedMCQA] \\ Physics [ \\MMLU Physics]\\Biology [\\MMLU Biology]\end{tabular} &  \\  \hline 

%  User Interaction & Active-Prompt~\cite{diao2023active} & \begin{tabular}[c]{@{}c@{}} CodeX code-davinci-002, \\text-davinci-002, \\ text-davinci-003 \end{tabular} & \begin{tabular}[c]{@{}c@{}}Arithmetic [\\ GSM8K, \\SVAMP,\\ASDiv, \\ AQuA, \\MAWPS] \\ Commonsense \\ Reasoning [\\ CSQA,\\StrategyQA,\\ Date, \\Sports, \\SayCan] \\ Symbolic\\ Reasoning [ \\Letter, \\ Coin] \end{tabular}  &  \\ 
%  \hline
%  % & Instruction Prompting and Tuning &  & \xmark &  \\ \hline

%  % & & \xmark & \cmark& FedAWA \\ \hline 

% \begin{tabular}[c]{@{}c@{}} Fine-Tuning and \\ Optimization \end{tabular}   & \begin{tabular}[c]{@{}c@{}}Automatic Prompt \\ Engineer~\cite{zhang2022automatic} \end{tabular}   & \begin{tabular}[c]{@{}c@{}} Ada, \\ babbage,\\ curie,\\ davinci,\\ text-ada-001,\\ text-babbage-001,\\ text-curie-001,\\ text-davanci-002 \end{tabular} & \begin{tabular}[c]{@{}c@{}} 24 Instruction Induction,\\
% BIG-Bench Instruction Induction \\(emotional understanding,\\ context-free question answering,\\ reading comprehension,\\ summarization, \\algorithms,\\ and various reasoning tasks) \\
% TruthfulQA \end{tabular}&  \\ \hline

% % & Automatic Prompt Engineer (APE) & \xmark & \cmark& FAVOR \\ \hline

% \begin{tabular}[c]{@{}c@{}} Knowledge-Based\\Reasoning and
% Generation \end{tabular}  & \begin{tabular}[c]{@{}c@{}} Automatic Reasoning\\and Tool-use~\cite{paranjape2023art} \end{tabular}  & \begin{tabular}[c]{@{}c@{}} GPT-3 (175B),\\Toolformer,\\InstructGPT \\(text-davinci-002)\end{tabular}  & \begin{tabular}[c]{@{}c@{}}SQUAD, \\TriviaQA,\\ SVAMP,\\ MAWPS,\\ MMLU \end{tabular} &  \\ \hline

%  % & Retrieval Augmented Generation (RAG) & \xmark & \cmark& FAVOR \\ \hline

% \begin{tabular}[c]{@{}c@{}} Improving Consistency\\and Coherence \end{tabular} & \begin{tabular}[c]{@{}c@{}} Contrastive\\Chain-of-Thought~\cite{chia2023contrastive} \end{tabular}   & GPT-3.5-Turbo (0301) & \begin{tabular}[c]{@{}c@{}} arithmetic [ \\GSM8K,\\SVAMP,\\ASDiv,\\AQuA,\\GSM-Hard] \\Factual QA [\\Bamboogle\\ StrategyQA] \end{tabular}&  \\ \hline

% \begin{tabular}[c]{@{}c@{}} Managing Emotions\\and Tone \end{tabular} 
%    & Emotion Prompting~\cite{li2023large} & \begin{tabular}[c]{@{}c@{}} T5,\\ Vicuna,\\ BLOOM,\\ Llama 2,\\ gpt-3.5-turbo,\\ GPT-4 \end{tabular}& \begin{tabular}[c]{@{}c@{}} 24 instruction \\ induction tasks, \\BIG-Bench \\Instruction \\Induction (BBII) \end{tabular}&  \\ \hline

%  & \begin{tabular}[c]{@{}c@{}}Structured  \\ Chain-of-Thought~\cite{zhang2022automatic}\end{tabular} & \begin{tabular}[c]{@{}c@{}} ChatGPT, \\Codex \end{tabular}& \begin{tabular}[c]{@{}c@{}} HumanEval,\\ MBPP,\\MBCPP \end{tabular} & \\
%  \cline{2-5}
%   %& Program of Thoughts & \cmark& FAVOR \\ \cline{2-5}

%  & Program of Thoughts~\cite{chen2022program} & \begin{tabular}[c]{@{}c@{}}code-davinci-002,\\ text-davinci-002,\\
%  gpt-3.5-turbo,\\PaLM,\\codegen-16B\end{tabular} &\begin{tabular}[c]{@{}c@{}} GSM8K,\\AQuA,\\SVAMP,\\TabWMP,\\FinQA,\\ConvFin,\\TATQA \end{tabular}&  \\ \cline{2-5}

% \begin{tabular}[c]{@{}c@{}}Code Generation and \\ Execution\end{tabular}   & Chain of Code Prompting~\cite{li2023chain} & \begin{tabular}[c]{@{}c@{}}text-ada-001, \\text-baggage-001, \\text-curie-001,\\text-davinci-003,\\gpt-3.5-turbo,\\gpt-4 \end{tabular} & \begin{tabular}[c]{@{}c@{}} BIG-Bench\\ Hard (BBH) - 23 tasks[ \\semantic reasoning, \\numerical reasoning, \\combination of \\semantic and \\numerical reasoning] \end{tabular} &  \\ \cline{2-5}

%  & Scratchpad Prompting~\cite{nye2021show} & GPT  & \begin{tabular}[c]{@{}c@{}} MBPP,\\MBPP-aug \end{tabular}&  \\ \hline

% \begin{tabular}[c]{@{}c@{}}Optimization \\and Efficiency\end{tabular}  & Optimization by Prompting~\cite{yang2023large} & \begin{tabular}[c]{@{}c@{}} text-bison,\\ PaLM 2-L,\\PaLM 2-L-IT,\\ gpt-3.5-turbo,\\gpt-4 \end{tabular}& \begin{tabular}[c]{@{}c@{}}GSM8,\\Multi-Arith,\\AQuA \end{tabular}&  \\ \hline

% Understanding User Intent & Rephrase and Respond~\cite{deng2023rephrase} & \begin{tabular}[c]{@{}c@{}}GPT-3.5-turbo-0613,\\ Vicuna-13b-v1.5, \\GPT-4-0613 \end{tabular}  & \begin{tabular}[c]{@{}c@{}}Knowledge Classification [\\Even day,\\Even month,\\Even year,\\Compare age]\\Commonsense Reasoning [\\CSQA,\\Dates]\\Symbolic Reasoning [ \\Last letter (2), \\Last letter (4),\\ Coin flip,\\ Sports] \end{tabular} &  \\ \hline

% \begin{tabular}[c]{@{}c@{}} Metacognition and\\Reflection \end{tabular} & \begin{tabular}[c]{@{}c@{}}Take a Step \\Back ~\cite{zheng2023take} \end{tabular}   &\begin{tabular}[c]{@{}c@{}} PaLM2-L,\\ GPT-4 \end{tabular}&\begin{tabular}[c]{@{}c@{}} TimeQA,\\ StrategyQA,\\ MMLU high-school\\ Physics,\\MuSiQue \end{tabular}&  \\ \hline

% \end{tabular}
% }
% \caption{A comparative summary of the literature and the proposed model}
% \label{tab:comparison}
% \end{table*}
% \end{center}










% \hline
% \begin{tabular}[c]{@{}c@{}}Metacognition and Self-Reflection\end{tabular} & Take a Step Back & Manual & Single & GPT-3.5-turbo, LLaMA 2 Chat (70B), GPT-4 \\
% \hline
% \end{tabular}
% }
% \end{center}
% \end{table*}

% \end{document}






% \begin{table*}
% \begin{center}
%  \caption{The performance evaluation of the proposed model using BERTScore.}
% \label{cla_result2}
% \scalebox{0.75}{
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \multirow{3}{*} {Category}& \multirow{2}{*} \begin{tabular}[c]{@{}c@{}}Prompting\\ Technique\end{tabular} & \multicolumn{3}{c|}{Comparison Scope} \\
% \cline{3-5}
%     &&Prompt Acquisition  & Prompt Stage &Language Model \\
% \hline

% % &&\multicolumn{3}{c}{ \textbf{Comparison Scope}}  
% % % \multicolumn{2}{c|}{Prompt Stage} &
% % % \multicolumn{2}{c}{InstructBLIP}  \\
% % \cline{2-6}
% %    \multirow{2}{2cm} {Category} & \begin{tabular}[c]{@{}c@{}}Prompting\\ Technique\end{tabular} &Prompt Acquisition  & Prompt Stage &Language Model  \\
% % \hline
% \begin{tabular}[c]{@{}c@{}}New Tasks Without \\Training Data\end{tabular}   & Zero-shot & Manual & Single\\
% & Few-shot & Manual & Single\\
% \hline
%  & CoT & Manual & Multi &PaLM 540B,
% code-davinci-002, LaMDA 137B  \\
%  & Auto-CoT &LM Generated & Multi&PaLM 540B,
% code-davinci-002, LaMDA 137B \\
%  Reasoning and Logic&Self-Consistency & Manual & Single&PaLM 540B,
% code-davinci-002, LaMDA 137B \\
%  &ToT & Retrieval Based & Multi&GPT-4 \\
%  &GoT & Retrieval Based & Multi&GPT-3,GPT-3.5,GPT-4,code-davinci-002,
% T5 \\
%  &S2A  & Manual & Single &GPT-4,
% Llama 2-70B \\
%  &ThoT& Hybrid  & \\
%  &CoT & & \\        
% \hline
% & CoVe & Retrieval Based & Multi&Llama 2 70B Chat,Llama 65B \\
% Reduce Hallucination&ReAct & Retrieval Based & Multi &PaLM-540B,GPT-3\\
% &RAG & Retrieval Based & Single&T5-11B, RAG-Token,RAG-Seq.,REALM,DPR \\
% &CoN &LM Generated &Multi&Llama 2,DPR \\
% &CoK & LM Generated & Multi&gpt-3.5-turbo-0613 \\
% \hline
% User Interaction&Active-Prompt & Manual & Single&CodeX code-davinci-002,
% text-davinci-003 \\
% \hline
% \begin{tabular}[c]{@{}c@{}}Fine-Tuning \\and Optimization\end{tabular} &APE & LM Generated & Single&davinci,text-ada-001,text-curie-001,text-davanci-002 \\
%  \hline
% \begin{tabular}[c]{@{}c@{}}Knowledge-Based \\ Reasoning and Generation\end{tabular}  & ART& Hybrid  & & GPT-3 (175B), Toolformer, InstructGPT \\
%  \hline
% \begin{tabular}[c]{@{}c@{}}Improving Consistency\\ and Coherence\end{tabular}  & CCoT&LM Generated & Multi &GPT-3.5-Turbo (0301)\\
%  \hline
% \begin{tabular}[c]{@{}c@{}}Managing Emotions\\ and Tone\end{tabular} & Emotion Prompting&Manual & Single &T5, Vicuna, BLOOM,Llama 2,gpt-3.5-turbo, GPT-4\\
%  \hline
% & SCoT&LM Generated & Multi&ChatGPT,
% Codex \\
% \begin{tabular}[c]{@{}c@{}}Code Generation\\ and Execution\end{tabular}  &PoT & Manual & Single &code-davinci-002, text-davinci-002, gpt-3.5-turbo\\
%   &CoC & Manual & Single&text-curie-001,
% text-davinci-003,gpt-3.5-turbo,gpt-4 \\
%  &Scratchpad Prompting & Manual & Single& GPT \\
%  \hline
% \begin{tabular}[c]{@{}c@{}}Optimization \\and Efficiency\end{tabular} & OPRO&Manual & Single&PaLM 2-L-IT, gpt-3.5-turbo,gpt-4 \\
% \hline
% \begin{tabular}[c]{@{}c@{}}Understanding \\ User Intent\end{tabular} & RaR& Manual & Single &GPT-3.5-turbo-0613, Vicuna-13b-v1.5,GPT-4-0613\\
% \hline
% \begin{tabular}[c]{@{}c@{}}Metacognition \\ and Self-Reflection\end{tabular} & Take a Step Back&Manual & Single&GPT-3.5-turbo, LLaMA 2 Chat (70B),GPT-4 \\
% \hline
% \end{tabular}
% }
% \end{center}
% \end{table*}

% \begin{table*}
% \begin{center}
%  \caption{Experiment 2 (V-2): Model performance across different testing setups. Here, A, T, and B refer to the Audio, Text, and Both shared, respectively. All the reported results are statistically significant}
% \label{cla_result2}
% \scalebox{0.75}{
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
% \hline
% \multirow{2}{2cm} {Augmentaion Factor}& \multirow{2}{2cm}{Model} & \multicolumn{3}{c|}{Precision (\%)} & \multicolumn{3}{c|}{Sensitivity (\%)} & \multicolumn{3}{c|}{F1-score (\%)} & \multicolumn{3}{c|}{Accuracy (\%)} \\
% \cline{3-14}
%     &&A & T & B & A & T & B& A & T & B& A & T & B \\
% \hline
 
% Without & DAIC & 58.75 & 60.75 & 60.88 & 48.45 & 50.10 & 47.97 & 53.10 & 54.92& 53.63 & 48.13 & 50.13 & 46.84 \\
% Subsampling  & MODMA & 60.00 & 60.00 & 60.00 &50.00 & 50.00 & 50.00 & 54.54 & 54.54 & 54.54 & 50.00 & 50.00 & 50.00 \\
%  & Combined & 71.06 & 70.00 & 70.00 & 4.72 & 50.00 & 50.00 & 58.51 & 58.33 & 58.33 & 49.60 & 50.00 & 50.00 \\
% \hline
% With & DAIC & 49.07 & 54.61 &\textbf{85.00} & 33.00 & 37.00 & \textbf{83.33} & 39.46 & 44.11 & \textbf{84.15} & 49.38 & 53.12 & \textbf{84.00} \\
% Subsampling & MODMA & 63.64 & 100.00 & \textbf{100.00 }& 41.18 & 70.59 & \textbf{77.27} & 50.00 & {87.18} & \textbf{87.18} & 58.82 & 85.29 & \textbf{85.29}\\
% (3:4) & Combined & 48.68 & 75.00 & \textbf{75.00} & 45.48  & 52.94 & \textbf{52.94} & 47.03 & 62.07& \textbf{62.07} & 48.77 & 67.65 & \textbf{67.65}\\
% \hline
% \end{tabular}
% }
% \end{center}
% \end{table*}









% \begin{center}
% \begin{table*}[h!]
% \centering
% \footnotesize
% % \scriptsize
% \scalebox{0.85}{
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% Category & \begin{tabular}[c]{@{}c@{}}Prompting Technique \end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluated \\ LLM(s) \end{tabular} & Dataset(s) & \begin{tabular}[c]{@{}c@{}}Task(s) \end{tabular} \\
% \hline

%  & Chain-of-Note~\cite{yu2023chainofnote} & \begin{tabular}[c]{@{}c@{}} Llama 2,\\ DPR \end{tabular} & \begin{tabular}[c]{@{}c@{}} NQ,\\ TriviaQA, \\WebQ \end{tabular} &  \\ \cline{2-5}
% \end{tabular}
% }
% \caption{A comparative summary of the literature and the proposed model}
% \label{tab:comparison}
% \end{table*}
% \end{center}

% \subsubsection{Appendices}
% You may move some of the contents of the paper into one or more appendices that appear after the main content, but before references. These appendices count towards the page limit and are distinct from the supplementary material that can be submitted separately through CMT. Such appendices are useful if you would like to include highly technical material (such as a lengthy calculation) that will disrupt the flow of the paper. They can be included both in papers submitted for review and in camera-ready versions; in the latter case, they will be included in the proceedings (whereas the supplementary materials will not be included in the proceedings).
% Appendices are optional. Appendices must appear after the main content.
% Appendix sections must use letters instead of Arabic numerals. In \LaTeX,  you can use the {\tt \textbackslash{}appendix} command to achieve this followed by  {\tt \textbackslash section\{Appendix\}} for your appendix sections.

% \subsubsection{Ethical Statement}

% Ethical Statement is optional. You may include an Ethical Statement to discuss  the ethical aspects and implications of your research. The section should be titled \emph{Ethical Statement} and be typeset like any regular section but without being numbered. This section may be placed on the References pages.

% Use
% \begin{quote}
%     {\tt \textbackslash{}section*\{Ethical Statement\}}
% \end{quote}

% \subsubsection{Acknowledgements}

% Acknowledgements are optional. In the camera-ready version you may include an unnumbered acknowledgments section, including acknowledgments of help from colleagues, financial support, and permission to publish. This is not allowed in the anonymous submission. If present, acknowledgements must be in a dedicated, unnumbered section appearing after all regular sections but before references.  This section may be placed on the References pages.

% Use
% \begin{quote}
%     {\tt \textbackslash{}section*\{Acknowledgements\}}
% \end{quote}
% to typeset the acknowledgements section in \LaTeX{}.


% \subsubsection{Contribution Statement}

% Contribution Statement is optional. In the camera-ready version you may include an unnumbered Contribution Statement section, explicitly describing the contribution of each of the co-authors to the paper. This is not allowed in the anonymous submission. If present, Contribution Statement must be in a dedicated, unnumbered section appearing after all regular sections but before references.  This section may be placed on the References pages.

% Use
% \begin{quote}
%     {\tt \textbackslash{}section*\{Contribution Statement\}}
% \end{quote}
% to typeset the Contribution Statement section in \LaTeX{}.

% \subsubsection{References}

% The references section is headed ``References'', printed in the same
% style as a section heading but without a number. A sample list of
% references is given at the end of these instructions. Use a consistent
% format for references. The reference list should not include publicly unavailable work.

% \subsubsection{Order of Sections}
% Sections should be arranged in the following order:
% \begin{enumerate}
%     \item Main content sections (numbered)
%     \item Appendices (optional, numbered using capital letters)
%     \item Ethical statement (optional, unnumbered)
%     \item Acknowledgements (optional, unnumbered)
%     \item Contribution statement (optional, unnumbered)
%     \item References (required, unnumbered)
% \end{enumerate}

% \subsection{Citations}

% Citations within the text should include the author's last name and
% the year of publication, for example~\cite{gottlob:nonmon}.  Append
% lowercase letters to the year in cases of ambiguity.  Treat multiple
% authors as in the following examples:~\cite{abelson-et-al:scheme}
% or~\cite{bgf:Lixto} (for more than two authors) and
% \cite{brachman-schmolze:kl-one} (for two authors).  If the author
% portion of a citation is obvious, omit it, e.g.,
% Nebel~\shortcite{nebel:jair-2000}.  Collapse multiple citations as
% follows:~\cite{gls:hypertrees,levesque:functional-foundations}.
% \nocite{abelson-et-al:scheme}
% \nocite{bgf:Lixto}
% \nocite{brachman-schmolze:kl-one}
% \nocite{gottlob:nonmon}
% \nocite{gls:hypertrees}
% \nocite{levesque:functional-foundations}
% \nocite{levesque:belief}
% \nocite{nebel:jair-2000}

% \subsection{Footnotes}

% Place footnotes at the bottom of the page in a 9-point font.  Refer to
% them with superscript numbers.\footnote{This is how your footnotes
%     should appear.} Separate them from the text by a short
% line.\footnote{Note the line separating these footnotes from the
%     text.} Avoid footnotes as much as possible; they interrupt the flow of
% the text.

% \section{Illustrations}

% Place all illustrations (figures, drawings, tables, and photographs)
% throughout the paper at the places where they are first discussed,
% rather than at the end of the paper.

% They should be floated to the top (preferred) or bottom of the page,
% unless they are an integral part
% of your narrative flow. When placed at the bottom or top of
% a page, illustrations may run across both columns, but not when they
% appear inline.

% Illustrations must be rendered electronically or scanned and placed
% directly in your document. They should be cropped outside \LaTeX{},
% otherwise portions of the image could reappear during the post-processing of your paper.
% When possible, generate your illustrations in a vector format.
% When using bitmaps, please use 300dpi resolution at least.
% All illustrations should be understandable when printed in black and
% white, albeit you can use colors to enhance them. Line weights should
% be 1/2-point or thicker. Avoid screens and superimposing type on
% patterns, as these effects may not reproduce well.

% Number illustrations sequentially. Use references of the following
% form: Figure 1, Table 2, etc. Place illustration numbers and captions
% under illustrations. Leave a margin of 1/4-inch around the area
% covered by the illustration and caption.  Use 9-point type for
% captions, labels, and other text in illustrations. Captions should always appear below the illustration.

% \section{Tables}

% Tables are treated as illustrations containing data. Therefore, they should also appear floated to the top (preferably) or bottom of the page, and with the captions below them.

% \begin{table}
%     \centering
%     \begin{tabular}{lll}
%         \hline
%         Scenario  & $\delta$ & Runtime \\
%         \hline
%         Paris     & 0.1s     & 13.65ms \\
%         Paris     & 0.2s     & 0.01ms  \\
%         New York  & 0.1s     & 92.50ms \\
%         Singapore & 0.1s     & 33.33ms \\
%         Singapore & 0.2s     & 23.01ms \\
%         \hline
%     \end{tabular}
%     \caption{Latex default table}
%     \label{tab:plain}
% \end{table}

% \begin{table}
%     \centering
%     \begin{tabular}{lrr}
%         \toprule
%         Scenario  & $\delta$ (s) & Runtime (ms) \\
%         \midrule
%         Paris     & 0.1          & 13.65        \\
%                   & 0.2          & 0.01         \\
%         New York  & 0.1          & 92.50        \\
%         Singapore & 0.1          & 33.33        \\
%                   & 0.2          & 23.01        \\
%         \bottomrule
%     \end{tabular}
%     \caption{Booktabs table}
%     \label{tab:booktabs}
% \end{table}

% If you are using \LaTeX, you should use the {\tt booktabs} package, because it produces tables that are better than the standard ones. Compare Tables~\ref{tab:plain} and~\ref{tab:booktabs}. The latter is clearly more readable for three reasons:

% \begin{enumerate}
%     \item The styling is better thanks to using the {\tt booktabs} rulers instead of the default ones.
%     \item Numeric columns are right-aligned, making it easier to compare the numbers. Make sure to also right-align the corresponding headers, and to use the same precision for all numbers.
%     \item We avoid unnecessary repetition, both between lines (no need to repeat the scenario name in this case) as well as in the content (units can be shown in the column header).
% \end{enumerate}

% \section{Formulas}

% IJCAI's two-column format makes it difficult to typeset long formulas. A usual temptation is to reduce the size of the formula by using the {\tt small} or {\tt tiny} sizes. This doesn't work correctly with the current \LaTeX{} versions, breaking the line spacing of the preceding paragraphs and title, as well as the equation number sizes. The following equation demonstrates the effects (notice that this entire paragraph looks badly formatted, and the line numbers no longer match the text):
% %
% \begin{tiny}
%     \begin{equation}
%         x = \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i
%     \end{equation}
% \end{tiny}%

% Reducing formula sizes this way is strictly forbidden. We {\bf strongly} recommend authors to split formulas in multiple lines when they don't fit in a single line. This is the easiest approach to typeset those formulas and provides the most readable output%
% %
% \begin{align}
%     x = & \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \nonumber \\
%     +   & \prod_{i=1}^n \sum_{j=1}^n j_i.
% \end{align}%

% If a line is just slightly longer than the column width, you may use the {\tt resizebox} environment on that equation. The result looks better and doesn't interfere with the paragraph's line spacing: %
% \begin{equation}
%     \resizebox{.91\linewidth}{!}{$
%             \displaystyle
%             x = \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i
%         $}.
% \end{equation}%

% This last solution may have to be adapted if you use different equation environments, but it can generally be made to work. Please notice that in any case:

% \begin{itemize}
%     \item Equation numbers must be in the same font and size as the main text (10pt).
%     \item Your formula's main symbols should not be smaller than {\small small} text (9pt).
% \end{itemize}

% For instance, the formula
% %
% \begin{equation}
%     \resizebox{.91\linewidth}{!}{$
%             \displaystyle
%             x = \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j
%         $}
% \end{equation}
% %
% would not be acceptable because the text is too small.

% \section{Examples, Definitions, Theorems and Similar}

% Examples, definitions, theorems, corollaries and similar must be written in their own paragraph. The paragraph must be separated by at least 2pt and no more than 5pt from the preceding and succeeding paragraphs. They must begin with the kind of item written in 10pt bold font followed by their number (e.g.: {\bf Theorem 1}),
% optionally followed by a title/summary between parentheses in non-bold font and ended with a period (in bold).
% After that the main body of the item follows, written in 10 pt italics font (see below for examples).

% In \LaTeX{} we strongly recommend that you define environments for your examples, definitions, propositions, lemmas, corollaries and similar. This can be done in your \LaTeX{} preamble using \texttt{\textbackslash{newtheorem}} -- see the source of this document for examples. Numbering for these items must be global, not per-section (e.g.: Theorem 1 instead of Theorem 6.1).

% \begin{example}[How to write an example]
%     Examples should be written using the example environment defined in this template.
% \end{example}

% \begin{theorem}
%     This is an example of an untitled theorem.
% \end{theorem}

% You may also include a title or description using these environments as shown in the following theorem.

% \begin{theorem}[A titled theorem]
%     This is an example of a titled theorem.
% \end{theorem}

% \section{Proofs}

% Proofs must be written in their own paragraph(s) separated by at least 2pt and no more than 5pt from the preceding and succeeding paragraphs. Proof paragraphs should start with the keyword ``Proof." in 10pt italics font. After that the proof follows in regular 10pt font. At the end of the proof, an unfilled square symbol (qed) marks the end of the proof.

% In \LaTeX{} proofs should be typeset using the \texttt{\textbackslash{proof}} environment.

% \begin{proof}
%     This paragraph is an example of how a proof looks like using the \texttt{\textbackslash{proof}} environment.
% \end{proof}


% \section{Algorithms and Listings}

% Algorithms and listings are a special kind of figures. Like all illustrations, they should appear floated to the top (preferably) or bottom of the page. However, their caption should appear in the header, left-justified and enclosed between horizontal lines, as shown in Algorithm~\ref{alg:algorithm}. The algorithm body should be terminated with another horizontal line. It is up to the authors to decide whether to show line numbers or not, how to format comments, etc.

% In \LaTeX{} algorithms may be typeset using the {\tt algorithm} and {\tt algorithmic} packages, but you can also use one of the many other packages for the task.

% \begin{algorithm}[tb]
%     \caption{Example algorithm}
%     \label{alg:algorithm}
%     \textbf{Input}: Your algorithm's input\\
%     \textbf{Parameter}: Optional list of parameters\\
%     \textbf{Output}: Your algorithm's output
%     \begin{algorithmic}[1] %[1] enables line numbers
%         \STATE Let $t=0$.
%         \WHILE{condition}
%         \STATE Do some action.
%         \IF {conditional}
%         \STATE Perform task A.
%         \ELSE
%         \STATE Perform task B.
%         \ENDIF
%         \ENDWHILE
%         \STATE \textbf{return} solution
%     \end{algorithmic}
% \end{algorithm}

% \section{\LaTeX{} and Word Style Files}\label{stylefiles}

% The \LaTeX{} and Word style files are available on the IJCAI--24
% website, \url{https://ijcai-24.org/}.
% These style files implement the formatting instructions in this
% document.

% The \LaTeX{} files are {\tt ijcai24.sty} and {\tt ijcai24.tex}, and
% the Bib\TeX{} files are {\tt named.bst} and {\tt ijcai24.bib}. The
% \LaTeX{} style file is for version 2e of \LaTeX{}, and the Bib\TeX{}
% style file is for version 0.99c of Bib\TeX{} ({\em not} version
% 0.98i). .

% The Microsoft Word style file consists of a single file, {\tt
%         ijcai24.docx}. This template differs from the one used for
% IJCAI--23.

% These Microsoft Word and \LaTeX{} files contain the source of the
% present document and may serve as a formatting sample.

% Further information on using these styles for the preparation of
% papers for IJCAI--24 can be obtained by contacting {\tt
%         proceedings@ijcai.org}.

% \appendix

% \section*{Ethical Statement}

% There are no ethical issues.

% \section*{Acknowledgments}

% The preparation of these instructions and the \LaTeX{} and Bib\TeX{}
% files that implement them was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Preparation of the Microsoft Word file was supported by IJCAI.  An
% early version of this document was created by Shirley Jowell and Peter
% F. Patel-Schneider.  It was subsequently modified by Jennifer
% Ballentine, Thomas Dean, Bernhard Nebel, Daniel Pagenstecher,
% Kurt Steinkraus, Toby Walsh, Carles Sierra, Marc Pujol-Gonzalez,
% Francisco Cruz-Mencia and Edith Elkind.


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai24}

\end{document}

