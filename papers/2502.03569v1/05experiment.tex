
\subsection{Datasets}

\name is evaluated on datasets and tasks in the biological and medical domains: cellular reprogramming experiments~(Figure~\ref{fig:data}a) and patient routine laboratory tests~(Figure~\ref{fig:data}b).

\xhdr{Cellular developmental trajectories} We introduce a novel benchmarking dataset, WOT. It is constructed using the Waddington-OT model, which simulates single-cell transcriptomic profiles of developmental time courses for individual cells \cite{schiebinger2019-ie}~(Figure~\ref{fig:data}a; Table~\ref{tab:data}). We also construct a paired counterfactual benchmarking dataset, WOT-CF (Table~\ref{tab:data}). We obtain condition embeddings of the activated transcription factors from ESM-2~\cite{lin2022language}. Refer to Appendix~\ref{appendix:cells} for further details.


\xhdr{Patient lab test trajectories}
%
We construct two real-world patient datasets of routine laboratory tests from eICU~\cite{pollard2018eicu} and MIMIC-IV~\cite{johnson2024mimic, johnson2023mimic, goldberger2000physiobank}~(Figure~\ref{fig:data}b; Table~\ref{tab:data}). In addition to a random split, we construct data splits with different levels of train/test split similarities using SPECTRA~\cite{ektefaie2024evaluating} to evaluate model generalizability~(Appendix Figure~\ref{fig:supp_spectra_cso}). For condition embeddings, we leverage pretrained embeddings of clinical codes from a clinical knowledge graph that integrates six existing databases of clinical vocabularies used in electronic health records~\cite{johnson2024unified}. Refer to Appendix~\ref{appendix:labs} for more details. 



\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{FIG/figure_domains.pdf}}
\caption{\name is evaluated on two real-world domains involving multivariate trajectories: \textbf{(a)}~cellular development and \textbf{(b)}~patient health. \textbf{(a)}~To study cellular development, fibroblast cells derived from mice can be artificially reprogrammed into various other cell states \textit{in vitro}. A cell's state is defined by its gene expression. Throughout reprogramming, a cell activates transcription factor (TF) genes at different time points to change its gene expression, thereby influencing its developmental trajectory. In this illustration, a mouse fibroblast is being reprogrammed over the span of 20 days~(D0-D20); color and shape represent cell state. On day 8, if the cell activates the Obox6 TF, the cell is on the path toward becoming an induced pluripotent stem cell (iPSC); whereas if it activates the Neurod4 TF, it is on the path toward becoming a neuron or astrocyte. \textbf{(b)}~The health of a human patient is often monitored through lab tests (e.g. blood sodium level, white blood cell count). The history of lab results across multiple patient visits (V1-V9) as well as candidate clinical interventions (e.g.,~medication) can be used to infer the most likely future trajectory of the patient's health. Illustrations from NIAID NIH BIOART Source~(see References). \nocite{fibroblast1,fibroblast2, fibroblast3, astrocyte, progenitor, petri, generic_immune, cajal, neuron, unidentified_offtarget, mouse, patient, syringe, wbc, hemoglobin, vial}}
\label{fig:data}
\end{center}
\vskip -0.35in
\end{figure}



\begin{table}[ht]
\caption{\textbf{Dataset statistics.} We construct three core datasets: WOT~(cellular developmental trajectories), eICU~(patient lab tests), and MIMIC-IV~(patient lab tests). We also construct a paired counterfactual cellular trajectories dataset, WOT-CF. $N$ is the number of sequences (i.e.,~cellular developmental trajectories, patient lab test trajectories), $V$ is the number of measured variables (i.e.,~gene expression, lab test), and $L$ is the length of the sequences.}
\label{tab:data}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}
{lcccc}
\toprule
\textbf{Dataset}  & $N$ & $V$ & \textbf{Mean $L$} & \textbf{Max $L$} \\
\midrule
WOT      & $3,000$ & $1,480$ & $27.03 \pm 6.04$ & $37$ \\
WOT-CF   & $2,546$ & $1,480$ & $27.01 \pm 5.98$ & $37$ \\
eICU     & $108,346$ & $17$ & $20.27 \pm 25.23$ & $858$ \\
MIMIC-IV & $156,310$ & $16$ & $15.56 \pm 24.43$ & $949$ \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.15in
\end{table}




\subsection{Setup}


\xhdr{Metrics}
%
We use standard metrics (MAE, RMSE, and $R^2$) to quantify sequence editing performance.

\xhdr{Baselines}
%
We evaluate \name against a traditional multivariate time series algorithm, Vector Autoregression~(VAR) model~\cite{lutkepohl2005new}. As \name can leverage any type of sequence encoder, we benchmark against the state-of-the-art condition-guided counterfactual sequence generation setup with different sequential data encoders: Transformer~\cite{vaswani2017attention, narasimhan2024time, jing2024towards, zhang2023survey} and xLSTM~\cite{beck2024xlstm}. We further evaluate \name against a state-of-the-art time series foundation model, MOMENT~\cite{goswami2024moment}; specifically, we finetune an adapter for the $1024$-dimensional embeddings generated by the frozen \texttt{MOMENT-1-large} embedding model. 


\xhdr{Ablations}
%
To investigate the effectiveness of the learned temporal concepts, we evaluate against an ablated model, SimpleLinear, in which temporal concepts are simply all ones; in other words, temporal concepts are not learned nor meaningful. This ablation is inspired by traditional linear models that excel when $\mathbf{x}_{t_j} \simeq \mathbf{x}_{t_i}$~\cite{toner2024analysis, ahlmann2024deep}. We also evaluate different versions of \name with and without an FFN layer in the concept encoder~$E$~(Appendix~\ref{appendix:figures}).

\xhdr{Implementation details}
%
Models are trained on a single NVIDIA A100 or H100 GPU. All models have comparable number of parameters as their \name-based counterparts. Refer to Appendix~\ref{appendix:implementation} for details and hyperparameter selection.
