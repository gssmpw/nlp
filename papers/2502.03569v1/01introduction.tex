\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{FIG/figure1.pdf}}
\caption{Illustrative comparison of \textbf{(a)} \name's controllable sequence editing and \textbf{(b)} existing sequence editing approaches. Unlike existing methods, controllable sequence editing generates counterfactual sequences (dotted lines) while preserving historical data to model the immediate or delayed effects of interventions.}
\label{fig:problem}
\end{center}
\vskip -0.35in
\end{figure}


Counterfactual thinking is a fundamental objective in biology and medicine~\cite{lee2024clinical}. ``What if" scenarios are critical for reasoning about the underlying mechanisms of a cell, patient, disease, and drug, and each decision can have tangible impact~\cite{bunne2024build, lee2024clinical}: \textit{What if we treat the cells with the candidate drug every hour or every 24 hours? What if we perform the surgery on the patient today or next year?} We should not only reason about the choice of the counterfactual condition (e.g.,~drug, surgery), but also its timing (e.g.,~when and how frequent). Thus, counterfactual generation requires precise and context-specific edits that adhere to temporal and structural constraints. For example, prescribing a medication to a patient should result in changes to the patient's trajectory only after the intervention time (i.e.,~the medical history prior to intervention should be unaffected to preserve temporal causality) and on only the relevant variables that are specific to the context of the intervention (i.e.,~the measurements unaffected by the intervention should be preserved).



Generative models in the language and vision domains enable precise editing guided by a description, such as textual prompts or condition tokens~\cite{zhang2023adding, gao2023editanything, ravi2024sam2, gong2024text, niu2024mitigating, gu2024token, zhou2024virtual}. These models are designed to gain more comprehensive~(i.e.,~global) and precise (i.e.,~local) control over the generation of text~\cite{chatzi2024counterfactual, niu2024mitigating, gu2024token, zhou2024virtual}, images~\cite{zhang2023adding, gao2023editanything, ravi2024sam2}, and even molecular structures~\cite{gong2024text, dauparas2022robust, zhang2024efficient}. Their outputs are expected to preserve the global integrity of the input while making precise local edits to satisfy the desired condition. Analogous to these models' consideration of spatial context to edit images~\cite{zhang2023adding, gao2023editanything} and protein pockets~\cite{dauparas2022robust, zhang2024efficient} via in-painting, our work leverages temporal context to perform precise editing on sequences. 



Controllable text generation (CTG) approaches, designed specifically to edit natural language sequences, have been extensively studied~\cite{zhang2023survey}. They excel in \textit{immediate sequence editing}:~predicting the next token or readout in the sequence under a counterfactual condition~\cite{niu2024mitigating, gu2024token, zhou2024virtual, chatzi2024counterfactual, zhang2023survey, bhattacharjee2024zero}. For example, if asked to predict the next word in the sentence ``Once upon a time, there lived a boy" under the counterfactual condition that the genre is horror, a CTG model may respond with ``alone" to convey vulnerability and loneliness. However, CTG models are unable to perform \textit{delayed sequence editing}: predicting a counterfactual trajectory at a future time step while maintaining causal consistency. For example, if asked to predict what would happen to the boy if he took a trip to New York City as an adult, a CTG model would struggle due to the multitude of possible answers. In the existing paradigm of CTG models, they cannot effectively utilize the given context to skip ahead to the future; instead, CTG models would need to be run repeatedly to fill in the temporal gap without any guarantee of ever satisfying the desired condition. As a result, CTG models are insufficient for other types of sequences for which both immediate and delayed sequence editing are necessary, such as cellular reprogramming and patient immune dynamics.


There exist two controllable time series generation approaches~\cite{jing2024towards, bao2024towards}, which utilize diffusion modeling to generate counterfactual time series. However, they are limited to univariate sequences and assume that the entire input sequence is affected~\cite{jing2024towards, bao2024towards}. These methods are thus insufficient in settings where edits are only allowed after time $t$ (i.e.,~cannot change historical data) and affect only certain sequences (i.e.,~preserve unaffected co-occurring sequences). In other words, they are unable to make precise local edits while preserving global causal consistency.



\xhdr{Present work}
%
We tackle the gaps in controllable sequence editing to enable \textit{temporally localized modifications at any time step while ensuring the adherence to temporal causality and the consistency of intrinsic dependencies within and across sequences}~(Figure~\ref{fig:problem}). Controllable sequence editing is a particularly complex task because it necessitates learning both the temporal dynamics in the sequences and the relationships between the desired condition and the sequences. The latter is most often unknown, hindering the model's ability to determine (1)~which sequences are affected by the condition, and (2)~when and how the sequences are affected based on the historical data and the guiding condition. These challenges prevent sequence editing approaches from generating precise and context-specific edits while preserving temporal and structural constraints. Further, the resulting condition-guided counterfactual sequences may not resemble the distribution of observed sequences. 



We develop \name (\longname), a controllable sequence editing approach for instance-wise counterfactual generation. \name learns temporal concepts that represent the trajectories of the sequences to enable accurate counterfactual generation guided by a given condition. We show that the learned temporal concepts help preserve temporal and structural constraints in the generated outputs. By design, \name is flexible with any type of sequential data encoder. We demonstrate through comprehensive experiments on four novel benchmark datasets in cellular reprogramming and patient immune dynamics that \name outperforms state-of-the-art models by up to $36.01\%$ and $65.71\%$ (MAE) on immediate and delayed sequence editing, respectively. We also show that any pretrained sequence encoder can gain controllable sequence editing capabilities when finetuned with \name. Moreover, \name outperforms baselines in zero-shot counterfactual generation of cellular trajectories by up to $14.45\%$ and $63.19\%$ (MAE) on immediate and delayed sequence editing, respectively. Further, precise edits via user interaction can be performed directly on \name's learned concepts. We demonstrate through real-world case studies that \name, given precise edits on specific temporal concepts, can generate realistic ``healthy" counterfactual trajectories for patients originally with type 1 diabetes mellitus.



\xhdr{Our contributions are threefold} (1)~We formalize controllable sequence editing for temporally localized modification of biomedical sequences at any time step given a desired condition while ensuring temporal causality. (2)~\name is a novel controllable sequence editing model for counterfactual generation. (3)~We release four datasets on cellular reprogramming and patient immune dynamics, and demonstrate the efficacy of \name on immediate and delayed sequence editing on cellular and patient trajectories.


