
\section{Data \& Experimental Setup}

In this section, we provide further details about data construction, data preparation, and experimental setup. We share code and instructions in our GitHub repository to reproduce the experiments in this paper: \url{https://github.com/mims-harvard/CLEF}.

\subsection{Cellular Developmental Trajectories}\label{appendix:cells}

Here, we describe the process of (1) simulating single-cell transcriptomic profiles of developmental time courses for individual cells and (2) preparing these trajectories for modeling.

\subsubsection{Simulating trajectories}

Cellular reprogramming experiments help elucidate cellular development~\cite{schiebinger2019-ie}. In these experiments, cells are manipulated and allowed to progress for a specific period of time before they undergo RNA sequencing~(RNA-seq), and we analyze the resulting RNA-seq data to observe their new cellular profiles~\cite{schiebinger2019-ie}. RNA-seq is a destructive process for the cell, meaning that the same cell cannot be sequenced at two different time points. Computational models are thus necessary to infer the trajectory of a cell.


\xhdr{Waddington-OT dataset and model}
%
Waddington-OT~\cite{schiebinger2019-ie} is a popular approach to reconstruct the landscape of cellular reprogramming using optimal transport~(OT). There are two components in Waddington-OT: (1)~\textit{a single-cell RNA-seq (scRNA-seq) dataset} of mouse cells from a reprogramming experiment, and (2)~\textit{an OT-based trajectory inference model} fitted on the scRNA-seq dataset. The scRNA-seq dataset consists of 251,203 mouse cells profiled from 37 time points (0.5-day intervals) during an 18-day reprogramming experiment starting from mouse embryonic fibroblasts. The trajectory inference model consists of transport matrices $\pi_{t_k,t_{k+1}}$ with dimensions $N \times M$ that relate all cells $\mathbf{x}^1_{t_k}, ..., \mathbf{x}^n_{t_k}$ profiled at time $t_k$ to all cells $\mathbf{x}^1_{t_{k+1}}, ..., \mathbf{x}^m_{t_{k+1}}$ profiled at time $t_{k+1}$. An entry at row $i$ and column $j$ of $\pi_{t_k,t_{k+1}}$ corresponds to the probability that $\mathbf{x}^j_{t_{k+1}}$ is a descendant cell of $\mathbf{x}^i_{t_k}$, as determined using optimal transport \cite{chizat2017scalingalgorithm}. Every cell in the scRNA-seq dataset is either pre-labeled as one of the 13 provided cell sets (i.e.,~induced pluripotent stem, stromal, epithelial, mesenchymal-epithelial transition, trophoblast, spongiotrophoblast, trophoblast progenitor, oligodenrocyte progenitor, neuron, radial glial, spiral artery trophoblast giant, astrocyte, other neural) or unlabeled. We cluster the unlabeled cells using Leiden clustering via \texttt{scanpy}~\cite{wolf2018-tb} at a resolution of 1, and define the resulting 27 unlabeled clusters as unique cell sets. As a result, each cell in the dataset belongs one and only one cell set. 


\xhdr{Simulating cell state trajectories} 
%
We define ``cell state" as the transcriptomic profile of a cell. Here, a transcriptomic profile is the log-normalized RNA-seq counts of the top $1,479$ most highly variable genes. To create a simulated trajectory of cell states for an individual cell undergoing reprogramming, we randomly and uniformly sample a cell profiled at time step $t_0$ (Day $0.0$) from the Waddington-OT scRNA-seq dataset, and generate via the transport matrix $\pi_{t_0,t_1}$ a probability distribution $\mathbb{P}_{t_1}$ over possible descendant cells $\mathbf{x}^1_{t_1}, \dots, \mathbf{x}^m_{t_1}$ at time step $t_1$ (Day $0.5$). We sample a cell from this distribution, and repeat the process until we reach either Day $18.0$ or a terminal state (i.e.,~neural, stromal, or induced pluripotent stem cell). After generating a trajectory composed of cells from the Waddington-OT scRNA-seq dataset through this process, we retrieve the transcriptomic profile of each cell to compose $\mathbf{x}_{:,t_0:t_T}$, where $T$ is the length of the trajectory.


\xhdr{Inferring conditions}
%
A condition $s_{t_i}$ is defined as the activation of a transcription factor~(TF) that leads a cell to transition from state $\mathbf{x}_{t_i}$ to descendant state $\mathbf{x}_{t_{i+1}}$. To infer such conditions, we perform differential expression analysis between cells from the same cell set as $\mathbf{x}_{t_i}$ (i.e.,~$\mathbf{x}^a \in A$) and cells from the same cell set as $\mathbf{x}_{t_{i+1}}$ (i.e.,~$\mathbf{x}^b \in B$). Using the $\texttt{wot.tmap.diff\_exp}$ function (via the \texttt{Waddington-OT} library), we identify the top TF that was significantly upregulated in $\mathbf{x}^a \in A$ compared to $\mathbf{x}^b \in B$. If no TFs are differentially expressed, then the condition is ``None." We retroactively perform this analysis on all pairs of consecutive cell states in a cell state trajectory $\mathbf{x}_{:,t_0:t_T}$ to obtain the full trajectory containing both cell states and TF conditions: $\tau = \{\mathbf{x}_{t_0}, s_{t_0}, \mathbf{x}_{t_1}, s_{t_1}, \cdots, s_{t_{T-1}}, \mathbf{x}_{t_T} \}$. In other words, $\tau$ represents a simulated trajectory of an individual cell undergoing the reprogramming process. Condition embeddings $\mathbf{z}_s \in \mathbb{R}^{5120}$ are obtained from the (frozen) pretrained ESM-2 embedding model~\cite{lin2022language}.

\xhdr{Generating matched counterfactual trajectories}
%
We additionally create pairs of matched counterfactual trajectories to evaluate a model's performance in zero-shot counterfactual generation. Each pair consists of an ``original" trajectory~$\tau_{og}$ and a ``counterfactual" trajectory~$\tau_{cf}$. First, we generate $\tau_{og}$ using the Waddington-OT model. Then, given a divergence time step~$D$, the first $D$ time steps of $\tau_{og}$ are carried over to $\tau_{cf}$ such that the first $D$ cell states and conditions of $\tau_{og}$ and $\tau_{cf}$ are exactly the same. The remaining states and conditions of $\tau_{cf}$ are sampled independently from $\tau_{og}$, resulting in an alternative future trajectory based on an alternative condition at time step~$D$. 

\textbf{Implementation note:} Because \name learns time embeddings based on the year, month, date, and hour of a given timestamp, we convert the time steps of each cell into timestamps. We set the starting time $t_0$ as timestamp \texttt{2000/01/01 00:00:00}, and add $10\times t_i$ hours to the converted timestamp of $t_{i-1}$.





\subsubsection{Experimental setup}

\xhdr{Generating data splits}
%
There are three cell sets (i.e.,~groups of cells with the same cell state label) that consist of cells from Day 0.0 in our post-clustering version of the Waddington-OT dataset. We refer to these cell sets as ``start clusters" because all initial cell states are sampled from one of these cell sets. Since the choice of start cluster can influence the likelihood of a cell's trajectory reaching certain terminal fates, we split our cellular trajectories into train, validation, and test sets based on their start cluster. This cell-centric data split allows us to evaluate how well a model can generalize to different distributions of trajectories. Start cluster \#1 is in the train set, start cluster \#3 is in the validation set, and start cluster \#2 is in the test set.


\xhdr{Zero-shot counterfactual generation}
%
The data split for zero-shot counterfactual generation is constructed such that the original trajectories $\tau_{og}$ are in the train or validation sets, and the counterfactual trajectories $\tau_{cf}$ are in the test set.




\subsection{Patient Lab Tests}\label{appendix:labs}

Here, we describe the process of (1) preprocessing electronic health records to extract longitudinal routine lab tests data and (2) preparing these trajectories for modeling.



\subsubsection{Constructing routine lab test trajectories}

We leverage two publicly available medical datasets: eICU~\cite{pollard2018eicu} and MIMIC-IV~\cite{johnson2024mimic, johnson2023mimic, goldberger2000physiobank}. Both datasets are under the PhysioNet Credentialed Health Data License 1.5.0 \cite{mimiciv_license}. The retrieval process includes registering as a credentialed user on PhysioNet, completing the CITI ``Data or Specimens Only Research" training, and signing the necessary data use agreements.

\xhdr{Processing patient datasets}
%
We process each dataset (i.e.,~eICU, MIMIC-IV) separately with the following steps. First, we extract the routine lab tests only (annotation available only in MIMIC-IV) and the most commonly ordered lab tests~(i.e.,~lab tests that appear in at least 80\% of patients). Next, we keep patients for whom we have at least one of each lab test. If there are multiple measurements of a lab test at the same time step~(i.e.,~year, month, date, hour, minute, and seconds), we take the mean of its values. We extract patients with more than one visit (or time step).

We define patients' conditions as medical codes, specifically International Classification of Diseases~(ICD)\nocite{icd}, of their diagnosis. Both eICU and MIMIC-IV use ICD-9 and ICD-10 codes. We extract the medical codes and their timestamps (multiple medical codes at a single time step is possible). Since the timestamps of diagnostic codes and lab tests are not necessarily the same (and there are fewer entries of diagnostic codes than lab orders), we merge them with a tolerance range of 12 hours (eICU) or two days (MIMIC-IV). We obtain (frozen) condition embeddings $\mathbf{z}_s \in \mathbb{R}^{128}$ (retrieved on~December~22,~2024) from an embedding model that has been pretrained on a clinical knowledge graph~\cite{johnson2024unified}. The clinical knowledge graph is constructed by integrating six existing databases of clinical vocabularies used in electronic health records: International Classification of Diseases~(ICD), Anatomical Therapeutic Chemical~(ATC) Classification, Systemized Nomenclature of Medicine - Clinical Terms~(SNOMED CT), Current Procedural Terminology~(CPT), Logical Observation Identifiers Names and Codes~(LOINC), and phecodes~\cite{johnson2024unified}.

\subsubsection{Generating data splits}

We generate a standard patient-centric random split for benchmarking model performance, and a series of increasingly challenging data splits via SPECTRA~\cite{ektefaie2024evaluating} to evaluate model generalizability.

\xhdr{Constructing data splits to evaluate model generalizability}
%
SPECTRA~\cite{ektefaie2024evaluating} creates a series of splits with decreasing cross-split overlap or similarity between the train and test sets. By training and testing models on these splits, we can assess model performance as a function of cross-split overlap. SPECTRA refers to this relationship as the spectral performance curve, which provides insight into how well a model generalizes to less similar data. When a new dataset split is encountered, it can be plotted as a point on this curve. The area under the spectral performance curve (AUSPC) serves as a metric of model generalizability and enables comparisons across models.

To generate a split with SPECTRA, a similarity definition and a SPECTRA parameter (SP) value between 0 and 1 are required. SP controls the level of cross-split overlap: values closer to 0 create splits resembling classical random splits, while values closer to 1 produce stricter splits with minimal or no overlap between train and test sets. For example, at an input of 1, no similar samples are shared between the train and test sets.

For eICU and MIMIC-IV, we define two patients as similar if: (1)~they are of the same gender, (2)~they are born in the same decade, and (3)~they share at least one ICD-9 or ICD-10 category. We exclude ICD-9 and ICD-10 codes that are present in more than 50\% of patients to avoid overly generic features. SPECTRA systematically prunes similar patients to produce splits. For this study, we generate 20 splits with SP values that are evenly spaced between 0 and 1. Given a train and test set, cross-split overlap is defined as the proportion of samples in the train set that are similar to at least one sample in the test set.


\subsubsection{Constructing cohorts of patients with type 1 diabetes mellitus}

To define a type 1 diabetes mellitus (T1D) patient cohort in eICU and MIMIC-IV, we identify patients with T1D and matched healthy individuals. A patient has T1D if the ICD-10 code~\texttt{E10} (or the equivalent ICD-9 code~\texttt{250}) is present in the electronic health records. Matched healthy patients are defined by three criteria. First, the patient must not contain any of the following ICD-10 (and ICD-9 equivalent) codes: \texttt{E11}, \texttt{E13}, \texttt{E12}, \texttt{E08}, \texttt{E09}, \texttt{R73}, and \texttt{O24}. An initial healthy patient cohort is constructed using these filtering codes. Next, we identify frequently co-occurring ICD codes between the initial set of patients and patients with T1D to filter out generic ICD codes (threshold $= 20$). Finally, healthy patients are matched with a T1D patient if: they are of the same gender, they are born in the same decade, and they share at least 50\% of ICD codes.



\section{Further Implementation Details}\label{appendix:implementation}

We provide code and instructions in our GitHub repository to implement \name, baselines, and ablations: \url{https://github.com/mims-harvard/CLEF}. For the implementation of baselines, we followed the authors' recommendations on model design and hyperparameter selection from the original publications.


\subsection{Hyperparameter Sweep}

For all models trained from scratch, the selection of hyperparameters are: dropout rate~$\in [0.3, 0.4, 0.5, 0.6]$, learning rate~$\in [0.001, 0.0001, 0.00001]$, and number of layers (or blocks in xLSTM) $\in [4, 8]$. As the number of heads must be divisible by the number of features, the number of heads for eICU (18 lab tests)~$\in [2, 3, 6, 9]$ and for others~$\in [4, 8]$. For xLSTM, additional hyperparameters are: 1D-convolution kernel size $\in [4, 5, 6]$ and QVK projection layer block size $\in [4, 8]$.


\subsection{Best Hyperparameters}

\xhdr{MIMIC-IV dataset} The best hyperparameters for models trained on the MIMIC-IV dataset are: dropout rate~$= 0.6$, learning rate~$= 0.0001$, number of layers (or blocks in xLSTM)~$= 8$, and number of heads~$= 4$. For xLSTM models, 1D-convolution kernel size~$= 4$ and QVK projection layer block size~$= 4$. For \name models, the number of FNN in the concept encoder~$= 1$ (Appendix Figures~\ref{fig:supp_benchmark_full_mae}-\ref{fig:supp_benchmark_full_rmse}).

\xhdr{eICU dataset} The best hyperparameters for  models trained on the eICU dataset are: dropout rate~$= 0.6$, learning rate~$= 0.0001$, number of layers (or blocks in xLSTM)~$= 8$, and number of heads~$= 6$. For xLSTM models, the number of heads~$= 2$, 1D-convolution kernel size~$= 4$ and QVK projection layer block size~$= 4$. For \name models, the number of FNN in the concept encoder~$= 1$ (Appendix Figures~\ref{fig:supp_benchmark_full_mae}-\ref{fig:supp_benchmark_full_rmse}).


\xhdr{Waddington-OT (WOT) dataset} The best hyperparameters for  models trained on the WOT dataset are: dropout rate~$= 0.6$, learning rate~$= 0.00001$, number of layers (or blocks in xLSTM)~$= 4$, number of heads~$= 8$. For xLSTM models, 1D-convolution kernel size~$= 4$ and QVK projection layer block size~$= 8$. For \name models, the number of FNN in the concept encoder~$= 0$ (Appendix Figures~\ref{fig:supp_benchmark_full_mae}-\ref{fig:supp_benchmark_full_rmse}).



\newpage



\section{Additional Figures and Tables}\label{appendix:figures}



\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=0.95\textwidth]{FIG/suppfig_benchmark_full.pdf}}
\caption{Benchmarking the performance of \name, baselines, and ablation models on \textbf{(a)} immediate and \textbf{(b)} delayed sequence editing. Performance is measured by MAE. The models are trained using a standard cell- or patient-centric random split. Not shown for visualization purposes are the performances of VAR models on eICU and MIMIC-IV datasets: on immediate sequence editing, MAE for eICU and MIMIC-IV are $55982.74$ and $886.05$, respectively; on delayed sequence editing, MAE for eICU and MIMIC-IV are $3.02 \times 10^{39}$ and $8.62 \times 10^{23}$, respectively.}
\label{fig:supp_benchmark_full_mae}
\end{center}
\end{figure*}



\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=0.95\textwidth]{FIG/suppfig_benchmark_full_rmse.pdf}}
\caption{Benchmarking the performance of \name, baselines, and ablation models on \textbf{(a)} immediate and \textbf{(b)} delayed sequence editing. Performance is measured by RMSE. The models are trained using a standard cell- or patient-centric random split. Not shown for visualization purposes are the performances of VAR models on eICU and MIMIC-IV datasets: on immediate sequence editing, MAE for eICU and MIMIC-IV are $135003.67$ and $1793.23$, respectively; on delayed sequence editing, MAE for eICU and MIMIC-IV are $5.84 \times 10^{39}$ and $1.59 \times 10^{24}$, respectively.}
\label{fig:supp_benchmark_full_rmse}
\end{center}
\end{figure*}


\clearpage



\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=0.55\textwidth]{FIG/suppfig_cross_split_overlap.pdf}}
\caption{Cross-split overlap (CSO) as a function of SPECTRA parameter (SP) for eICU and MIMIC-IV datasets. CSO is defined as the number of samples in the test set that are similar to at least one sample in the train set. SP is an internal parameter used by SPECTRA to control the CSO of generated data splits. CSO decreases as SP increases.}
\label{fig:supp_spectra_cso}
\end{center}
\end{figure*}


\bigskip



\begin{table}[ht]
\caption{Generalizability of \name, baselines, and ablations on eICU and MIMIC-IV datasets in immediate and delayed sequencing. Performance is measured by the area under the spectral performance curve (AUSPC) for MAE~(Appendix Figure~\ref{fig:supp_spectra_full}) or RMSE~(Appendix Figure~\ref{fig:supp_spectra_full_rmse}). Smaller AUSPC values indicate better performance.}
\label{tab:auspc}
\vskip 0.15in
\begin{center}
\begin{tiny}
\begin{tabular}
{c|cccc|cccc}
\toprule
\textbf{Model}  & \multicolumn{4}{|c}{\textbf{eICU}} & \multicolumn{4}{|c}{\textbf{MIMIC-IV}} \\
                & \multicolumn{2}{|c}{Immediate} & \multicolumn{2}{c}{Delay} & \multicolumn{2}{|c}{Immediate} & \multicolumn{2}{c}{Delay} \\
                & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE \\
\midrule
Transformer                      & $27.06 \pm 0.98$ & $59.83 \pm 1.14$ & $22.59 \pm 1.21$ & $50.29 \pm 0.56$ & $40.87 \pm 0.15$ & $71.77 \pm 0.21$ & $44.61 \pm 0.19$ & $80.38 \pm 0.32$ \\
\name-Transformer (FFN = 0)      & $15.16 \pm 1.09$ & $32.95 \pm 2.47$ & $14.36 \pm 1.07$ & $34.27 \pm 2.12$ & $32.79 \pm 1.41$ & $57.76 \pm 3.39$ & $35.65 \pm 1.73$ & $65.10 \pm 4.43$ \\
\name-Transformer (FFN = 1)      & $10.99 \pm 0.31$ & $27.57 \pm 0.27$ & $9.25 \pm 0.60$ & $27.69 \pm 0.22$ & $21.35 \pm 3.16$ & $36.92 \pm 5.46$ & $23.83 \pm 3.26$ & $44.11 \pm 5.83$ \\
\midrule
xLSTM                            & $28.47 \pm 0.63$ & $62.28 \pm 1.38$ & $23.11 \pm 0.91$ & $52.53 \pm 1.98$ & $40.75 \pm 0.30$ & $71.90 \pm 0.40$ & $44.31 \pm 0.24$ & $80.38 \pm 0.33$ \\
\name-xLSTM (FFN = 0)            & $16.73 \pm 2.16$ & $35.43 \pm 6.01$ & $15.32 \pm 2.10$ & $34.68 \pm 7.09$ & $32.06 \pm 1.13$ & $53.42 \pm 2.18$ & $33.88 \pm 1.98$ & $57.73 \pm 3.63$ \\
\name-xLSTM (FFN = 1)            & $11.35 \pm 0.11$ & $28.09 \pm 0.08$ & $9.04 \pm 0.18$ & $26.21 \pm 0.48$ & $21.04 \pm 2.32$ & $37.50 \pm 4.60$ & $22.63 \pm 2.61$ & $42.12 \pm 5.03$ \\
\midrule
MOMENT                           & $53.49 \pm 0.03$ & $90.54 \pm 0.03$ & $48.83 \pm 0.02$ & $82.50 \pm 0.02$ & $46.55 \pm 0.01$ & $77.22 \pm 0.01$ & $50.59 \pm 0.02$ & $85.72 \pm 0.01$ \\
\name-MOMENT (FFN = 0)           & $47.69 \pm 0.33$ & $82.18 \pm 0.34$ & $40.10 \pm 0.44$ & $72.70 \pm 0.46$ & $44.01 \pm 0.35$ & $73.83 \pm 0.63$ & $46.88 \pm 0.38$ & $81.20 \pm 1.27$ \\
\name-MOMENT (FFN = 1)           & $47.56 \pm 1.60$ & $82.81 \pm 2.88$ & $39.91 \pm 1.65$ & $72.54 \pm 3.20$ & $42.92 \pm 0.52$ & $70.72 \pm 1.96$ & $45.75 \pm 0.65$ & $77.35 \pm 2.77$ \\
\bottomrule
\end{tabular}
\end{tiny}
\end{center}
\end{table}



\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=0.95\textwidth]{FIG/suppfig_spectra_full.pdf}}
 \caption{Generalizability of \name, baselines, and ablation models on \textbf{(a)}~eICU and \textbf{(b)}~MIMIC-IV patient datasets in immediate and delayed sequence editing. Performance is measured by MAE. As the SPECTRA parameter increases, the train/test split similarity decreases~(Appendix Figure~\ref{fig:supp_spectra_cso}). The area under the spectral performance curve (AUSPC) evaluation is in Appendix Table~\ref{tab:auspc}.}
\label{fig:supp_spectra_full}
\end{center}
\end{figure*}


\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=0.95\textwidth]{FIG/suppfig_spectra_full_rmse.pdf}}
\caption{Generalizability of \name, baselines, and ablation models on \textbf{(a)}~eICU and \textbf{(b)}~MIMIC-IV patient datasets in immediate and delayed sequence editing. Performance is measured by RMSE. As the SPECTRA parameter increases, the train/test split similarity decreases~(Appendix Figure~\ref{fig:supp_spectra_cso}). The area under the spectral performance curve (AUSPC) evaluation is in Appendix Table~\ref{tab:auspc}.}
\label{fig:supp_spectra_full_rmse}
\end{center}
\end{figure*}




\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=0.85\textwidth]{FIG/suppfig_wot_cf_full.pdf}}
\caption{Benchmarking the performance of \name, baselines, and ablation models on zero-shot \textbf{(a)} immediate and \textbf{(b)} delayed counterfactual generation of cellular developmental trajectories. Performance is measured by MAE (top row) and RMSE (bottom row).}
\label{fig:supp_wot_cf_full}
\end{center}
\end{figure}



\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=0.85\textwidth]{FIG/suppfig_pt_int_T1D.pdf}}
\caption{\name-generated counterfactual patients via intervention on temporal concepts. Observed and \name patients are compared to quantify the differences between their lab test trajectories as a result of the intervention to halve the \textbf{(a)}~glucose levels in T1D patients from the MIMIC-IV-T1D cohort, \textbf{(b)}~white blood cell (WBC) levels in T1D patients from the MIMIC-IV-T1D cohort, and \textbf{(c)}~WBC levels in T1D patients from the eICU-T1D cohort. \textbf{(d)}~After intervening on \name to halve WBC levels, we observe whether the resulting \name patients' trajectories are ``healthier" or ``sicker" compared to other patients in the real-world cohort~(top). Further, we investigate whether the intervention effects are compounded when simultaneously reducing glucose and WBC levels by half~(bottom).}
\label{fig:supp_t1d}
\end{center}
\end{figure}
