


\xhdr{Sequence editing} The sequence editing task has been defined in language and time series modeling via different terms, but share a core idea: Given a sequence and an alternative condition (e.g.,~sentiment, attribute), generate a counterfactual sequence with the desired properties. Counterfactual sequence generation is an autoregressive process in language~\cite{chatzi2024counterfactual} but a diffusion process in time series~\cite{jing2024towards, bao2024towards}. Prompting is often used to guide the generation of a sequence, both textual and temporal, with a desired condition~\cite{zhang2023survey, bhattacharjee2024zero, jing2024towards, bao2024towards}. However, existing approaches are unable to generate counterfactual multivariate sequences, preserve relevant historical data, and ensure time-sensitive interventions. Temporal sequence editing approaches assume that sequences are univariate and conditions affect the entire sequence~\cite{jing2024towards, bao2024towards}. While incorporating a structural causal model for token sampling can help preserve certain attributes during counterfactual text generation, the counterfactual statements may be inconsistent with real-world causal models~\cite{chatzi2024counterfactual}.


\xhdr{Concept-based learning} Concepts can be thought of as abstract atomic ideas or concrete tokens of text or images~\cite{the2024large, lai2023faithful}. Concept-based learning has been used to explain (e.g.,~predict the concepts observed in the sample) or transform black-box models into more explainable models~(e.g.,~allow users to intervene on learned concepts)~\cite{koh2020concept, shin2023closer, ismail2023concept, lai2023faithful, laguna2024beyond, van2024enforcing}. While concepts have been used in sequence generation, they have not yet been used for conditional generation. The adoption of concept-based learning for counterfactual prediction is limited to image classification, where concepts are intervened on during training to simultaneously learn the label and explanation~\cite{dominici2024climbing}. Further, there is a consistent and widely accepted trade-off between accuracy and interpretability in concept-based models.


\xhdr{Leveraging trajectories as inductive biases} Understanding sequential data as trajectories (e.g.,~increasing, decreasing, constant) is more natural for human interpretation than individual values~\cite{kacprzyk2024towards}. Many modeling approaches on temporal data extract dynamic motifs as inductive biases to improve their interpretability~\cite{kacprzyk2024towards, goswami2024moment, cao2024tempo}.  Such temporal patterns can be used for prompting large pretrained models to perform time series forecasting~\cite{cao2024tempo}, suggesting that trajectories can capture more universal and transferrable insights about the temporal dynamics in time series data. Trajectories have yet to be adopted for counterfactual sequence generation.



