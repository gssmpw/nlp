\section{Related work}
Word learning in LMs is most commonly measured via word-level surprisal (negative log-probability, \citealp{hale2001probabilistic}). \citet{chang2022word} define a surprisal threshold below which words are said to be learned. They find that BERT and GPT-2 learn frequent function words early, unlike children, who first utter nouns and verbs. \citet{portelance2023predicting} show that in LSTMs trained on child-directed speech, surprisal correlates with word-level age of acquisition. \citet{chang2024characterizing} observe that learning curves for surprisal values are stable for frequent tokens, while infrequent tokens are ``forgotten'' again over pre-training. \citet{shafiabadi2025surprisal} introduce anti-surprisal (in incorrect contexts) to track false usage, which also fluctuates over pre-training. These studies cast word learning as the ability to anticipate words given a preceding sequence of tokens, i.e. estimating its expectedness in a given syntactic (and semantic) context. We note a certain conceptual leap to the original works on surprisal, where it is primarily viewed as an incremental measure of processing difficulty in syntactic comprehension \cite{levy2008expectationbased, demberg2009computational}. A simple word like \textit{dog} might be surprising and therefore hard to parse in some contexts, but very expected in others, independently of it being already learned on the pure word level. A further methodological drawback of surprisal as a measure of word learning is that it corresponds almost directly to the next-token prediction objective LMs are trained on. This contrasts with typical probing paradigms used, e.g., in the domain of syntax, which often implement the idea to ``challenge'' models in minimal pair-style set-ups that cannot be observed directly in training, thereby testing implicit linguistic knowledge rather than observed patterns in corpus data. In a similar vein, we want to probe the word knowledge of an LM at a fundamental level, and beyond surface-level word sequence patterns that LMs are known to excel in predicting. We want to know if the learner knows that the word \textit{doggie} exists in the English language, but \textit{moggie} does not.

While lexical decision has been used to study lexical abilities in humans, it remains an underexplored benchmark. \citet{legodais2017comparing} show that character-based LSTMs achieve about 95\% accuracy on such tasks. \citet{lavechin2023babyslm} find that speech-based LMs need significantly more input to still perform poorly (56.8\%) than phoneme-level LSTMs (75.4\%) on a phonetic dataset. For the same data,  \citet{goriely2024babble} find that regular GPT-2 models achieve 70\% accuracy and a character-based GPT-2 reaches nearly 90\%. \citet{bunzeck2025small} report near-perfect lexical decision accuracy for regular Llama models, while phoneme models perform at 60--70\%. None of these studies report learning trajectories.