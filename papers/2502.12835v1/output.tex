% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{float}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{amsmath} 
\DeclareFontEncoding{LS1}{}{}
\DeclareFontSubstitution{LS1}{stix}{m}{n}
\DeclareRobustCommand{\mathvisiblespace}{%
  \mathord{\text{\usefont{LS1}{stixscr}{m}{n}\symbol{"B6}}}%
}

\newcommand{\sina}[1]{\textcolor{blue}{S: #1}}


\title{Subword models struggle with word learning, but surprisal hides it}


\author{Bastian Bunzeck \and Sina Zarrieß \\
  Computational Linguistics, Department of Linguistics \\
  Bielefeld University, Germany \\
  \texttt{\{bastian.bunzeck, sina.zarriess\}@uni-bielefeld.de}}

% TLDR: Character LMs outperform subword LMs in the lexical decision task and learn word knowledge in a more developmentally plausible manner.
\begin{document}
\maketitle
\begin{abstract}
We study word learning in subword and character language models with the psycholinguistic lexical decision task. While subword LMs struggle to discern words and non-words with high accuracy, character LMs solve this task easily and consistently. Furthermore, when comparing word learning and syntactic learning, both processes are separable in character LM where word learning predates syntactic learning, whereas these processes are simultaneous in subword LM. This raises questions about the adequacy of subword LMs for modeling language acquisition and positions character LMs as a viable alternative.
\end{abstract}


\section{Introduction}

When humans acquire first language(s), they \textit{first} learn to recognize single words
%, mostly from short, fragmentary utterances \cite{cameron-faulkner2003construction,bunzeck2024richness}, 
\textit{before} understanding the grammatical processes governing them \cite{tomasello1992first,behrens2021constructivist}. This simple fact about language acquisition has found surprisingly little attention in the increasing amount of work that treats LMs as models of language learners \cite{warstadt2022what,portelance2024roles}. While word learning in children is well studied, the implicit word learning processes in LMs are not. Current studies overwhelmingly focus on syntax \cite{mueller2022coloring,choshen2022grammarlearning}, or investigate word learning in close connection to syntax through surprisal \cite{chang2022word,portelance2023predicting,shafiabadi2025surprisal,ficarra2025distributional}. Architecture-wise, a key limitation to the precise study of word learning is subword tokenization (e.g. BPE, \citealp{gage1994new}), which splits words into linguistically and cognitively implausible units \cite{beinborn2023analyzing,arnett2025why}.

\begin{figure}[t!]
\centering
\includegraphics[width=0.95\linewidth]{viz/graphique.pdf}
\vspace{-0.1cm} 
\caption{Word learning in natural/artificial learners}
\vspace{-0.5cm} 
\label{fig:bla}
\end{figure}

To gauge word learning in a syntax-independent manner, we use the psycholinguistic lexical decision task \cite{meyer1971facilitation,legodais2017comparing}, i.e. deciding which word in given word/non-word pair is real (e.g. \textit{doggie} vs. \textit{moggie}, cf. Figure \ref{fig:bla}). We find that models with character-level tokenization and therefore more fine-grained representations learn this task quickly and reliably. In contrast, subword LMs of all sizes perform much worse in a syntax-independent lexical decision setting and only reach comparable accuracy when stimuli are measured through surprisal, ``unexpectedness'' in syntactic contexts. By comparing word and syntactic learning (measured via BLiMP, \citealp{warstadt2020blimp}), we further find that character models quickly acquire lexical knowledge and only later develop syntactic knowledge. In subword models, however, word learning happens later and concurrently with syntax learning, bringing further evidence against the cognitive plausibility of subword tokenization. This shows how elementary decisions (like choice of tokenization) can tremendously influence the learning dynamics and trajectories that can be observed in LMs, a fact that should receive more scrutiny in studies of LMs as models of language acquisition.

\section{Related work}

Word learning in LMs is most commonly measured via word-level surprisal (negative log-probability, \citealp{hale2001probabilistic}). \citet{chang2022word} define a surprisal threshold below which words are said to be learned. They find that BERT and GPT-2 learn frequent function words early, unlike children, who first utter nouns and verbs. \citet{portelance2023predicting} show that in LSTMs trained on child-directed speech, surprisal correlates with word-level age of acquisition. \citet{chang2024characterizing} observe that learning curves for surprisal values are stable for frequent tokens, while infrequent tokens are ``forgotten'' again over pre-training. \citet{shafiabadi2025surprisal} introduce anti-surprisal (in incorrect contexts) to track false usage, which also fluctuates over pre-training. These studies cast word learning as the ability to anticipate words given a preceding sequence of tokens, i.e. estimating its expectedness in a given syntactic (and semantic) context. We note a certain conceptual leap to the original works on surprisal, where it is primarily viewed as an incremental measure of processing difficulty in syntactic comprehension \cite{levy2008expectationbased, demberg2009computational}. A simple word like \textit{dog} might be surprising and therefore hard to parse in some contexts, but very expected in others, independently of it being already learned on the pure word level. A further methodological drawback of surprisal as a measure of word learning is that it corresponds almost directly to the next-token prediction objective LMs are trained on. This contrasts with typical probing paradigms used, e.g., in the domain of syntax, which often implement the idea to ``challenge'' models in minimal pair-style set-ups that cannot be observed directly in training, thereby testing implicit linguistic knowledge rather than observed patterns in corpus data. In a similar vein, we want to probe the word knowledge of an LM at a fundamental level, and beyond surface-level word sequence patterns that LMs are known to excel in predicting. We want to know if the learner knows that the word \textit{doggie} exists in the English language, but \textit{moggie} does not.

While lexical decision has been used to study lexical abilities in humans, it remains an underexplored benchmark. \citet{legodais2017comparing} show that character-based LSTMs achieve about 95\% accuracy on such tasks. \citet{lavechin2023babyslm} find that speech-based LMs need significantly more input to still perform poorly (56.8\%) than phoneme-level LSTMs (75.4\%) on a phonetic dataset. For the same data,  \citet{goriely2024babble} find that regular GPT-2 models achieve 70\% accuracy and a character-based GPT-2 reaches nearly 90\%. \citet{bunzeck2025small} report near-perfect lexical decision accuracy for regular Llama models, while phoneme models perform at 60--70\%. None of these studies report learning trajectories.

\section{Models, data, and pre-training}

We train triplets of increasingly larger Llama models \cite{touvron2023llama} with character/subword tokenization on the BabyLM 10M corpus \cite{choshen2024call}, which contains developmentally plausible data. Hyperparameters, losses, etc., are found in Appendix \ref{sec:model-stuff}. As additional ablations, we test subword Pythias \cite{biderman2023pythia} and character/subword GPT-2 models \citep{goriely2024babble}.%, trained on different amounts of data (cf. Table \ref{tab:results-exp12}).

\begin{table*}[t!]
\footnotesize
\centering
%\begin{tabular}{@{}c|l|r|c|cc|cc|cc@{}}
\begin{tabular}{@{}clrccccccc@{}}
\toprule
 &  & &  & \multicolumn{2}{c}{Lexical decision} & \multicolumn{2}{c}{Surprisal} & \multicolumn{2}{c}{Anti-surprisal} \\ 
Tokenization & Model & Parameters & Data size & highFrq & lowFrq & highFrq & lowFrq & highFrq & lowFrq \\ \midrule
\multirow{10}{*}{\rotatebox[origin=c]{90}{Subword (BPE)}} & \multirow{6}{*}{Pythia} & 14M & \multirow{6}{*}{825GB} & 66.6\% & 62.5\% & 90.5\% & 85.5\% & 71.4\% & 77.7\% \\
 &  & 70M & & 72.5\% & 68.8\% & 94.5\% & 94.0\% & 77.0\% & 83.6\% \\
 &  & 160M & & 77.8\% & 73.0\% & 96.4\% & 95.8\% & 78.0\% & 85.7\% \\
 &  & 410M & & 81.9\% & 78.1\% & 97.7\% & 97.9\% & 77.1\% & 84.1\% \\
 &  & 1B & & 87.5\% & 83.2\% & 97.7\% & 97.9\% & 76.6\% & 83.8\% \\
 &  & 1.4B & & 87.8\% & 81.6\% & 97.9\% & 97.9\% & 76.5\% & 84.7\% \\ \cmidrule(l){2-10}
 & GPT-2 & 97.5M & 100M words & 35.6\% & 79.1\% & 99.0\% & 99.2\% & 84.7\% & 86.9\% \\ \cmidrule(l){2-10}
 & \multirow{3}{*}{Llama} & 2.51M & \multirow{3}{*}{10M words} & 70.9\% & 58.4\% & 86.7\% & 70.9\% & 78.6\% & 67.7\% \\
 &  & 7.77M & & 79.5\% & 63.2\% & 91.3\% & 78.1\% & 81.1\% & 72.9\% \\
 &  & 30.03M & & 83.6\% & 68.6\% & 92.7\% & 81.1\% & 83.7\% & 76.1\% \\ \midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{Character}} & GPT-2 & 85.3M & 100M words & 98.7\% & \textbf{97.3\%} & \textbf{99.8\%} & \textbf{99.4\%} & 98.0\% & \textbf{96.3\%} \\ \cmidrule(l){2-10}
 & \multirow{3}{*}{Llama} & 0.49M & \multirow{3}{*}{10M words} & 97.6\% & 83.0\% & 98.2\% & 84.3\% & 98.0\% & 83.1\% \\
 &  & 3.73M & & 98.9\% & 90.2\% & 99.4\% & 90.3\% & 98.5\% & 88.8\% \\
 &  & 21.94M & & \textbf{99.0}\% & 93.3\% & \textbf{99.8\%} & 94.7\% & \textbf{99.0\%} & 92.5\% \\ \midrule
\end{tabular}
\vspace{-0.2cm} 
\caption{Accuracy for lexical decision (i) without context, (ii) measured through surprisal and (iii) anti-surprisal}
\vspace{-0.5cm} 
\label{tab:results-exp12}
\end{table*}

\section{Experiments}

We follow the idea of forced-choice lexical decision \cite{baddeley1993spottheword, legodais2017comparing}, where participants are presented with an existing word and a synthesized non-word, and must decide which is real. To test LMs at this, we generate minimal pairs of words/non-words, akin to established linguistic benchmarks such as BLiMP, with \texttt{wuggy} \cite{keuleers2010wuggy}. We derive 1,000 non-words (e.g. \textit{monding}) each from 1,000 high-frequency/low-frequency words (e.g. \textit{sending}), which preserve syllable-bigram frequencies, match their origin words in length and look graphematically plausible (more details in Appendix \ref{sec:tokenization}). 

\paragraph{Lexical decision} 
For a word/non-word pair $(w,*w)$, we measure $-log(P(w|\mathvisiblespace))$ and $-log(P(*w|\mathvisiblespace))$, i.e. how ``surprised'' a LM is by the word in the context of a prepended whitespace. If $-log(P(w|\mathvisiblespace)) < -log(P(*w|\mathvisiblespace))$, the LM's lexical decision is correct.\footnote{We use \texttt{minicons} \cite{misra2022minicons} to measure this value averaged over all subwords/characters of a token, as this library fixes common problems with surprisal, tokenization, and whitespace in subword models \citep{pimentel2024how,oh2024leading}.} As autoregressive LMs are sequence prediction models, we need a preceding context for which we can calculate surprisal. A single whitespace is the most neutral starting token available (and for subword models also signals that the first subword is word-initial).
%\footnote{Results with the \texttt{huggingface} perplexity \cite{jelinek1977perplexity} implementation are comparable.}

\paragraph{Surprisal} To measure LMs' knowledge of words presented in regular syntactic contexts, we calculate the surprisal of words and non-words $(w,*w)$ as $-log(P(w_i|w_{n<i}))$, i.e. the degree to which the LM is ``surprised'' by the word in the context of plausible preceding tokens. We create stimuli by sampling sentences that contain the existing words from OpenSubtitles \citep{lison2016opensubtitles2016} and substituting them with matching non-words for the false stimuli. If $-log(P(w_i|w_{n<i})) < -log(P(*w_i|w_{n<i}))$, the LM's decision is correct.
%\paragraph{RandomSurprisal}

\paragraph{Anti-surprisal} We create negative samples by selecting sentences that our original words do not occur in, and then randomly placing words/non-words into these sentences at the same index. By doing so, we compromise between lexical decision and surprisal measurement. Words are presented in the context of existing words, but without plausible syntactic/semantic signals in the surrounding data. In line with \citet{shafiabadi2025surprisal}, we call this measure anti-surprisal. In contrast to them (and \citealp{ficarra2025distributional}), however, we are not interested in whether models are surprised by unfitting words in context, but only want to provide some positive lexical signal (without other signals). Again, if $-log(P(w_i|w_{n<i})) < -log(P(*w_i|w_{n<i}))$, the LM's decision is correct.


\paragraph{Learning trajectories}
To assess when word learning happens in relation to syntax learning, we further evaluate intermediate checkpoints of our models on our lexical tests and BLiMP as a syntactic benchmark. In line with previous studies \cite{chang2022word,viering2023shape}, we space our checkpoints logarithmically -- 10 for the first 10\% of training, 9 more for the remaining 90\%. For the Pythia models, we extract similarly spaced checkpoints (the GPT-2 models do not provide checkpoints, so we exclude them). 


\section{Results}

\paragraph{Lexical decision} The lexical decision results (Table \ref{tab:results-exp12}) show a strong contrast between character and subword models. Character models achieve near-perfect accuracy (97–99\%) on high-frequency words, regardless of model size. The performance on low-frequency words steadily increases with model size and reaches a near-perfect level for our largest character Llama and the character GPT-2. On the other hand, all BPE models get surprisingly low scores on high-frequency words: The smallest Pythia model discriminates between word and non-words with an accuracy of only 67\%, and the BPE GPT-2 actually performs below the chance baseline. Even the largest and best BPE model reaches only 87.8\% on high-frequency words -- almost 10\% less than the smallest character model. 

Scaling laws generally hold, with larger models outperforming smaller ones. Interestingly, in the BPE models there is a consistent gap between high and low-frequency words which cannot be closed by the larger model sizes. The smaller character models also show a significant gap between high and low-frequency words, but this gap narrows considerably with larger model sizes. These results point to substantial differences in how BPE and characters models learn words. Such a surprising lack of ability in distinguishing words from non-words (where no context is given) is a blatant, hitherto overlooked gap in BPE-based LMs.

\paragraph{Surprisal and anti-surprisal} Results for the second experiment (Table \ref{tab:results-exp12}) differ from those for lexical decision. In the surprisal setting, the difference between BPE and character models is less pronounced. On high-frequency data, nearly all models (except the smallest BPE Llama) achieve over 90\% accuracy. Still, larger character models yield the very best results. In the low-frequency data condition, the pattern is similar, though scores are generally lower. Very large BPE models outperform our Llamas there, but the character GPT-2 remains superior. This may be attributed to the limited lexical exposure of our Llamas, trained on only 10M tokens. In the anti-surprisal setting, character models again drastically outperform BPE models and achieve nearly perfect scores on high-frequency data, while BPE models only reach 70--80\% accuracy. This setting is the only one where Pythia models get better scores on low-frequency data, with a gap of 6--8\%, which increases with model size (not the case for BPE Llamas or character models). This is another indicator for the entanglement of word learning and syntactic learning in subword models. It is plausible that, for high-frequency words, the BPE models have strong expectations about which word should come next in a certain context, and because this expectation is not matched by the real (but ill-fitting) word, a made-up none word is generally preferred.
In any case BPE models require context to perform lexical decision tasks consistently, catching up to character models if (and \textit{only} if) provided with additional syntactic/semantic information. While random context somewhat aids BPE models, a substantial gap remains between the largest BPE models and the character models, where performance remains excellent, even in the presence of implausible contexts.

\begin{figure}[t!]
\centering
%\hspace*{-0.5cm}  
\includegraphics[width=\linewidth]{viz/lmplot-averaged2-narrow.pdf}
\vspace{-0.7cm} 
\caption{Selected lexical and syntactic learning curves}
\vspace{-0.5cm} 
\label{fig:linguistic-curves-averaged-n}
\end{figure}


\paragraph{Learning trajectories} We display learning curves for syntactic agreement phenomena, lexical decision and both surprisal conditions in Figure \ref{fig:linguistic-curves-averaged-n} (complete curves reproduced in Appendix \ref{sec:curves}). For character models, the high-frequency, low-frequency, and syntactic curves are clearly separated. On high-frequency data, word learning is rapid and follows power-law curves; the low-frequency scores improve a little later and at a lower rate, but with the same trajectories. Syntactic phenomena improve later, mostly in s-shaped curves (e.g. det.-noun agreement), or are not learned at all in small models (subj.-verb agreement). In contrast, the syntactic and lexical curves for BPE models form sheaves of s-shaped trajectories. There is \textit{no} principle difference in learning dynamics between the syntactic and the lexical level,\footnote{To corroborate these findings, we also calculate Spearman-rank correlation between ordered accuracies for lexical tasks and BLiMP paradigms (Results in Appendix \ref{sec:correlations}).} improvements occur simultaneously. This further confirms the results of our previous experiments: in BPE models, word learning is extremely dependent on syntax learning and words cannot be recognized reliably outside of plausible contexts. Additionally, these different levels of learning cannot be disentangled in BPE models, whereas in character models syntactic learning \textit{follows} word learning.\footnote{Interestingly, the learning process is not finished when accuracy curves stabilize. We provide an analysis of the absolute differences between response values in Appendix \ref{sec:delta}.}


\section{Discussion and conclusion}

This study set out to explore how LMs learn what valid words \textit{are}, not when words are validly \textit{used}. We have shown that lexical decision complements surprisal-based approaches and reveals difficulties in word learning that surprisal hides: subword LMs actually struggle with it, whereas character models master this task with ease. In subword LMs, lexical and syntactic learning are inseparable (unlike in humans, see \citealp{brinchmann2019there,donnelly2025separability}), whereas word learning in character models precedes syntactic learning; the processes are related, yet separable. The \textit{a priori} token splitting in subword models basically preempts word learning in them, whereas character models pass through more plausible developmental stages. Why this is the case remains open to further inquiry, but it is hard to imagine that arbitrary subword units lead to human-like, cognitively plausible word-level representations (like in exemplar models of lexical storage, cf. \citealp{bybee2010language}), whereas character models possibly offer a more justified level of granularity for them. In any case, as we have shown, decisions about the representational levels of LMs tremendously influence their learning pathways.
%- wenn man modelle echt zur erforschung von spracherwerb benutzen will, muss man sich mehr gedanken über repräsentationen machen; immer noch offene fragen, wie sich das dann mit anderen tests verhält (generierung, semantik, etc.?)


\section*{Limitations}

The generalizability of our findings is constrained by a few factors. The present study has only focused on the English language, but it is plausible that other languages with different writing systems or graphematic and phonotactic rule systems exhibit different patterns under different tokenization schemes. Here, phonetic transcriptions might provide a viable alternative, but real narrow transcriptions that accurately capture the whole breadth of human input are scarce and extremely costly and laborious to manually produce. Besides, for the character LMs, we focus only on small models, as very large models with such tokenization, especially ones providing intermediate checkpoints, are nonexistent at this moment. It would still be interesting to see how they compare to subword models in a setting where parameter size and training data are greatly increased. 

From a developmental perspective, word learning in humans also includes other processes than statistical pattern recognition from the input: semantic aspects and real-world reference are equally important, as are multimodal input and communicative intent. The ongoing form-vs.-function debate on LMs \cite{mahowald2024dissociating} has begun to consider these aspects, and further studies should aim at incorporating them. 

Finally, we also want to mention that there are attempts to add more linguistic theory to tokenizers. Looking into different tokenizers that, for exameple, try to be more morphology-aware \cite{hofmann2022embarrassingly,bauwens2024bpeknockout} or implement other optimization tricks \cite{schmidt2024tokenization} could yield even more fine-grained points of comparison, but for the present study and its limited scope we focused on the most popular tokenization scheme (BPE) and the most linguistically minimalist alternative (characters).

\section*{Ethical considerations}

Due to the nature of this work, no concrete ethical aspects or repercussions need to be discussed. However, we would like to stress that, of course, BabyLMs are \textit{not} real babies, but only abstractions of a very specific part of their learning capacity (frequency-driven, domain-general learning mechanisms such as entrenchment or resonance), and therefore all claims about their implications for language development in the real world should be interpreted in this light.

%\section*{Acknowledgments}
%This research has been funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) -- CRC-1646, project number 512393437, project A02.

\bibliography{bastian}

\appendix

\section{Model hyperparameters and training details}
\label{sec:model-stuff}

\begin{table*}[htbp]
\centering
\footnotesize
\begin{tabular}{@{}l|rrrrrr@{}}
\toprule
 & \texttt{small-char} & \texttt{medium-char} & \texttt{large-char} & \texttt{small-bpe} & \texttt{medium-bpe} & \texttt{large-bpe} \\ \midrule
Embedding size & 128 & 256 & 512 & 128 & 256 & 512 \\
Hidden size & 128 & 256 & 512 & 128 & 256 & 512 \\
Layers & 4 & 8 & 12 & 4 & 8 & 12 \\
Attention heads & 4 & 8 & 12 & 4 & 8 & 12 \\
Context size & 128 & 128 & 128 & 128 & 128 & 128 \\
Vocab. size & 102 & 102 & 102 & 8,002 & 8,002 & 8,002 \\
Parameters & 486,016 & 3,726,592 & 21,940,736 & 2,508,416 & 7,771,392 & 30,030,336 \\ \bottomrule
\end{tabular}
\caption{Model hyperparameters for our self-trained Llama models}
\label{tab:model-params}
\end{table*} 

\begin{figure*}[htb]
\centering
\includegraphics[width=\linewidth]{viz/loss-curves.pdf}
\caption{Loss curves for our self-trained Llama models}
\label{fig:loss-curves}
\end{figure*}

We use the \texttt{transformers} library \cite{wolf2020transformers} to train our models. The corresponding hyperparameters are listed in Table \ref{tab:model-params}. We opted for very small LMs and little training data because the BabyLM paradigm has consolidated itself as a well-established method of developmentally plausible language modeling \cite{warstadt2023findings,choshen2024call}. Also, we noticed the word learning to occur quite rapidly in our models, so we argue that small LMs offer more fine-grained opportunities for investigating these processes -- in larger LMs, even a singular training step can already influence performance on such a brittle task like lexical decision tremendously.

The subword models feature a considerably higher number of parameters, as the embedding layer of transformer language models accounts for a quite large share of overall model parameters. In light of our vastly different vocabulary sizes (102 for character models, 8,002 for subword models), these differences are not surprising. The subword tokenizer is a regular BPE tokenizer self-trained with the \texttt{tokenizers} library and contains 8,000 subword tokens, a beginning-of-sequence token and a combined end-of-sequence/padding token. The character tokenizer contains these two special tokens and all printable ASCII characters, which are sufficient to represent all graphemes of the English language.

\begin{figure*}[htb]
\centering
\includegraphics[width=0.9\linewidth]{viz/tokenization-stats.pdf}
\caption{Pairplot displaying (i) number of tokens of words, (ii) frequency scores from CELEX \cite{baayen1995celex2} and (iii) number of tokens of non-words (for BPE and character tokenization)}
\label{fig:tokens}
\end{figure*}

Our models were trained on a Apple M2 Pro processor with the MPS backend. For the small models, training lasted approx. 20min, for the medium-sized models it took approx. 1h and for the largest models approx. 10h (no principal differences between character and subword models). We share our models on the HuggingFace hub under [link anonymized].

Loss curves for models can be found in Figure \ref{fig:loss-curves}. For the test loss, we evaluated perplexity on a held-out portion of our training corpus that is comparable in composition. We find no principal differences in loss development, although the character models converge faster. Larger models also tend to converge faster and generally reach smaller absolute loss values. As the similar train and test loss curves indicate, all Llama models succeed in optimizing for their next-token prediction objective. However, it remains questionable how much these scores are constrained by the comparatively small capacity of our BabyLMs.

\section{Data creation and tokenization analysis}
\label{sec:tokenization}

\paragraph{Data creation}
Word frequency influences lexical decision performance greatly \cite{mcclelland1981interactive,allen2005evidence}. To incorporate this effect into our study, we create two distinct data sets from words included in \texttt{wuggy}: (i) high-frequency stimuli with a frequency score over 7.0 (occurrences per 1M words in BNC, COCA and other English corpora, as reported in CELEX, \citealp{baayen1995celex2}) and (ii) low-frequency stimuli with a frequency score over 0.0 but below 0.7 (so at least one order of magnitude lower). We opted to rely on the CELEX frequency scores because we compare models trained on different corpora -- our models are trained on the 10M BabyLM corpus, the models by \citet{goriely2024babble} are trained on the 100M BabyLM corpus and the Pythia models are trained on The Pile \cite{gao2020pile}. As such, frequency scores from these corpora would taint analyses of other models and hinder comparability. For the contextualised stimuli, we sample sentences from the OpenSubtitles \cite{lison2016opensubtitles2016} portion of the BabyLM 2024 corpus \cite{choshen2024call}. Both Wuggy and BabyLM data are licensed under the MIT license\footnote{As per license information found at \url{https://github.com/WuggyCode/wuggy} and \url{https://github.com/babylm/evaluation-pipeline-2024}.}, therefore we release our own stimuli artifacts on HuggingFace under [link anonymized] under the same license. The data contain no information that names or uniquely identifies individual people or offensive content, and are commonly used in computational linguistics.

\begin{figure*}[ht!]
\centering
\includegraphics[width=\linewidth]{viz/lmplot-full2.pdf}
\caption{Learning curves for all paradigms in BLiMP and high/low frequency lexical decision data, separated for models (rows) and phenomenon sets (columns)}
\label{fig:linguistic-curves}
\end{figure*}

\paragraph{Analysis of tokenization}
To further assess the influence that these frequency scores have on the resulting tokenization for our own models, we offer a brief analysis: Figure \ref{fig:tokens} shows a pairplot between three numerical variables -- (i) the number of tokens that our original words are split into, (ii) the number of tokens that the derived non-word are split into and (iii) the corresponding frequency score from CELEX. While the three plots on the diagonal axis show a layered kernel density estimate (KDE) for each individual variable, the other plots are scatterplots which visualize the relationship between the variables. The data points are colored for their tokenization scheme. 

In the upper left and lower right plot we can see that both real and non-words are split similarly in the two distinct tokenization schemes. Words split by the BPE tokenizer tend to have fewer tokens, mostly between one and six. For the character-based tokenizer, a normal distribution is visible, with its peak at six tokens.

The upper right and lower left scatter plots show the relationship between tokenization for real and non-words. The character-level tokenization exhibits perfect alignment between both kinds of stimuli, they are always split into the exact same number of tokens. Subword tokenization is slightly skewed towards the non-word tokens. This means that non-words are more often split into more tokens than real words, although the reverse case is not completely infrequent.

\section{Full learning curves for BLiMP and word learning}
\label{sec:curves}

Figure \ref{fig:linguistic-curves} shows the full learning curves of all phenomena included in BLiMP (individual syntactic paradigms belonging to one phenomenon are displayed in the same sub-figure) as well as our own lexical benchmarks (all displayed in individual sub-figures), for our six self-trained models and the six Pythia models that we compare them to. We fit a fifth-order polynomial curve to the individual data points and display it on a logarithmic scale. 

It should be noted that we plot the number of the checkpoint on the x-scale. However, the individual amounts of actual textual data seen between these checkpoints differs vastly between our self-trained models (10M lexical tokens) and the Pythia models (825GB of textual data; as the dataset has since been taken down, no lexical token counts are possible anymore).

\section{Correlations between word learning and syntactic learning}
\label{sec:correlations}

As an additional measure of commonalities between word learning and syntactic learning, we calculate Spearman-rank correlation scores between ordered accuracy scores for our lexical tasks and BLiMP paradigms. Table \ref{tab:correlations} shows the underlying numerical values for the correlation heatmap provided in Figure \ref{fig:correlation-heat} (please note that the heatmap is rotated in comparison to the table). All scores are statistically significant ($p<0.05$). Due to the similar learning curves found in Figure \ref{fig:linguistic-curves-averaged-n}, we average accuracy scores over all lexical phenomena (lexical decision and both surprisal settings), and then calculate correlations between them (both high and low frequency) and the coarse-grained BLiMP phenomena. For BPE models, lexical performance is highly correlated with more than half of the BLiMP phenomena. The character models show much weaker correlation with syntactic learning. This further confirms our findings about the strong entanglement of lexical and syntactic learning in subword models and their weaker ties in character models.

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{viz/correlation-heat.pdf}
\caption{Correlation heatmap}
\label{fig:correlation-heat}
\end{figure}



\begin{table*}[htbp]
\scriptsize
\centering
\begin{tabular}{@{}l|rr|rr|rr|rr|rr|rr@{}}
\toprule
 & \multicolumn{2}{c}{\texttt{small-char}} & \multicolumn{2}{c}{\texttt{medium-char}} & \multicolumn{2}{c}{\texttt{large-char}} & \multicolumn{2}{c}{\texttt{small-bpe}} & \multicolumn{2}{c}{\texttt{medium-bpe}} & \multicolumn{2}{c}{\texttt{large-bpe}} \\ 
BLiMP phenomenon & highFrq & lowFrq & highFrq & lowFrq & highFrq & lowFrq & highFrq & lowFrq & highFrq & lowFrq & highFrq & lowFrq \\ \midrule
Anaphor agr. & -0.393 & -0.435 & 0.580 & 0.277 & 0.332 & 0.555 & 0.569 & 0.559 & 0.930 & 0.910 & 0.772 & 0.759 \\
Argument structure & 0.339 & 0.589 & 0.467 & 0.825 & 0.545 & 0.895 & 0.949 & 0.959 & 0.970 & 0.987 & 0.979 & 0.986 \\
Binding & -0.718 & -0.629 & 0.660 & 0.918 & 0.653 & 0.922 & 0.901 & 0.889 & 0.992 & 0.993 & 0.979 & 0.977 \\
Control raising & 0.904 & 0.837 & 0.909 & 0.776 & 0.791 & 0.892 & 0.777 & 0.780 & 0.974 & 0.974 & 0.930 & 0.936 \\
Det.-noun agr. & 0.686 & 0.870 & 0.524 & 0.869 & 0.521 & 0.890 & 0.989 & 0.989 & 0.990 & 0.993 & 0.994 & 0.989 \\
Ellipsis & -0.912 & -0.766 & -0.285 & 0.209 & 0.180 & 0.662 & 0.857 & 0.822 & 0.897 & 0.868 & 0.865 & 0.856 \\
Filler gap & -0.765 & -0.586 & -0.554 & -0.146 & -0.200 & 0.209 & -0.715 & -0.722 & -0.589 & -0.602 & -0.063 & -0.031 \\
Irregular forms & 0.612 & 0.724 & 0.507 & 0.856 & 0.397 & 0.787 & -0.116 & -0.098 & 0.636 & 0.662 & 0.816 & 0.832 \\
Island effects & -0.937 & -0.840 & -0.862 & -0.657 & -0.556 & -0.214 & -0.321 & -0.334 & 0.473 & 0.418 & -0.088 & -0.094 \\
NPI licensing & 0.535 & 0.641 & 0.748 & 0.782 & 0.616 & 0.568 & -0.425 & -0.407 & 0.520 & 0.526 & -0.069 & -0.049 \\
Quantifiers & -0.476 & -0.202 & 0.059 & 0.299 & 0.547 & 0.848 & 0.789 & 0.767 & 0.779 & 0.770 & 0.716 & 0.695 \\
Subj.-verb agr. & -0.386 & -0.400 & 0.329 & 0.529 & 0.351 & 0.730 & 0.963 & 0.961 & 0.982 & 0.981 & 0.967 & 0.974 \\ \bottomrule
\end{tabular}
\caption{Spearman-rank correlation scores between ordered accuracy scores for our lexical tasks and BLiMP paradigms}
\label{tab:correlations}
\end{table*}

\section{Final BLiMP scores for all models}
\label{sec:full-blimp}

We reproduce the final syntactic evaluation scores for all models that we incorporated in our lexical analyses in Table \ref{tab:results-blimp}. Generally, scores improve with larger models and with more training data. Most strikingly, subword models are consistently superior to comparable character models trained on the same amount of data. These differences, however, are most pronounced for the small models trained on very little data, like our Llama models trained on 10M tokens (7\% for smallest models, 2\% for largest models). For the comparable GPT-2 models trained on 100M tokens, the gap becomes much smaller (0.4\%).

\begin{table}[htb]
\footnotesize
\centering
\begin{tabular}{@{}l|l|r|c@{}}
\toprule
Tok. & Model & Params & BLiMP score \\ \midrule
\multirow{10}{*}{\rotatebox[origin=c]{90}{Subword (BPE)}} & \multirow{6}{*}{Pythia} & 14M & 65.86\%  \\
 &  & 70M & 73.30\% \\
 &  & 160M & 77.50\%  \\
 &  & 410M & 81.63\%  \\
 &  & 1B & 82.21\%  \\
 &  & 1.4B & 81.92\%  \\ \cmidrule(l){2-4}
 & GPT-2 & 85M & 77.80\% \\ \cmidrule(l){2-4}
 & \multirow{3}{*}{Llama} & 2.51M & 59.80\%  \\
 &  & 7.77M & 64.55\%  \\
 &  & 30.03M & 64.56\%  \\ \midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{Character}} & GPT-2 & 85M & 77.40\% \\ \cmidrule(l){2-4}
 & \multirow{3}{*}{Llama} & 0.49M & 52.69\%  \\
 &  & 3.73M & 51.07\% \\
 &  & 21.94M & 62.14\% \\ \midrule
\end{tabular}
\caption{BLiMP scores for all models}
\vspace{-0.3cm} 
\label{tab:results-blimp}
\end{table}

\section{Development of word/non-word differences}
\label{sec:delta}

In Figure \ref{fig:delta-narrow}, we plot the average difference between word and non-word negative log-probability values across training (for high-frequency data). Positive scores indicate preference for real words. For the character models, the differences are less pronounced and get most extreme at the end of pre-training (where accuracy scores do not change anymore), especially for the lexical decision data, which is already consistent at very early training stages. For the BPE models, we see that at the beginning they actually prefer non-words in the lexical decision task. Only after the first 10\% of training they \textit{begin} to discern words and non-words. Figure \ref{fig:delta-curves-low} shows the equivalent difference curves to Figure \ref{fig:delta-narrow} for our low-frequency data. While general tendencies remain the same, the absolute differences are generally lower and the differences between the curves are less pronounced.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.97\linewidth]{viz/lmplot-differences-high-narrow.pdf}
\caption{Average differences between surprisal scores}
\label{fig:delta-narrow}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.97\linewidth]{viz/lmplot-differences-low-narrow.pdf}
\caption{Average differences between surprisal scores (for low-frequency data)}
\label{fig:delta-curves-low}
\end{figure}

\end{document}