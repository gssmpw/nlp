%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed


\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{color,array}
\usepackage{xcolor}
\usepackage[misc]{ifsym}

\usepackage{microtype}
\usepackage{subfigure} 
\usepackage[colorlinks,urlcolor=blue,linkcolor=blue,citecolor=blue]{hyperref}

\usepackage{amssymb}
\usepackage{amsmath}
% \usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{cite}
\usepackage{siunitx}
\usepackage{bm}
\usepackage{cancel}
% \usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage{longtable,tabularx}
\usepackage{multirow}
\usepackage{placeins}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usetikzlibrary{patterns}
\usepackage{mathrsfs} 

\usepackage{hyperref}


\newtheorem{condition}{Condition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}


\def\be{\begin{equation}}
\def\ee{\end{equation}}
\newcommand{\YLcom}[1]{{\color{blue} [{\bf Comment: }#1]}}

\newcommand{\MK}[1]{{#1}}
\newcommand{\YL}[1]{{#1}}
\newcommand{\TR}[1]{{#1}}
\newcommand{\LY}[1]{{#1}}

\newtheorem{lemma}{Lemma}


\newcommand{\YLnew}[1]{{\color{blue} #1}}



\usepackage{mathtools}

\DeclareRobustCommand{\legendlineBlue}[1]{%
  \tikz[baseline=(a.south)]{\node[#1, inner sep=.8ex, outer sep=0] (a) {};}%
}


\setlength{\floatsep}{5pt plus 2pt minus 2pt}
\setlength{\textfloatsep}{5pt plus 2pt minus 2pt}
\setlength{\intextsep}{5pt plus 2pt minus 2pt}



\title{\LARGE \bf
On Space-Filling Input Design for Nonlinear Dynamic Model Learning: \\
A Gaussian Process Approach
}


\author{Yuhan Liu, Máté Kiss, Roland Tóth and Maarten Schoukens% <-this % stops a space
\thanks{This work is funded by the European Union (ERC, COMPLETE, 101075836). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.}% <-this % stops a space
\thanks{Yuhan Liu, Máté Kiss, Roland Tóth and Maarten Schoukens are with the Control Systems Group, Department of Electrical
Engineering, Eindhoven University of Technology, 5600 MB Eindhoven, The
Netherlands.
        {\tt\small (email: y.liu11@tue.nl).}}%
\thanks{Roland Tóth is also with the Systems and Control Laboratory, HUN-REN Institute for Computer Science and Control, 1111 Budapest, Hungary.
        }%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}


While optimal input design for linear systems has been well-established, no systematic approach exists for nonlinear systems, where robustness to extrapolation/interpolation errors is prioritized over minimizing estimated parameter variance.
To address this issue,
we develop a novel \emph{identification-oriented} space-filling input design strategy for efficient covering of a well-defined region of interest. 
By placing a Gaussian Process (GP) prior on the joint input-state space, the proposed strategy leverages the GP posterior variance to construct a space-filling promoting input design cost function from the Bayesian point of view. This accordingly enables maximization of the coverage in the region of interest, thereby facilitating the generation of informative datasets. Furthermore, we theoretically prove that minimization of the cost function indicates the space-filling behavior. 
The effectiveness of the proposed strategy is demonstrated on both an academic example and a mass-spring-damper system, highlighting its potential practical impact on efficient exploration in identification scenarios for nonlinear dynamical systems.
\end{abstract}
%
% \begin{IEEEkeywords}Space filling, Input design, Gaussian process, System identification, Dynamical system. 
% \end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:1}

Nonlinear system identification \cite{schoukens2019nonlinear} is a well-established topic, and has been applied in a broad range of domains such as mechanics and physics \cite{noel2017nonlinear,worden2018evolutionary}. \YL{In general, the quality of the model derived from identification is substantially dependent on the data collected during the identification procedure and input design plays a significant role in ensuring data informativity.} In the nonlinear case, this especially concerns the extent to which the measurement dataset \YL{achieves effective data coverage} of the input, output and state space in the region of interest, ensuring a comprehensive exploration of the dynamical system. However, such an input design for dynamical
systems is challenging because the current output depends on the past system inputs and outputs.



The overall process aiming at overcoming these challenges is often called \emph{identification-oriented} input design and its objective is that \TR{given an expected modeling accuracy}, obtain the information required to model the system dynamics at the lowest experimental cost \cite{schoukens2019nonlinear}. 
There has been extensive research conducted on \emph{identification-oriented} input design for linear systems \cite{tanaskovic2014optimal,annergren2017application,bombois2021robust}.  \TR{State-of-the-art methods} minimize the variance on the parameter estimate of an assumed parametric model of the system by maximizing the Fisher information matrix $\mathcal{M}$. \TR{Such input design has been considered both from the nominal and robust point of view to} optimize some forms of $\mathcal{M}$ (D-optimality \cite{galil1980d}, A-optimality \cite{chernoff1953locally}, E-optimality \cite{ehrenfeld1955efficiency}). In the linear system case, this problem effectively translates into a convex optimization, where the solution is given by an optimal \TR{design of the} power spectrum of the input signal~\cite{pronzato2008optimal}.


In contrast to the input design for linear systems, \TR{there is no general systematic framework available to address the input design problem for a wide range of nonlinear systems due to their dependence on both frequency-domain characteristics and time-domain evolution. Solution methods exist only for simple nonlinear systems, such as  Hammerstein and Wiener \cite{de2016d}.} 
Furthermore, \TR{given that the dominant source of errors in nonlinear system identification is most often the generalization error, i.e., model errors on regions of the operating space that are not contained in the estimation dataset \cite{schoukens2019nonlinear}, input design that minimizes the variance of the parameter estimate is not justified.} Hence, an input design approach should be adopted to minimize the model error over a considered range of operations of the system of interest. 

A space-filling input ensures that the model errors are well-behaved and that the estimated model using this dataset would generalize well over the full region of interest.
Various methods \cite{smits2021genetic,smits2024space,smits2022space,KISS2024562} have been explored for space-filling input design in nonlinear systems, and it has also been shown that accurate model estimation can be obtained with the space-filling dataset in a set-membership setting \cite{novara2007experiment}.
Existing space-filling promoting cost functions and algorithms \cite{heinz2017iterative,smits2024space,peter2019fast} mainly penalize the mutual distance between data points.
\YL{While substantial work has been done on exploring various space-filling promoting cost functions, they are typically limited to some specific input classes (e.g. piecewise constant inputs \cite{smits2021genetic,smits2022space,smits2024space}), and do not consider constraints on the input (e.g. spectral constraints). Additionally, active learning, which shares a similar spirit with input design, is used to actively explore the space by selecting sampling points that maximize the variance-based cost \cite{buisson2020actively} or information gain \cite{treven2023optimistic}. 
Many active learning approaches follow a MPC-like procedure where the next sampling point (input sequence in MPC) is actively selected based on a specific information criterion. In contrast, space-filling input design, as an experiment design method, emphasizes the thorough coverage of the entire space by solving an offline design problem, without the need for state measurements of the system. Similarly to MPC-based methods, a model of the system needs to be available.
 

In this letter, we study how to efficiently generate an informative dataset that can \TR{maximize the space coverage for a fixed length of the experiment}. To this end, we propose a simple, yet effective \emph{identification-oriented} \emph{Gaussian process} (GP)-based space-filling input design strategy, where a GP prior is placed on the joint input-state space.
The underlying philosophy of the proposed method \TR{is to solve the filling problem by} minimizing the total posterior variance w.r.t. the user-selected anchor points distributed in a region of interest. The main contributions of this letter are summarized as: 
\begin{itemize} 
\item[C1)]
A novel GP-based space-filling input design algorithm is presented by developing a variance-based cost function to explore a user-defined region of interest efficiently; 
\item[C2)] Rigorous theoretical analysis is provided to illustrate that the cost function minimization leads to a space-filling behavior regardless of the hyperparameter choice; 
\item[C3)] The proposed approach is not limited to a specific input class, facilitating flexible choice of parameterization and allowing for a rather wide range of input constraints.
\end{itemize} 

The remainder of this letter is organized as follows. We describe the problem formulation in Section \ref{sec:2} and summarize the background in Section \ref{sec:3}.  The proposed GP-based space-filling input design method is detailed in Section \ref{sec:4}.
Experiment results are given in Section \ref{sec:5}, followed by the conclusions in Section \ref{sec:6}.  
}


\section{\YL{Problem Formulation}}
\label{sec:2}
\subsection{Data-generating system}
\label{subsection:dg}

Consider the following discrete-time state-space representation of the data-generating system: 
\be
\label{1}
\begin{aligned}
x(k+1) & =f(x(k), u(k)), \\
y(k) & =g(x(k), u(k)),
\end{aligned}
\ee
where $u(k)\in\mathcal{U}\subseteq\mathbb{R}^{n_\mathrm{u}}$, $x(k)\in\mathcal{X}\subseteq\mathbb{R}^{n_\mathrm{x}}$, and $y(k)\in\mathcal{Y}\subseteq\mathbb{R}^{n_\mathrm{y}}$ are the input, state and output signals of the system at time instant $k\in\mathbb{Z}$, respectively, and $\mathcal{Z}:\mathcal{U}\times\mathcal{X}$ denotes the joint input-state space. The smooth nonlinear functions $f(\cdot):\mathcal{Z}\to\mathcal{X}$ and $g(\cdot):\mathcal{Z}\to\mathcal{Y}$ represent the \YL{known model of the system} with $f\in\mathcal{C}_1$. 
% \LY{Assume that the measurement of $x(k)$ is available.}

\begin{condition}
\label{cond:controllable} 
% ({{Controllability for NL-SS system}})
The state-space representation  \eqref{1} is controllable, which indicates that for any two states $x_1,x_2\in\mathcal{X}$ there exists a finite time $T_{\mathrm{d}}\in\mathbb{Z}$ and an admissible control function $u:[0,T_{\mathrm{d}}]\to\mathcal{U}$ such that $x_2 = \phi(T_{\mathrm{d}},0,x_1,u)$, where $\phi(t,0,x_0,u)$ denotes the unique solution of  \eqref{1} at time $t\geq0$ for a particular input function $u(\cdot)$ and initial condition $x(0) = x_0$.
\end{condition}



\subsection{\YL{Space-Filling Input Design Problem}}
\label{subsection:sf}


\YL{It is worth noting that the optimal input design problem corresponds to a  \emph{chicken-egg dilemma} because to optimally design an input signal for which the output response will be informative w.r.t. the identification objective, perfect knowledge of the underlying system dynamics in \eqref{1} is necessary, while in order to completely capture the dynamics of \eqref{1} from data, we need a data set that is informative w.r.t. our model structure. This inherent interdependency leads to a challenge in effectively addressing both tasks concurrently within the system identification cycle.} In this letter, we mainly consider the space-filling input design problem under a known nonlinear system model \eqref{1} irrespectively that it is the full or only the currently available model knowledge about the system to be identified. 

\begin{remark}
Under the condition that the model is unknown or it is approximately known with uncertainties, an iterative procedure can be adopted. \LY{Specifically, one may begin by assuming a region to explore and perform identification based on the dataset collected within this region. Subsequently, using the refined model, the space-filling input design can be repeated, taking into account the previously measured datasets}. This alternating procedure continues until it achieves an adequate level of modeling accuracy.
\end{remark}


Consider the set of parameterized input signals as:
\be
\label{para-u}
\mathscr{U}(\theta):=\LY{\{\theta\in{\Theta},k\in\mathbb{T}|u(k;\theta):\mathbb{T}\times \Theta\to\mathbb{R}^{n_\mathrm{u}}\}}%
\ee
\LY{where $\mathbb{T}$ is a finite interval in $\mathbb{Z}$} and satisfies the condition:

\begin{condition}
\label{cond:u}
\LY{The partial derivatives of any signal in $\mathscr{U}(\theta)$ w.r.t. the parameter $\theta\in{\Theta}$ exist and are bounded over $\forall k\in\mathbb{T}$,
where ${\Theta}\subseteq\mathbb{R}^{n_\mathrm{\theta}}$ is considered to be a compact set. }
% Furthermore, the length of $\mathscr{U}(\theta)$ is finite.}
\end{condition}

Note that there are no restrictions on the type and parametrization of the input signals in $\mathscr{U}(\theta)$. In other words,  \eqref{para-u} can represent a wide class of parametric input signals such that Condition \ref{cond:u} is satisfied. Here are two examples:
\begin{itemize}
\item Multisine signal parameterized by $\theta\triangleq\mathrm{vec}(A_l,\phi_l)\in\mathbb{R}^{n_\theta}$ with $n_\theta=2N_f$%:
\begin{small}
\be
\label{multisine}
\mathscr{U}_{\text{ms}}(\theta)\!:=\!\left\{\! \LY{k\!\in\!\mathbb{T}}\bigg|u(k;\theta)\!=\!\sum_{l=0}^{N\!_f\!-\!1}\!A_l\!\sin\!\left(\!2\pi l\frac{f_0}{f_s}\!k\!+\!\phi_l\!\right)\!\right\}\!
\ee
\end{small}%
where $A_l\in \mathbb{R}^{n_\mathrm{u}}$ and $\phi_l\in [0,2\pi]$ denote the amplitude and phase, respectively, \MK{$f_0$} represents the base frequency of the multisine, and $f_s$ is the sampling frequency.
\item Piecewise constant signal parameterized by  $\theta\triangleq\mathrm{vec}(A_l,p_l)\in\mathbb{R}^{n_\theta}$ with $n_\theta=2N+1$:
\be
\label{piecewise}
\mathscr{U}_{\text{pw}}(\theta)\!:=\!\left\{\! \LY{k\!\in\!\mathbb{T}}\bigg|u(k;\theta)\!=\!\sum_{l=0}^{N\!_p-1}\!A_l\delta\!\left(p_{l-1},p_l,k\right)\!\right\}  
\ee
where $A_l\in \mathbb{R}^{n_\mathrm{u}}$ denotes the amplitude levels and 
\be
\delta(p_{l-1},p_l,k) = \begin{cases}1, & p_{l-1} \leq k<p_{l} \\ 0, & \text{otherwise}\end{cases}
\ee denotes the duration of each amplitude level with dwell time $p_l \in\mathbb{T}$.
\end{itemize}


As discussed in Section \ref{sec:1}, \LY{a space-filling input facilitates an identified model with a small error, allowing \TR{the model to represent well}} over the region of interest, which is defined~as:
\begin{definition}
\label{ROI}
A region of interest is a compact subset of the joint input-state space $\tilde{\mathcal{Z}}\subseteq\mathcal{Z}$, where the behavior of the system is of particular interest for data-driven modeling.
\end{definition}



The primary goal of this letter is to design the parameter vector $\theta\in{\Theta}$ such that when the input signal $u(k;\theta)$ is applied to the assumed, known system  \eqref{1}, the resulting dataset is expected to achieve an effective space-filling property, ensuring a good coverage within $\tilde{\mathcal{Z}}$.






\section{Gaussian Process Models}
\label{sec:3}
% \subsection{\YL{Input Design for Data-driven Model Learning}}
\LY{In this section, we introduce some basic concepts from GP modeling}. GP regression \cite{rasmussen2003gaussian} is a well-known tool for constructing nonparametric, probabilistic models directly from data, and allows for flexible specification of prior assumptions on the system.
\LY{To construct a \emph{hypothetical} model of $f:\mathcal{Z}\to\mathcal{X}$, we consider $h:\mathcal{Z}\to\mathcal{X}$ as a GP.}
In terms of definition, a \emph{Gaussian Process} $\mathcal{GP}: \mathbb{R}^{n_\mathrm{z}} \to \mathbb{R}$ assigns to every point $ {z} \in \mathbb{R}^{n_\mathrm{z}}$ a random variable ${h}({z})$ taking values in $\mathbb{R}$ such that, for any finite set of points $\{{z}_{\tau}\}_{\tau=1}^N \subset \mathbb{R}^{n_\mathrm{z}}$, the joint probability distribution of $\mathcal{GP}\left( {z}_1\right), \ldots, \mathcal{GP}\left( {z}_{N}\right)$ is  Gaussian. A GP can be fully characterized by its mean function $\mu(z):\mathcal{Z}\to\mathbb{R}$ and its covariance function $\kappa(z,z'):\mathcal{Z}\times\mathcal{Z}\to\mathbb{R}$ and  it is denoted as:
$h(z)\sim\mathcal{GP}\left({\mu}({z}), {\kappa}\left( {z},  {z}^{\prime}\right)\right)$.


Given a training dataset $\mathcal{D}=\{Z,H\}$ with the training input matrix $Z =\mathrm{vec}(z_1,...,z_N)\in\mathbb{R}^{N\times n_\mathrm{z}}$ and the training output $H =\mathrm{vec}(h_1,...,h_N)\in\mathbb{R}^{N\times 1}$, with $h_i=h(z_i)$. Under the assumption that function $h(z)$ follows a GP, one can define the Gaussian prior distribution for %the function 
$h$ as $\mathbb{P}\left(H\right) = \mathcal{N}(H\mid 0,K_N)$  where  $K_N\in\mathbb{R}^{N\times N}$ is the Gram matrix with entries $[K_N]_{i,j} = \kappa(z_i,z_j)$. Given a query point $\bar{z}\in\mathbb{R}^{n_\mathrm{z}}$, the joint distribution of $H$ and $\bar{h}(\bar{z})$ is:
{\setlength\abovedisplayskip{1mm} \setlength\belowdisplayskip{1mm}
\be
\left(\begin{array}{c}
H \\ 
\bar{h}\end{array} \Bigg| \begin{array}{c}
Z \\
\bar{z}
\end{array}\right) \sim \mathcal{N}\left( \begin{array}{c}
{0}
\end{array},\begin{bmatrix}
{K}_{N} & {\kappa}(\bar{z},Z)^{\top} \\
{\kappa}(\bar{z},Z) & {\kappa}(\bar{z},\bar{z})
\end{bmatrix}\right).
\ee}%
Then conditioning the joint distribution on  $\mathcal{D}$ leads to the predictive distribution of $\bar{h}(\bar{z})$ given an input $\bar{z}$:
{\setlength\abovedisplayskip{1mm} \setlength\belowdisplayskip{1mm}
\be
\mathbb{P}(\bar{h}\mid\bar{z},\mathcal{D}) 
=\mathcal{N}(\bar{h}\mid \mu(\bar{h}\mid\bar{z},\mathcal{D}), \mathrm{Var}(\bar{h}\mid\bar{z},\mathcal{D}))
 \ee}%
 with the mean and variance expressed respectively as:
{\setlength\abovedisplayskip{1mm} \setlength\belowdisplayskip{1mm}
\begin{subequations}
\begin{align}
\label{mean}\mu(\bar{h}\mid\bar{z},\mathcal{D})&\!=\! {\kappa}(\bar{z},Z)K_N^{-1}H,\\
 \label{var}
 \mathrm{Var}(\bar{h}\mid\bar{z},\mathcal{D})&\!=\!{\kappa}(\bar{z},\bar{z})\!-\!{\kappa}(\bar{z},Z)K_N^{-1}{\kappa}(\bar{z},Z)^{\top}.
 \end{align}
\end{subequations}}%

\begin{remark}
\label{remarl:variance}
According to  \eqref{var}, it can be deduced that the posterior variance evaluated at $\bar{z}$ only depends on the GP inputs $Z$, which corresponds to the \TR{particular property of Gaussian distribution}. Hence, the posterior variance of a GP inherently represents the distribution of GP inputs $Z$ \TR{in the given dataset over the space $\mathcal{Z}$}. 
\end{remark}




\section{GP-based Space-filling Input Design}
\label{sec:4}
\YL{In this section, we propose a GP-based space-filling input design algorithm, enabling the acquisition of an informative dataset for data-driven model learning that achieves good coverage within the region of interest $\tilde{\mathcal{Z}}$. The main philosophy behind this is to firstly place a GP prior on a latent \emph{hypothetical} model $\hat{x}(k+1)=\hat{f}(x(k),u(k))$ of the data-generating system  \eqref{1}, and then construct the space-filling promoting cost function from the Bayesian point of view. } 


\subsection{Classical Space-filling Design}\label{sec:4:1}
In this subsection, we briefly introduce the concept of the classical space-filling design method \cite{johnson_minimax_1990}.

\begin{definition}
\label{distance}
A function ${d}(x_1,x_2):\mathcal{X}\times\mathcal{X}\to\mathbb{R}_0^{+}$ is a distance-based metric if it satisfies:
\begin{itemize}
\item $d(x_1,x_2)=d(x_2,x_1)$, $\forall {x_1,x_2}\in\mathcal{X}$,
\item $d(x_1,x_2)\geq0$ with equality if and only if $x_1=x_2$,
\item $d(x_1,x_2)\leq d(x_1,x_3)+d(x_2,x_3)$,  $\forall {x_1,x_2}$ and $x_3 \in\mathcal{X}$.
\end{itemize}
\end{definition}



\begin{definition}
\label{SF}
% ({Minimax space-filling property of a dataset} )
Consider a dataset $\mathcal{D}_N=\{z_i\}_{i=1}^N$ and a region of interest ${\tilde{\mathcal{Z}}}$. 
Let $d(\varsigma,\mathcal{D}_N)=\min_{z_i\in\mathcal{D}_N}d(\varsigma,z_i)$ denote the minimum distance w.r.t. $\varsigma$ where $d(\cdot,\cdot)$ is given in Definition \ref{distance}. Then
{\setlength\abovedisplayskip{1pt} \setlength\belowdisplayskip{1pt}
\be
\LY{\rho({\mathcal{D}_N})} = \max_{\varsigma\in\tilde{\mathcal{Z}}}d(\varsigma,\mathcal{D}_N)
\ee}% 
is the so-called filling distance indicating the radius of the largest ball that can be placed in $\tilde{\mathcal{Z}}$ which does not contain any point in $\mathcal{D}_N$.
Specifically, if there exists a constant $\epsilon$ such that $\LY{\min_{\mathcal{D}_N}\rho({\mathcal{D}_N})}<\epsilon, ~\epsilon>0$, then \LY{$\mathcal{D}_N$ is said to have the at least $1/\epsilon$ density w.r.t $\tilde{\mathcal{Z}}$, i.e., the $\epsilon$-space filling property.}
\end{definition}

The classical space-filling design often focuses on the distribution of the points within $\mathcal{D}_N$, without considering the temporal dynamics between the points, that is, the underlying dynamics that generate these data points are neglected. Hence, we highlight the nonlinear dynamical systems and design \emph{identification-oriented} space-filling input signals.


\subsection{Space-filling Promoting Cost Function}
The space-filling promoting cost function is constructed based on the dataset $\mathcal{D}_N=\{z_j\}_{j=1}^{N}$ generated by the parameterized input signal $u(k;\theta)$ after applying it to the assumed model of the system  \eqref{1}. Based on the dataset $\mathcal{D}_N$, one has  following data matrix $Z = \mathrm{vec}(z_1,\ldots,z_N)\in\mathbb{R}^{N\times n_z}$ 
where $z_i = \mathrm{vec}(x(i),u(i;\theta))$, $i\in\mathbb{I}_{i=1}^{N}$.


Additionally, to facilitate the design of the cost function, we define anchor points as follows in our learning scenario:
\begin{definition}
\label{def:inf}
Given a region of interest $\tilde{\mathcal{Z}}\subseteq\mathcal{Z}$, the \TR{anchor dataset} is characterized by $\mathcal{D}_\mathrm{I}:=\{\tilde{z}_i=\mathrm{vec}(\tilde{x}_i,\tilde{u}_i)\}_{i=1}^{M}$, which contains $M$ pre-defined anchor points from $\mathcal{Z}$ to capture the dynamics, thereby gridding the region of interest~$\tilde{\mathcal{Z}}$.
\end{definition}



\YL{We place a GP prior on the latent \emph{hypothetical} model $\hat{x}(k+1)=\hat{f}(x(k),u(k))$ of the known data-generating system  \eqref{1}, which results in $\hat{f}\sim\mathcal{GP}\left({\mu}({z}), {\kappa}\left( {z},  {z}^{\prime}\right)\right)$.
% latent input-output space to construct a nonparametric and probabilistic synthetic model $\hat{f}$ with, where 
The kernel function $\kappa\left({z},{z}^{\prime}\right)$ satisfies the following assumption:}
\begin{condition}
\label{assump:kernel}
\YL{The kernel function $\kappa(\|z-z'\|)=\kappa(z,z')$ is chosen as a degenerating, monotonically decreasing function w.r.t. $\|z-z'\|$.
}
\end{condition}

The decreasing monotonicity of $\kappa(z,z')$ indicates that the association between the anchor points and $Z$ diminishes with the increase of the Euclidean distance $\|z-z'\|$.
The kernel is 
usually chosen from the exponential family, e.g., the \emph{Squared Exponential Automatic Relevance Determination} (SEARD) 
$\kappa\left({ {z}},  {  {z}}'\right)\!=\!\sigma_{\mathrm{f}}^{2} \exp \left(-\frac{1}{2}( {  {z}}\!-\!  {  {z}}')^{\top} {\Lambda}^{-1}( {  {z}}\!-\! {  {z}}')\right)\!$ is taken as a prior with $\sigma_{\mathrm{f}}^{2}\in\mathbb{R}^{+}$ and~$\Lambda = \mathrm{diag}(\ell_1^2,...,\ell_{n_\mathrm{z}}^2)\in\mathbb{R}^{n_\mathrm{z}\times n_\mathrm{z}}$.






To obtain a good coverage for the region of interest, the posterior variance at each anchor point:
{\setlength\abovedisplayskip{1mm} \setlength\belowdisplayskip{1mm}
\be
\label{varcost}
\mathrm{Var}(\hat{f}\mid\tilde{z}_i,\mathcal{D}_N)={\kappa}(\tilde{z}_i,\tilde{z}_i)-{\kappa}(\tilde{z}_i,Z)K_N^{-1}{\kappa}(\tilde{z}_i,Z)^{\top}
\ee}%
should be minimized. This leads to optimizing the scalar-valued space-filling promoting cost function defined~as:
{\setlength\abovedisplayskip{1pt} \setlength\belowdisplayskip{1pt}
\be
\label{GPcost-data}
\mathcal{V}(\mathcal{D}_N):=\frac{1}{M} \sum_{i=1}^{M} \mathrm{Var}(\hat{f}\mid\tilde{z}_i,\mathcal{D}_N)
\ee}%
based on the dataset $\mathcal{D}_N$.

We would like to find a  parametrized input signal $u(k;\theta)$ such that the resulting dataset $\mathcal{D}_N(\theta)$ \LY{is expected to achieve the $1/\epsilon$ density}, ensuring a good coverage within the region of interest $\tilde{\mathcal{Z}}$. To this end, when taking the dynamical system \eqref{1} into account, the posterior variance at each anchor point can be rewritten in a $\theta$-dependent form:
{\setlength\abovedisplayskip{1mm} \setlength\belowdisplayskip{1mm}
\be
\label{varcost-theta}
\mathrm{Var}(\hat{f}\!\mid\!\tilde{z}_i,\!\mathcal{D}_N\!(\theta))\!=\!{\kappa}(\tilde{z}_i,\!\tilde{z}_i)\!-\!{\kappa}(\tilde{z}_i,\!Z_\theta)K_N^{-1}{\kappa}(\tilde{z}_i,\!Z_\theta)^{\top}.
\ee}%
The space-filling input design problem can be formalized~as:
\vspace{-2mm}
{\setlength\abovedisplayskip{1pt} \setlength\belowdisplayskip{1pt}
\begin{subequations}
\begin{align}
\underset{\theta\in{\Theta}}{\text{min}} 
 & \quad \mathcal{W}({\theta};\mathcal{D}_N(\theta)) \label{f1}\\
 \text{s.t.} & \quad x(0)=x_0, \quad u(0)=u_0,\\
 & \quad x(j+1) = f(x(j),{u}(j;\theta)),\\
 &\quad \mathcal{D}_N(\theta)\!:=\!\{{Z}^{(j)}_\theta\!=\!\mathrm{vec}(x(j),u(j;\theta))\}_{j=1}^{N},
\end{align}
\end{subequations}}%
where 
{\setlength\abovedisplayskip{1pt} \setlength\belowdisplayskip{1pt}
\be
\label{GPcost-data-input}
\mathcal{W}(\theta;\mathcal{D}_N(\theta)):=\frac{1}{M} \sum_{i=1}^{M} \mathrm{Var}(\hat{f}\mid\tilde{z}_i,\mathcal{D}_N(\theta)).
\ee}%

The implementation of the proposed method is summarized in Algorithm \ref{alg:1}.

\begin{algorithm}[tb]
   \caption{\YL{GP-based Space-Filling Input Design for Nonlinear Dynamic Model Learning}}
   \label{alg:1}
\begin{algorithmic}
   \STATE{\bf{Step 1:}} \YL{Select the number of samples $N$, learning rate $\alpha$ and learning threshold $\delta$. Set the iteration index $\iota=0$. Initialize $u(k;\theta^{(0)})$ with $\theta^{(0)}=\theta_0$};
   \STATE{\bfseries Step 2:}  Select the region of interest $\tilde{\mathcal{Z}}$ together with the \YL{anchor dataset $\mathcal{D}_\mathrm{I}$};
   \STATE {\bfseries Step 3:} \YL{Set the GP kernel $\kappa(z,z')$ with pre-defined hyperparameters $\Lambda$, $\sigma_{\mathrm{f}}^{2}$};
   % on $\tilde{\mathcal{Z}}$
   \STATE {\bfseries Step 4:}
   \YL{\WHILE{$\|\theta^{(\iota+1)}-\theta^{(\iota)}\|\geq\delta$}
   \STATE Apply $u(k;\theta^{(\iota)})$ on  \eqref{1} and obtain $\mathcal{D}_N(\theta^{(\iota)})$;
    \FOR{$i=1$ {\bfseries to} $M$}
   \STATE Compute $\mathrm{Var}(\hat{f}~|~\tilde{z}_i,\mathcal{D}_N(\theta^{(\iota)}))$ according to  \eqref{varcost-theta};
   \ENDFOR
   \STATE Compute $\mathcal{W}(\theta^{(\iota)};\mathcal{D}_N(\theta^{(\iota)}))$ according to  \eqref{GPcost-data-input};
   \STATE Set $\theta^{(\iota+1)}=\theta^{(\iota)}-\alpha\nabla_{\theta}\mathcal{W}(\theta^{(\iota)};\mathcal{D}_N(\theta^{(\iota)}))$;
   % Solve $\theta^{(\iota+1)}=\mathrm{argmin}_{ \theta\in\Theta}~ \mathcal{W}(\theta^{(\iota)};\mathcal{D}_N)$;
   \STATE Set $\iota\gets\iota+1$;
    \ENDWHILE}

   \YL{\STATE {\bfseries Step 5:} Set $\hat{\theta}\gets\theta^{(\iota)}$, and adopt $u(k;\hat{\theta})$ as the space-filling input signal.}
   
   
\end{algorithmic}
\end{algorithm}


\vspace{-1mm}
\subsection{Theoretical Analysis}

We first illustrate the space-filling property of  $\mathcal{D}_N$ w.r.t. a space-filling anchor dataset $\mathcal{D}_\mathrm{I}$  by selecting a proper kernel function, which is stated in the following lemma.


\begin{lemma}
\label{lemma:1}
Consider a dataset $\mathcal{D}_N$ and a region of interest $\tilde{\mathcal{Z}}$ which is characterized by $M$ pre-defined anchor points  $\mathcal{D}_\mathrm{I}=\{\tilde{z}_i\}_{i=1}^{M}$.
The GP-based space-filling promoting cost function is appropriately defined as \eqref{GPcost-data}, \TR{with a kernel function $\kappa(z,z')$ satisfying Condition \ref{assump:kernel}}. If:
\begin{itemize}
\item $\mathcal{V}(\mathcal{D}_N)=0$; 
\item \LY{The anchor dataset $\mathcal{D}_\mathrm{I}$ has the  $1/\epsilon$ density;}
\item \LY{The size $N$ of dataset $\mathcal{D}_N$ is finite;}
\end{itemize}
then $\mathcal{D}_N$  \LY{is guaranteed to have the at least $1/\epsilon$ density w.r.t.~$\tilde{\mathcal{Z}}$}.
\end{lemma}

\begin{proof}
The first step is to prove that the posterior variance at $\tilde{z}_i$: $\mathrm{Var}(\hat{f}\!\mid\!\tilde{z}_i,\!\mathcal{D}_N)\!=\!{\kappa}(\tilde{z}_i,\tilde{z}_i)\!-\!{\kappa}(\tilde{z}_i,\!Z)K_N^{-1}{\kappa}(\tilde{z}_i,Z)^{\top} \!=\! 0$ \emph{if and only if} there exists $j\in\mathbb{I}_1^{N}$ such that  $\tilde{z}_i = z^{(j)}$.

(\emph{Sufficiency}) Assume that $\tilde{z}_i \in \mathcal{D}_N$, i.e., there exists $j\in\mathbb{I}_1^{N}$ such that  $\tilde{z}_i = z^{(j)}$. This indicates that ${\kappa}(\tilde{z}_i,Z)={\kappa}(z^{(j)},Z)$ is the $j^{\mathrm{th}}$ row of the Gram matrix $K_N$, that is,
${\kappa}(\tilde{z}_i,Z) = \mathrm{e}_jK_N$ 
where $ \mathrm{e}_j = ( \delta_{1,j}, \dots, \delta_{N,j} )$ is a standard basis of $\mathbb{R}^{N}$ and 
$\delta_{j,i} = 1$ if  $j = i$,  and  $\delta_{j,i} = 0$ if  $j \neq i$.
Then we have: $\mathrm{Var}(\hat{f}\mid\tilde{z}_i,\mathcal{D}_N)=\kappa(z^{(j)},z^{(j)})-\mathrm{e}_jK_NK_N^{-1}K_N^{\top}\mathrm{e}_j^{\top}
=\kappa(z^{(j)},z^{(j)})-\mathrm{e}_jK_N\mathrm{e}_j^{\top}
=\kappa(z^{(j)},z^{(j)})- \kappa(z^{(j)},z^{(j)})=0$, where $\mathrm{e}_jK_N\mathrm{e}_j^{\top}$ denotes the $j^{\mathrm{th}}$ diagonal element of $K_N$. This constitutes the proof of the sufficiency.

(\emph{Necessity}) We show the necessity part by contradiction. Suppose that
$\tilde{z}_i \notin \mathcal{D}_N$,
and ${\kappa}(\tilde{z}_i,\tilde{z}_i)-{\kappa}(\tilde{z}_i,Z)K_N^{-1}{\kappa}(\tilde{z}_i,Z)^{\top}=0$ still holds. \LY{Since the kernel function $\kappa(\cdot,\cdot)$ is positive definite, the augmented matrix  $\left[\begin{smallmatrix} K_N & \kappa(\tilde{z}_i, Z)^{\top} \\ \kappa(\tilde{z}_i, Z) & \kappa(\tilde{z}_i, \tilde{z}_i) \end{smallmatrix}\right] \succ 0$
holds for all $\tilde{z}_i \notin \mathcal{D}_N$. By the Schur complement condition for positive definiteness, the Schur complement of $K_N$  must be positive, that is, ${\kappa}(\tilde{z}_i,\tilde{z}_i)-{\kappa}(\tilde{z}_i,Z)K_N^{-1}{\kappa}(\tilde{z}_i,Z)^{\top}>0$, $\forall \tilde{z}_i \notin \mathcal{D}_N$.} This contradicts with the assumption, which implies that only if $\tilde{z}_i \in \mathcal{D}_N$,
$\mathrm{Var}(\hat{f}\mid\tilde{z}_i,\mathcal{D}_N)=0$ holds.




Next, based on the results above, we need to further prove that the cost $\mathcal{V}(\mathcal{D}_N)=0$ \emph{if and only if} every anchor point has data on it.

(\emph{Sufficiency}) Assume that $\forall i\in\mathbb{I}_1^{M}$, the anchor point $\tilde{z}_i$
 has at least one data $z^{(j)}$ on it. Based on the proof in the first step, we have
$ \mathrm{Var}(\hat{f}\mid\tilde{z}_i,\mathcal{D}_N)=0$.
This leads to $\mathcal{V}(\mathcal{D}_N)=0$ .

(\emph{Necessity}) Suppose that $\mathcal{V}(\mathcal{D}_N)=0$. According to its definition given in \eqref{GPcost-data}, it can be easily derived that
$\frac{1}{M} \sum_{i=1}^{M} \mathrm{Var}(\hat{f}\mid\tilde{z}_i,\mathcal{D}_N)=0$.
The non-negative posterior variance implies that $\mathrm{Var}(\hat{f}\mid\tilde{z}_i,\mathcal{D}_N)=0$ for all $i\in\mathbb{I}_1^{M}$. Again, based on the proof in the first step, $\mathrm{Var}(\hat{f}\mid\tilde{z}_i,\mathcal{D}_N)=0$ holds if and only if $\tilde{z}_i \in \mathcal{D}_N$. Therefore, every anchor point $\tilde{z}_i$ must be a point in $\mathcal{D}_N$, i.e., 
$\exists \, z^{(j)} \in Z$  such that  $z^{(j)} = \tilde{z}_i$.
Since the anchor dataset $\mathcal{D}_\mathrm{I}$ is designed to be space-filling inside the entire region of interest $\tilde{\mathcal{Z}}$, the dataset $\mathcal{D}_N$ must cover $\tilde{\mathcal{Z}}$ as well. This completes the proof.
\end{proof}

Based on Lemma~\ref{lemma:1}, subsequently, we show the $\epsilon$-space filling property of a dataset $\mathcal{D}_N(\theta)$ which is generated by the optimized input applied to the dynamical system. \LY{To achieve this, the following condition is assumed to be satisfied for a set of parametrized input signal $\mathscr{U}(\theta)$ given in \eqref{para-u}:}
\begin{condition}
\label{cond:4}
\LY{There exists a sufficiently flexible parametrization for $\mathscr{U}(\theta)$ such that it can realize any possible input sequence, for example, $u(k;\theta)=\theta_k, \forall k\in\mathbb{Z}$.}
\end{condition}

Then the main theorem is stated as follows:
\begin{theorem}
\label{theo}
Consider system \eqref{1} which satisfies Condition \ref{cond:controllable} and the parameterized input signal $u(k;{\theta})$ which satisfies Conditions \ref{cond:u} and \ref{cond:4}. The GP-based space-filling promoting cost function is defined as \eqref{GPcost-data-input} \YL{on the basis of the $\epsilon$-space filling anchor dataset $\mathcal{D}_\mathrm{I}=\{\tilde{z}_i\}_{i=1}^{M}$}, with a kernel function $\kappa(z,z')$ satisfying Condition \ref{assump:kernel}. Then the dataset $\mathcal{D}_N(\hat{\theta})$ generated by the optimized input signal $u(k;\hat{\theta})$ achieves the \LY{$\epsilon$-space filling} property in the region of interest $\tilde{\mathcal{Z}}$ with a finite data length $N\geq M \times (T_{\mathrm{d}}+1)$, where $\hat{\theta}=\mathrm{arg\,min}_{\theta\in\Theta}~ \mathcal{W}(\theta;\mathcal{D}_N(\theta)).$
\end{theorem}

\begin{figure*}[t]
\centering 
\includegraphics[width=5.3in]{Figs/fig_1.pdf}
\vspace{-4mm}
\caption{Space-filling performance of $\mathcal{D}_N(\theta)$ (\protect\tikz[baseline=-0.5ex] \protect\draw[line width=0.3mm, blue] (-0.08,-0.08) -- (0.08,0.08) (-0.08,0.08) -- (0.08,-0.08);) under different numbers of anchor points (\protect\tikz[baseline=-0.5ex] \protect\draw[red, fill=red] (0,0) circle (0.03cm);) $M=4,9,16$ with $N=40$ when the optimized $\mathcal{W}({\theta};\mathcal{D}_N(\theta))\to 0$, where \protect\tikz[baseline=-0.5ex] \protect\draw[fill=red!30, draw=red!30] (-0.15, -0.08) rectangle (0.15, 0.08); represents the largest ball in $\tilde{\mathcal{Z}}$ that does not contain any point in $\mathcal{D}_N(\theta)$. The boxplot illustrates that $\mathcal{D}_N(\theta)$ always exhibits a lower $\rho$ than the theoretically guaranteed value ($\epsilon = 2.8, 1.4, 0.94)$ over 20 Monte Carlo realizations with $\theta_k^{(0)}\sim\mathcal{N}(0,1)$, $\forall k\in\mathbb{I}_1^{N}$.
}\label{fig:LTI}  
\end{figure*}


\begin{proof}
On the basis of Condition \ref{cond:controllable}, it is obvious that starting from  any $z_j=\mathrm{vec}(x(j),u(j))\in\mathcal{Z}$, it takes 1 step to reach $x(j+1)$, and another $T_\mathrm{d}$ steps to reach any anchor point $\tilde{z}_i=\mathrm{vec}(\tilde{x}(i),\tilde{u}(i))\in\tilde{\mathcal{Z}}$, $\forall i\in\mathbb{I}_1^M,j\in\mathbb{I}_1^N$.
Hence, all $M$ anchor points can be explored with a finite data length $N\geq M \times (T_{\mathrm{d}}+1)$.
Condition \ref{cond:4} guarantees that the parametrization of $\mathscr{U}({\theta})$ is sufficiently flexible to generate any value of the input. 
Combining again with Condition \ref{cond:controllable}, it can be deduced that there exists a parameter vector $\theta\in\Theta$ such that the corresponding input signal $u(k;{\theta})$ can generate a dataset $\mathcal{D}_N(\theta)$ that satisfies $\mathcal{V}(\mathcal{D}_N)=0$.
Then according to Lemma \ref{lemma:1}, one can readily conclude that the dataset $\mathcal{D}_N(\hat{\theta})$  generated by  $u(k;\hat{\theta})$ optimized based on the cost function \eqref{GPcost-data-input} \LY{is guaranteed to have the $1/\epsilon$ density w.r.t. $\tilde{\mathcal{Z}}$.} 
\end{proof}



\begin{remark}
Theorem \ref{theo} implies that minimizing \eqref{GPcost-data-input} leads to the $\epsilon$-space filling regardless of the hyperparameter choice. It is worth mentioning that our proposed space-filling input design strategy is not concerned with the GP mean estimate, as the standard GP \emph{regression} does. In practice, the selection of $\sigma_{\mathrm{f}}^{2}$ and $\Lambda$ can make the problem easier to optimize.
Moreover, in the finite data case, the length scale $\Lambda$ should be chosen considering both the interval between anchor points and the size of $N$ to ensure sufficient coverage.
\end{remark}

\vspace{-1mm}
\section{\YL{Simulation Experiments}}
\label{sec:5}
In this section, simulation results for a linear system and a nonlinear mass-spring-damper system are presented to demonstrate the effectiveness of our proposed strategy. 
Note that to quantify the space-filling behavior of the proposed algorithm, we employ the distance-based metric presented in Section \ref{sec:4:1} to verify whether the dataset $\mathcal{D}_N(\theta)$ also exhibits classical space-filling properties. The criterion $\rho$  w.r.t. a dataset $\mathcal{D}$ is denoted~as:
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\be
\label{rho-metric}
\rho(\mathcal{D}) = \max_{\varsigma\in\tilde{\mathcal{Z}}}\min_{z_j\in\mathcal{D}}\|\varsigma-z_j\|_{Q},~\quad \forall  j\in\mathbb{I}_1^{|\mathcal{D}|},
\ee}%
where $\varsigma$ represents a set of uniformly gridded points over $\tilde{\mathcal{Z}}$, with 100 points along each dimension, $\|\varsigma-z_j\|_{Q}=\sqrt{(\varsigma-z_j)^{\top}Q(\varsigma-z_j)}$
denotes the normalized distance between $\varsigma$ and $z_j$.
\vspace{-2mm}
\subsection{Linear system example}
Consider the continuous-time linear time-invariant system:
{\setlength\abovedisplayskip{1pt} \setlength\belowdisplayskip{1pt}
\be
\label{16}
\dot{x} = Ax + Bu
\ee}%
where $A=\begin{bmatrix}
0 & 1 \\
-0.3 & -0.5
\end{bmatrix}$ and $B=\begin{bmatrix}
0 \\
1
\end{bmatrix}$. The model \eqref{16} is discretized using zero-order hold on the input with a sample time of $T_s = 1$s. The region of interest is selected as  $\tilde{\mathcal{Z}} = [-2,2] \times [-2,2]$. The input sequence is a fully parameterized signal $u(k;\theta)=\theta_k$ with  $N=40$ samples, where the input value at each time step is treated as an optimization variable. Initially, $\theta_k^{(0)}\sim\mathcal{N}(0,1)$ for all $k\in\mathbb{I}_1^{N}$.
Figure~\ref{fig:LTI} illustrates the space-filling performance under different numbers of anchor points when the optimized cost tends to 0 with $Q$ set as an identity matrix. 
It can be observed that $\mathcal{D}_N(\theta)$ is lower than the theoretically guaranteed space-filling property ($\epsilon = 2.8, 1.4, 0.94$) for 20 Monte Carlo realizations.
Moreover, as  $M$ increases, the radius of the largest ball decreases, indicating a corresponding increase in density, thereby leading to improved space-filling performance.
\vspace{-1mm}
\subsection{Nonlinear mass-spring-damper system example}
We consider a nonlinear mass-spring-damper which consists of a horizontally moving mass fixed in a rail connected by a spring to the ceiling. The system is excited by an external force $F$.
By first-principle modeling, one can derive the dynamics in terms of state-space representation \cite{KISS2024562}:
{\setlength\abovedisplayskip{1mm} \setlength\belowdisplayskip{1mm}
\begin{equation}
\label{MSD}
\dot{x}_1\!=\! x_2,
\dot{x}_2 \!=\! \left(F\! - \!b\eta^{-1}(x_1)\!\left(\alpha(x_1) - l\right){x_1} \!-\! c x_2\right)/m
\end{equation}}%
where $x_1$ and $x_2$ denote the position and velocity of the moving body, respectively, and $\eta(x_1)=\sqrt{x_1^2+a^2}$. The physical parameters are set as $l=0.17$~m, $a=0.25$~m, $m=5$~kg,  $b=800$~N/m, and  $c=10$~Ns/m.


Regarding the parametric input signal, a periodic multisine excitation according to  (\ref{multisine}) is selected with $N_f=92$ excited frequencies between $[1,10]$ Hz, a sampling frequency $f_s = 100$ Hz, and $N = 1024$ points per period for this example and with $k_{\mathrm{min}}= 11$ and $k_{\mathrm{max}} = 102$. 
The optimization is performed by tuning both the amplitudes and the phases of each frequency component, denoted as $\theta = \mathrm{vec}(A_l,\phi_l),l\in\mathbb{I}_{0}^{N_f-1}$.
Specifically, the initialization of the phase $\phi_l$ is realized by sampling from a uniform distribution $\mathcal{U}(0,2\pi)$, and the initial value for the amplitude $A_l$ is selected as 100 $N$.  In addition, considering the output limitations of the actuators, we set the upper bound of $A_l$ as 200 $N$. To simplify the optimization problem, we assume that all frequency components share the same amplitude. The region of interest is selected as $\tilde{\mathcal{Z}}:=[-400,400]\times[-2,2]\times[-20,20]$, with  $M = 512$ uniformly distributed anchor points $\tilde{z}_i$ with $i\in\mathbb{I}_{i}^{M}$. The kernel hyperparameters are chosen as $\sigma_{\mathrm{f}}^{2}=10$ and  $\Lambda^{1/2} = \mathrm{diag}(120, 0.6, 6)$.

A 3D depiction of the considered domain can be observed in Fig.~\ref{fig:isometricView}. Although the initial data is highly concentrated around the center of the domain, after optimization a clear space-filling behavior can be observed. 
It is important to emphasize that these results were observed using a signal with $N=1024$ samples while exciting only a limited range of frequencies. 
Table~\ref{tab:1} provides a quantitative evaluation of the value of $\rho(\mathcal{D}_N(\theta))$ and $\rho(\mathcal{D}_\mathrm{I})$ (mean value over 10 Monte Carlo simulations) under different sizes of $\mathcal{D}_\mathrm{I}$ before and after optimization with the normalization weight  $Q = \mathrm{diag}\left(\frac{1}{400^2},\frac{1}{2^2},\frac{1}{20^2}\right)$.
 It can be observed that minimizing the proposed GP-based space-filling promoting cost function  \eqref{GPcost-data-input} likewise leads to the minimization of the classical criterion \eqref{rho-metric}. Even though the optimization is nonlinear in this scenario, which prevents the optimization cost from reaching 0 and results in $\rho(\mathcal{D}_N(\theta))$  being slightly larger than  $\rho(\mathcal{D}_\mathrm{I})$  in the $M=512$ case, our proposed method still achieves adequate space-filling behavior.


\begin{figure}[t]
\centering     
\includegraphics[width=1.15in]{Figs/MSD.pdf} 
\caption{An illustration of the nonlinear mass-spring-damper system.}
\label{fig:4}
\end{figure}

\begin{figure}[t]
\centering     
\includegraphics[width=2.4in]{Figs/fig_7.pdf}  
% 2D_isometricView
\vspace{-3mm}
\caption{\TR{Comparison of initial dataset $\mathcal{D}_N(\theta^{0})$(\protect\tikz \protect\draw[line width=0.8mm, color={rgb:red,0;green,0.43;blue,0.75}] (0,0) -- (0.08,0);) and optimized points $\mathcal{D}_N(\hat{\theta}) $(\protect\tikz \protect\draw[line width=0.8mm, color={rgb:red,0.85;green,0.35;blue,0.13}] (0,0) -- (0.08,0);) in the 3D space. The anchor points are shown by the shaded~cube.}}
\label{fig:isometricView}
\end{figure}

\begin{table}[H]
        \centering
        \caption{Quantitative evaluation of $\rho$ under different sizes of $\mathcal{D}_\mathrm{I}$}
        \vspace{-3mm}
        \label{tab:1}
        \renewcommand\arraystretch{1.2}
        \begin{tabular}{cccc}
        \toprule[1.5pt]
        \multicolumn{2}{c}{Num. of anchor points} & $\rho(\mathcal{D}_N(\theta))$ & $\rho(\mathcal{D}_\mathrm{I})$        \\ \hline
        \multirow{2}{*}{$M=512$}      & Init.     & 1.2756               & \multirow{2}{*}{0.2449} \\ \cline{2-2}
                                                                  & Opt.      &  \bf{0.3029}              &                    \\ \hline
        \multirow{2}{*}{$M=216$}      & Init.     & 1.2862             & \multirow{2}{*}{0.3429} \\ \cline{2-2}
                                                                  & Opt.      & \bf{0.3229}           &                    \\ \hline
        \multirow{2}{*}{$M=125$}      & Init.     & 1.2816            & \multirow{2}{*}{0.4286} \\ \cline{2-2}
                                                                  & Opt.      & \bf{0.3500}              &                    \\ \bottomrule[1.2pt]
        \end{tabular}
        \end{table}
        \vspace{-1mm}






\vspace{-1mm}
\section{Conclusion}
\label{sec:6}
% This letter presented a solution to the space-filling input design problem. The proposed method accomodates a wide range of parametrizations of the input and applies to a wide range of nonlinear dynamical systems.
% xxx
% The effectiveness of the approach is 

This letter presents an effective identification-oriented GP-based space-filling input design strategy for nonlinear dynamical model learning. A GP prior is placed on the latent joint input-state space, resulting in an innovative space-filling promotion cost function from a Bayesian perspective. Theoretical analysis has been provided to demonstrate the equivalence between the convergence of the cost function and the expected space-filling behavior.
The introduced approach provides
new perspectives into the input design for data-driven model learning, especially in nonlinear cases lacking systematic design methods \MK{and with the flexible parameterization of the input signal}. The effectiveness of the proposed strategy has
been analyzed and demonstrated by numerical simulations.

\vspace{-2mm}


\bibliographystyle{IEEEtran}
\bibliography{references}










\end{document}
