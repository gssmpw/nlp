\begin{algorithm*}


\caption{Process Reinforcement through Implicit Rewards (PRIME)}
\textbf{Input} supervised fine-tuned model $\pi_{\theta_{\text{init}}}$; ground truth outcome reward verifier $R_{GT}$; instruction dataset $\mathcal{D}$; hyperparameters $\beta$, $\epsilon$, M, N, K, $\gamma$.

\textbf{Notation} $r_i$ represents the outcome reward of $\mathbf{y}_i$, and $r_i^t$ denotes its process reward at step $t$.
Define $r_\phi \left(\mathbf{y}\right) = \beta \log \frac{\pi_\phi(\mathbf{y})}{\pi_{\text{ref}}(\mathbf{y})}$, $\mathbf{x}$ is omitted for simplicity.
\begin{algorithmic}[1]
\State Initialize policy model $\pi_\theta \leftarrow \pi_{\theta_{\text{init}}}$, $\pi_{\theta_{\text{old}}} \leftarrow \pi_{\theta_{\text{init}}}$, implicit PRM $\pi_{\phi} \leftarrow \pi_{\theta_{\text{init}}}$, reference model $\pi_{\text{ref}} \leftarrow \pi_{\theta_{\text{init}}}$
\For{iteration = 1, \dots, N}
    \State Sample batch of prompts $\mathcal{B} \sim \mathcal{D}$
    \State $\mathcal{T} \leftarrow \{\}$
    % \State $\mathcal{P} \leftarrow \{\}$
    \For{each instruction $\mathbf{x} \in \mathcal{B}$}
        \State Generate $K$ responses: $\{\mathbf{y}_1, ..., \mathbf{y}_K\} \sim \pi_\theta(\cdot|\mathbf{x})$
        \State Compute ground truth rewards: $r_i = R_{GT}\left(\mathbf{x}, \mathbf{y}_i\right)$ for $i \in \{1,...,K\}$

        \State Collect correct responses $\mathcal{C}_\mathbf{x} = \{\mathbf{y}_i | r_i = 1\}$
        \If{$|\mathcal{C}_\mathbf{x}| / K > 0.2$ and $|\mathcal{C}_\mathbf{x}| / K < 0.8$}
            \State Add all (instruction, response, ground truth reward) triples to $\mathcal{T}$:
            \[\mathcal{T} \leftarrow \mathcal{T} \cup \{(\mathbf{x}, \mathbf{y}_i, r_i) | \mathbf{y}_i \in \{\mathbf{y}_1, ..., \mathbf{y}_K\}, r_i = R_{GT}(\mathbf{x}, \mathbf{y}_i)\}\]
            % \State Add all correct-incorrect pairs to $\mathcal{P}$:
            % \[\mathcal{P} \leftarrow \mathcal{P} \cup \{(x, y_c, y_r) | y_c \in \mathcal{C}_x, y_r \in \mathcal{R}_x\}\]
        \Else
            \State Drop this instruction and continue
        \EndIf
    \EndFor


    \For{epoch = 1, \dots, M}

        \State Forward pass $\pi_\phi$ on each $(\mathbf{x}, \mathbf{y}, r) \in \mathcal{T}$ to obatin implicit process reward $r^t$:
            \[r^t = \beta \log \frac{\pi_\phi(y_{t}|\mathbf{y}_{<t})}{\pi_{\text{ref}}(y_{t}|\mathbf{y}_{<t})}\]
            
        \State Update Implicit PRM $\pi_\phi$ by CE loss on $(\mathbf{x}, \mathbf{y}, r) \in \mathcal{T}$:
            \[
            \mathcal{L}_{\text{CE}}(\phi) = \mathbb{E}_{\left(\mathbf{x},\mathbf{y},r\right)\sim\mathcal{T}} \left[ r \cdot \log \sigma \left( r_\phi \left(\mathbf{y}\right) \right) + (1-r) \cdot \log\left( 1 - \sigma \left( r_\phi \left(\mathbf{y}\right) \right) \right) \right]
            \]

        \State Compute advantages $\hat{A}$. Here we use RLOO with $\{\mathbf{y}_1, ..., \mathbf{y}_K\}$ of the same $x$:
        % \[\hat{A} = \underbrace{\frac{1}{K} \sum_{i=1}^{K}\left[r_i-\frac{1}{K-1} \sum_{j \neq i} r_j\right]}_\text{RLOO with outcome rewards} +  \underbrace{\frac{1}{K} \sum_{i=1}^{K} \sum_{t=1}^{|\mathbf{y}_i|} \gamma^t \cdot \left[r_i^t-\frac{1}{K-1} \sum_{j \neq i} \frac{r_\phi \left(\mathbf{y}_j\right)}{|\mathbf{y}_j|}\right]}_\text{RLOO with implicit process rewards}\]
        \[\hat{A}_i^t = \underbrace{r_i-\frac{1}{K-1} \sum_{j \neq i} r_j}_\text{RLOO with outcome rewards} +  \underbrace{\sum_{s=t}^{|\mathbf{y}_i|} \gamma^{s-t} \cdot \left[r_i^s-\frac{1}{K-1} \sum_{j \neq i} \frac{r_\phi \left(\mathbf{y}_j\right)}{|\mathbf{y}_j|}\right]}_\text{RLOO with implicit process rewards}\]


        \State Update policy $\pi_\theta$ by PPO objective:
        \[\mathcal{L}_{\text{CLIP}}(\theta) = \mathbb{E}_{(\mathbf{x},\mathbf{y})\sim\mathcal{T}}\left[\min\left(\frac{\pi_\theta(\mathbf{y})}{\pi_{\theta_{\text{old}}}(\mathbf{y})}\hat{A}, \text{clip}\left(\frac{\pi_\theta(\mathbf{y})}{\pi_{\theta_{\text{old}}}(\mathbf{y})}, 1-\epsilon, 1+\epsilon\right)\hat{A}\right)\right]\]
    
        
    \EndFor
    % \State Update old parameters: $\theta_{\text{old}} \leftarrow \theta$, $\phi_{\text{old}} \leftarrow \phi$

\EndFor
\end{algorithmic}
\textbf{Output} Fine-tuned policy model $\pi_\theta$

\end{algorithm*}