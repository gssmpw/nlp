\begin{algorithm}

\caption{Process Reinforcement through Implicit Rewards (PRIME)}
\textbf{Input} Language model $\pi_{\theta_{\text{init}}}$; outcome reward verifier $r_o$; dataset $\mathcal{D}$; sample number $K$; total iteration $N$.

% \textbf{Notation} $r_i$ represents the outcome reward of $\mathbf{y}^i$, and $r^i_t$ denotes its process reward at step $t$.
% Define $r_\phi \left(\mathbf{y}\right) = \beta \log \frac{\pi_\phi(\mathbf{y})}{\pi_{\text{ref}}(\mathbf{y})}$, $\mathbf{x}$ is omitted for simplicity.
\begin{algorithmic}[1]
\State Initialize policy model $\pi_\theta \leftarrow \pi_{\theta_{\text{init}}}$, $\pi_{\theta_{\text{old}}} \leftarrow \pi_{\theta_{\text{init}}}$, implicit PRM $\pi_{\phi} \leftarrow \pi_{\theta_{\text{init}}}$, reference model $\pi_{\text{ref}} \leftarrow \pi_{\theta_{\text{init}}}$
\For{iteration = 1, \dots, N}
    \State Sample batch of prompts $\mathcal{B} \sim \mathcal{D}$
    % \State $\mathcal{T} \leftarrow \{\}$
    % \State $\mathcal{P} \leftarrow \{\}$
       % \State \hao{for the following 5 lines, i'd wrap them in to for each $(x, y)\in \mathcal{B}$}
        \State Generate $K$ responses: $\{\mathbf{y}^1, ..., \mathbf{y}^K\} \sim \pi_\theta(\cdot|\mathbf{x})$ for $\mathbf{x} \in \mathcal{B}$
        \State Compute outcome rewards: $r_o\left(\mathbf{y}^{1:K}\right)$
        \State Apply accuracy filter (\S \ref{sec:prime_init_filter}) on all prompts: $\mathcal{T} \leftarrow \text{Filter}(\mathbf{x}, \mathbf{y}^{1:K}, r_o\left(\mathbf{y}^{1:K}\right))$ for $\mathbf{x} \in \mathcal{B}$
        \State Forward pass $\pi_\phi, \pi_\text{ref}$ on each $(\mathbf{x}, \mathbf{y}) \in \mathcal{T}$ to obatin implicit process reward $r_\phi(y_t)$ with Eq.~\ref{eq:pr} 
        \State Update Implicit PRM $\pi_\phi$ by CE loss on $(\mathbf{x}, \mathbf{y}, r_o\left(\mathbf{y}\right)) \in \mathcal{T}$:
            \[
            \mathcal{L}_{\text{CE}}(\phi) = -\mathbb{E}_{\left(\mathbf{x},\mathbf{y},r_o\left(\mathbf{y}\right)\right)\sim\mathcal{T}} \left[ r_o\left(\mathbf{y}\right) \cdot \log \sigma \left( r_\phi \left(\mathbf{y}\right) \right) + (1-r_o\left(\mathbf{y}\right)) \cdot \log\left( 1 - \sigma \left( r_\phi \left(\mathbf{y}\right) \right) \right) \right]
            \]

        \State Compute advantages $A$ with Eq.~\ref{eq:adv} %Here we use RLOO with $\{\mathbf{y}_1, ..., \mathbf{y}_K\}$ of the same $x$:
        % \[\hat{A} = \underbrace{\frac{1}{K} \sum_{i=1}^{K}\left[r_i-\frac{1}{K-1} \sum_{j \neq i} r_j\right]}_\text{RLOO with outcome rewards} +  \underbrace{\frac{1}{K} \sum_{i=1}^{K} \sum_{t=1}^{|\mathbf{y}_i|} \gamma^t \cdot \left[r_i^t-\frac{1}{K-1} \sum_{j \neq i} \frac{r_\phi \left(\mathbf{y}_j\right)}{|\mathbf{y}_j|}\right]}_\text{RLOO with implicit process rewards}\]
        \State Update policy $\pi_\theta$ by PPO loss in Eq.~\ref{eq:clip}
    \State Update old parameters: $\theta_{\text{old}} \leftarrow \theta$%, $\phi_{\text{old}} \leftarrow \phi$

\EndFor
\end{algorithmic}
\textbf{Output} Optimized policy model $\pi_\theta$

\label{algo:prime}
\end{algorithm}