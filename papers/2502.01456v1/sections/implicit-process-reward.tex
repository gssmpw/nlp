% \section{Reinforcement Learning and Challenges in Dense Rewards} 
\section{Reinforcement Learning for LLMs and the Challenges of Incoporating Dense Rewards}


Reinforcement Learning (RL) aims to learn an optimal policy $\pi_\theta$ that maximizes the expected cumulative discounted reward, namely return, when interacting with an environment.
In the context of autoregressive language modeling, state at step $t$ is the concatenation of prompt $\mathbf{x}$ and current response $\mathbf{y}_{<t}$, and the action is the $t$-th token or step $y_t$.


\subsection{RL Preliminaries for LLMs}
\textbf{Policy Gradient.}
Policy gradient is a fundamental algorithm that directly optimizes this objective. Central to this approach is the advantage function $A_t$, which quantifies how much better an action is compared to alternatives in a given state:
\vspace{-10pt}
\begin{equation}
    \nabla_\theta J(\theta) = \mathbb{E}_{\mathbf{x} \sim \mathcal{D},\mathbf{y} \sim \pi_\theta}\left[\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(y_t|\mathbf{y}_{<t}) A_t\right]
\end{equation}
where $\left(\mathbf{x},\mathbf{y}\right)$ represents a pair of input and output. $\mathbf{x}$ is omitted for brevity. In practice, the advantage function is implemented as cumulative discounted rewards subtracting a baseline:
\begin{equation}
\label{eq:reinforce}
    A_t = \sum_{s=t}^T \gamma^{s-t} r(y_s) - b
\end{equation}
$\gamma \in [0,1]$ is a discount factor that optionally decays future rewards, and $r(y_s)$ is the reward provided by the environment at time step $s$ with $x$ and $\mathbf{y}_{<s}$ being omitted in conditions.
Eq.~\ref{eq:reinforce} is the general formula of the Monte-Carlo (MC) advantage estimate, which indicates that, the high-quality and dense reward at each step is crucial for RL.
Different choices of $b$ include, e.g. directly using values \cite{williams1992simple}, group average of rewards~\citep{deepseek-math}, and leave-one-out average of rewards \cite{ahmadian2024back, Kool2019Buy4R}.


\textbf{Value Models.} 
Though the MC estimate is unbiased, it suffers from high variance because of the reliance on all future actions and rewards, which can be random and noisy.
Value models, which predict expected accumulated rewards starting from a state, are adopted to help reduce the variance in advantage estimation, such as Generalized Advantage Estimation (GAE;~\citealp{SchulmanMLJA15}):
$A_t^{\text{GAE}(\gamma,\lambda)} = \sum_{s=0}^{\infty} (\gamma\lambda)^s \delta_{t+s}$,
where $\delta_t = r(y_t) + \gamma V(\mathbf{y}_{<t+1}) - V(\mathbf{y}_{<t})$ is the temporal difference (TD) error~\citep{sutton1988learning}, $V$ is a value model, and $\lambda$ controls the bias-variance tradeoff in advantage estimation.
PPO \citep{schulman2017proximal} is a representative of such actor-critic algorithms that explicitly train a value model along with the policy.

\looseness=-1
\textbf{Reward Sparsity.}
% A common issue in RL for LLMs is the sparse rewards.
Although dense rewards can be naturally integrated into the advantage function through Eq. \ref{eq:reinforce},
unfortunately, only outcome reward models (ORMs) are available in most practices of LLMs, i.e., only the final token bears a meaningful reward while intermediate tokens receive no rewards~\citep{rafailov2024direct,deepseek-math,deepseekai2025deepseekr1incentivizingreasoningcapability}. In this bandit setting, $r(y_t)=0$ for $t<T$ while $r(y_T)$ can be non-zero, and Eq. \ref{eq:reinforce} becomes $A = r(y_T) - b$.
% \ganqu{We can say more about the drawbacks of sparse rewards. }
This formulation, while simpler, can suffer from reward sparsity issues as the policy receives feedback only at the end of the entire generation.
This may (1) encourage spurious solutions with incorrect processes but correct answers, (2) largely reduce sample efficiency in training, and (3) encounter the credit assignment problem~\citep{sutton2018reinforcement}.
These drawbacks could be further amplified on complicated tasks, which require more thinking and execution steps, urging the need of dense rewards~\citep{uesato2022solving,Lightman2023LetsVS}.
%\lifan{i add this back:}
Some may consider employing a value model to mitigate the problem, as it predicts values at every step $t$. However, previous work showed that value models may not be able to solve the reward sparsity issue effectively due to training challenges, despite the additional computation overhead~\citep{deepseek-math,ahmadian2024back}. 
We will also empirically validate this claim in \S \ref{sec:ppo}.



\subsection{Key Challenges in Scalable Dense Rewards}
\label{sec:challenges}
The way to mitigate the reward sparsity problem is to adopt dense reward models, namely PRMs, which score model responses over each token or step.
However, it is usually infeasible in practice to incorporate dense rewards into online RL because of three critical challenges in implementation.


\looseness=-1
\textbf{C1. Process rewards are hard to define.}
It is difficult to collect step-level labels since reasoning steps do not naturally occur in sequences. Although tokens are easily distinguishable, annotating labels for each token is too costly.
Moreover, defining the absolute correctness of intermediate processes as dense rewards can be ambiguous, as some incorrect steps can also positively contribute to the final answer by pruning searching branches \citep{Openai2024OpenAIOS,deepseekai2025deepseekr1incentivizingreasoningcapability}. 


\textbf{C2. PRM online updates are not scalable.}
It is crucial to prevent reward overoptimization or reward hacking, which requires the reward model or value model to be updated online along with the policy model~\citep{schulman2017proximal,Gao2022ScalingLF}. 
However, training PRMs often requires extensive nuanced step-level annotation, which is infeasible in online RL training.
Therefore, this brings about considerable scalability and generalization concerns in dense rewards for RL.

\textbf{C3. Explicit reward modeling brings extra cost.}
Training reward models requires extensive annotation and broad data coverage to ensure a good balance between adaptability to the policy distribution and generalization to distribution shifts.
Hence, the explicit training stage introduces a very costly data collection and an additional training overhead, especially for PRMs which typically require stepwise labels.

Notably, a concurrent work shares similar conclusions and thus is impeded from incorporating PRMs into their large-scale RL training \citep{deepseekai2025deepseekr1incentivizingreasoningcapability}.
