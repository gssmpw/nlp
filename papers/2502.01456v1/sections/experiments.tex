\section{Experiments}
%\hao{general thought on this exp:suggest specifying which curves to read when talking about the conclusions}


\subsection{Imitation Warmup}
We focus on mathematical and coding problems in this paper. For models, we start with Qwen2.5-Math-7B-Base~\citep{yang2024qwen25mathtechnicalreportmathematical} for its great mathematical capabilities. We first performed supervised finetuning for RL preparation. 

% \textbf{Data Format.} We designed an action-centric chain-of-thought reasoning framework, where the policy model chooses one of 7 actions at each step and stops after executing each action. \huayu{finished?}

\looseness=-1
\textbf{Data Construction.}
To construct the SFT dataset, we collect reasoning instructions from several open-source datasets. For completion, we employed LLaMA-3.1-70B-Instruct~\citep{grattafiori2024llama3herdmodels} to answer the instructions, with a system prompt requesting the model to perform action-centric chain-of-thought. 
We finally obtained 230K SFT data, the detailed sources and statistics can be found in \S~\ref{sec:sft_data_training_details}.

\textbf{SFT Results.}
After finetuning, the performance of our SFT model is reported in Figure~\ref{fig:results}.
Compared to baselines, Eurus-2-7B-SFT lags Qwen2.5-Math-7B-Instruct on all mathematics benchmarks.

\subsection{RL Settings}
\textbf{Rule-based Outcome Verifier.}
% We focus on math and coding.
% \huayu{You say OV sometimes but did not mention OV here. Maybe title like "OV as Rule-based Outcome Rewards}
Consistent with recent research that adopts exact match with ground truth as unhackable rewards~\citep{Gao2024OnDE, Lambert2024TLU3P,deepseekai2025deepseekr1incentivizingreasoningcapability}, we define the rule-based ground truth outcome verifiers (OV) for math and coding as follows: 
% \begin{align}
% r_o^{\text{math}}(y)= \begin{cases}1, & \text { if answer matched } \\ 0, & \text { if answer not matched } \end{cases} \quad \text{  } \quad r_o^{\text{coding}}(y)= \frac{\sum \text{passed test cases}}{\sum \text{test cases}}
% \end{align}
\begin{align*}
r_o^{\text{math}}(\mathbf{y}) &= \begin{cases}1, &\text{matched} \\ 0,&\text{otherwise} \end{cases} \quad
r_o^{\text{code}}(\mathbf{y}) =\frac{\sum \text{\#passes}}{\sum \text{\#test cases}}
\end{align*}



\textbf{Hyperparameters.}
We use veRL~\citep{sheng2024hybridflow} to conduct experiments.
By default, we initialize the Implicit PRM with SFT model and retain the SFT model for reference logprobs. For hyperparameters, we use a constant $5\times 10^{-7}$ learning rate together with AdamW optimizer for policy model, and use a $10^{-6}$ learning rate for PRMs. Both policy and PRMs use a batch size of 256 and micro batchsize of 8. The rollout stage collects 256 prompts and samples 4 responses for each prompt. We set $\beta=0.05$ for PRM training. We set KL coefficient to 0 in all experiments.

\textbf{Evaluation Benchmarks.}
We evaluate on 7 reasoning benchmarks, focusing on competition-level mathematics and programming tasks, including AIME 2024~\citep{li2024numinamath}, AMC~\citep{li2024numinamath}, MATH-500~\citep{hendrycks2021measuring}, Minerva Math~\citep{lewkowycz2022solving}, OlympiadBench~\citep{he-etal-2024-olympiadbench}, LeetCode~\citep{guo2024deepseek}, and LiveCodeBench (v2)~\citep{jain2024livecodebench}.


\input{tables/dense_rewards_results}
\input{figures/effect_of_dense_rewards}
\subsection{Main Results}

As shown in Figure~\ref{fig:results} and Table~\ref{tab:dense_rewards_results}, Eurus-2-7B-PRIME achieves substantial improvements on key reasoning benchmarks over the SFT version of the model, leading to 15.1\% improvement on average, and over 20\% on AMC and AIME competitions. Besides, Eurus-2-7B-PRIME achieves 26.7\% pass@1 on AIME 2024, surpassing GPT-4o, Llama-3.1-70B-Instruct, and Qwen2.5-Math-7B-Instruct, demonstrating its excellent reasoning ability.

%\input{tables/main_results}


\subsection{Dense Rewards v.s. Sparse Rewards}
We first validate the effect of dense rewards compared to RLOO with outcome rewards only. 
We train this model for 240 steps. For PRIME, we use the same setting and train the model for 592 steps. We plot the training rewards measured by the outcome verifier and test accuracy in Figure \ref{fig:dense_rewards}. \textbf{Compared with sparse reward, PRIME  takes 40\% of the training steps to achieve the same training rewards as RLOO and improves the final rewards by 6.9\%, with lower variances.} On downstream tasks, PRIME also consistently outperforms OV only setup. Detailed results are listed in Table~\ref{tab:dense_rewards_results}.
\vspace{-10pt}

% \S \ref{sec:effeoc_of_dense_rewards}.

% \hao{this section is pretty heavy.
% consider starting a new section below
% }
\section{Analysis}
\input{figures/effect_of_online_prm}
\subsection{Design Choices for the Implicit PRM}
\label{sec:design}
The Implicit PRM is the key component of PRIME, and its design choices greatly affect RL. In this section, we explore two major factors: (1) the initialization model and (2) the update mechanism.

\textbf{SFT model initializes a good PRM.}
Conventionally, we need to collect data to train RMs and PRMs, and then we can use them in RL. However, the Implicit PRM is a language model, so we can initialize it from any language model with the same tokenizer as the policy model. To investigate whether it is still necessary to train a PRM in advance, we conduct experiments with different PRM initialization strategies: with the SFT model itself and with a specially trained PRM. For the later one, we train EurusPRM from Eurus-2-7B-SFT with additional 500K data generated by Llama3.1 and Qwen2.5 series (data details in \S~\ref{sec:app_prm_data}). 
\input{figures/prm_acc_online}

We report the experiment results in Figure~\ref{fig:online_offline_prm}. \textbf{Surprisingly, directly using Eurus-2-7B-SFT to initialize the PRM greatly outperforms EurusPRM which was trained on more samples.} We conjecture that initializing policy model and PRM from the same model largely alleviates the distribution shift issue, as the PRM is only trained on the online rollouts from the policy model.

\looseness=-1
\textbf{Online PRM update is essential.}
To verify the effect of online PRM update, we pair the correct and wrong samples and calculate the PRM prediction accuracy using $r_\phi(\mathbf{y})$.
We report the PRM classification accuracy in Figure~\ref{fig:prm_acc_online}. 
The figure clearly shows that, \textbf{online update mitigates overoptimization and reward hacking.}
The offline PRM, though starting with high accuracy, gradually drops during RL training procedure due to distribution shift. In contrast, online PRMs that are trained on policy rollouts show the reverse curve.


This is further validated with training rewards and downstream performance. To breakdown, Eurus-2-7B-SFT is both used as PRM initialization and the reference model in the main experiment, so the PRM is totally trained from scratch, which means the initial PRM outputs zero reward for all tokens. Therefore, Figure~\ref{fig:dense_rewards} also demonstrates the effect of online PRM update. For EurusPRM initialization, the online run outperforms the offline run as well in Figure~\ref{fig:online_offline_prm}.


\subsection{Reference Model Choice is Flexible} 

\input{figures/comparision_of_policy_sft_ref}
\input{figures/effect_of_ref_policy}
% We introduce online PRM, which updates with policy model rollouts and their corresponding verifier outcomes. Here we demonstrate the importance of online updates for PRMs. We compare two settings, where the online PRM is initialized by Eurus-2-7B-SFT and the offline PRM is EurusPRM-Stage1\hanbin{This seems to be the first mention of EurusPRM-Stage1, which may require a quote or additional explanation}. As shown in Figure \ref{fig:online_prm}, we can see that online PRM outperforms offline PRM by a large margin on both training and test sets. 


%\subsection{Effect of Reference Policy}
%\lifan{a bit confusing about reference policy and ref model for implicit prm. elaborate a bit before diving into details?}\ganqu{the same, merged. But this is not so imporatnt, consider moving to Appendix}
We implement two variants of our algorithms to explore the effect of reference model of implicit PRM, one using the initial SFT model as the reference model (SFT ref) while the other using the running policy's old logprobs as reference (policy ref), as shown in Figure~\ref{fig:policy_ref}. The policy ref simply adopts the old logprob of the policy model as  $\pi_{\text{ref}}$, while the SFT ref remains the initial SFT model for an additional $\pi_{\text{ref}}$ calculation. We compare their performance in this section. 

From the training rewards in Figure \ref{fig:effect_of_ref_policy}, we find the two strategies are close and have pros and cons in different aspects: The Q value calculated by implicit PRM is the expectation under the distribution of the reference model. So the updating policy could natrually serve as the reference.
On the other hand, KL divergence calculation is only allowed when the initial SFT model is retained.



\input{figures/effect_of_single_double_forward}
\subsection{Single-Forward v.s. Double-Forward}
% \lifan{review current forward flow so we know what's single forward?}\lifan{and move to appendix?}
Since our implicit PRM is concurrently updated in training, for each rollout stage, we can update the PRM before the policy model and use the updated PRM to re-calculate the process rewards, which we call the double-forward setting. We investigate the impact of double-forward in both the training and test phases. Our default setting applies single-forward, which uses process rewards from old PRMs. We plot PRM accuracy on rollouts and training rewards in Figure \ref{fig:single_double_forward}.


% \input{tables/effect_of_single_double_forward}
Accordingly, we find that double-forward could increase PRM accuracy, but the training rewards remain close between the two methods. %We also compare the average test accuracy of single and double-forward\hanbin{there is a simple table in our blog} in Table \ref{tab:effect_of_single_double}. Their performances are also close. Single double-forward brings more computation overhead, we recommend the single-forward setting in practice.




% \begin{wrapfigure}{r}{0.5\textwidth}
% \centering
%     \includegraphics[width=\linewidth]{figures/images/train_rewards_baseline.pdf}
%     \caption{PRIME also benefits REINFORCE, GRPO, and PPO, achieving similar improvement as RLOO.} 
%     \label{fig:baseline}
%     \vspace{-15pt} % 调整为合适的负值
% \end{wrapfigure}
\subsection{PRIME with Other RL Algorithms}
\label{sec:other_algo}
% We mainly examplify PRIME combined with RLOO in this paper, 
As we stated before, PRIME is equally applicable to other RL algorithms beyond RLOO.
% seamlessly connected with other RL algorithms. 
In this section, we implement PRIME with REINFORCE~\citep{williams1992simple}, GRPO~\citep{deepseek-math}, and PPO~\citep{schulman2017proximal}. Similarly to RLOO, we only modify the advantage estimation functions and leave the clip surrogate loss unchanged. 


% Detailed functions can be found in \S \ref{sec:diff_rl_algo}. 
First of all, We compare different REINFORCE-like advantage estimators including REINFORCE, GRPO, and RLOO, toggling the existence of implicit process reward. To make different algorithms compatible with the compound of outcome verifier reward and process reward, we accordingly make adaptions similar to Eq. \ref{eq:adv}. For GRPO, we have
\begin{equation}
        A^i_t = \underbrace{\frac{r_{o}\left(\mathbf{y}^i\right)-\text{mean}( r_o\left(\mathbf{y}^j\right))}{\text{std}( r_o\left(\mathbf{y}^j\right))}}_\text{GRPO with outcome rewards} + \underbrace{\sum_{s=t}^{|\mathbf{y}^i|} \gamma^{s-t} \cdot \left[\frac{r_\phi(y^i_s)-\text{mean}\left(\frac{r_\phi \left(\mathbf{y}^j\right)}{|\mathbf{y}^j|}\right)}{\text{std}\left(\frac{r_\phi \left(\mathbf{y}^j\right)}{|\mathbf{y}^j|}\right)}\right]}_\text{GRPO with implicit process rewards}.
\end{equation}
For REINFORCE, we have
\begin{equation}
A^i_t = \underbrace{r_o\left(\mathbf{y}^i\right)}_\text{REINFORCE with outcome rewards} + \underbrace{\sum_{s=t}^{|\mathbf{y}^i|} \gamma^{s-t} \cdot r_\phi(y^i_s)}_\text{REINFORCE with implicit process rewards}.
\end{equation}

From Figure~\ref{fig:baseline} and Table~\ref{tab:diff_rl_algo}, We show that PRIME boosts these algorithms on both efficiency and performance as it does with RLOO. PRIME contributes consistently regardless of the policy update method, making it a generic algorithm. 
It indicates that \textbf{PRIME is a general plug-in for almost any RL algorithm for LLM.}, which largely extends the use cases of PRIME.

Moreover, the PPO variant of PRIME provides no performance gain, demonstrating that the additional computation cost from the critic model is redundant. This makes it possible to compensate for the expense of the process reward model by using REINFORCE-like algorithms with simpler advantage estimators. Finally, we choose the best-performing RLOO as the advantage estimator in our algorithm. 


\input{tables/diff_rl_algo}

\begin{figure}[t] % 图片浮动环境
    \centering % 居中对齐
    \begin{minipage}[t]{0.46\textwidth} % 左侧图片
        \includegraphics[width=\linewidth]{figures/images/train_rewards_baseline.pdf}
        \caption{PRIME also benefits REINFORCE, GRPO, and PPO, achieving similar improvement as RLOO.} 
        \label{fig:baseline}
    \end{minipage}
    \hspace{5pt}
    \begin{minipage}[t]{0.51\textwidth} % 右侧图片
        \centering
        \includegraphics[width=\linewidth]{figures/images/train_rewards_ppo.pdf}
        \caption{Comparison of value models and reward models. We show that value models, either the original PPO one or Implicit PRM, is substaintially worse than reward models.}
        \label{fig:ppo}
    \end{minipage}
\end{figure}
\subsection{Value or Reward, How to Use the Implicit PRM? }
\label{sec:ppo}
% \begin{wrapfigure}{r}{0.5\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/images/train_rewards_ppo.pdf}
%     \caption{Comparison of value models and reward models. We show that value models, either the original PPO one or Implicit PRM, is substaintially worse than reward models.}
%     \label{fig:ppo}
%     \vspace{-15pt} % 调整为合适的负值
% \end{wrapfigure}



%The Implicit PRM can provide process rewards, and they intrinsically function similarly to value models. 
Besides using process rewards to estimate returns, we can also employ the Implicit PRM to predict values for advantage estimation in Eq. \ref{eq:reinforce}.
Therefore, we compare four variants of MC estimate to determine the best way to incorporate dense supervision.
Recall that the Implicit PRM has $v_{\phi}(\mathbf{y}_{<t+1}) = \sum_{i=1}^{t} \beta \log \frac{\pi_\phi(y_{i}|\mathbf{y}_{<i})}{\pi_\text{ref}(y_{i}|\mathbf{y}_{<i})}$ with the process reward being $r_{\phi}(y_t) = v_{\phi}(\mathbf{y}_{<t+1}) - v_{\phi}(\mathbf{y}_{<t})$, and we assume a ground-truth outcome verifier $r_o$, $\gamma=1$, then we represent the variants as follows:

(1) REINFORCE: $A_t = r_o(\mathbf{y})$.

(2) On top of (1), using \textbf{a linear-head value model} $V$ to calculate the baseline: $A_t = r_o(\mathbf{y}) - V(\mathbf{y}_{<t})$. This is the original PPO in Figure \ref{fig:baseline} as we set $\gamma=1$ and $\lambda=1$.

(3) On top of (1), using \textbf{values from the Implicit PRM} to serve as the baseline: $A_t = r_o(\mathbf{y}) - v_\phi(\mathbf{y}_{<t})$. This is equivalent to PPO with its value model being replaced by values from the Implicit PRM when $\gamma=1$ and $\lambda=1$.

(4) On top of (1), using \textbf{process rewards from the Implicit PRM} to calculate the return: $A_t = r_o(\mathbf{y}) + \sum_{s=t}^T r_{\phi}(y_s) $. This is the REINFORCE w/ PRIME in Figure \ref{fig:baseline}.
% = r_o(\mathbf{y}) + v_{\phi}(\mathbf{y}_{<T+1}) - v_{\phi}(\mathbf{y}_{<t})


% (5) On top of (4), using \textbf{process rewards from the Implicit PRM} to calculate the return: $A_t = \sum_{s=t}^T (r_s + v_{\phi}(\mathbf{y}_{<s+1}) - v_{\phi}(\mathbf{y}_{<s}))) - V(\mathbf{y}_{<t}) = r_o(\mathbf{y}) - V(\mathbf{y}_{<t})$. This is the PPO w/ PRIME in Figure \ref{fig:baseline}.\lifan{is this setup necessary?}







% Recall that the advantage is estimated by $A_t^{\text{GAE}(\gamma=1,\lambda=1)} = \sum_{s=0}^{T-s} \delta_{t+s}$ and the Implicit PRM has $v_{\phi}(\mathbf{y}_{<t+1}) = \sum_{i=1}^{t} \beta \log \frac{\pi_\phi(y_{i}|\mathbf{y}_{<i})}{\pi_\text{ref}(y_{i}|\mathbf{y}_{<i})}$ and $r_\phi(y_t) = v_{\phi}(\mathbf{y}_{<t+1}) - v_{\phi}(\mathbf{y}_{<t}) $.
% We assume a ground-truth outcome verifier with $r_t=0$ for $t<T$ and $r_T=r_o\left(\mathbf{y}\right)$, then we represent the variants as follows:

% (1) The original PPO, with a value model $V$ initialized from the SFT model and a new linear head: $\delta_t = r_t + V(\mathbf{y}_{<t+1}) - V(\mathbf{y}_{<t})$, $A_t^{\text{GAE}} = r_o(\mathbf{y}) - V(\mathbf{y}_{<t})$. 

% (2) Keep the reward side intact but replace the value model with the value predicted by Implicit PRM: 
% $\delta_t = 0 + v_{\phi}(\mathbf{y}_{<t+1}) - v_{\phi}(\mathbf{y}_{<t})$ for $t<T$ and $\delta_T = r_o(\mathbf{y}) - v_\phi(\mathbf{y}_{<T})$, $A_t^{\text{GAE}} = r_o(\mathbf{y}) - v_{\phi}(\mathbf{y}_{<t})$.

% (3) Keep the linear-head value model $V$ intact while using Implicit PRM to provide dense rewards: 
% $\delta_t = r_t + r_\phi(y_t) + V(\mathbf{y}_{<t+1}) - V(\mathbf{y}_{<t})$, $A_t^{\text{GAE}} = r_o(\mathbf{y}) - v_{\phi}(\mathbf{y}_{<t}) - V(\mathbf{y}_{<t})$.

% (4) Discard the value model: $\delta_t = r_t$, which becomes REINFORCE: $A_t^{\text{GAE}} = r_o(\mathbf{y})$.

% (5) Discard the value model while using implict PRM as the reward model as in PRIME: $\delta_t = 0 + r_\phi(y_t)$ for $t<T$ and $\delta_T = r_o(\mathbf{y}) + r_\phi(y_T)$, $A_t^{\text{GAE}} = r_o(\mathbf{y}) + v_{\phi}(\mathbf{y}_{<T+1}) - v_{\phi}(\mathbf{y}_{<t})$.

Figure~\ref{fig:ppo} reports the results. 
Comparing PPO and REINFORCE, we find that an additional value model does not benefit policy performance. 
Notably, using rewards from the Implicit PRM to calculate returns, which is the default setting in PRIME, greatly outperforms all three baselines, regardless of where the values come from. This indicates that PRMs work better than value models in RL for LLMs.
%both two kinds of value models (PPO value model and Implicit PRM as value model) fall behind reward models. 


% \subsection{Length}

\subsection{``Zero'' Experiments}
\label{sec:app_zero}
\input{figures/zero_7}
\input{figures/zero_32}
\citet{deepseekai2025deepseekr1incentivizingreasoningcapability} proposed DeepSeek-R1-Zero, which is directly trained from a base model with reinforcement learning. To further investigate the ``Zero'' setting, we also perform RL from Qwen2.5-Math-7B-Base and Qwen2.5-32B-Base~\citep{qwen2.5}, skipping the SFT phase. 
We present the experimental results in Figure~\ref{fig:zero_7} and Figure~\ref{fig:zero_32}. The observations are as follows:

(1) \textbf{RL from base model is suprisingly efficient and effective.} Comparing PRIME from Qwen2.5-Math-7B and Eurus-2-7B-SFT, the ``Zero'' setting converges much faster. This indicates that directly performing RL from a base model might be a strong alternative to the conventional SFT-RL pipeline.

(2) \textbf{Larger models benefit more.} Comparing 7B and 32B models, we see that the 32B model gains more on both training rewards and test performance. This is aligned with the conclusion in \citet{deepseekai2025deepseekr1incentivizingreasoningcapability}.

(3) \textbf{Saturation could be a potential issue.} Although PRIME-Zero obtains impressive performance gain, we find it quickly saturated at a very early stage (about 50 steps), which hinders further improvement like in \citet{deepseekai2025deepseekr1incentivizingreasoningcapability}. This is possibly attributed to the decrease of response diversity, and we leave this as future work.


