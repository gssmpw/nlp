\section{PRIME}


\looseness=-1
To address the above challenges, we propose PRIME, a scalable online RL method with dense rewards.
The key insight of PRIME is to apply \textit{implicit process rewards}, which are derivable from the Implicit PRM that is trained with only outcome labels~\citep{yuan2024freeprocessrewardsprocess}.
This property enables us to update the PRMs online to avoid reward hacking. 
We then design a flexible framework to incorporate implicit process rewards with outcome rewards into any kind of MC advantage estimate.
PRIME is illustrated in Figure \ref{fig:prime-algo} and Algorithm \ref{algo:prime}.
Next, we will detail the implicit process rewards (\S \ref{sec:prime_prm}) and how we leverage them to calculate advantages (\S \ref{sec:prime_loss_adv}), and introduce other techniques we used (\S \ref{sec:prime_init_filter}).



\subsection{Enabling Scalable Reward Update with Implicit Reward Modeling}
\label{sec:prime_prm}

\looseness=-1
We consider dense rewards from the Implicit PRM because of the scalability. 
In short, Implicit PRM enables training an ORM with outcome labels only while repurposing it as a PRM at inference. 
The training stage is the same as standard ORM pipelines, with the only difference being representing the reward as $r_\phi(\mathbf{y}):= \beta \log \frac{\pi_\phi(\mathbf{y})}{\pi_\text{ref}(\mathbf{y})}$, where $\pi_\phi$ is the RM and $\pi_\text{ref}$ is the reference model, both of which are causal LMs. At inference, the process rewards are obtained by:
\begin{equation}
\label{eq:pr}
    r_\phi(y_t) := \beta \log \frac{\pi_\phi(y_{t}|\mathbf{y}_{<t})}{\pi_\text{ref}(y_{t}|\mathbf{y}_{<t})}
\end{equation}


\input{algos/prime-simple}

\looseness=-1
In PRIME, upon rollouts being generated and graded by the (ground truth) outcome verifier, we \textbf{update the Implicit PRM online with on-policy rollouts and outcome supervision} and then \textbf{calculate token-level dense rewards to estimate advantages}, which solves C1 and C2 mentioned in \S \ref{sec:challenges} respectively:
(1) To prevent overoptimization and reward hacking, it is crucial to update reward models online. However, updating previous PRMs \citep{Lightman2023LetsVS} requires annotating step labels on the latest policy rollouts, which is neither efficient nor scalable during online RL.
In contrast, the Implicit PRM only demands outcome labels to train due to its special reward representation, and thus it can be easily updated with policy rollouts and outcome labels or rewards, both of which have already been collected to update the policy model.
(2) Unlike common PRMs that produce only step-level rewards, the Implicit PRM provides more fine-grained \textit{token-level} rewards at no additional cost. This addresses the ambiguity in identifying steps in LLM responses while not introducing extra overhead, making it easy to combine with any RL algorithms for advantage estimation.

\input{figures/prime-algo}
\iffalse
\begin{equation}
    \label{eq:ce}
    \mathcal{L}_{\text{CE}} = l \cdot \log \sigma \left( \beta \log \frac{\pi_\phi(\mathbf{y})}{\pi_\text{ref}(\mathbf{y})} \right)+ (1 - l) \cdot \log \left[ 1 - \sigma \left( \beta \log \frac{\pi_\phi(\mathbf{y})}{\pi_\text{ref}(\mathbf{y})} \right) \right]
\end{equation}
\fi



\subsection{Advantage Estimation and Policy Update}
\label{sec:prime_loss_adv}



\textbf{Estimating advantages using Monte Carlo estimator with a leave-one-out baseline.}
After obtaining token-level dense rewards, we calculate advantages based on either MC estimators or GAE.
To determine the advantage function in PRIME, we compare GAE with several MC estimators, including REINFORCE~\citep{williams1992simple}, RLOO~\citep{ahmadian2024back}, and GRPO~\citep{deepseek-math}. Experimental details and results can be found in \S \ref{sec:other_algo}.

We find that MC estimators, despite being simpler, are strong enough to produce stable results. Therefore, we choose MC estimate as our advantage function and despite PRIME being compatible with any baseline estimation approaches, we instantiate it with a leave-one-out baseline from $K$ samples \citep{ahmadian2024back} in this paper, as it performs better in the experiments:
\begin{equation}
    A^i = r(\mathbf{y}^i_T)-\frac{1}{K-1} \sum_{j \neq i}r(\mathbf{y}^j_T)
\end{equation}
where $r(\mathbf{y}^i_T)$ denotes the reward of $i$-th response at final step $T$, $K$ is the number of samples for one prompt. The leave-one-out (LOO) baseline helps reduce variances.


More specifically, we use an Implicit PRM $\pi_\phi$ and an outcome verifier or reward model $r_o$.
We calculate the return of implicit process rewards and outcome rewards separately if both are available, since directly mixing their values may lead to numerical instability~\citep{deepseek-math}.
\textbf{For implicit process rewards}, we perform a three-step process to calculate return:
(1) Use the averaged implicit process rewards to calculate the leave-one-out baseline;
(2) Normalize the process reward at step $t$ by subtracting the baseline;
(3) Calculate the discounted return for each response.
\textbf{For outcome rewards}, we directly adopt LOO without any modification.
Finally, the advantage is set to the combination of both returns:
\begin{equation}
\label{eq:adv}
    \begin{aligned}
        A^i_t = &\underbrace{\sum_{s=t}^{|\mathbf{y}^i|} \gamma^{s-t} \cdot \left[r_\phi(y^i_s)-\frac{1}{K-1} \sum_{j \neq i} r_\phi \left(\mathbf{y}^j\right)\right]}_\text{RLOO with implicit process rewards}+\underbrace{r_o\left(\mathbf{y}^i\right)-\frac{1}{K-1} \sum_{j \neq i} r_o\left(\mathbf{y}^j\right)}_\text{RLOO with outcome rewards}
    \end{aligned}
\end{equation}

\textbf{Updating policy with PPO clip surrogate loss.}
We adopt PPO clip surrogate loss for more stable policy updates: 
\begin{equation}
\begin{aligned}
\label{eq:clip}
    L_{\text{CLIP}}(\theta) = &\mathbb{E}_t\Biggl[\min\biggl(\frac{\pi_\theta(y_t|\mathbf{y}_{<t})}{\pi_{\theta_{\text{old}}}(y_t|\mathbf{y}_{<t})}A_t,\text{clip}\Bigl(\frac{\pi_\theta(y_t|\mathbf{y}_{<t})}{\pi_{\theta_{\text{old}}}(y_t|\mathbf{y}_{<t})}, 1-\epsilon, 1+\epsilon\Bigr)A_t\biggr)\Biggr]
\end{aligned}
\end{equation}
where $\epsilon$ is a clipping parameter. The loss prevents the updated policy from deviating too far from the original distribution, which is the prerequisite of importance sampling. 
The legitimacy of importance sampling then enables the reuse of rollouts sampled in previous steps, thus improving sampling efficiency. 








\subsection{Other Techniques}
\label{sec:prime_init_filter}
\textbf{Initializing PRM with SFT/base model.}
In practice, we find that the starting policy model itself serves as a decent initialization of PRM, bypassing the PRM training stage. This solves C3 in \S \ref{sec:challenges} and even outperforms a dedicatedly trained PRM, as shown in \S~\ref{sec:design}.


\input{figures/online_prompt_filter}
\textbf{Online Prompt Filtering.}
As we sample multiple trajectories for each prompt, we introduce online prompt filtering which filters prompts within a certain accuracy range.
This (1) preserves only the prompts within a certain median-level difficulty range~\citep{yang2024qwen25mathtechnicalreportmathematical} and (2) balances data distribution for the Implicit PRM online training.

We present the ablation study results in Figure \ref{fig:online_prompt_filter} using RLOO with outcome rewards only, from which we can see that the online prompt filter largely lowers the variance of RL training. 

\textbf{How PRIME addresses challenges in \S \ref{sec:challenges}.} In summary, as illustrated in Figure \ref{fig:prime-algo} and Algorithm \ref{algo:prime}, PRIME adopts implicit process rewards for efficient PRM online update (C2), then integrates token-level dense rewards with outcome rewards in MC advantage estimate (C1). The PRMs are directly initialized from SFT or base models, which foregoes explicit reward modeling (C3). 














