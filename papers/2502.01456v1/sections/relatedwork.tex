\section{Related Work}
\textbf{RL for LLM Reasoning.}
In the context of LLMs, reinforcement learning has been widely used for aligning human preferences~\citep{christiano2017deep,ouyang2022training,Cui2023ULTRAFEEDBACKBL}, but the open-source community mostly adopt the data-driven imitation learning methods~\citep{Yuan2024AdvancingLR,Yue2024MAmmoTH2SI,wei2024magicoder,liu2024acemath} to enhance the reasoning capabities of LLMs.
Over the past few months, the paradigm gradually shifted. OpenAI o1~\citep{jaech2024openai} first showed the tremendous potential of large-sacle RL for reasoning LLMs, and recent works have verified the scaling effect of the simple RL recipe with merely outcome rewards~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,team2025kimi}.
Meanwhile, the role of dense rewards in RL remains underexplored, which is the main focus of PRIME.


\textbf{Implicit Rewards.}
Implicit rewards are broadly adopted in LLM alignment~\citep{rafailov2024direct,chen2024noise,Azar2023IPO,Ethayarajh2024KTOMA,Rosset2024DirectNO,chen2024bootstrapping}. ~\citet{rafailov2024r} first showed that optimizing DPO objective learns a Q function implicitly. 
\citet{zhou2024weak} utilized implicit rewards in PPO, and showed that dense implicit rewards are better than sparse ones. 
~\citet{yuan2024freeprocessrewardsprocess} further extended the conclusion to any loss funtion optimizing Eq.~\ref{eq:pr}.