% \input{algos/prime}






\newpage
% \section{Additional Results}
% \label{sec:app_results}

% \subsection{Reference Model Choice is Flexible} 

% \input{figures/comparision_of_policy_sft_ref}
% \input{figures/effect_of_ref_policy}
% % We introduce online PRM, which updates with policy model rollouts and their corresponding verifier outcomes. Here we demonstrate the importance of online updates for PRMs. We compare two settings, where the online PRM is initialized by Eurus-2-7B-SFT and the offline PRM is EurusPRM-Stage1\hanbin{This seems to be the first mention of EurusPRM-Stage1, which may require a quote or additional explanation}. As shown in Figure \ref{fig:online_prm}, we can see that online PRM outperforms offline PRM by a large margin on both training and test sets. 


% %\subsection{Effect of Reference Policy}
% %\lifan{a bit confusing about reference policy and ref model for implicit prm. elaborate a bit before diving into details?}\ganqu{the same, merged. But this is not so imporatnt, consider moving to Appendix}
% We implement two variants of our algorithms to explore the effect of reference model of implicit PRM, one using the initial SFT model as the reference model (SFT ref) while the other using the running policy's old logprobs as reference (policy ref), as shown in Figure~\ref{fig:policy_ref}. The policy ref simply adopts the old logprob of the policy model as  $\pi_{\text{ref}}$, while the SFT ref remains the initial SFT model for an additional $\pi_{\text{ref}}$ calculation. We compare their performance in this section. 

% From the training rewards in Figure \ref{fig:effect_of_ref_policy}, we find the two strategies are close and have pros and cons in different aspects: The Q value calculated by implicit PRM is the expectation under the distribution of the reference model. So the updating policy could natrually serve as the reference.
% On the other hand, KL divergence calculation is only allowed when the initial SFT model is retained.



% \input{figures/effect_of_single_double_forward}
% \subsection{Single-Forward v.s. Double-Forward}
% % \lifan{review current forward flow so we know what's single forward?}\lifan{and move to appendix?}
% Since our implicit PRM is concurrently updated in training, for each rollout stage, we can update the PRM before the policy model and use the updated PRM to re-calculate the process rewards, which we call the double-forward setting. We investigate the impact of double-forward in both the training and test phases. Our default setting applies single-forward, which uses process rewards from old PRMs. We plot PRM accuracy on rollouts and training rewards in Figure \ref{fig:single_double_forward}.


% % \input{tables/effect_of_single_double_forward}
% Accordingly, we find that double-forward could increase PRM accuracy, but the training rewards remain close between the two methods. %We also compare the average test accuracy of single and double-forward\hanbin{there is a simple table in our blog} in Table \ref{tab:effect_of_single_double}. Their performances are also close. Single double-forward brings more computation overhead, we recommend the single-forward setting in practice.


% \subsection{Results of Different RL Algorithms}
% \label{sec:diff_rl_algo}

% \input{tables/diff_rl_algo}
% % The results of different RL algorithms on Llama-3.1-8B are listed in Table \ref{tab:diff_rl_algo}. Since we use a different base model and dataset for the pilot study, the benchmarks used here are slightly different from the main experiments. From the results, we find that REINFORCE-like algorithms, despite being simpler than PPO, are strong enough to produce stable results. In this paper, we choose the best performing RLOO as our RL algorithm.

% We ablate PRIME and different RL algorithms with their variants and find that the PRIME algorithm achieves the best performance for several reasons. 

% First of all, We compare different REINFORCE-like advantage estimators including REINFORCE, GRPO, and RLOO, toggling the existence of implicit process reward. To make different algorithms compatible with the compound of outcome verifier reward and process reward, we accordingly make adaptions similar to Eq. \ref{eq:adv}. For GRPO, we have
% \begin{equation}
%         A^i_t = \underbrace{\frac{r_{o}\left(\mathbf{y}^i\right)-\text{mean}( r_o\left(\mathbf{y}^j\right))}{\text{std}( r_o\left(\mathbf{y}^j\right))}}_\text{GRPO with outcome rewards} + \underbrace{\sum_{s=t}^{|\mathbf{y}^i|} \gamma^{s-t} \cdot \left[\frac{r_\phi(y^i_s)-\text{mean}\left(\frac{r_\phi \left(\mathbf{y}^j\right)}{|\mathbf{y}^j|}\right)}{\text{std}\left(\frac{r_\phi \left(\mathbf{y}^j\right)}{|\mathbf{y}^j|}\right)}\right]}_\text{GRPO with implicit process rewards}.
% \end{equation}
% For REINFORCE, we have
% \begin{equation}
% A^i_t = \underbrace{r_o\left(\mathbf{y}^i\right)}_\text{REINFORCE with outcome rewards} + \underbrace{\sum_{s=t}^{|\mathbf{y}^i|} \gamma^{s-t} \cdot r_\phi(y^i_s)}_\text{REINFORCE with implicit process rewards}.
% \end{equation}
% As shown in Table \ref{tab:diff_rl_algo}, PRIME contributes consistently regardless of the policy update method, making it a generic algorithm. 

% \iffalse
% On top of that, a process reward model prevails in providing token-level signals compared to a value estimation model. On the one hand, apparently the PRIME algorithm with RLOO advantage estimator surpasses the PPO implementation with only the verifier outcome reward model. This means while both provide online updating token-level signals, the PRIME algorithm is essentially distinct from a TD error critic model. On the other hand, we can also derive a value model from \cite{yuan2024freeprocessrewardsprocess} and the corresponding advantage estimator. 
% % \zefan{do we need to add superscript i?}
% \begin{proposition}\label{prop:implicit_value} 
% Consider an autoregressive process with an outcome reward at the last step~($T$). Define the Q value as $q_\theta^t(\mathbf{y}_{<t}, y_t):= \sum_{i=1}^{t} \beta \log \frac{\pi_\theta(y_{i}|\mathbf{y}_{<i})}{\pi_\text{ref}(y_{i}|\mathbf{y}_{<i})}$. 
% \begin{equation}
% V_t(\mathbf{y}_{<t}) = q_\theta^{t-1}(\mathbf{y}_{<t-1},\mathbf{y}_{t-1})
% \end{equation}

% \begin{equation}
% \hat A^{\text{GAE}(1,\lambda)}_t = \lambda^{T-t} \cdot
% \left[ \underbrace{r_o\left(\mathbf{y}\right)}_\text{PPO with outcome rewards} - \quad q_\theta^t(\mathbf{y}_{<t}, y_t) 
% \right] + \sum_{s=0}^{T-t} \lambda^l \cdot r_\phi(y_s)
% \end{equation}

% \end{proposition}

% \begin{proof}
% We set $r_t=0$ for $t<T$ and $r_T=r_o\left(\mathbf{y}\right)$. The above definition of $q^t_\theta$ is the exponential average of undiscounted outcome reward, so we omit the discount factor $\gamma$ for brevity. 
% \begin{equation}
% \begin{aligned}
% V_t(\mathbf{y}_{<t}) =& q_\theta^{t-1}(\mathbf{y}_{<t-1}, y_{t-1}) - r_{t-1} \\
% =&q_\theta^{t-1}(\mathbf{y}_{<t-1}, y_{t-1}) \nonumber
% \end{aligned}
% \end{equation}

% \begin{equation}
% \begin{aligned}
% \delta_t:=&r_t+V_{t+1}(\mathbf{y}_{<t-1})-V_t(\mathbf{y}_{<t})\\
% =&
% \begin{cases}
%   r_\phi(y_t),& 0\leq t < T,\\
%   r_o(\mathbf{y})-q_\theta^{t-1}(\mathbf{y}_{<t-1}), & t = T.
% \end{cases} \nonumber
% \end{aligned}
% \end{equation}

% \begin{equation}
% \hat A^{\text{GAE}(1,\lambda)}_t = \sum_{s=0}^{T-t} \lambda^l \cdot \delta_{t+s} \nonumber
% \end{equation}
% \end{proof}

% Based on the above proposition, we implement PRIME as a value model instead of a reward model, keeping its initialization and online update. As shown in Table \ref{tab:diff_rl_algo}, PRIME performs better as a reward model rather than a value model. We hypothesize that this phenomenon is related to the benefit of using advantage as process reward~\cite{setlur2024rewarding} and leave this topic for future exploration. 
% \fi

% Moreover, the PPO variant of PRIME provides no performance gain, demonstrating that the additional computation cost from the critic model is redundant. This makes it possible to compensate for the expense of the process reward model by using REINFORCE-like algorithms with simpler advantage estimators. 

% Finally, we choose the best-performing RLOO as the advantage estimator in our algorithm. 




% For PPO with value models,
% \begin{equation}
% \underbrace{r_o\left(\mathbf{y}^i\right)}_\text{PPO with outcome rewards} + \underbrace{\sum_{s=t}^{|\mathbf{y}^i|} \gamma^{s-t} \cdot r_\phi(y^i_s)}_\text{PPO with implicit process rewards} - V(\mathbf{y}_{<t})

% For GRPO,


% From the results, we find that REINFORCE-like algorithms, despite being simpler than PPO, are strong enough to produce stable results. In this paper, we choose the best-performing RLOO as our RL algorithm.


% \input{tables/dense_rewards_results}
% \subsection{Effect of dense rewards}
% \label{sec:effeoc_of_dense_rewards}
% Table ~\ref{tab:dense_rewards_results} shows the detailed results of PRIME and RLOO w/ outcome verifier (OV). We can see that at the same 240 steps, the model trained by PRIME is generally better than the model trained by outcome rewards, leading to a 4-point performance gap. PRIME could further enhance the model with more training steps. 



% \subsection{Effect of Reference Policy}
% \label{sec:effeoc_of_ref_policy}
% We delve into the comparative analysis of different reference policy implementations. As shown in Figure \ref{fig:effect_of_ref_policy}, The left one (policy ref) simply adopts the old logprob of policy model as  $\pi_{\text{ref}}$, while the right one (SFT ref) remains the initial SFT model for an additional $\pi_{\text{ref}}$ calculation.

% \subsection{``Zero'' Experiments}
% \label{sec:app_zero}
% \input{figures/zero_7}
% \input{figures/zero_32}
% \citet{deepseekai2025deepseekr1incentivizingreasoningcapability} proposed DeepSeek-R1-Zero, which is directly trained from a base model with reinforcement learning. To further investigate the ``Zero'' setting, we also perform RL from Qwen2.5-Math-7B-Base and Qwen2.5-32B-Base~\citep{qwen2.5}, skipping the SFT phase. 
% We present the experimental results in Figure~\ref{fig:zero_7} and Figure~\ref{fig:zero_32}. The observations are as follows:

% (1) \textbf{RL from base model is suprisingly efficient and effective.} Comparing PRIME from Qwen2.5-Math-7B and Eurus-2-7B-SFT, the ``Zero'' setting converges much faster. This indicates that directly performing RL from a base model might be a strong alternative to the conventional SFT-RL pipeline

% (2) \textbf{Larger models benefit more.} Comparing 7B and 32B models, we see that the 32B model gains more on both training rewards and test performance. This is aligned with the conclusion in \citet{deepseekai2025deepseekr1incentivizingreasoningcapability}.

% (3) \textbf{Saturation could be a potential issue.} Although PRIME-Zero obtains impressive performance gain, we find it quickly saturated at a very early stage (about 50 steps), which hinders further improvement like in \citet{deepseekai2025deepseekr1incentivizingreasoningcapability}. This is possibly attributed to the decrease of response diversity, and we leave this as future work.



\section{SFT Data \& Training Details}
\label{sec:sft_data_training_details}
\input{tables/actions}
\input{tables/sft_data_stat}

We first perform supervised finetuning on the base model to get a starter model for RL. 

\textbf{Action-centric chain-of-thought reasoning.} We apply imitation learning (supervised finetuning) as a warmup stage to teach models to learn certain reasoning patterns. To this end, we first design an action-centric chain-of-thought reasoning framework.
Table \ref{tab:actions} shows the actions in the action-centric chain-of-thought reasoning framework. When the model generates answers, it conducts multi-step reasoning and chooses one of the 7 actions at each step. The response begins with the ASSESS action and ends with the OUTPUT action.

\textbf{Construction of the SFT dataset.} To construct the SFT dataset, we collect reasoning instructions from several open-source datasets. It is noteworthy that we did not include many datasets with ground-truth answers in SFT, even though they are of higher quality. However, we reserve them for later RL training. The reason is that we aim to use different datasets for SFT and RL to diversify the exploration in RL, and we consider ground-truth more essential in RL than in SFT.  For completion, we employ LLaMA-3.1-70B-Instruct to answer the instructions, with a system prompt requesting the model to perform an action-centric chain-of-thought. Table \ref{tab:sft_data_stat} summarizes the key statistics of the datasets used for SFT. The datasets span mathematics, coding, and biomedicine. We finally obtain 230K SFT data and the average response length is 1390 tokens.

\textbf{SFT Training.}  During the SFT phase, we conduct full parameter fine-tuning with a learning rate of 1e-05, utilizing the AdamW optimizer alongside a cosine annealing learning rate schedule and a warmup ratio of 0.1. The batch size was set to 96, with a fixed random seed of 42. The model was trained on 230K datasets for 3 epochs.









\section{RL Data Preprocessing}
\label{sec:rl_data_process}
\subsection{RL Data Collection and Preprocessing}

We curate a high-quality RL training dataset of mathematics and coding problems with outcome verifiers (LaTeX answers for math and test cases for coding). For math, we source from NuminaMath-CoT~\citep{li2024numinamath},  which contains about 860K math problems. The problems span from Chinese high school mathematics to International Mathematical Olympiad competition questions. For coding, we source from APPS~\citep{apps}, CodeContests~\citep{li2022competition}, TACO~\citep{li2023taco}, and Codeforces\footnote{\url{https://huggingface.co/datasets/MatrixStudio/Codeforces-Python-Submissions}}. To further increase data quality, we conduct detailed cleaning and filtering. Finally, we retain 457k math problems and 27k coding problems.

\subsection{Data Filtering and Question-Type Classification}

The preprocessing pipeline employs a systematic rule-based approach to filter and classify mathematical problems to create a high-quality dataset with solvable problems, appropriate difficulty levels, and correct solutions. We exclude problems containing figures or diagrams since they require visual processing capabilities. We also remove proof questions due to difficulties in answer verification. Based on specific patterns, the remaining problems are classified into question-answering, multiple-choice, or fill-in-the-blank questions. Since fill-in-the-blank questions comprise less than 400 examples compared to the much larger set of multiple-choice questions, we focus solely on multiple-choice questions for further processing.

\subsection{Converting to Direct Question-Answer Format}


We transform multiple-choice questions into a direct question-answer format through three sequential stages: rule-based filtering, LLM-based filtering, and LLM-based formatting.

We first identify and remove questions that inherently require multiple-choice options - specifically, those where comparing specific statements or properties is essential to the problem-solving process. These questions cannot be meaningfully converted to a direct question-answer format. The initial filtering employs simple rule-based pattern matching, searching for keywords like "following" and "statement" that typically indicate option-dependent problems.

Following the rule-based filtering, we employ Llama-3.1-8B-Instruct to perform a more nuanced classification of the remaining questions. Our pilot study revealed that while the LLM occasionally misclassifies questions, it tends to err on the conservative side - marking potentially convertible questions as requiring options rather than the reverse. Given our large dataset, we accepted this conservative approach to maintain quality.

For questions classified as convertible, we implement a two-phase reformatting process: 1) Question Reformatting: Removing choice indicators and restructuring the question to elicit direct answers. 2) Solution Reformatting: Converting multiple-choice solutions into step-by-step derivations, ensuring all final answers are presented in standard LaTeX boxed format. This systematic approach maintains mathematical rigor while creating a standardized format suitable for downstream applications.

\subsection{Problem and Solution Validation}

The final stage involves merging all question-answer pairs and performing LLM-based comprehensive validation. We identify two key aspects in validation: solvability and correctness.

We leverage state-of-the-art mathematical reasoning models, including QwQ-32B-Preview~\citep{qwq-32b-preview} and Qwen2.5-Math-72B-Instruct~\citep{yang2024qwen25mathtechnicalreportmathematical}, employing a self-consistency approach to determine problem solvability, and if solvable, verify the correctness of solutions provided in the original dataset.


To enhance validation accuracy, we first analyzed sample problems to identify characteristics of solvable and unsolvable cases and created synthetic unsolvable problems featuring missing conditions or logical contradictions. Based on these samples, we developed specialized prompts to improve the models' ability to distinguish solvability. Each problem undergoes five independent validation attempts, where the LLM: 1) Provides step-by-step solutions using LaTeX formatting. 2) Identifies unsolvability due to missing conditions or logical contradictions. 3) Generates complete reasoning traces for solvable problems. 4) Presents final answers in standardized LaTeX boxed format (\texttt{\textbackslash boxed\{...\}}). 5) Document any impediments to solution completion.



We evaluate two key consistency measures across multiple validation attempts: 1) Status Consistency: agreement on problem solvability. 2) Answer Consistency: consistency of solutions across different attempts and agreement between generated solutions and ground truth. The final dataset retains only problems that demonstrate consistent solvability across validation attempts, agreement in solutions across multiple attempts, and alignment with ground truth answers. This rigorous validation process ensures the resulting dataset comprises well-defined, solvable problems with verified, accurate solutions.



\subsection{PRM Data}
\label{sec:app_prm_data}
\input{tables/stage1_data_stat}
% \input{tables/stage2_data_stat}
The dataset statistics of training EurusPRM are shown in Table~\ref{tab:stage1_data_stat}.