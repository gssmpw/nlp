\input{figures/results}
\section{Introduction}
Dense process rewards, which provide feedback at each intermediate step rather than only the whole trajectory, have proven effective in inference-time scaling of large language models (LLMs) on challenging reasoning tasks~\citep{uesato2022solving,Lightman2023LetsVS,Wang2023MathShepherdVA,yuan2024freeprocessrewardsprocess}.
On the training side, they also present superiorities in the reinforcement learning (RL) of LLMs, particularly in improving training efficiency~\citep{sutton2018reinforcement} and credit assignment~\citep{leike2018scalable} compared with sparse outcome rewards.
However, successful applications of dense rewards in RL for LLMs are limited~\citep{setlur2024rewarding}, as current industry-leading models primarily depend on verifiable outcome rewards and have not yet demonstrated meaningful progress with dense rewards~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,team2025kimi}.

We identify the central challenge as \textit{how to acquire and utilize high-quality dense rewards at scale}, which enables online process reward model (PRM) update efficiently.
The reason is that, optimizing towards a static reward model eventually leads to overoptimization or reward hacking~\citep{Gao2022ScalingLF} due to distribution shift. 
Ideally, this can be solved by improving the reward model online~\citep{leike2018scalable}. However, acquiring dense process labels for training is prohibitively more expensive. Existing methods either need to build complicated human annotation pipelines~\citep{Lightman2023LetsVS} or rely on estimation-based methods, which require about 10$\times$ more rollouts for each step than sampling only the response-level trajectories~\citep{Wang2023MathShepherdVA,kazemnejad2024vineppo}.
Neither of them is scalable in online RL. 
Moreover, to the best of our knowledge, it remains underexplored how to incorporate dense rewards into RL for LLMs. %\lifan{may not be accurate}

\looseness=-1
In this work, we propose Process Reinforcement through Implicit Rewards (PRIME), a scalable framework for enhancing reasoning capabilities via efficient reinforcement learning with dense token-level rewards. 
%PRIME addresses the challenge of reward sparsity by integrating outcome-aligned process supervision \lifan{another word?}\hao{i actually think we don't need this sentence} into policy training. 
At its core, the framework employs recently proposed implicit process reward modeling~\citep{yuan2024freeprocessrewardsprocess} to train dense reward models with only outcome-level labels.
This enables PRIME to perform online learning of reward signals using only outcome labels on policy rollouts, thereby fundamentally mitigating reward hacking while maintaining the same computational cost as traditional outcome reward models (ORMs).
Besides scalability, PRIME also (1) serves as a general method to fuse token-level dense rewards and sparse outcome rewards by calculating their returns separately before summing together, which is compatible with diverse RL algorithms~\citep{williams1992simple,Kool2019Buy4R,deepseek-math,ahmadian2024back,schulman2017proximal}; (2) eliminates the dedicated reward modeling stage, which is required by existing works, by simply initializing from the SFT model or even the base model (\S~\ref{sec:app_zero}).
% Besides scalability, PRIME also eliminates the explicit reward modeling stage. This is because the Implicit PRM is inherently a language model and we can simply use the supervised fine-tuned model for initialization.
In summary, starting from one single language model, the PRIME framework can efficiently accomplish the generation of dense rewards, the initialization and updating of reward models, as well as the reinforcement learning (RL) training of the policy model.


%Recent work answered the first question by presenting the implicit process reward modeling (PRM) objective to solve the scalability of dense rewards \citep{yuan2024freeprocessrewardsprocess}.
% Without any process label, Implicit PRM is trained as an outcome reward model (ORM) and then used as a PRM. 
% However, despite some recent progress on PRM, existing work mostly focuses on its application at inference \lifan{cite}, while few of them employ dense rewards in RL training \lifan{cite}.
%It is unclear where process rewards can be incorporated, e.g. initialization of value models or estimation of returns, and how they can be updated online. Therefore, in this work, we seek the \textbf{scalable} path towards advanced reasoning capabilities with \textbf{efficient reinforcement learning} through dense rewards.


\iffalse
 \setlength{\itemindent}{-2em}
\begin{itemize}
  \item RQ1: How to provide dense rewards in reinforcement learning remains underexplored.
  
  A: Utilizing token-level rewards directly in advantage/return estimation, rather than values.
  \item RQ2: PRM online update is unscalable given costly process labels.
  
  A: Adopting Implicit PRMs which can be online updated with only policy rollouts and outcome labels.
  \item RQ3: The explicit reward modeling stage consumes a huge amount of data.
  
  A: Using the SFT model as the initialization of the Implicit PRM, which not only bypasses the reward modeling but also improves training performance.
  
\end{itemize}

This is based on three benefits Implicit PRM brings:
(1) \textbf{Dense Reward:} Implicit PRM directly learns a Q-function and provides rewards for \textit{each token}, which alleviates the reward sparsity issue without the need for an extra value model; 
(3) \textbf{Scalability:} Implicit PRM can be online updated with only outcome labels. Therefore, we can directly update the PRM with on-policy rollouts given outcome verifiers, which mitigates the distribution shift as well as scalability issues for PRMs. 
(3) \textbf{Simplicity:} Implicit PRM is inherently a language model. Therefore, it is unnecessary to train a PRM beforehand since the SFT model itself already serves as a strong starting point.
\fi


\input{tables/comparison}
In experiments, we train Qwen2.5-Math-7B-Base~\citep{yang2024qwen25mathtechnicalreportmathematical} with PRIME after a lightweight SFT warmup stage. Compared to RL using outcome rewards only, PRIME achieves a $2.5\times$ sample efficiency gain and a $6.9\%$ performance improvements on challenging math problems.
As shown in Figure \ref{fig:results}, through PRIME, we successfully achieve substantial improvement on key mathematical reasoning benchmarks over the SFT model, leading to \textbf{16.7\%} improvement on average, and over \textbf{20\%} on AMC\&AIME competitions. Our final model Eurus-2-7B-PRIME surpassed Qwen2.5-Math-7B-Instruct on five key mathematical benchmarks. 
Notably, this is achieved with only $10\%$ of the data used by Qwen-Math, as in Table \ref{tab:comparision}.


Our analysis shows that updating the PRM online is key to the success of PRIME (\S \ref{sec:design}).
We also show that PRIME could generally boost various RL algorithms, including RLOO \citep{ahmadian2024back}, REINFORCE~\citep{williams1992simple}, PPO~\citep{schulman2017proximal}, and GRPO~\citep{deepseek-math} (\S \ref{sec:other_algo}). 
In terms of the design choices of advantage estimate, 
we observe that Implicit PRMs are better to be used as reward models than value models (\S \ref{sec:ppo}).


