\section{Inference Scaling with Implicit PRM}\hao{this is maybe too similar to the implicit PRM paper. both are strong works but we should draw a clear boundary between them}\lifan{if we keep this part in the paper, should emphasize the two-stage training method.}
Despite RL, implicit PRM could further scale inference-time computation through Best-of-N sampling. In this section, we present EurusPRM, a SOTA-level open-source PRM for Best-of-N sampling.

\subsection{Settings}
We introduce a two-stage training pipeline \lifan{give it a name} upon Qwen2.5-Math-7B-Instruct for EurusPRM. We collect instructions with ground truth and employ Qwen2.5-Math-7B-Base, Llama-3.1-8B-Base/Instruct, Llama-3.1-70B-Instruct, Qwen2.5-72B-Instruct, and our SFT model to sample rollouts. Training datasets statistics can be found in the Appendix \ref{sec:prm_data}.

\textbf{Stage 1: Training on Complete Response-level Rollouts.} We apply the above $L_{CE}$ to train implicit PRM. We used a learning rate of  5e-7 and a batch-size of 64 for training. 

\textbf{Stage 2: Training on Manufactured Partial Step-level Pairs.} We start the second-stage training on top of the first-stage model with fine-grained step-level labels. To obtain step-level labels, we employ Llama-3.1-70B-Inst and Qwen2.5-72B-Inst to insert nuance errors into correct solutions. We also mix response-level data in this stage. The model was continually trained with $L_{CE}$ with a learning rate of 5e-7 and a batch-size of 64.

During the evaluation phase, we adapt Eurus-2-7B-SFT, Qwen2.5-7B-Instruct, and Llama-3.1-70B-Instruct as generation models to evaluate the performance of our implicit PRM. For all models, we set the sampling temperature to 0.5 and the p of the top-p sampling to 1.

\subsection{Best-of-N Sampling}
We use Best-of-64 as our evaluation metric. The weighting methods are different for several PRMs below. For Skywork-o1-Open-PRM-Qwen-2.5-7B, we use a simple average reward across all steps. For EurusPRM-Stage 1, we use the minimum reward across all steps. For EurusPRM-Stage 2, we use the accumulative rewards. The results are shown in Table x\hanbin{Maybe pick a good table and put it here, and put the rest in appendix}.