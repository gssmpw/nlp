\PassOptionsToPackage{table}{xcolor}
\documentclass[dvipsnames]{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}
% \usepackage[table]{xcolor}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{hyperref}
%\usepackage{longtable}

\usepackage{url}
\usepackage{wrapfig,lipsum}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{fancyvrb}
\usepackage{pythonhighlight}
\usepackage{fvextra}
% \usepackage[table,xcdraw,dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{tabularray}
\usepackage{framed}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{algpseudocode}
\usepackage{algorithm}

\tcbuselibrary{listings} % Load the listings library
%\PassOptionsToPackage{prologue,dvipsnames}{xcolor}
\lstset{
  breaklines=True,%自动换行
  columns=flexible,%不随便添加空格,只在已经有空格的地方添加空格,
  basicstyle=\small,
  % backgroundcolor=\color{gray!10},
  % rulesepcolor= \color{gray!50},
  % frame=shadowbox,
}
\usepackage[tikz]{bclogo}
\usepackage{enumitem}
\newenvironment{itemize*}%
 {\leftmargini=20pt\begin{itemize}%
  \setlength{\itemsep}{3pt}%
  \setlength{\parskip}{0pt}%
  }%
 {\end{itemize}}
\newenvironment{enumerate*}%
 {\begin{enumerate}%
  \setlength{\itemsep}{0pt}%
  \setlength{\parskip}{0pt}}%
 {\end{enumerate}}

\definecolor{msftBlack}{RGB}{0,0,0}
\newcommand{\finding}[1]{
	\begin{bclogo}[couleur= msftBlack!05, epBord=1, arrondi=0.1, logo=\bcetoile, marge=6, ombre=true, blur, couleurBord=msftBlack!10, tailleOndu=2, sousTitre ={\em #1}]{} 
	\end{bclogo}
}

\NewDocumentCommand{\lifan}
{ mO{} }{\textcolor{cyan}{\textsuperscript{\textit{lifan}}\textsf{\textbf{\small[#1]}}}}
\NewDocumentCommand{\hanbin}
{ mO{} }{\textcolor{orange}{\textsuperscript{\textit{hanbin}}\textsf{\textbf{\small[#1]}}}}
\NewDocumentCommand{\hao}
{ mO{} }{\textcolor{purple}{\textsuperscript{\textit{hao}}\textsf{\textbf{\small[#1]}}}}

%\title{UltraFeedback: Boosting Language Models with High-quality Feedback}
\title{Process Reinforcement through \\ Implicit Rewards}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Ganqu Cui$^{2,1\dagger}$\thanks{Core Contributors.}, Lifan Yuan$^{3}$\thanks{Project Lead.}\hspace{0.35em}$^{*}$, Zefan Wang$^{1*}$, Hanbin Wang$^{4*}$, Wendi Li$^{1*}$, \\ \textbf{Bingxiang He}$^{1*}$,
\textbf{Yuchen Fan$^{2,5*}$}, \textbf{Tianyu Yu$^{1*}$}, \textbf{Qixin Xu$^{1*}$}, \textbf{Weize Chen$^{1}$}, \textbf{Jiarui Yuan}$^{1}$, \\ \textbf{Huayu Chen}$^{1}$, \textbf{Kaiyan Zhang}$^{1}$, \textbf{Xingtai Lv}$^{1}$, \textbf{Shuo Wang}$^{1}$, \textbf{Yuan Yao}$^{1}$, \textbf{Xu Han}$^{1}$, \\ \textbf{Hao Peng}$^{3}$, 
\textbf{Yu Cheng}$^{2,6}$, \textbf{Zhiyuan Liu}$^{1}$, \textbf{Maosong Sun}$^{1}$, \textbf{Bowen Zhou}$^{2,1}$, \textbf{Ning Ding}$^{1\dagger}$\\
$^1$Tsinghua University \quad
$^2$Shanghai AI Lab \quad
$^3$University of Illinois Urbana-Champaign \hfill\\
$^4$Peking University \quad
$^5$Shanghai Jiaotong University \quad
$^6$CUHK
\\
\texttt{cuiganqu@pjlab.org.cn} \quad
\texttt{lifan4@illinois.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
% \usepackage{markdown}
\usepackage{microtype}
% \usepackage{listings}
\hypersetup{
	colorlinks=true,
	linkcolor=red,
	filecolor=blue,      
	urlcolor=blue,
	citecolor=blue,
}
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\vspace{-10pt}
\begin{center}
    \url{https://github.com/PRIME-RL/PRIME}
\end{center}
\begin{abstract}
Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. 
While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of 
training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking.
To address these challenges, we propose PRIME (\underline{\textbf{P}}rocess \underline{\textbf{R}}einforcement through \underline{\textbf{IM}}plicit r\underline{\textbf{E}}wards), which enables online PRM updates using only policy rollouts and outcome labels through \textit{implict process rewards}. 
PRIME combines well with various advantage functions and forgoes the dedicated reward model training phase that existing approaches require, substantially reducing the development overhead. 
We demonstrate PRIME's effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1\% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10\% of its training data.\footnote{Models and data are available at: \url{https://github.com/PRIME-RL/PRIME}.}
\end{abstract}



\input{sections/intro}
\input{sections/implicit-process-reward}
\input{sections/prime}
\input{sections/experiments}
% \input{sections/inference_scaling}
\input{sections/relatedwork}

\section{Conclusion}
As the fuel of LLMs, data, will be depleted in the near future, we are entering a new era of search and exploration, which is exemplified by reinforcement learning~\citep{sutton2019bitter}. This work develops PRIME, which produces and leverages dense rewards in online RL for LLM reasoning. Throughout the experiments, we validate that PRIME (1) greatly benefits sample efficiency and policy performance, (2) is easy to use with minimum cost, and (3) is a general method that works with broad RL algorithms together.

% \section*{Impact Statement}
% This paper presents PRIME whose goal is to advance the field of reinforcement learning for LLMs. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.





\bibliography{icml2024}
\bibliographystyle{iclr2024_conference}

\appendix


\input{sections/appendix}

\end{document}
