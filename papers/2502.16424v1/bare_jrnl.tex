 
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper. 
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/
%\usepackage{indentfirst}
\setlength{\parindent}{2em}

\documentclass[journal]{IEEEtran}

% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}
\usepackage{multirow}
%\usepackage{multicolumn}
%\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{amsthm,amssymb}
\usepackage{mathrsfs}
\usepackage{algorithmic}
\usepackage{makecell}%解决公式在表格中位置太小的问题
\usepackage{easyReview}
\usepackage{booktabs}%解决表格横向粗细问题
\setcellgapes{3pt}
\usepackage{verbatim}
\setcellgapes{3pt}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage[justification=centering]{caption}
\usepackage{color}
\usepackage[subfigure]{tocloft}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{amsmath, bm}
\usepackage{hyperref}

% 格式设置
\hypersetup{hidelinks,
	colorlinks=true,
	allcolors=blue,
	pdfstartview=Fit,
	breaklinks=true}
%\usepackage [numbers,sort&compress,super,square]{natbib}


% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MSemCom UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\usepackage{cite}

% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
%\usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
%\usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Lightweight Vision Model-based Multi-user Semantic Communication Systems}

\author{Feibo Jiang, \textit{Senior Member, IEEE}, Siwei Tu, Li Dong,
%Kezhi Wang, \textit{Senior Member, IEEE}, 
 Kezhi Wang, \textit{Senior Member, IEEE}, Kun Yang, \textit{Fellow, IEEE}, Ruiqi Liu, \textit{Senior Member, IEEE}, Cunhua Pan, \textit{Senior Member, IEEE}, Jiangzhou Wang, \textit{Fellow, IEEE} %Xiaohu You, \textit{Fellow, IEEE}
	\thanks{
		Feibo Jiang (jiangfb@hunnu.edu.cn) is with Hunan Provincial Key Laboratory of Intelligent Computing and Language Information Processing, Hunan Normal University, Changsha 410081, China.
		
		Siwei Tu (tusiwei@hunnu.edu.cn) is with School of Information Science and Engineering, Hunan Normal University, Changsha 410081, China.
		
		Li Dong (Dlj2017@hunnu.edu.cn) is with the School of Computer Science, Hunan University of Technology and Business, Changsha 410205, China, and also with the Xiangjiang Laboratory, Changsha 410205, China
		
		Kezhi Wang (Kezhi.Wang@brunel.ac.uk) is with the Department of Computer Science, Brunel University London, UK.
		
		Kun Yang (kunyang@essex.ac.uk) is with the School of Computer Science and Electronic Engineering, University of Essex, Colchester, CO4 3SQ, U.K., also with Changchun Institute of Technology.
		
		
		Ruiqi Liu (richie.leo@zte.com.cn) is with the Wireless and Computing Research Institute, ZTE Corporation, Beijing 100029, China.
		
		Cunhua Pan (cpan@seu.edu.cn) is with the National Mobile Communications Research Laboratory, Southeast University, Nanjing 210096, China.
		
		Jiangzhou Wang (j.z.wang@seu.edu.cn) is with the National Mobile Communications Research Laboratory, Southeast University, Nanjing, China, and also with the Purple Mountain Laboratories, Nanjing, China.
		
	%	Xiaohu You (xhyu@seu.edu.cn) is with the National Mobile Communications Research Laboratory, Southeast University, Nanjing, China, and also with the Purple Mountain Laboratories, Nanjing, China.
	}
	%	\thanks{The paper was submitted on \today.}\\
}


%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

%\author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
%        John~Doe,~\IEEEmembership{Fellow,~OSA,}
%        and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
%\thanks{M. Shell was with the Department
%of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
%GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
%\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Submitted for Review}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.

\begin{abstract}
Semantic Communication (SemCom) is a promising new paradigm for next-generation communication systems, emphasizing the transmission of core information, particularly in environments characterized by uncertainty, noise, and bandwidth constraints. However, existing image SemCom systems face several challenges, such as inefficient knowledge base construction, insufficient semantic encoding, and lack of multi-user semantic sharing. To address these issues, we propose a Lightweight Vision Model-based Multi-user Semantic Communication System (LVM-MSC). First, we construct a Lightweight Knowledge Base (LKB) based on the fast Segment Anything Model (SAM). LKB incorporates the extensive image knowledge of the SAM model while significantly reducing the number of parameters through its convolutional architecture. Next, we design an Efficient Semantic Codec (ESC) based on the Masked AutoEncoder (MAE) architecture. ESC enhances semantic compression at both the pixel and semantic levels and implements lightweight semantic decoding tailored for user devices. Furthermore, we  propose a Multi-user Semantic Sharing (MSS) transmission for the multi-user SemCom. By calculating the similarity of semantic information among different users in the sharing semantic space, we unify the transmissions of similar semantic information through broadcasting, further improving the transmission efficiency. Finally, simulation results demonstrate the feasibility and effectiveness of the proposed LVM-MSC system.
\end{abstract}

\begin{IEEEkeywords}
	Semantic communication; Large vision model; SAM; Masked autoencoder; Multi-user communication.
\end{IEEEkeywords}

\section{Introduction}
With the rapid advancement of technologies such as the Internet of Things (IoT), Augmented Reality (AR), and Virtual Reality (VR), future communication networks must accommodate increasingly diverse and complex application scenarios \cite{wang2015multi}. The data transmission demands in these scenarios far exceed the capabilities of current 5G networks. Traditional approaches that focus solely on increasing transmission rates or expanding bandwidth are becoming insufficient in addressing the exponential growth in connected devices and data volume \cite{letaief2019roadmap}. 

In 6G, the design of Semantic Communication (SemCom) systems is emerging as a prominent research focus \cite{kountouris2021semantics}. Unlike traditional communication systems, the core of SemCom lies in extracting the ``meaning" of the transmitted information. By leveraging knowledge bases shared between the sender and receiver, semantic information can be effectively ``interpreted" at the receiver's end, enabling further data compression. This approach goes beyond merely correctly transmitting and receiving bits, placing greater emphasis on the semantic content carried in the messages. This paradigm shift offers a novel pathway for efficient information transmission, particularly in communication environments characterized by limited bandwidth, high mobility, or limited network resources. 6G systems are expected to feature intelligent communication capabilities, allowing not only data transmissions but also intelligent data processing and perception, thereby significantly enhancing communication performance and efficiency \cite{xie2021deep}.

With the integration of computer vision and deep learning, numerous researchers have proposed different types of image SemCom systems. Convolutional Neural Networks (CNNs) excel at extracting image features, significantly influencing semantic perception \cite{weng2021semantic}. AutoEncoders play a critical role in compressing and reconstructing semantic information due to their ability to capture compact representations \cite{luo2022autoencoder}. Additionally, Vision Transformers (ViTs), which leverage attention mechanisms to model long and short-range dependencies in images, demonstrate exceptional performance for high-resolution images and complex scenarios \cite{yoo2023role}. Meanwhile, the emergence of generative Artificial Intelligence (AI) and Large AI Models (LAMs) has introduced novel methods for building high-performance SemCom systems \cite{jiang2024large}. SemCom systems based on Large Language Models (LLMs), such as GPT-4 \cite{achiam2023gpt} and LLaMA \cite{touvron2023llama}, and Large Vision Models (LVMs), such as SAM \cite{kirillov2023segment} and Stable Diffusion \cite{rombach2021highresolution}, have been proposed and demonstrated tremendous potential. However, under constraints of limited communication resources and the increasing demands of user communication requirements, current research on image SemCom systems still faces the following challenges:

\begin{enumerate}[]
	\item {\it{Inefficient Knowledge Base Construction:}}
	Traditional SemCom systems typically rely on large-scale rule sets or Knowledge Graphs (KGs) as their knowledge bases \cite{shi2021semantic}, extracting information through supervised learning. However, collecting, summarizing, and updating these knowledge bases requires complex operations and high costs. LVMs inherently possess extensive world knowledge, offering a significant advantage in constructing knowledge bases for SemCom systems. Nonetheless, such models come with substantial parameter scales, high deployment costs, and lengthy inference times, making it difficult to meet the latency requirements of SemCom systems \cite{wang2024uses}. Therefore, there is an urgent need for a low-cost and high-efficiency solution for lightweight knowledge base construction.
	
	
	\item {\it{Insufficient Semantic Encoding:}}
	Images, as natural signals, exhibit significant pixel redundancy, resulting in low information density. Image semantic encoders based on CNNs and ViTs encode all pixels of an image, eliminating redundancy only at the semantic level but failing to address redundancy at the pixel level \cite{xie2023communication}. This approach not only consumes substantial bandwidth but also compromises the quality of key semantic information for primary semantic objects in the image. We propose that high-quality image semantic codec should eliminate information redundancy at both the pixel and semantic levels, enabling the transmission of high-density semantic information. 
	
	\item {\it{Lack of Multi-User Semantic Sharing:}} 
	As the number of users and applications increases, point-to-point communication systems face performance bottlenecks when handling large-scale, multi-user data. Although some multi-user SemCom systems have been proposed, they rarely consider the sharing of information in the semantic space among multiple users \cite{xie2022task}. SemCom systems process information in the semantic space, where the commonalities and differences in semantic information provide a foundation for designing multi-user SemCom systems based on the semantic relevance. By leveraging advanced image feature representation techniques, such as those provided by LVMs, it is possible to accurately identify the semantic information of different users. \textcolor{black}{Multi-user Semantic Sharing (MSS) transmission} aims to fully leverage shared semantic information in the semantic space, thereby reducing redundant bandwidth consumption caused by the repeated transmission of semantically similar data among different users.
\end{enumerate}

Lightweight vision models represent one of the cutting-edge research areas in the development of LVMs. To address challenges in applying LVMs to edge systems, such as high resource consumption and slow inference speeds, researchers have adopted techniques such as knowledge distillation, model quantization, and pruning \cite{gou2021knowledge} \cite{yao2022zeroquant}. These methods aim to reduce model parameters and enhance inference efficiency while maintaining performance, making them highly suitable for resource-limited SemCom systems. Hence, we propose a Lightweight Vision Model-based Multi-user Semantic Communication System (LVM-MSC) that leverages lightweight LVMs. The innovations of LVM-MSC are as follows:

\begin{enumerate}[]
	\item {\it{Lightweight Knowledge Base:}}
	To address the challenges of high resource consumption and slow inference speed associated with constructing knowledge bases using LVMs, we develop a Lightweight Knowledge Base (LKB) based on Fast Segment Anything Model (SAM), an optimized lightweight SAM. Fast SAM replaces the original ViT model with a simpler CNN model, achieving a 10-fold reduction in parameters and a 50-fold acceleration in inference speed. While maintaining performance, this approach enables the rapid construction of the LKB, aiding SemCom systems in accurately perceiving the key semantic objects in images.
	
	\item {\it{Efficient Semantic Codec:}}
	To achieve more efficient semantic compression, we design an Efficient Semantic Codec (ESC) based on the Masked AutoEncoder (MAE) architecture. ESC's encoder divides semantic encoding into two stages: the low-level (pixel) encoding stage, which uses the semantic objects and their location information identified by the LKB to perform adaptive masking, reducing redundant pixels in the image unrelated to the semantic objects; and the high-level (semantic) encoding stage, which uses a ViT-based image encoder for high-quality semantic encoding to eliminate redundant semantic information. At the receiver end, the semantic decoder requires only a small amount of semantic information to accurately reconstruct the semantic objects in the image and rebuild the original image through the statistical properties between pixels. Additionally, the ESC is designed with an asymmetric structure, where the computationally intensive encoder part is deployed at the base station, and the computationally simpler decoder part is deployed at the user device.
	
	\item {\it{Multi-user Semantic Sharing Transmission:}} 
	From the semantic perspective, we leverage the powerful feature representation capabilities of LVMs to construct a multi-user semantic space, where image data from different users is distinguished, and semantically identical or similar feature vectors are merged to form shared semantic signals. 
	During communication, the shared and private semantic information are transmitted separately, reducing the overall information volume. On the receiving end, for each user, the shared and private semantic information are recombined to reconstruct the original images, thereby achieving more efficient multi-user SemCom systems.
	

\end{enumerate}

The remainder of this paper is structured as follows: Section \uppercase\expandafter{\romannumeral2}  reviews the related work; Section \uppercase\expandafter{\romannumeral3}  presents the system model; Section \uppercase\expandafter{\romannumeral4}  provides a detailed description of the proposed LVM-MSC; Section \uppercase\expandafter{\romannumeral5} discusses the experimental setup and results; Section \uppercase\expandafter{\romannumeral6} concludes the paper.

\section{Related Work}
\subsection{Image SemCom systems}
Zhang et al. \cite{zhang2023predictive} proposed a Predictive and Adaptive Deep Coding (PADC) framework to achieve flexible bitrate optimization in image SemCom, ensuring specific transmission quality requirements. The PADC framework is capable of transmitting images over wireless channels with minimal bandwidth consumption while maintaining the Peak Signal-to-Noise Ratio (PSNR) constraints for each image data. 
Liu et al. \cite{liu2024novel} introduced a novel image SemCom method that combines Dynamic Decision Generation Networks (DDGN) and Generative Adversarial Networks (GANs) to effectively compress and transmit images while maintaining a high compression ratio and reducing distortion in the reconstructed images. This method demonstrated superior performance in low Signal-to-Noise Ratio (SNR) wireless communication environments. 
Pan et al. \cite{pan2023image} proposed an image segmentation-based SemCom system for efficient visual data transmission in vehicular networks. The system extracts semantic information from images using a multi-scale semantic information extractor based on Swin Transformer at the sender side, and at the receiver side, and it uses a semantic information decoder and reconstructor to combat wireless channel noise and reconstruct image segmentation, thereby achieving high compression ratios and robust transmission performance under limited spectrum resources.

All of these works adopted a direct semantic encoding approach for images, aiming to eliminate redundancy at the semantic level. However, they did not consider information compression at the pixel level of the image.

\subsection{Large AI models for SemCom systems}
Jiang et al. \cite{jiang2024large} proposed a large-model-based multimodal SemCom framework, which utilizes Multimodal Language Models (MLMs) to facilitate the conversion between multimodal and unimodal data while maintaining semantic consistency. They also introduced a personalized LLM knowledge base to perform personalized semantic extraction or recovery, effectively addressing the issue of semantic ambiguity. 
Zhao et al. \cite{zhao2024lamosc} presented a SemCom system called LaMoSC, which employs an LLM-driven multimodal fusion framework to reconstruct original visual information. The system integrates visual and textual multimodal feature inputs through an end-to-end encoder-decoder network to improve visual transmission quality under low SNR conditions. 
Wang et al. \cite{wang2024uses} proposed an LLM-based end-to-end learning SemCom model, which leverages the semantic understanding capabilities of LLMs to design semantic encoders and decoders. This model enhances semantic fidelity and cross-scene generalization by using subword-level tokenization, gradient-based rate adapters, and task-specific fine-tuning, improving performance under various channel encoding-decoding rate requirements.

However, these SemCom systems overlook the high energy consumption and latency caused by the enormous parameter numbers of LAMs, as well as the resource constraints of edge devices in mobile communication systems.

\subsection{Multi-user SemCom systems}
Xie et al. \cite{xie2022task} studied task-oriented multi-user SemCom systems and proposed a Transformer-based framework to unify the transmitter architecture for different tasks, including image retrieval, machine translation, and visual question answering. They introduced the DeepSC-IR, DeepSC-MT, and DeepSC-VQA models for unimodal and multimodal multi-user systems, respectively. 
Li et al. \cite{li2023non} proposed a Non-Orthogonal Multiple Access (NOMA)-based multi-user SemCom system (NOMASC), which supports semantic transmission for multiple users with different source information modalities. The system uses asymmetric quantizers and neural network models for symbol mapping and intelligent multi-user detection. 
Mu et al. \cite{mu2022semantic} introduced an innovative heterogeneous semantic and bit multi-user communication framework that adopts a semi-NOMA scheme to effectively facilitate heterogeneous semantic and bit multi-user communication. They also proposed an opportunistic semantic and bit communication method to alleviate the early and late rate difference issues in NOMA.

These multi-user SemCom systems rely on physical resources to differentiate user signals and fail to exploit the commonalities and differences between the information of different users in the semantic space. As LAMs become increasingly capable of extracting more precise semantic information, leveraging the commonality among semantics transmitted by different users to further enhance the efficiency of SemCom systems represents a novel research direction.

\section{System Model}
As shown in Fig. \ref{fig:sys}, we consider a multi-user SemCom system. The system consists of  a transmitter (base station) and multiple receivers (users). 
At the base station , $K$ source images are jointly semantic and channel encoded and transmitted via the downlink channel. At the receiver, the received signals are jointly semantic and channel decoded to reconstruct the source data images.

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=17cm]{sys.png}
	\caption{The structure of system model.}
	\label{fig:sys}
\end{figure*}

\subsection{Base station}
We denote the source image data of the $k$-th user as ${\textbf P}_k \in \mathbb{R}^{C \times H \times W}$, where $C$, $W$ and $H$ are the channel, width and height of the source image, respectively. Each source contains the semantic information. The semantic information is extracted first by
\begin{equation}
	\label{equ:1}
	\textbf Z_k={S\left( {F\left ( {\textbf P}_k;{\theta} \right ) ,{\textbf P}_k ;{\alpha }_k} \right)}
\end{equation}
where ${\textbf Z}_k \in \mathbb{R}^{L_S \times D_S}$ is the semantic information with length $L_S$ and feature dim $D_S$. ${{S}\left( {;{{{\alpha }}_k}} \right)}$ is the semantic encoder for the $k$-th user with learnable parameters ${{{\alpha }}_k}$. ${{F}\left( {;{{{\theta }}}} \right)}$ is the knowledge base named LKB with a fixed parameter $\theta$ shared by all users. ${{F}\left( {{\textbf P}_k;{{{\theta }}}} \right)}$ is the location information of key semantic objects retrieved by LKB from ${\textbf P}_k$ and participates in semantic encoding. Due to the limited communication resource and complex communication environment for wireless communications, the semantic information of the $k$-th user is compressed by
\begin{equation}
	{\textbf X}_k = C\left( {\textbf Z}_k;{\beta }_k \right)
\end{equation}
where ${\textbf X}_k \in {\mathbb{C} }^{L_S \times D_C}$ is the transmitted complex signal with feature dim $D_C < D_S$  and ${ C\left( { ;{{{\beta }}_k}} \right)}$ is the $k$-th user channel encoder with learnable parameters ${{{\beta }}_k}$. The channel encoder in SemCom compresses semantic information to reduce the number of transmitted symbols, as well as improve the robustness to channel variations. 
After the channel encoding process, signal power normalization is carried out to satisfy the transmission power constraint
\begin{equation}
	\frac{1}{L_S \times D_C} \mathbb{E}\left[\left\|\textbf{X}_k\right\|_{2}^{2}\right] \leq P_{S},
\end{equation}
where $P_{S}$ denotes the maximum allowable power per transmit-
ter. This step ensures that the power of the transmitted signal
$\textbf{X}_k$ remains within the predefined limit, optimizing the use of available transmission power.

\subsection{Downlink channel}
When the transmitted signal passes a Multiple-Input Multiple-Output (MIMO) physical channel, the received signal, ${\bf Y}_k$, at the receiver can be expressed as
\begin{equation}\label{eq3}
	{\bf Y}_k = {\bf H}_k {\bf X}_k + {\bf N}_k,
\end{equation}
where ${\bf H}_k$ is the channel matrix between the base station and the $k$-th user. For the Rayleigh fading channel, the channel coefficient follows ${\cal CN}(0,1)$; for the Rician fading channel, it follows ${\cal CN}(\mu,\sigma^2)$ with $\mu = \sqrt{r/(r+1)}$ and $\sigma = \sqrt{1/(r+1)}$, where $r$ is the Rician coefficient. $ {\bf N}_k$ denotes the circular symmetric Gaussian noise. The elements of ${\bf N}_k$ are i.i.d with zero mean and variance ${\sigma }_{n}^{2}$, and SNR is defined by $\sum\limits_k {{{\left\| {{\textbf{H}_k}{\textbf{X}_k}} \right\|}^2}} /\sigma _n^2$.

Subsequently, the transmission signals are recovered by the Linear Minimum Mean-Squared Error (L-MMSE) detector with the estimated Channel State Information (CSI)
\begin{equation}
	\label{equ:5}
	{\bf{\widehat X}}_k = {{\bf{\widehat H}}^H}{\left( {{\bf{\widehat H}}{{{\bf{\widehat H}}}^H} + \sigma_n^2 {\bf{I}}} \right)^{ - 1}}{\bf Y}_k
\end{equation}
where ${\bf{\widehat X}}_k$ is the recovered transmission signals at the $k$-th user, $\widehat {\bf H} = {\bf H} + {\Delta {\bf H}} $ is the estimated CSI, in which ${\Delta {\bf H}}$ is the estimation error with  ${\Delta {\bf H}} \in {\cal CN}(0, \sigma^2_e)$. 

\subsection{Receiving user}
The semantic information from the $k$-th user, ${\hat{\textbf{Z}}_k} \in \mathbb{R}^{L_S \times D_C}$, is recovered by the  channel decoder as
\begin{equation}
	{\hat{\textbf{Z}}_k} = C^{ - 1}\left( {\hat {\textbf{X}_k};{\gamma}_k } \right)
\end{equation}
where ${ C^{ - 1}}\left( {;{{\gamma}}_k } \right)$ is channel decoder for the $k$-th user with the learned parameters ${\gamma}_k$. The channel decoder aims to decompress the semantic information while mitigating the effects of channel distortion and inter-user interference. Ultimately, the semantic information is semantic decoded and reconstructed into an image
\begin{equation}
	\label{equ:7}
	{\textbf{Q}}_k  = S^{-1} \left( {\hat {\textbf{Z}_k};{\varphi}_k} \right)
\end{equation}
where ${\textbf{Q}}_k$ is the reconstructed image.  $ S^{-1}(;{\varphi}_k)$ is the semantic decoder for the $k$-th user with learning parameters ${\varphi}_k$.

\section{Proposed LVM-MSC System}
In this section, we provide the implementation details of the proposed LVM-MSC system, as illustrated in Fig. \ref{fig:main} (using two users as an example).

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=16cm]{main.png}
	\caption{The structure of the proposed SemCom system.}
	\label{fig:main}
\end{figure*}

\subsection{System overview}
\subsubsection{LKB}
To construct a lightweight knowledge base, we first deploy an LKB at the base station, based on the Fast SAM model. The LKB encompasses extensive visual knowledge and can swiftly and accurately locate semantic objects in an image \cite{zhao2023fast}. It facilitates the adaptive masking process of the ESC by discarding irrelevant background pixels, thereby improving encoding efficiency at the pixel level. As illustrated in Fig. \ref{fig:main}, the LKB identifies key semantic objects in different source image data by marking them with rectangular bounding boxes.

\subsubsection{ESC encoder}
We utilize the ESC, consisting of an encoder and decoder, to perform efficient semantic encoding and decoding. First, the ESC encoder references the location information generated by the LKB to adaptively mask a portion of the source image pixels that are weakly associated with semantic objects. Subsequently, the semantic encoder, composed of multiple Transformer encoder layers, encodes the remaining critical pixels to generate high semantic-density information. As shown in Fig. \ref{fig:main}, for a single animal image, background pixels exhibit a weak correlation with the animal itself. Thus, most background pixels are adaptively masked by the ESC, while the ESC encoder performs high-quality semantic encoding on the remaining critical pixels.

\subsubsection{Semantic comparator}
At the base station, a semantic comparator evaluates the similarity of semantic information from different users in the semantic space. Each piece of semantic information is divided into two parts: private semantic information specific to each user and shared semantic information common to all users. The shared semantic information represents similar features across different semantic information, such as the grassland in the background and the white fur on different animals illustrated in Fig. \ref{fig:main}.

\subsubsection{\textcolor{black}{MSS} transmission}
The shared semantic information is encoded using a public channel encoder and transmitted through a public downlink channel to all receiving users, thereby reducing redundant semantic transmissions. Private semantic information, on the other hand, is encoded and transmitted through channel encoders and downlinks corresponding to individual users. At the receiver end, each user receives both the shared and private semantic information. These are then combined and decoded using the user-specific channel decoder.

\subsubsection{Channel encoder and decoder}
The encoded semantic information is processed through a channel encoder to ensure effective transmission over the physical channel. The channel encoder consists of an autoencoder, where the feature dimensions of the hidden layers progressively decrease to achieve data compression. A perceptron neural network is used to simulate the channel, which essentially represents a mapping between inputs and outputs. \textcolor{black}{This mapping is primarily determined by the network's weights and biases, the weights represent the gain or attenuation characteristics of the channel, while the biases simulate random noise within the channel. The specific noise characteristics depend on the channel model and may include additive Gaussian noise, multipath fading, or other distortion factors.} At the receiver side, a channel decoder is employed to decode the transmitted information. To maintain consistency, the channel decoder adopts an autoencoder structure that is the inverse of the channel encoder.

\subsubsection{ESC decoder}
At the receiver side, the ESC decoder, composed of multiple Transformer decoder layers, functions as a semantic decoder to reconstruct the original image from the limited semantic information. It is worth noting that the structures of the semantic encoder and decoder are asymmetrical. The semantic encoder, which extracts semantic information, has a larger number of parameters and is deployed at the base station, whereas the semantic decoder, responsible for reconstructing the image, has fewer parameters and is deployed on the user-end devices.

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=17cm]{train.png}
	\caption{Training of the proposed SemCom system.}
	\label{fig:train}
\end{figure*}

\subsection{Training algorithm}
As shown in Fig. \ref{fig:train} and Algorithm \ref{alg:lvm-isc}, the training process of LVM-MSC consists of three phases: \texttt{Train Semantic Codec}, \texttt{Train Channel Codec}, and \texttt{Train Whole Network}. It is important to note that the Fast SAM is a pre-trained vision model; therefore, the LKB does not participate in the training of the SemCom system.

The first phase is to train the semantic codec. The semantic codec, ${\mathrm S\left( {;{{\alpha }_k}}\right)}$ and ${\mathrm S^{-1}\left( {;{{\varphi }_k}}\right)}$, will be trained firstly by the Mean Square Error (MSE) loss function \cite{koksoy2006multiresponse}, which enables the model to turn images into semantic information by learning the distribution of pixels. The MSE loss function is represented by
\begin{equation}
	\label{equ:8}
	\mathcal{L}_{\mathrm{MSE,1}}=\mathbb{E}\left[\left\|{\boldsymbol{\textbf{Q} }}_k-\boldsymbol{\textbf{P} }_k\right\|_{2}^{2}\right].
\end{equation}

In the second training phase, the channel codec \(C(;\beta_k)\) and \(C^{-1} (;\gamma_k)\) are also trained to learn the compression and decompression of image semantic information, with the channel distortion being addressed through the MSE loss function
\begin{equation}
	\label{equ:9}
	\mathcal{L}_{\mathrm{MSE,2}}=\mathbb{E}\left[\left\|{{ \hat {\textbf{Z} }}_k}-\boldsymbol{\textbf{Z} }_k\right\|_{2}^{2}\right].
\end{equation}

Finally, the whole network is trained, with the loss function designed as follows
\begin{equation}
	\label{equ:10}
	\mathcal{L}_{\mathrm{MSE,3}}=\mathbb{E}\left[\left\|{\boldsymbol{\textbf{Q} }}_k-\boldsymbol{\textbf{P} }_k\right\|_{2}^{2}+\left\|{{ \hat {\textbf{Z} }}_k}-\boldsymbol{\textbf{Z} }_k\right\|_{2}^{2}\right].
\end{equation}

\subsection{Lightweight knowledge base}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=8cm]{msam.png}
	\caption{The workflow of LKB.}
	\label{fig:msam}
\end{figure}

Fast SAM is a lightweight LVM capable of quickly locating semantic objects in an image. It uses a CNN model as its backbone, achieving a 10-fold compression in the parameters and a 50-fold acceleration in inference speed compared to the SAM model \cite{zhao2023fast}. This makes it highly suitable for deployment on edge devices with extremely fast inference speeds. We first introduce Fast SAM to construct LKB. LKB perceives the location information of key semantic objects in the image and passes it to the semantic encoder. The workflow of LKB is shown in Fig. \ref{fig:msam}. For a given input image $\textbf{P}$, the process of LKB perceiving the semantic object location information, denoted as $loc$, consists of the following two stages.

\subsubsection{All-instance segmentation}
First, we use a CNN-based backbone network along with a Feature Pyramid Network (FPN) \cite{lin2017feature} to extract multi-scale features from the input image $\textbf{P}$
\begin{equation}
	\label{equ:11}
	\textbf{F}_P= \mathrm{FPN}\big(\mathrm{CNN}(\textbf{P})\big)
\end{equation}
where $\textbf{F}_P$ is the feature maps containing multi-scale features. $\mathrm{CNN}\left ( \cdot \right ) $ is the CNN-based backbone network. $\mathrm{FPN}\left ( \cdot \right ) $ is the FPN model which is used to fuse image features at different levels.

Subsequently, we employ a CNN-based segment branch to obtain the prototype mask and mask coefficients of the image. The final instance mask is obtained through a linear combination of these prototype masks and coefficients, completing the image segmentation task. The instance mask is represented as a binary image, where the regions containing semantic objects in the image are indicated \cite{wang2023yolov7}, this process can be expressed as follows
\begin{equation}
	\textbf{p} =\mathrm f _{pt}\left ( \textbf{F}_P \right ), 
\end{equation}
\begin{equation}
	\textbf{c} =\mathrm f _{cf}\left ( \textbf{F}_P \right ), 
\end{equation}
\begin{equation}
	\label{equ:14}
	\textbf{M}_i =\sigma\left(\sum_{j=1}^{k^{\prime}}\textbf{c} _{i,j}\cdot \textbf{p} _j\right)\quad i=1,2,...,N,
\end{equation}
where $\mathrm{f} _{pt}\left ( \cdot \right ) $ and $\mathrm{f} _{cf}\left ( \cdot \right )$ are functions used in the segment branch to generate prototype codes $	\textbf{p}\in\mathbb{R}^{H^{\prime}\times W^{\prime}\times k^{\prime}}$ and mask coefficients $	\textbf{c}\in\mathbb{R}^{N\times k^{\prime}}$. $k^{\prime}$ is the number of prototype codes. $H^{\prime}$ and $W^{\prime}$ are the spatial dimensions of the prototype codes. $N$ is the number of target instances detected in the image. $\textbf{M}_i$ represents the $i$-th instance's mask. $\textbf{c}_{i,j}$ is the $j$-th prototype code's coefficient for the $i$-th instance. $\textbf{p} _j$ is the $j$-th prototype code. $\sigma\left ( \cdot \right ) $ is a function used for normalization \cite{wang2023yolov7}.

\subsubsection{Goal-oriented selection}
The communicating parties agree on the semantic objects to be transmitted. The LKB is capable of semantic perception, extracting the matching objects from all-instance segmentation \(\textbf{M}=\{\textbf{M}_i\}$ and obtaining their location information. Suppose the communication parties use a piece of text \( t \) to confirm the semantic objects. We use the text encoder of CLIP \cite{radford2021learning} to encode the text \( t \) into a feature representation and calculate the similarity between this representation and each mask \( \textbf{M}_i \) to select the location information of the specified semantic objects.
\begin{equation}
	\label{equ:15}
	\textbf{Sim}_i=\mathrm{cos}\bigl(\textbf{M}_i,\mathrm{CLIP}(t)\bigr)=\frac{\textbf{M}_i\cdot \mathrm{CLIP}(t)}{|\textbf{M}_i||\mathrm{CLIP}(t)|},\quad i=1,2,...,N
\end{equation}
\begin{equation}
	\label{equ:16}
	loc=arg\max_i \textbf{Sim}_i
\end{equation}
where $\mathrm{CLIP}\left ( \cdot \right ) $ is the text encoder of CLIP. $loc$ refers to the location information of the final selected semantic objects in the image.
The workflow of LKB is illustrated in Algorithm \ref{alg:sam}.

\begin{algorithm}[H]
	\caption{Training Functions of LVM-MSC}
	\label{alg:lvm-isc}
	\begin{algorithmic}[1]
		\REQUIRE Training dataset $\mathcal{D}$, batch size $B$
		\STATE \texttt{Function: Train Semantic Codec}
		\STATE \textbf{Input:} Mini-batch data $\{\textbf{P}_{k,j}\}_{j=n}^{n+B}$ from $\mathcal{D}$
		\FOR{$j = n$ to $n+B$}
		\STATE $\textbf{Z}_{k,j} \leftarrow S(\textbf{P}_{k,j}; \alpha_k)$.
		\STATE $\textbf{Q}_{k,j} \leftarrow S^{-1}(\textbf{Z}_{k,j}; \varphi_k)$.
		\STATE Compute $\mathcal{L}_{\text{MSE,1}}$ using Eq. (\ref{equ:8}) with $\textbf{P}_{k,j}$ and $\textbf{Q}_{k,j}$.
		\STATE Update $\alpha_k, \varphi_k$ using gradient descent with $\mathcal{L}_{\text{MSE,1}}$.
		\ENDFOR
		\RETURN $S(; \alpha_k)$ and $S^{-1} (; \varphi_k)$.
		\vspace{1em}
		
		\STATE \texttt{Function: Train Channel Codec}
		\STATE \textbf{Input:} Semantic features of images $\{\textbf{Z}_{k,j}\}_{j=n}^{n+B}$
		\FOR{$j = n$ to $n+B$}
		\STATE $\textbf{X}_{k,j} \leftarrow C(\textbf{Z}_{k,j}; \beta_k)$.
		\STATE Transmit $\textbf{X}_{k,j}$ over the channel.
		\STATE Receive $\textbf{Y}$, perform signal detection using Eq. (\ref{equ:5}) to get $\hat{\textbf{X}}_{k,j}$.
		\STATE $\hat{\textbf{Z}}_{k,j} \leftarrow C^{-1}(\hat{\textbf{X}}_{k,j}; \gamma_k)$.
		\STATE Compute $\mathcal{L}_{\text{MSE,2}}$ using Eq. (\ref{equ:9}) with ${\textbf{Z}}_{k,j}$ and $\hat{\textbf{Z}}_{k,j}$.
		\STATE Update $\beta_k, \gamma_k$ using gradient descent with $\mathcal{L}_{\text{MSE,2}}$.
		\ENDFOR
		\RETURN $C(; \beta_k)$ and $C^{-1} (; \gamma_k)$.
		\vspace{1em}
		
		\STATE \texttt{Function: Train Whole Network}
		\STATE \textbf{Input:} Mini-batch data $\{\textbf{P}_{k,j}\}_{j=n}^{n+B}$ from $\mathcal{D}$
		\FOR{$j = n$ to $n+B$}
		\STATE Repeat lines 3-7 and 13-19 to get $\textbf{Q}_{k,j}$.
		\STATE Compute $\mathcal{L}_{\text{MSE,3}}$ using Eq. (\ref{equ:10}).
		\STATE Update $\alpha_k, \beta_k, \gamma_k, \varphi_k$ using gradient descent with $\mathcal{L}_{\text{MSE,3}}$.
		\ENDFOR
		\RETURN $S(; \alpha_k)$, $S^{-1} (; \varphi_k)$, $C(; \beta_k)$, and $C^{-1} (; \gamma_k)$.
	\end{algorithmic}
\end{algorithm} 

\begin{algorithm}
	\caption{Constructing the LKB}
	\label{alg:sam}
	\begin{algorithmic}[1]
		\REQUIRE Source image data \(\textbf{P}\)
		\ENSURE The location of semantic objects \(loc\)
		\STATE Obtain the all-instance segmentation \(\textbf{M}\) of image \(\textbf{P}\), following Eqs. (\ref{equ:11})-(\ref{equ:14}).
		\STATE Extract the location information \(loc\) from the segmentation results based on the textual representation \(t\) of semantic objects, following Eqs. (\ref{equ:15}) and (\ref{equ:16}).
		\STATE{Return \(loc\).}
	\end{algorithmic}
\end{algorithm}

\subsection{Efficient semantic codec}
The MAE is a vision model that has undergone self-supervised learning on a large amount of image data. During the encoding phase, the input image is divided into several patches, many of which are randomly masked. The encoder only processes a small number of patches and attempts to reconstruct the image during the decoding phase \cite{he2022masked}. As a result, MAE achieves a very high compression rate when extracting semantic information from the image.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=7.7cm]{mae.png}
	\caption{\textcolor{black}{The workflow of ESC.}}
	\label{fig:mae}
\end{figure}

In the proposed LVM-MSC, we introduce an efficient semantic codec called ESC based on MAE. The ESC leverages the location information of semantic objects provided by the LKB to replace the original random masking operation in MAE with a targeted approach that preferentially retains the pixels of semantic objects. This ensures that a larger proportion of the pixels involved in semantic encoding are derived from semantic objects, achieving both efficient semantic encoding and accurate semantic transmission. The workflow is illustrated in Fig. \ref{fig:mae}. For a given image \(\textbf{P}\) and the semantic information \(loc\) of object location provided by LKB, the semantic encoding process produces semantic information \(\textbf{Z}\) and reconstructs the image \(\textbf{Q}\) through decoding. The detailed process is as follows.

\subsubsection{Adaptive mask}
First, we incorporate the location information \(loc\) provided of semantic objects by the LKB to prioritize masking the background pixels of \(\textbf{P}\). This eliminates redundant pixels unrelated to semantic objects, thereby improving the efficiency of semantic encoding. It is important to note that this does not imply completely discarding background pixels during the encoding process. Instead, background pixels are assigned a higher probability of being masked, while pixels belonging to semantic objects are less likely to be masked. The implementation details are as follows: suppose the image \(\textbf{P}\) is divided into multiple patches, with the indices of each patch represented by the set \(\mathcal{I} = \{1, 2, \ldots\}\). Therefore, our masking strategy can be expressed as Eq. (\ref{equ:17}), and \( P_i \) is the probability of the \( i \)-th image patch being masked

\begin{equation}
	\label{equ:17}
	P_i = 
	\begin{cases} 
		P_r, & i \in R \\ 
		1 - P_r, & i \in R^c 
	\end{cases}
\end{equation}
where \( P_r \) represents the probability of an image patch located in \( loc \) being masked. \(R\) is the set of image patches locatedin \( loc \) and \(R^c\) is the set of image patches located outside \(loc\). Since the goal of LVM-MSC is to transmit the semantics of objects in the image with higher accuracy, \( P_r \) should be set to a value less than 0.5. This ensures that more background patches are masked, thereby improving the quality of the semantic encoding.
\textcolor{black}{Therefore, each image patch follows a Bernoulli distribution \cite{weisstein2002bernoulli}, determining whether it is masked or not. During forward propagation, a single sampling operation for each image patch is sufficient to determine the masking status of all image patches.}

\begin{comment}
To implement this masking strategy, we define a random variable \( m_i \) based on these probabilities to indicate whether the \( i \)-th patch is masked, as shown in Eq. (\ref{equ:18}). According to the rule, \( m_i \) follows the distribution described in Eq. (\ref{equ:19}) \cite{weisstein2002bernoulli}. In training stage, it suffices to randomly sample \( m_i \) according to \( P_i \). 

\begin{equation}
	\label{equ:18}
m_i =
\begin{cases}
	1, & \text{patch } i \text{ is masked} \\
	0, & \text{patch } i \text{ is not masked}
\end{cases} 
\end{equation}

\begin{equation}
	\label{equ:19}
m_i \sim \text{Bernoulli}(P_i)
\end{equation}
\end{comment}

\subsubsection{Semantic encoding}
The semantic encoder-decoder framework in LVM-MSC consists of multiple layers of Multi-Head Self-Attention (MSA) layers and Feed-Forward (FF) layers \cite{vaswani2017attention}. Specifically, the semantic encoder, deployed at the base station, stacks 24 layers to effectively learn the hierarchical features of the image. In contrast, the semantic decoder, deployed at edge devices, stacks only 8 layers, making it significantly lighter and reducing the deployment burden on edge devices. The MSA layer excels at capturing the global dependencies among different positions in the image sequence, while the FF layer processes each position's representation independently, enabling the capture of finer-grained local features \cite{vaswani2017attention}. Assuming the set of unmasked patches used for semantic encoding and their corresponding index set are denoted as \( \textbf{P}_{keep} \) and \( I_{keep} \), respectively, the outputs of the MSA and FF layers can be expressed as follows
\begin{equation}
	\label{equ:18}
\textbf{M} _{msa, i}=\left\{\begin{array}{cc}\mathrm{MSA} \left({\mathrm{LN} }\left(\textbf{P} _{keep}\right)\right)+\textbf{P} _{keep}, & i=1 \\\mathrm{MSA} \left({\mathrm{LN} }\left(\textbf{M} _{ff, i-1}\right)\right)+\textbf{M} _{ff, i-1}, & 2 \leq i \leq 24	\end{array}\right.
\end{equation}
\begin{equation}
	\label{equ:19}
	\textbf{M} _{ff, i}=\mathrm{GeLU}\left(\textbf{W} _{b,f} \cdot \mathrm{LN}\left(\textbf{M} _{msa, {i}}\right)+\textbf{b} _{b, f}\right)+\textbf{M} _{msa, i}
\end{equation}
where \( \text{MSA}(\cdot) \) represents the MSA operator, and \( \text{LN}(\cdot) \) denotes the layer normalization operator. \( \textbf{W}_{b,f} \) and \( \textbf{b}_{b,f} \) are the weights and biases of the FF layer, respectively. \( \text{Gelu}(\cdot) \) represents the activation function. Finally, the output of the semantic encoder can be expressed as
\begin{equation}
	\label{equ:20}
	\textbf{Z}=\mathrm{LN}\left ( \textbf{M} _{ff, 24} \right ) .
\end{equation}

\subsubsection{Semantic decoding}
At the receiver side, the semantic decoder reconstructs the image \( \textbf{Q} \) from the channel-decoded semantic information \( \hat{\textbf{Z}} \). First, the semantic decoder generates zero-valued feature vectors for all masked patches based on the mask position indices \( I-I_{\text{keep}} \) and combines them with \( \hat{\textbf{Z}} \) to form a representation of the same shape as \( \textbf{P} \) \cite{he2022masked}.
\begin{equation}
	\label{equ:21}
	\hat{\textbf{Z} }^{\prime}=\mathrm{Concat}\left(\hat{\textbf{Z} },\mathrm{Copy}(\mathrm{\textbf{zero} },I-I_{keep})\right)
\end{equation}
where \( \textbf{zero} \) represents a zero vector, \( \mathrm{Copy}(\cdot) \) denotes the copy operation, and \( {\hat{\textbf{Z}}}^\prime \) is the combined semantic information of the image used for semantic decoding.

Finally, the semantic decoder then continues to decode \( {\hat{\textbf{Z}}}^\prime \) to generate \( \textbf{Q} \).
\begin{equation}
		\label{equ:22}
	\textbf{M} _{msa, i}^\prime=\left\{\begin{array}{cc}\mathrm{MSA} \left({\mathrm{LN} }\left({\hat{\textbf{Z}}}^\prime\right)\right)+{\hat{\textbf{Z}}}^\prime, & i=1 \\\mathrm{MSA} \left({\mathrm{LN} }\left(\textbf{M} _{ff, i-1}^\prime \right)\right)+\textbf{M} _{ff, i-1}, ^\prime& 2 \leq i \leq 8	\end{array}\right.
\end{equation}
\begin{equation}
		\label{equ:23}
	\textbf{M} _{ff, i}^\prime=\mathrm{GeLU}\left(\textbf{W} _{b,f} \cdot \mathrm{LN}\left(\textbf{M} _{msa, {i}}^\prime\right)+\textbf{b} _{b, f}\right)+\textbf{M} _{msa, i}^\prime
\end{equation}
\begin{equation}
	\label{equ:24}
	\textbf{Q}=\mathrm{LN}\left ( \textbf{M} _{ff, 8}^\prime \right ) .
\end{equation}

The workflow of ESC is illustrated in Algorithm \ref{alg:esc}.

\begin{algorithm}
	\caption{Efficient Semantic Codec}
	\label{alg:esc}
	\begin{algorithmic}[1]
		\REQUIRE Source image data \(\textbf{P}\), the location information \(loc\) of semantic objects
		\ENSURE The recovered image \(\textbf{Q}\)
		\STATE Perform adaptive masking on the image \(\textbf{P}\) based on \(loc\), as described in Eq. (\ref{equ:17}).
		\STATE Conduct semantic encoding on the unmasked pixels to obtain the semantic information \(\textbf{Z}\), following Eqs. (\ref{equ:18})-(\ref{equ:20}).
		\STATE Decode the received semantic information \({\hat{\textbf{Z}}}^\prime\) on the receiver side to reconstruct the image \(\textbf{Q}\), as outlined in Eqs. (\ref{equ:21})-(\ref{equ:24}).
		\STATE{Return \(\textbf{Q}\).}
	\end{algorithmic}
\end{algorithm}

\subsection{Multi-user semantic sharing transmission}
Numerous studies have demonstrated that the semantic information of different images can exhibit significant similarities \cite{zhang2023model}. For example, as shown in Fig. \ref{fig:mdma}, the grass background and the white fur pixel regions in two images share semantic similarity. The proposed image SemCom system leverages the precise perception and representation capabilities of LVMs to map the source images of different users into a semantic space. A semantic comparator is then employed in this space to identify, analyze, and extract shared and private semantic information from the semantic information of multiple users. By modeling user-specific characteristics, the system differentiates the semantic information of different users. During wireless transmission, signals containing shared semantic information are superimposed and reused, thereby reducing the amount of information ultimately transmitted.

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=17cm]{mdma.png}
	\caption{\textcolor{black}{Semantic shared transmission for multi-user communication systems.}}
	\label{fig:mdma}
\end{figure*}

Taking the SemCom system with two users as an example, the left part of Fig. \ref{fig:mdma} illustrates how the semantic comparator extracts shared and private information from the semantic information of multiple users, while the right part of Fig. \ref{fig:mdma} provides specific examples showing the different meanings of shared and private information. \textcolor{black}{Referring to Fig. \ref{fig:mdma}, we further explain how the LVM-MSC system utilizes the semantic comparator to extract shared and private semantic information through steps (1)-(2). Additionally, with steps (3)-(4), we detail how the system encodes, transmits, and decodes these information.}

\subsubsection{Semantic encoding}
As is shown in Fig. \ref{fig:mdma}, each user independently utilizes a semantic encoder \(S\left ( ;\alpha _k \right ) \) to extract semantic information \(\textbf{Z}_k \in \mathbb{R}^{L_s \times D_s}\) from the source image data \(\textbf{P}_k\), as described in Eq. (\ref{equ:1}). Each vector \(\textbf{Z}_{k,i} \ (i = 1, 2, \ldots, L_s)\) in \(\textbf{Z}_k\) can be interpreted as the semantic information corresponding to a specific image patch.

\subsubsection{Semantic comparation}
Certain regions of different images may exhibit high similarity in content, making the presence of similar semantics inevitable. This forms the basis for the existence of shared semantic information. Referring to the determination method proposed by Zhang \cite{zhang2023model}, we calculate the variance of corresponding \(\textbf{Z}_{k,i} \ (i=1, 2, \ldots, L_s)\) across different \(\textbf{Z}_k\), as shown in Eq. (\ref{equ:27}) \textcolor{black}{where \(\mathrm{Var}(\cdot)\) is the operation that calculates the variance of a vector,} and compute their difference. For scenarios involving more than two users, we calculate the variance differences for semantic information between all pairs of users, as given in Eq. (\ref{equ:28}). Finally, the average of these variance differences is taken as \(d_i\), as described in Eq. (\ref{equ:29}).
\begin{equation}
	\label{equ:27}
	\sigma_{k,i}^2=\mathrm{Var} (\textbf{Z} _{k,i}),~i=1,2~...L_s
\end{equation}
\begin{equation}
	\label{equ:28}
	\Delta\sigma_{{j,i}}^2=\sigma_{{j,i}}^2-\sigma_{{j+1,i}}^2,{~i=1,2~...L_s,~j=1,2~...K-1}
\end{equation}
\begin{equation}
	\label{equ:29}
	d_i=\frac{1}{K-1}\sum_{j=1}^{K-1}\Delta\sigma_{j,i}^2,~i=1,2~...L_s
\end{equation}

If the difference \(d_i\) is less than a preset threshold \(\epsilon\), we consider $\textbf{Z}_{k,i}$ to be sufficiently close in the semantic space. In this case, it is identified as shared semantic information, and the mean of $\textbf{Z}_{k,i}$ is taken as the shared semantic information. Otherwise, $\textbf{Z}_{k,i}$ is treated as private information for the \(k\)-th user. As illustrated in Fig. \ref{fig:mdma}, shared semantic information may represent common semantics across different sources, such as the white feather of the white cat and white dog or the shared grassy background in the figures. On the other hand, private semantic information reflects unique semantics of the sources, such as the differing facial structures of the cat and dog. In summary, the shared and private semantic information obtained by the semantic comparator are denoted as
\begin{equation}
	\label{equ:30}
	\{\textbf{Z}_{pub},\textbf{Z}_{pri}\}=S_c(\textbf{Z} )
\end{equation}
where \(\textbf{Z} = \left[ \textbf{Z}_1, \textbf{Z}_2, \ldots, \textbf{Z}_K \right] \in \mathbb{R}^{K \times L_s \times D_s}\) represents the semantic information of all users. \(S_c\) is the semantic comparator. \(\textbf{Z}_{pub} \in \mathbb{R}^{1 \times L_{pub} \times D_s}\) denotes the shared semantic information among the \(K\) users, and \(\textbf{Z}_{pri} = \left[ \textbf{Z}_{pri,1}, \textbf{Z}_{pri,2}, \ldots, \textbf{Z}_{pri,K} \right] \in \mathbb{R}^{K \times L_{pri} \times D_s}\) is the set of private information. It is worth noting that \(L_{pub}\) and \(L_{pri}\) sum to \(L_s\), as the comparison is performed along the sequence dimension of the semantic information.

\subsubsection{Channel encoding}
As shown in Fig. \ref{fig:main}, after semantic encoding and comparison are completed, private semantic information is still encoded using channel encoders corresponding to each user individually, while shared semantic information is encoded using an additional shared channel encoder within the system. 
\begin{equation}
	\textbf{X} _{pub}=C_{pub}(\textbf{Z} _{pub})
\end{equation}
\begin{equation}
	\textbf{X} _{pri,k}=C(\textbf{Z} _{pri,k};\beta _k)
\end{equation}
where \(C_{pub}\) represents the shared channel encoder, and \(C(;\beta _k)\) denotes the channel encoder for the \(k\)-th user. \(\textbf{X}_{pub} \in \mathbb{C}^{{1 \times L_{\mathrm{pub}}} \times D_c}\), and \(\textbf{X}_{pri} = [\textbf{X}_{pri,1}, \textbf{X}_{pri,2}, \dots, \textbf{X}_{pri,K}] \in \mathbb{C}^{{K \times L_{\mathrm{pri}}} \times D_c}\) are the transmitted complex signals.

\subsubsection{Channel decoding}
Subsequently, \(\textbf{X}_{pub}\) is transmitted to all receiving users through a shared channel, while \(\textbf{X}_{pri,k}\) is transmitted to the \(k\)-th user through a dedicated channel. Once the signals reach the receiver, each user utilizes a channel decoder to decode the combination of shared and private semantic information.
\begin{equation}
	\label{equ:33}
	\hat{\textbf{Z} }_k=C^{-1}\left(\mathrm{Cat}(\widehat{\textbf{Z} }_{pub},\widehat{\textbf{Z} }_{pri,k});\gamma_k\right)
\end{equation}
where \(C^{-1}(;\varphi_k)\) is the channel decoder for the \(k\)-th user.  $\mathrm{Cat}(\cdot)$ is matrix concatenation operation.
The final semantic information \(\hat{\textbf{Z}}_k\) is decoded by the semantic decoder \(S^{-1}(;\varphi_k)\) to reconstruct the image \(\textbf{Q}_k\), as is shown in Eq. (\ref{equ:7}).

The workflow of semantic shared transmission is illustrated in Algorithm \ref{alg:mdma}.

\begin{algorithm}
	\caption{Multi-user Semantic Sharing Transmission}
	\label{alg:mdma}
	\begin{algorithmic}[1]
		\REQUIRE Source image data \(\textbf{P}_i \ (i=1, 2, \ldots, K)\)
		\ENSURE The recovered image \(\textbf{Q}_i \ (i=1, 2, \ldots, K)\)
		\STATE The \(k\)-th user applies the semantic encoder \(S\left ( ;\alpha _k \right ) \) to perform semantic encoding on \(\textbf{P}_k\), obtaining the semantic information \(\textbf{Z}_k\), as described in Eq. (\ref{equ:1}). 
		\STATE The semantic comparator calculate the shared and private semantic information of all users based on Eqs. (\ref{equ:27})-(\ref{equ:30}). 
		\STATE The system transmits the shared and private semantic information through the shared and private channels, respectively.  
		\STATE The receiving user combines the shared and private semantic information and performs channel and semantic decoding to obtain \(\textbf{Q}_i\) according to Eq. (\ref{equ:33}) and Eq. (\ref{equ:7}).
		\STATE{Return  \(\textbf{Q}_i\).}
	\end{algorithmic}
\end{algorithm}

\section{Simulation Results}

\subsection{Simulation settings}
The image dataset used in this study is PASCAL VOC2012 \cite{everingham2010pascal}, which contains over 12,000 images spanning 20 categories of everyday objects such as humans, vehicles, and animals. During the experiments, the dataset is evenly divided to distinguish multiple users.

The detailed experimental configuration is as follows: The LKB consists of 68 M parameters \cite{zhao2023fast}. The efficient semantic encoder, based on MAE, includes 24 layers of Vision Transformer with a feature dimension of 1024 \cite{he2022masked}. The channel encoder is composed of a linear layer with an input feature dimension of 1024 and an output feature dimension of 128. To maintain consistency, the channel decoder adopts a structure inverse to the channel encoder. The semantic decoder, based on the MAE decoder, consists of 8 layers of Vision Transformer. During adaptive masking of ESC, the default masking probability \(P_r\) for semantic object patches is set to 0.3. The threshold \(\epsilon\) for the semantic comparator is set to 0.1 by default. The channel used in the experiments is trained for 10 epochs with a learning rate of \(2 \times 10^{-4}\).    

The training and testing environment includes Windows 11, Python 3.8, PyTorch 2.0.1, and CUDA 11.8. The computational resources are provided by a 12th Gen Intel(R) Core(TM) i7-12700H 2.30 GHz CPU and an NVIDIA GeForce RTX 4060 Laptop GPU.  

\begin{comment}
	内容...

\subsection{Evaluation metrics}
We evaluate the LVM-MSC system from three perspectives.

\subsubsection{LKB Evaluation}
The LKB is compared against the SKB used in LAM-SC \cite{jiang2024large} and several other models of the same type that have potential as knowledge bases. The comparison covers various aspects, including the number of parameters and inference time, to assess the cost of LKB.

\subsubsection{ESC Evaluation}
We use the unmodified MAE as a baseline semantic encoder-decoder for comparison. The image reconstruction quality within the regions identified by LKB is compared between the images reconstructed by MAE and those reconstructed by ESC. Metrics such as PSNR and Structural Similarity Index Measure (SSIM) \cite{hore2010image} are used to evaluate the quality of key semantic object reconstruction in LVM-MSC. Additionally, we investigated the impact of different masking probabilities for semantic object patches \(P_r\) on the aforementioned metrics.

\subsubsection{Multi-User System Evaluation}
Since shared semantic information is transmitted uniformly to all receiving users, the semantic shared transmission for multi-user communication systems inevitably reduces the bandwidth required compared to single-user SemCom systems. To quantitatively analyze this, we compared the proportion of data transmission reduction relative to the total transmitted data under different thresholds of shared semantic information \(\epsilon\), as the number of users in the system gradually increased from 2 to 10. %The reduction in transmitted data is calculated as the shared semantic information data volume multiplied by the number of users minus one.}
\end{comment}
\subsection{Performance evaluation for LKB}
The number of parameters is a crucial factor determining whether a model can be deployed on devices with varying computational capabilities. Smaller parameter sizes result in lower costs for constructing corresponding knowledge bases. Moreover, SemCom has stringent latency requirements; hence, shorter inference time for the knowledge base can significantly enhance the SemCom experience. We compare LKB with SKB \cite{jiang2024large} and similar models, SAM2 \cite{ravi2024sam2} and ViTDet \cite{Li2022ExploringPV}. The comparison results are shown in Fig. \ref{fig:fkbexp}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=8.5cm]{fkbexp.png}
	\caption{Comparison of LKB with other KBs and models.}
	\label{fig:fkbexp}
\end{figure}

As shown in Fig. \ref{fig:fkbexp}, LKB significantly outperforms other models in terms of parameter size and inference time. This advantage is attributed to its use of a compact model architecture while maintaining performance comparable to other models. Consequently, LKB demonstrates the lowest construction cost and is best aligned with the latency requirements of SemCom.

\subsection{\textcolor{black}{Performance evaluation for ESC}}
PSNR and Structural Similarity Index Measure (SSIM)\cite{hore2010image} are commonly used metrics for evaluating image quality. PSNR measures the quality of an image by calculating the MSE between the original and compressed or processed images, with higher values indicating that the image quality is closer to the original. SSIM, on the other hand, compares the similarity between two images in terms of brightness, contrast, and structure. A value closer to 1 indicates greater similarity between the images \cite{hore2010image}.

First, we compare the PSNR and SSIM of reconstructed images in the regions specified by LKB under AWGN and Rayleigh fading channel, where ESC and standard MAE are used as the semantic encoder-decoder. These metrics effectively demonstrate the system's reconstruction quality for critical semantic objects. The experimental results are shown in Fig. \ref{fig:whole}.

\begin{figure}[htbp]
	\centering
	\subfigure[]{
		\includegraphics[width=8.5cm]{escexp1psnr.png}
		\label{fig:subfig1}
	}
	\quad
	\subfigure[]{
		\includegraphics[width=8.5cm]{escexp1ssim.png}
		\label{fig:subfig2}
	}
	\caption{\textcolor{black}{Simulation experiment about ESC and MAE. (w/ means with and the same below)}}
	\label{fig:whole}
\end{figure}

As shown in Fig. \ref{fig:subfig1} and Fig. \ref{fig:subfig2}, the PSNR and SSIM of reconstructed images in the regions specified by LKB are significantly higher when ESC is used as the semantic encoder-decoder than MAE. This demonstrates that ESC not only retains MAE's efficient image encoding capabilities but also achieves higher-quality transmission of semantic information for image objects. The efficiency of semantic encoding is attributed to the selection of only a small number of pixels during the encoding process, while the improvement in reconstruction quality is due to the majority of encoded pixels being derived from semantic objects, reducing interference from the background.

To further investigate the impact of the sampling probability of semantic object patches \(P_r\) on the quality of the reconstructed image, we conduct experiments with different values under the AWGN channel and Rayleigh fading channel, as shown in Fig. \ref{fig:whole2}.

As seen in Fig. \ref{fig:subfig3} and Fig. \ref{fig:subfig4}, the best reconstruction quality is achieved when the sampling probability of semantic object patches is set to 30\%. The reconstruction quality for probabilities greater than 30\% (40\%) is lower than at 30\%. Compared to the random masking used by MAE, the adaptive masking significantly improves the reconstruction quality of the regions containing the semantic objects in the image. However, the reconstruction quality is still affected by the masking probability of background patches, which cannot be completely discarded.

\begin{figure}[htbp]
	\centering
	\subfigure[]{
		\includegraphics[width=8.5cm]{escexp2psnr.png}
		\label{fig:subfig3}
	}
	\quad
	\subfigure[]{
		\includegraphics[width=8.5cm]{escexp2ssim.png}
		\label{fig:subfig4}
	}
	\caption{\textcolor{black}{Simulation experiment about ESC setting.}}
	\label{fig:whole2}
\end{figure}

\subsection{Performance evaluation for MSS transmission}
%\textcolor{black}{A key advantage of multi-user communication systems over single-user systems is their ability to effectively reduce transmission bandwidth. 
Shared semantic information is widely present across different images, making bandwidth reduction in multi-user communication systems an inherent benefit. However, as the number of users increases, previously identified shared semantic information may no longer qualify due to significant deviations in data from newly added users, potentially leading to a slight increase in bandwidth requirements. In the simulation experiments, we set different thresholds for shared semantic information \(\epsilon\) and gradually increase the number of users from 2 to 10. We specifically analyze the proportion of reduced transmission data relative to the total transmitted data, and the results are presented in Fig. \ref{fig:mdmaexp}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=8.5cm]{mdmaexp.png}
	\caption{The proportion of reduced transmission data in relation to the number of system users.}
	\label{fig:mdmaexp}
\end{figure}

\textcolor{black}{As shown in Fig. \ref{fig:mdmaexp}, under varying thresholds for shared semantic information \(\epsilon\), the percentage of reduced transmission data increases as the number of users increases, but the rate of increase gradually slows and eventually slightly declines. Additionally, higher thresholds for shared semantic information result in consistently greater shared semantic information and a more gradual downward trend. Initially, the increase in the number of users broadens the possibilities for different parts of the images to contribute to shared semantic information, leading to an upward trend in the curve. However, as the number of users becomes larger, the proportion of private semantic information in the images gradually increases, causing the percentage of reduced transmission data to slightly decline.}

\subsection{SemCom performance evaluation}
To evaluate the performance of the LVM-MSC system in image classification tasks, we compare it with two SC systems based on CNN (JSCC) \cite{bourtsoulatze2019deep} and ViT (WITT) \cite{yang2023witt} under both AWGN and Rayleigh fading channels. The image classification dataset used is CIFAR-10, and the performance metric for evaluation is classification accuracy. The experimental results are shown in Fig. \ref{fig:scexp}.

\begin{figure}[htbp]
	\centering
	\subfigure[AWGN]{
		\includegraphics[width=8.5cm]{scexpawgn.png}
		\label{fig:scawgn}
	}
	\quad
	\subfigure[Rayleigh Fading]{
		\includegraphics[width=8.5cm]{scexpfade.png}
		\label{fig:scfade}
	}
	\caption{\textcolor{black}{Comparison of performance of LVM-MSC and other SemCom systems in classification tasks.}}
	\label{fig:scexp}
\end{figure}

As seen in Fig. \ref{fig:scexp}, under both channel environments, LVM-MSC and WITT outperform JSCC in terms of classification accuracy. This is because both LVM-MSC and WITT utilize semantic encoders based on Transformer architecture, which enables them to extract image features more effectively than the CNN-based JSCC. Moreover, at higher SNR levels, LVM-MSC demonstrates superior performance to WITT. This can be attributed to the collaborative function of the semantic encoder and LKB in LVM-MSC, which allows the system to focus more on the main objects in the image while minimizing the impact of background noise and other irrelevant information.

\section{Conclusion}
This paper proposed an innovative LVM-MSC system, aiming at addressing the challenges of inefficient knowledge base construction, insufficient semantic encoding, and lack of MSS in existing image SemCom systems. The core of the system lies in the development of a LKB, which utilizes the Fast SAM model to rapidly and accurately perceive the locations of key semantic objects in images. Additionally, an ESC based on MAE was proposed, which selectively encodes the pixels of key semantic objects to adaptively enhance compression rates, while achieving joint semantic and channel coding. Furthermore, we constructed a multi-user semantic space and introducted a multi-user SemCom scheme that facilitates semantic sharing. Through simulation experiments, we validated the feasibility and effectiveness of the LVM-MSC system, demonstrating its advantages in reducing bandwidth requirements and enhancing image reconstruction quality in multi-user environments. %Overall, the LVM-MSC system offers a novel solution for efficient and intelligent SemCom in future 6G networks, showcasing its potential and value in handling large-scale, multi-user data transmission.

\bibliographystyle{IEEEtran}
\bibliography{bare_jrnl_bobo}

\begin{comment}
\section*{Biographies}
\textbf{Feibo Jiang} received his B.S. and M.S. degrees in School of Physics and Electronics from Hunan Normal University, China, in 2004 and 2007, respectively. He received his Ph.D. degree in School of Geosciences and Info-physics from the Central South University, China, in 2014. He is currently an associate professor at the Hunan Provincial Key Laboratory of Intelligent Computing and Language Information Processing, Hunan Normal University, China. His research interests include artificial intelligence, fuzzy computation, Internet of Things, and mobile edge computing.

\textbf{Yubo Peng} received the B.S. degree from Hunan Normal University, Changsha, China, in 2019, where he is currently pursuing the master’s degree with the College of Information Science and Engineering. His main research interests include federated learning and semantic communication.

\textbf{Li Dong} received the B.S. and M.S. degrees in School of Physics and Electronics from Hunan Normal University, China, in 2004 and 2007, respectively. She received her Ph.D. degree in School of Geosciences and Info-physics from the Central South University, China, in 2018. She is currently an associate professor at Hunan University of Technology and Business, China. Her research interests include machine learning, Internet of Things, and mobile edge computing.

\textbf{Kezhi Wang} received the Ph.D. degree in Engineering from the University of Warwick, U.K. He was with the University of Essex and Northumbria University, U.K. Currently, he is a Senior Lecturer with the Department of Computer Science, Brunel University London, U.K. His research interests include wireless communications, mobile edge computing, and machine learning.

\textbf{Kun Yang} received his PhD from the Department of Electronic Electrical Engineering of University College London (UCL), UK. He is currently a Chair Professor in the School of Computer Science Electronic Engineering, University of Essex, leading the Network Convergence Laboratory (NCL), UK. He is also an affiliated professor at UESTC, China. Before joining in the University of Essex at 2003, he worked at UCL on several European Union (EU) research projects for several years. His main research interests include wireless networks and communications, IoT networking, data and energy integrated networks and mobile computing. He manages research projects funded by various sources such as UK EPSRC, EU FP7/H2020 and industries. He has published 400+ papers and filed 30 patents. He serves on the editorial boards of both IEEE (e.g., IEEE TNSE, IEEE ComMag, IEEE WCL) and non-IEEE journals (e.g., Deputy EiC of IET Smart Cities). He was an IEEE ComSoc Distinguished Lecturer (2020-2021). He is a Member of Academia Europaea (MAE), a Fellow of IEEE, a Fellow of IET and a Distinguished Member of ACM. 

\textbf{Cunhua Pan} received the B.S. and Ph.D. degrees from the School of Information Science and Engineering, Southeast University, Nanjing, China, in 2010 and 2015, respectively. He was a Research Associate with the University of Kent, Canterbury, U.K., from 2015 to 2016. He held a postdoctoral position with the Queen Mary University of London, London, U.K., from 2016 and 2019, and was a Lecturer from 2019 to 2021. He has been a Full Professor with Southeast University since 2021. He has published over 120 IEEE journal papers. His research interests mainly include reconfigurable intelligent surfaces (RIS), intelligent reflection surface, ultrareliable low-latency communication, machine learning, UAV, Internet of Things, and mobile-edge computing.

\textbf{Xiaohu You} received the M.S. and Ph.D. degrees in electrical engineering from Southeast University, Nanjing, China, in 1985 and 1988, respectively. Since 1990, he has been with the National Mobile Communications Research Laboratory, Southeast University, where he is currently the Director and a Professor. From 1999 to 2002, he was a Principal Expert of the C3G Project, responsible for organizing China 3G Mobile Communications Research and Development Activities. From 2001 to 2006, he was a Principal Expert of the China National 863 Beyond 3G FuTURE Project. Since 2013, he has been a Principal Investigator of the China National 863 5G Project. He has contributed over 200 IEEE journal articles and two books in the areas of adaptive signal processing and neural networks, and their applications to communication systems. His research interests include mobile communication systems, and signal processing and its applications.
Dr. You was selected as an IEEE Fellow for his contributions to the development of mobile communications in China in 2011. He was a recipient of the National 1st Class Invention Prize in 2011. \end{comment}
\begin{comment}
\appendix[Prompt using for semantic disambiguation]
\textcolor{black}{
	You're an expert in linguistics; I'm going to give you a bit of background information, and a text; You need to modify the text with background information to eliminate ambiguity or errors in the text.
	}

\textcolor{black}{
	Important: Your changes to the text must be based on the background information, and the revised text must be consistent with the original meaning of the text, focusing on the ambiguity caused by the polysemy in the text.
}

\textcolor{black}{
	An example is provided in \(\langle example \rangle \) and \( \langle /example \rangle\):
	\(\langle example \rangle \)\\
	\(\langle background \quad information \rangle \): A woman who had given birth was complaining.\\
	\(\langle original \quad text \rangle \): The woman say: I can't bear children, they seldom listen to me.\\
	\(\langle revised \quad text \rangle \): The woman say: I can't stand the presence of children, they seldom listen to me.\\
	\( \langle /example \rangle\)
}

\textcolor{black}{
	Here is the background information and the text you need to change:\\
	\(\langle background \quad information \rangle \): Your background information.\\
	\(\langle original \quad text \rangle \): Your text need changing.\\
}

\appendix[Prompt using for joining coding]
\textcolor{black}{
You are a linguist and I am going to give you a piece of text and a percentage range that you need to compress the number of words in the text while keeping the semantics of the text unchanged, and the ratio of the compressed text length to the length of the original text must remain within the percentage range that I have provided. 
Here is the text you need to compress and the percentage range:\\
\(\langle text \rangle \): Your text need compressed.\\
\(\langle range \rangle \): Your compression range.\\
}
\end{comment}






\newpage

\end{document}





