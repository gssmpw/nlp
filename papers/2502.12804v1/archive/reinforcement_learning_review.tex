\subsection{Reinforcement learning}
\label{sec:background_rl}

RL is a framework for learning to optimize sequential decision making under uncertainty. It emerged from Bellman's foundational work on optimal control theory in the 1950s, specifically dynamic programming and Markov Decision Processes (MDP) \cite{bellman_theory_1954,bellman_markovian_1957}. MDPs model decision making processes as an agent, that takes actions, and the environment, with which it interacts. The environment transitions to a new state in response to an action, with a feedback signal (reward) provided to the agent. The method by which the agent decides on actions, in response to observation of the environment state, is termed the policy. RL algorithms describe how to iteratively improve the policy to maximize reward in the long term or within a fixed number of decision steps (an episode). RL is distinct from other machine learning paradigms, supervised and unsupervised learning, by removing the train-test dichotomy and instead learning from an interactive environment that produces data (observations and rewards). The policy is improved on the basis of this data, which in turn affects the data collection process, and presents a non-stationary optimization objective. A key distinction in RL algorithms is whether they learn from data collected using the current policy being optimized (on-policy learning) or from data collected using any policy, including previous or exploratory policies (off-policy learning) - the latter offering more data efficiency but potentially greater instability during training.

While dynamic programming is restricted to problems with completely known dynamics, reinforcement learning allows the foundational ideas of state-value estimation and policy iteration to be applied to problems with incomplete knowledge of the state transition dynamics, as in the case of dynamic networks with unknown future traffic. This broad applicability of reinforcement earning techniques was enabled by later developments in the 80s and 90s of Q-learning \cite{watkins_learning_1989}, temporal difference learning \cite{sutton_learning_1988}, and actor-critic algorithms. The modern era of reinforcement learning was brought in by ground-breaking works from researchers at what is now Google Deepmind in the domains of Atari videogames \cite{mnih_human-level_2015} and the boardgame Go \cite{silver_mastering_2016}. These works demonstrated the effectiveness of RL algorithms, specifically Q-learning, when combined with deep neural networks (DNN) as function approximators.

We do not provide details of reinforcement learning theory here and instead refer the reader to Sutton and Barto's authoritative textbook \cite{sutton_reinforcement_2018}. However, to aid the discussion of RL applied to DRA problems, we outline the taxonomy of the RL algorithms as follows:

\textbf{Model-free} RL algorithms learn optimal behavior through direct interaction with the environment, without explicitly modeling its dynamics. These algorithms can be further categorized into:
\begin{itemize}
\item \textbf{Action-value methods}, such as Q-learning and SARSA, which learn to estimate the value of taking specific actions in different states. These methods were the first to be developed in RL.
\item \textbf{Policy gradient methods}, which directly optimize the policy parameters to maximize expected rewards. These methods can be applied to continuous action spaces and can naturally handle stochastic policies. Policy gradient methods are enhanced by using an Actor-Critic architecture, including algorithms such as A2C, PPO, and off-policy variants like DDPG and SAC, which reduce variance in the policy gradient by using a learned value function (critic) to estimate the value of each state, rather than using the full Monte Carlo returns.

\end{itemize}
\textbf{Model-based} RL algorithms use an explicit model of the environment's dynamics alongside the policy. The model may be an available simulator, as in AlphaGo \cite{silver_mastering_2016} or can be learned, as in MuZero \cite{silver_general_2018}. The model is used to simulate experience for a search process (e.g. Monte Carlo Tree Search) over possible trajectories, enabling multi-step planning to maximize reward. This leads to considerably higher performance in complex tasks, at the cost of much higher computational complexity \cite{brown_superhuman_2019}.
In the context of optical networks, only model-free algorithms have been applied to DRA problems thus far in the research literature.


% \begin{equation}
%     \label{eq:bellman_optimality}
%     V^*(s) = \max_a \left( R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right)
% \end{equation}
% Equation \ref{eq:bellman_optimality} is the Bellman optimality equation, a fundamental principle in RL that defines the optimal value function. It states that the optimal value of a state is equal to the expected return for the best action from that state. This equation forms the basis for many RL algorithms, including value iteration and Q-learning, and provides a mathematical framework for understanding how agents can make optimal decisions in sequential decision-making problems.

% \begin{equation}
%     \label{eq:bellman_optimality_q}
%     Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q^*(s',a')
% \end{equation}
% Equation \ref{eq:bellman_optimality_q}, the Q-function version of the Bellman optimality equation, defines the optimal action-value function in reinforcement learning. It represents the expected return of taking action a in state s and then following the optimal policy thereafter. This equation is crucial in Q-learning and other value-based methods, as it allows for optimal action selection without requiring a model of the environment. The Q-function directly incorporates the action into the value estimation, making it particularly useful for control problems.