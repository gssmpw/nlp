% Background contains technical details necessary to understand the work

% First subsection covers resource allocation and routing umbrella term (mention how this differentiates between routing on a network with scalar capcity vs vector, and also that the definition excludes multicast/NFV/VONE. We focus on unicast with link resources (no node resource considerations)), then RWA, RSA, RMSA.

\section{Background}
\label{sec:background}


\subsection{DRA problems in optical networks}

%\subsubsection*{Network Capacity and Low-Margin Operation}
%The optical data communication infrastructure that underpins metro, inter-data center, national and continental scale networks presents a range of optimization problems. The objective of these problems is to maximize the network throughput (the sum of realized point-to-point data transmission connections) within the constraints of available resources, most crucially the available bandwidth on links.

%The complexity of these systems has grown as networks have become increasingly flexible in the parameters of each transmitted channel; including launch power, symbol rate, modulation format and channel spacing. This flexibility is enabled by now-ubiquitous coherent transponders with advanced DSP, and ROADMs that support varying spectrum granularity. The highly configurable nature of each channel opens opportunities to increase network utilization through low-margin operation \cite{auge_can_2013}. Unallocated margins and system margins \cite{pointurier_design_2017} can be reduced with this technology by matching the requested and available channel capacity and reach, and mitigating non-linear effects through launch power allocation, respectively. Moving from fixed-grid (50GHz or greater channel spacing) to flex-grid (minimum 6.25GHz channel) spacing allows more precise matching of requested data transmission capacity to bandwidth. Network capacity is further enhanced by judicious allocation of routes and bandwidth to network traffic, which is the primary task of DRA problems.

\subsubsection*{Motivation for dynamic operation and RL}
As discussed by Aug\'e \cite{auge_can_2013} and Pointurier \cite{pointurier_design_2017}, optical networks must operate with a margin in the resources serving the required demand, which could be reduced to increase the network throughput or reduce costs. Dynamic operation allows the allocated resources to vary temporally in response to shifting demand, reducing margins. The time constraints of dynamic operation may preclude exact solution methods, but a trained RL agent can compute an allocation in sub-second time \cite{di_cicco_deep_2022}.

RL is appropriate for these problems because they possess three characteristics that merit the application of AI \cite{hassabis_nobel_2024}: 1) a large combinatorial search space, 2) a clear objective function for optimization, and 3) plentiful training data and/or accurate and efficient simulators for data generation. 
%\cite{nevin_techniques_2022,di_cicco_deep_2022}.

\subsubsection*{Traffic models}
Network traffic comprises a set of requests to connect source and destination nodes with fixed data rates. Traffic can be considered as either static, dynamic, or incremental. 

Static traffic assumes knowledge of all traffic requests that the network must accommodate. For dynamic traffic, connection requests are served on-demand without knowledge of future requests, and active connections expire randomly. The request arrival and expiry times are sampled from probability density functions (PDF), often assumed to be inverse exponential \cite{chen_deeprmsa_2019}. Incremental traffic lies between static and dynamic in stochasticity: requests are not known in advance but do not expire once allocated. Dynamic traffic is considered a paradigm for future optical networks, that have the necessary systems in place to enable real-time response to changing network conditions \cite{lord_flexible_2022}.

%In current production networks, traffic demand is typically forecast for a given period e.g. one year, and resources allocated and configured in advance. Additional unforecast demand may then be added incrementally. Dynamic traffic is therefore a paradigm for future optical networks, that have all the necessary software and hardware systems in place to enable real-time response to changing network conditions  \cite{lord_flexible_2022}.
%In this paper, we consider only dynamic traffic, as this paradigm offers the , though we note that previous research into wavelength-routed networks found dynamic operation only increased total capacity in networks with wavelength conversion capability \cite{zapata-beghelli_dynamic_2008}.

%The traffic model spatial characteristics are defined by the traffic matrix. %In dynamic or incremental settings, the traffic matrix elements are normalized probabilities of a connection between the nodes corresponding to the column and row indices \cite{nevin_techniques_2022}. %In static settings, a traffic matrix element may represent the total data rate between the node pair \cite{jaumard_decomposition_2023}. %It is common practice to consider uniform traffic matrices for DRA problems.

\subsubsection*{Problem variants}
The classic optimization problem in an optical network is Routing and Wavelength Assignment (RWA) for fixed-grid networks or Routing and Spectrum Assignment (RSA) for flex-grid networks, where spectrum is divided into frequency slot units (FSU) \cite{mukherjee_springer_2020}. Further degrees of freedom in the optimization are added by considering the selection of \underline{M}odulation formats (R\underline{M}SA), and the fiber \underline{C}ore or spectral transmission \underline{B}and utilized by each channel in the case of multi-core or multi-band networks (R\underline{C}MSA/R\underline{B}MSA). Launch \underline{P}ower has also been considered as a parameter in the optimization objective for dynamic networks (R\underline{P}MSA) \cite{ives_routing_2015} \cite{arpanaei_launch_2023}.% and is a relatively under-investigated problem variant.  

While most DRA problems in optical networks from the literature are concerned with point-to-point connections, some consider virtual networking tasks such as virtual optical network embedding (VONE) \cite{gong_virtual_2014,doherty_deep_2023} or virtual network function placement (VNF) \cite{zhou_applications_2022}. We choose to focus on RWA/RSA/RMSA in this paper because they are the most widely studied DRA problems in the context of optical networks and they form a core sub-task of variants such as VNF or VONE.

% Overview paper of problem types (also inlcudes details on ML/RL but very limited and with poor understanding): \cite{zhang_overview_2020}


\subsubsection*{Constraints}
Three fundamental constraints govern resource allocation in optical networks:
%transparent\footnotemark optical networks:

%\footnotetext{This assumes lightpaths are routed "transparently" at nodes without optoelectronic conversion for the purposes of signal regeneration or wavelength conversion.}

\begin{enumerate}[itemsep=0pt]
    \item Spectrum Continuity: A lightpath must use identical FSU along its entire route from source to destination, without wavelength conversion.
    \item Spectrum Contiguity: FSU allocated to a connection must be adjacent, as optical signals occupy a continuous range of frequencies determined by the required data rate, and therefore baud rate and modulation format.
    \item No Reconfiguration: Active lightpaths cannot be reallocated to different spectrum slots or routes, meaning allocation decisions are permanent while connections remain active.
\end{enumerate}

The 'No Reconfiguration' constraint is not a physical limitation but an operational assumption that active services cannot be disrupted. Reconfiguration may sometimes be desirable, especially in flex-grid networks that may suffer from spectral fragmentation \cite{gerstel_elastic_2012}, and has been used in a production network by Meta Platforms Inc. to free up spectral resources \cite{balasubramanian_targeted_2023}.



\subsubsection*{Solution methods}
%The possible methods to solve the DRA problems depend on whether the traffic is static or dynamic. Static traffic allows analytical approaches for exact combinatorial optimization, most notably Integer Linear Programming (ILP). ILP formulations have been suggested for DRA problems \cite{walkowiak_ilp_2016}, but the application of ILP to realistic problem sizes of networks with capacity for thousands of connection requests remains computationally infeasible \cite{jaumard_decomposition_2023}.% The RWA problem for static traffic has been shown to be NP-hard \cite{chlamtac_lightpath_1992}. Consequently, the computational complexity scales super-polynomially with the topology size and traffic load for RWA and related problems.


Allocation of static traffic is a NP-hard combinatorial optimization problems \cite{chlamtac_lightpath_1992}, for which the computational complexity of finding a solution scales super-polynomially with the space of possible allocations. Exact solution methods such as Integer Linear Programming (ILP) have been formulated for static traffic \cite{walkowiak_ilp_2016,jaumard_decomposition_2023} but show limited scalability\footnotemark and are infeasible for dynamic traffic without a priori knowledge of all requests. \footnotetext{\cite{jaumard_decomposition_2023} Jaumard et al. scale their ILP formulation to 690 requests on the USNET topology (24 nodes and 86 links) with 380 FSU per link, without considering distance-adaptive modulation formats.}

For dynamic traffic, any allocation decision must be taken within the constraint of the time interval between request arrivals. No standard limit has been defined for this constraint in the literature but it could be on the order of seconds, with a lower limit set by the switching time of reconfigurable optical add-drop multiplexers (currently 1ms to 100ms, depending on the switching technology \cite{goto_lcos-based_2024}).

Many simple heuristic algorithms \cite{vincent_scalable_2019,abkenar_best_2016,wright_minimum-_2015,tang_heuristic_2022,savory_congestion_2014} have been proposed for these problems, with the goal to minimize resources, lightpath distances and required bandwidth. Heuristics have the advantages of fast execution time and deterministic and interpretable allocation decisions.

Machine learning approaches to DRA problems have included nature-inspired techniques like particle swarm optimization (PSO) \cite{hassan_chaotic_2009} and genetic algorithms (GA) \cite{barpanda_genetic_2011}. RL is advantageous over these approaches as it can compute an allocation faster \cite{di_cicco_deep_2022} after an extensive training period, as explored in the next section.

\subsection{Reinforcement learning}
\label{sec:background_rl}

RL is a framework for learning to optimize sequential decision making under uncertainty. It emerged from Bellman's foundational work on optimal control theory in the 1950s, specifically dynamic programming and Markov Decision Processes (MDP) \cite{bellman_theory_1954,bellman_markovian_1957}. MDPs model decision-making processes as an agent that takes actions in an environment to maximize a reward signal. The method of action selection is a mapping from states to actions termed the policy, which can be expressed in tabular form or approximated by a neural network (NN). The use of NN for function approximation is sometimes distinguished as "Deep" RL. Tabular RL has been applied to optical networks \cite{terki_routing_2023}, but function approximation with NN is widely used \cite{chen_deeprmsa_2019,shimoda_mask_2021,tang_heuristic_2022,xu_deep_2022,cheng_ptrnet-rsa_2024} when the set of state-action pairs becomes infeasibly large to be tabulated \cite{sutton_reinforcement_2018}.

The reader is referred to Sutton and Barto's authoritative textbook \cite{sutton_reinforcement_2018} for details. However, to aid the discussion of RL applied to DRA problems, several terms are defined here.

RL algorithms can be classified as model-based or model-free. Model-based RL uses a model of the environment to plan future actions, but so far no works have applied this paradigm to DRA problems in optical networks. Model-free RL algorithms learn through direct interaction with the environment, without planning. Model-free algorithms can be further categorized into:
\begin{itemize}
\item \textbf{Action-value methods}, such as Q-learning, which learn to estimate the value of taking specific actions in different environment states. These methods were the first to be developed in RL \cite{watkins_learning_1989,sutton_learning_1988} and have been used for route selection in optical networks \cite{bryant_q-learning_2022}.
\item \textbf{Policy gradient methods}, which directly optimize the policy parameters to maximize expected rewards. This is subtly different from action-value methods, which optimize an action-value function. Policy gradient methods are enhanced by using an Actor-Critic architecture, as in algorithms such as A2C \cite{mnih_asynchronous_2016} and PPO \cite{schulman_proximal_2017}, which reduce variance in the policy gradient by using a learned value function (critic) to estimate the value of each state. Policy gradient methods have been used in many works on DRA in optical networks, such as A2C for DeepRMSA \cite{chen_deeprmsa_2019}. 
\end{itemize}

%Policy gradient methods are superior to action-value methods when the environment has stochastic dynamics (such as in a dynamic traffic scenario) due to their ability to learn stochastic policies \cite{sutton_reinforcement_2018}. It has been demonstrated that in many sequential decision-making problems, the policy is a simpler function to learn than an action-value function \cite{simsek_why_2016}. We therefore propose that policy gradient methods are better suited than action-value methods to DRA problems in optical networks.}

%\textbf{Model-based} RL algorithms use an explicit model of the environment's dynamics alongside the policy. The model may be an available simulator, as in AlphaGo \cite{silver_mastering_2016} or can be learned, as in MuZero \cite{silver_general_2018}. The model is used to simulate experience for a search process (e.g. Monte Carlo Tree Search) over possible sequences of actions, enabling multi-step planning to maximize reward. This leads to considerably higher performance in complex tasks, at the cost of much higher computational complexity \cite{brown_superhuman_2019}.

%Only model-free algorithms have been applied to DRA problems thus far in the research literature.


% \begin{equation}
%     \label{eq:bellman_optimality}
%     V^*(s) = \max_a \left( R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right)
% \end{equation}
% Equation \ref{eq:bellman_optimality} is the Bellman optimality equation, a fundamental principle in RL that defines the optimal value function. It states that the optimal value of a state is equal to the expected return for the best action from that state. This equation forms the basis for many RL algorithms, including value iteration and Q-learning, and provides a mathematical framework for understanding how agents can make optimal decisions in sequential decision-making problems.

% \begin{equation}
%     \label{eq:bellman_optimality_q}
%     Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q^*(s',a')
% \end{equation}
% Equation \ref{eq:bellman_optimality_q}, the Q-function version of the Bellman optimality equation, defines the optimal action-value function in reinforcement learning. It represents the expected return of taking action a in state s and then following the optimal policy thereafter. This equation is crucial in Q-learning and other value-based methods, as it allows for optimal action selection without requiring a model of the environment. The Q-function directly incorporates the action into the value estimation, making it particularly useful for control problems.