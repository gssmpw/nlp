{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://export.arxiv.org/api/{method_name}?{parameters}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=&terms-0-field=title&classification-computer_science=y&classification-physics_archives=all&classification-include_cross_list=include&date-year=&date-filter_by=date_range&date-from_date=2025-02-1&date-to_date=2025-02-2&date-date_type=submitted_date&abstracts=hide&size=200&order=-announced_date_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://export.arxiv.org/api/query?search_query=cat:cs*+AND+submittedDate:[20250201+TO+20250228]'\n",
    "expected_papers = 11574"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the ids based on the expected/desired number of papers\n",
    "# The API allows 2000 papers per request, with no more than one request being made every 3 seconds\n",
    "# However, I found that using less max papers and sleeping for 5 seconds between requests works best, as the results are more consistent\n",
    "def get_paper_ids(base_url, expected_papers):\n",
    "    start_search_index = 0\n",
    "    max_papers = 1000\n",
    "    max_search = max_papers\n",
    "    rounds = 1\n",
    "\n",
    "    paper_ids = []\n",
    "\n",
    "    while start_search_index < expected_papers:\n",
    "        paginated_url = base_url + f'&start={start_search_index}&max_results={max_search}'\n",
    "        print(f\"Starting round {rounds} with url: {paginated_url}\")\n",
    "        print(\"Getting response from arxiv...\")\n",
    "        response = requests.get(paginated_url)\n",
    "        print(\"Finding IDs...\")\n",
    "        root = ET.fromstring(response.text)\n",
    "        entry_id = root.findall('.//{http://www.w3.org/2005/Atom}entry/{http://www.w3.org/2005/Atom}id')\n",
    "        if entry_id is not None:\n",
    "            paper_id = [item.text for item in entry_id]\n",
    "            paper_ids.extend(paper_id)\n",
    "\n",
    "        start_search_index += max_papers\n",
    "        max_search += max_papers\n",
    "        rounds += 1\n",
    "        print(\"Sleeping...\")\n",
    "        time.sleep(5)\n",
    "\n",
    "\n",
    "    return paper_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ids\n",
    "ids = get_paper_ids(base_url, expected_papers)\n",
    "print(len(ids))\n",
    "\n",
    "# Remove duplicates\n",
    "new_ids = list(set(ids))\n",
    "print(len(new_ids))\n",
    "final_ids = []\n",
    "\n",
    "# Creating final list of ids without http://arxiv.org/abs/\n",
    "for item in new_ids:\n",
    "    final_ids.append(item.replace(\"http://arxiv.org/abs/\", \"\"))\n",
    "print(final_ids[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read from the id file created above\n",
    "final_ids = json.loads(open(\"ids.json\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "def download_papers(final_ids):\n",
    "    for final_id in final_ids:\n",
    "\n",
    "        print(f\"Downloading... {final_id}\")\n",
    "\n",
    "        url = f\"https://arxiv.org/e-print/{final_id}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Create temp directory structure\n",
    "        temp_dir = \"temp/papers\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "        # Save the file\n",
    "        filename = os.path.join(temp_dir, f\"{final_id}\")\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        print(f\"Extracting files...\")\n",
    "\n",
    "        # Create output directory\n",
    "        output_dir = f\"papers/{final_id}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Try to extract as tar.gz\n",
    "        try:\n",
    "            with tarfile.open(filename, \"r:gz\") as tar:\n",
    "                members = tar.getmembers()\n",
    "                \n",
    "                # Filter for only .tex files\n",
    "                tex_files = [member for member in members if member.name.endswith('.tex')]\n",
    "                \n",
    "                # Extract only the .tex files\n",
    "                for tex_file in tex_files:\n",
    "                    tar.extract(tex_file, path=output_dir)\n",
    "                    print(f\"Extracted: {tex_file.name}\")\n",
    "        except:\n",
    "            print(f\"Failed to extract: {final_id}, it may not be a tar.gz file: {response.headers['Content-Type']}\")\n",
    "\n",
    "        # Delete the temporary file\n",
    "        os.remove(filename)\n",
    "        print(f\"Temporary file deleted: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_papers(final_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
